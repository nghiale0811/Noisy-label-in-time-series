{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hmm-fit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px3RmzMhFa-A",
        "outputId": "625cf742-844c-4d5a-8ec6-a4b830dd5017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.2.7-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 20.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 81 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 129 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.2.7\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install hmmlearn\n",
        "from hmmlearn import hmm\n",
        "from torch.distributions import uniform\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
        "import datetime as dt\n",
        "import time\n",
        "import random\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Datasets using HMM\n",
        "\n",
        "def random_transmat(n_states):\n",
        "    matrix = np.random.rand(n_states, n_states)\n",
        "    return matrix/matrix.sum(axis=1)[:,None]\n",
        "\n",
        "def random_startprob(n_states):\n",
        "    startprob = np.random.rand(n_states)\n",
        "    return startprob/startprob.sum()\n",
        "\n",
        "def random_means(n_features):\n",
        "    return np.random.randint(5, size=(n_features,n_features))\n",
        "\n",
        "def generate_hmm(n_states, n_features , length):\n",
        "    #GENERATING A MODEL\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = random_startprob(n_states)\n",
        "    model.transmat_ = random_transmat(n_states)\n",
        "\n",
        "    model.means_ = random_means(n_features)\n",
        "    model.covars_ = np.tile(np.identity(n_features), (n_features, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "    dataset=[]\n",
        "    states = []\n",
        "\n",
        "    \n",
        "    X, Z = model.sample(length)\n",
        "    dataset.append(np.array(X))\n",
        "    states.append(Z)\n",
        "\n",
        "    dataset = np.stack(dataset)\n",
        "    \n",
        "    return np.array(X), np.array(Z)\n",
        "\n",
        "def sliding_windows(dataset, Z, window_length = 10):\n",
        "    lis = []\n",
        "    targets=[]\n",
        "\n",
        "    window_length = 10\n",
        "    if len(dataset) >= window_length+1:\n",
        "        for i in range(0, len(dataset)-window_length, 1):\n",
        "\n",
        "            x_window = dataset[i:i+window_length, :]\n",
        "            z_window = Z[i:i+window_length]\n",
        "\n",
        "            lis.append(x_window)\n",
        "            targets.append(z_window[-1])\n",
        "    return np.array(lis), np.array(targets)\n",
        "\n",
        "def generate_time_dependent_flip(length, startprob, transmat):\n",
        "    #GENERATING A MODEL\n",
        "\n",
        "\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = startprob\n",
        "    model.transmat_ = transmat\n",
        "\n",
        "    #this doesn't actually matter for us\n",
        "    model.means_ = np.array([[0.0, 0.0], \n",
        "                             [5.0, 10.0]])\n",
        "    model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "\n",
        "\n",
        "    X, Z = model.sample(length)\n",
        "\n",
        "    \n",
        "    return Z"
      ],
      "metadata": {
        "id": "xpW126KEFfM3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Injecting Noise into Labels\n",
        "\n",
        "#Given a flip_mask, flip an input\n",
        "def flip(array, flip_mask):\n",
        "    flipped_array = np.logical_xor(array, flip_mask, out=array)\n",
        "    return flipped_array\n",
        "\n",
        "#Class Independent / Time Independent\n",
        "def flip_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Dependent / Time Independent\n",
        "def flip_labels_class(array, flip_probability_0, flip_probability_1):\n",
        "    flip_mask = []\n",
        "    for elem in array:\n",
        "        if elem == 0:\n",
        "            to_flip = np.random.binomial(1, flip_probability_0, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "        else:\n",
        "            to_flip = np.random.binomial(1, flip_probability_1, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "            \n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Independent / Time Dependent\n",
        "def flip_labels_time(array, startprob, transmat):\n",
        "    flip_mask = generate_time_dependent_flip(len(array), startprob, transmat)[0]\n",
        "\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "\n",
        "#Class Dependent / Time Dependent\n",
        "#This can be achieved by careful design of the transition matrix (transmat)"
      ],
      "metadata": {
        "id": "OgyRemBtNY8o"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset,Z = generate_hmm(2,3,100)"
      ],
      "metadata": {
        "id": "oW2zggR4NdQS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmIkYpDuNgTw",
        "outputId": "aae844c8-15e3-4293-ae9a-287a6624f259"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startprob = random_startprob(2)\n",
        "transmat = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])"
      ],
      "metadata": {
        "id": "PXjUWBM5NjW5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                     flip_probability= None, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=None, transmat=None):\n",
        "    \n",
        "    #Generate Data\n",
        "    dataset, states_true = generate_hmm(n_states, n_features , length)\n",
        "    \n",
        "    if method == \"basic\":\n",
        "        states_flipped = (flip_labels_basic(states_true, flip_probability))\n",
        "        \n",
        "    elif method == \"class\":\n",
        "        states_flipped = (flip_labels_class(states_true, flip_probability_0, flip_probability_1))\n",
        "        \n",
        "    elif method == \"time\":\n",
        "        states_flipped = (flip_labels_time(states_true, startprob, transmat))\n",
        "    \n",
        "    #RESHAPE INTO WINDOWS\n",
        "    dataset_windows, states_true = sliding_windows(dataset, states_true, window_length= window_length)\n",
        "    \n",
        "    #RESHAPE INTO WINDOWS\n",
        "    _, states_flipped = sliding_windows(dataset, states_flipped, window_length= window_length)\n",
        "    \n",
        "    x_train = dataset_windows[:int(train_ratio*len(dataset_windows)),:,:]\n",
        "    x_test = dataset_windows[int(train_ratio*len(dataset_windows)):,:,:]\n",
        "    \n",
        "   \n",
        "    #Flip The Labels according to method\n",
        "    \n",
        "    \n",
        "    y_train_true = states_true[:int(train_ratio*len(dataset_windows))]\n",
        "    y_test_true = states_true[int(train_ratio*len(dataset_windows)):]\n",
        "    \n",
        "    y_train_flipped = np.array(states_flipped[:int(train_ratio*len(dataset_windows))])\n",
        "    y_test_flipped = np.array(states_flipped[int(train_ratio*len(dataset_windows)):])\n",
        "    \n",
        "    \n",
        "    return x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped"
      ],
      "metadata": {
        "id": "1q2hceLfNoDS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "OphpChUuPAo9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, feature_size, n_state, hidden_size=128, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
        "                 seed=random.seed('2021')):\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_state = n_state\n",
        "        self.seed = seed\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.rnn_type = rnn\n",
        "        self.regres = regres\n",
        "        self.return_all = return_all\n",
        "        \n",
        "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "\n",
        "        self.regressor = nn.Sequential(nn.BatchNorm1d(num_features=self.hidden_size),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Dropout(0.3),\n",
        "                                       nn.Linear(self.hidden_size, self.n_state),\n",
        "                                       nn.Softmax(-1))\n",
        "\n",
        "    def forward(self, input, past_state=None, **kwargs):\n",
        "        input = input.to(self.device)\n",
        "        self.rnn.to(self.device)\n",
        "        self.regressor.to(self.device)\n",
        "        if not past_state:\n",
        "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
        "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            all_encodings, encoding = self.rnn(input, past_state)\n",
        "        else:\n",
        "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
        "        \n",
        "        if self.regres:\n",
        "            if not self.return_all:\n",
        "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
        "            else:\n",
        "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
        "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
        "        else:\n",
        "            return encoding.view(encoding.shape[1], -1)\n",
        "        \n",
        "        \n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
        "            \n",
        "def save_ckpt(generator_model, output_dir, data):\n",
        "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
        "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
        "    os.makedirs(check_pt_dir, exist_ok=True)\n",
        "    torch.save(generator_model.state_dict(), fname)\n",
        "    \n",
        "def get_accuracy(model, loader):\n",
        "    \n",
        "    correct, total = 0, 0\n",
        "    for xs, ts in loader:\n",
        "        xs.to(device)\n",
        "        ts.to(device)\n",
        "        zs = model(xs)\n",
        "        \n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        ts = torch.argmax(ts, 1)\n",
        "\n",
        "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
        "        total += int(ts.shape[0])\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "g_bwMeNQSvCD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, n_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    history = dict(train=[], val=[])\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
        "    \n",
        "    best_loss = 10000.0\n",
        "    \n",
        "    n=0\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t0 = time.time()\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "        train_losses=[]\n",
        "        model = model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = (time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input= batch[0].to(device)\n",
        "            b_target =  batch[1].to(device)\n",
        "            iters.append(n)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(b_input)\n",
        "\n",
        "            target = torch.argmax(b_target, 1)\n",
        "\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if n % 10 == 0:\n",
        "                iters_sub.append(n)\n",
        "                \n",
        "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
        "                #print(get_accuracy(model, train_dataloader))\n",
        "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "        training_time = (time.time() - t0)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        val_losses = []\n",
        "        model = model.eval()\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "    \n",
        "        history['train'].append(train_loss)\n",
        "        \n",
        "        print(f'Epoch {epoch}: train loss {train_loss} ')\n",
        "    plt.style.use('seaborn-white')\n",
        "    plt.plot(history['train'])\n",
        "\n",
        "    plt.title('LSTM  Training Curves')\n",
        "    plt.ylabel('CE Loss')\n",
        "    plt.xlabel('Epoch Number')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    plt.show()\n",
        "    \n",
        "    return model.eval(), history\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    #INFERENCE ON TEST SET\n",
        "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
        "    \n",
        "    y_test = y_test.numpy()\n",
        "    y_pred = predictions.cpu().numpy()\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    #print(accuracy)\n",
        "    #print(f1)\n",
        "    #print(precision)\n",
        "    return accuracy\n",
        "\n",
        "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
        "  train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train.reshape(-1, 1)))\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "  x_test = torch.flip(torch.tensor(x_test, dtype=torch.float), [1,2])\n",
        "  y_test = torch.tensor(y_test_true, dtype=torch.int)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = LSTMClassifier(3, #num features \n",
        "                  2, #num classes,\n",
        "                  seed = seed,\n",
        "                  rnn=\"GRU\" #rnn type    \n",
        "            )\n",
        "\n",
        "  model, history = train_model(model, train_loader, 128, 5e-4)\n",
        "  acc = evaluate_model(model, x_test, y_test)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "ZfHKnAlSTDiC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f6HsUdvpPGMi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='mnist', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.0005, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# get dataset\n",
        "#==============================================================================\n",
        "\n",
        "def driver(seed, train, test, nrnn = False):\n",
        "  \n",
        "  model = NoisyRNN(input_dim=int(3), output_classes=2, n_units=args.n_units, \n",
        "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "                add_noise=args.add_noise, mult_noise=args.mult_noise).to(device)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)        \n",
        "  noise = torch.randn(1,693,10,3).float()\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # set random seed to reproduce the work\n",
        "  #==============================================================================\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # Model summary\n",
        "  #==============================================================================\n",
        "  print(model)    \n",
        "  print('**** Setup ****')\n",
        "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "  print('************')    \n",
        "    \n",
        "\n",
        "  if args.optimizer == 'SGD':\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "  elif  args.optimizer == 'Adam':\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  else:\n",
        "      print(\"Unexpected optimizer!\")\n",
        "      raise \n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # training and testing\n",
        "  count = 0\n",
        "  loss_hist = []\n",
        "  test_acc = []\n",
        "\n",
        "  t0 = timeit.default_timer()\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      lossaccum = 0\n",
        "      \n",
        "      for step, (x, y) in enumerate(train):\n",
        "          count += 1\n",
        "          \n",
        "          # Reshape data for recurrent unit\n",
        "          inputs = Variable(x.view(-1, 10, int(3))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "          targets = Variable(y).to(device)\n",
        "\n",
        "                  \n",
        "          # send data to recurrent unit    \n",
        "          output = model(inputs, mode='train')\n",
        "          loss = loss_func(output, targets.long())\n",
        "          \n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()          \n",
        "          \n",
        "          if args.gclip != 0.0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "              \n",
        "          optimizer.step() # update weights\n",
        "          lossaccum += loss.item()\n",
        "\n",
        "          if args.model == 'test':\n",
        "              D = model.W.weight.data.cpu().numpy()  \n",
        "              u, s, v = np.linalg.svd(D, 0)\n",
        "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "      loss_hist.append(lossaccum)    \n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          total_num = 0\n",
        "          for data, target in test: \n",
        "              data, target = data.to(device), target.to(device)               \n",
        "              output = model(data.view(-1, 10, int(3)))                  \n",
        "              \n",
        "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "              #print(output.shape)\n",
        "              #print(pred.shape)\n",
        "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "              total_num += len(data)\n",
        "          \n",
        "          accuracy = correct / total_num\n",
        "          test_acc.append(accuracy)\n",
        "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "      if nrnn == True:\n",
        "          B = model.B.data.cpu().numpy()            \n",
        "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "          A = 0.5 * (A + A.T)\n",
        "          e, _ = np.linalg.eig(A)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "          \n",
        "          C = model.C.data.cpu().numpy()            \n",
        "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "          e, _ = np.linalg.eig(W)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "              \n",
        "              \n",
        "\n",
        "      # schedule learning rate decay    \n",
        "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "  print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "\n",
        "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "  pickle.dump(data,f)\n",
        "  f.close()\n",
        "\n",
        "  return max(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbqUW26PJZz",
        "outputId": "e5b6c673-d55d-4e5a-ca89-87bcd10feb72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [1, 2, 3]"
      ],
      "metadata": {
        "id": "Of8YS2vvcNL2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"No noise\"\n",
        "flip_probability_0 = 0.1\n",
        "flip_probability_1 = 0.2\n",
        "rnn_arr1 = []\n",
        "nrnn_arr1 = []\n",
        "lstm_arr1 = []\n",
        "seed_arr1 = []\n",
        "method_arr1 = []\n",
        "specify_arr1 = []\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method = \"class\", \n",
        "                     flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1, startprob=None, transmat=None)\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_true))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_true))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=25, shuffle=True)\n",
        "\n",
        "for seed in seeds:\n",
        "  acc = driver(seed, train_loader, test_loader)\n",
        "  acc2 = driver(seed, train_loader, test_loader, nrnn = True)\n",
        "  acc3 = lstm_driver(seed, x_test, y_test_true, x_train, y_train_true)\n",
        "  rnn_arr1.append(acc)\n",
        "  nrnn_arr1.append(acc2)\n",
        "  lstm_arr1.append(acc3)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(None)"
      ],
      "metadata": {
        "id": "0KIIDEL9fsrd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca5bbb20-a9ff-4478-8e72-0cab8ba231b4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5770 | test accuracy: 0.576\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6423 | test accuracy: 0.576\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6057 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5710 | test accuracy: 0.576\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6293 | test accuracy: 0.609\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5302 | test accuracy: 0.576\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5847 | test accuracy: 0.599\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4059 | test accuracy: 0.572\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3694 | test accuracy: 0.606\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5426 | test accuracy: 0.623\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7419 | test accuracy: 0.636\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4626 | test accuracy: 0.710\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7737 | test accuracy: 0.626\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.4938 | test accuracy: 0.714\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4042 | test accuracy: 0.744\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6982 | test accuracy: 0.710\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4612 | test accuracy: 0.737\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5757 | test accuracy: 0.754\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6511 | test accuracy: 0.751\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2440 | test accuracy: 0.781\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2401 | test accuracy: 0.734\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6783 | test accuracy: 0.788\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4919 | test accuracy: 0.795\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.3820 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.9321 | test accuracy: 0.798\n",
            "Epoch:  25 Iteration:  1820 | train loss: 1.0813 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4003 | test accuracy: 0.785\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1276 | test accuracy: 0.801\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1244 | test accuracy: 0.798\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1118 | test accuracy: 0.801\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6846 | test accuracy: 0.801\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2186 | test accuracy: 0.795\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.9974 | test accuracy: 0.798\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0901 | test accuracy: 0.795\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1717 | test accuracy: 0.795\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5621 | test accuracy: 0.795\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.3191 | test accuracy: 0.798\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1287 | test accuracy: 0.795\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3190 | test accuracy: 0.795\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1394 | test accuracy: 0.795\n",
            "Epoch:  40 Iteration:  2870 | train loss: 1.0106 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7144 | test accuracy: 0.795\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1641 | test accuracy: 0.795\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1412 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2214 | test accuracy: 0.788\n",
            "Epoch:  45 Iteration:  3220 | train loss: 1.2765 | test accuracy: 0.788\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1073 | test accuracy: 0.795\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6015 | test accuracy: 0.788\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0751 | test accuracy: 0.788\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.1266 | test accuracy: 0.788\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.3630 | test accuracy: 0.788\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1514 | test accuracy: 0.788\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0631 | test accuracy: 0.788\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6342 | test accuracy: 0.788\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1438 | test accuracy: 0.788\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1317 | test accuracy: 0.788\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.8489 | test accuracy: 0.788\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1226 | test accuracy: 0.788\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.4560 | test accuracy: 0.788\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4094 | test accuracy: 0.788\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2043 | test accuracy: 0.788\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7449 | test accuracy: 0.788\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1103 | test accuracy: 0.788\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3676 | test accuracy: 0.788\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1654 | test accuracy: 0.788\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3015 | test accuracy: 0.788\n",
            "Epoch:  66 Iteration:  4690 | train loss: 1.6589 | test accuracy: 0.788\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7994 | test accuracy: 0.788\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5483 | test accuracy: 0.788\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1693 | test accuracy: 0.788\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0811 | test accuracy: 0.788\n",
            "Epoch:  71 Iteration:  5040 | train loss: 1.1649 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2095 | test accuracy: 0.788\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1123 | test accuracy: 0.788\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.8811 | test accuracy: 0.788\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.5403 | test accuracy: 0.788\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.9024 | test accuracy: 0.788\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1081 | test accuracy: 0.788\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3101 | test accuracy: 0.788\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1764 | test accuracy: 0.788\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4783 | test accuracy: 0.788\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7045 | test accuracy: 0.788\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2433 | test accuracy: 0.788\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2578 | test accuracy: 0.788\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.9537 | test accuracy: 0.785\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6144 | test accuracy: 0.785\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2296 | test accuracy: 0.785\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.4863 | test accuracy: 0.788\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.4517 | test accuracy: 0.788\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1316 | test accuracy: 0.788\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0526 | test accuracy: 0.785\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1507 | test accuracy: 0.785\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2917 | test accuracy: 0.788\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.0041 | test accuracy: 0.785\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7739 | test accuracy: 0.788\n",
            "Epoch:  95 Iteration:  6720 | train loss: 1.1233 | test accuracy: 0.788\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.8107 | test accuracy: 0.785\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1726 | test accuracy: 0.788\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2027 | test accuracy: 0.788\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2443 | test accuracy: 0.788\n",
            "total time:  80.32933064600002\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5967 | test accuracy: 0.576\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6336 | test accuracy: 0.576\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6005 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5564 | test accuracy: 0.576\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6315 | test accuracy: 0.653\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5144 | test accuracy: 0.589\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5892 | test accuracy: 0.636\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4057 | test accuracy: 0.572\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3708 | test accuracy: 0.613\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5611 | test accuracy: 0.620\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7720 | test accuracy: 0.626\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4825 | test accuracy: 0.707\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7177 | test accuracy: 0.626\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5447 | test accuracy: 0.707\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4119 | test accuracy: 0.714\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6975 | test accuracy: 0.717\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4682 | test accuracy: 0.697\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6093 | test accuracy: 0.754\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5966 | test accuracy: 0.744\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2696 | test accuracy: 0.771\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2446 | test accuracy: 0.727\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6318 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5192 | test accuracy: 0.785\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.3226 | test accuracy: 0.781\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.8678 | test accuracy: 0.798\n",
            "Epoch:  25 Iteration:  1820 | train loss: 1.0576 | test accuracy: 0.801\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3789 | test accuracy: 0.795\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1260 | test accuracy: 0.801\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1196 | test accuracy: 0.801\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0961 | test accuracy: 0.801\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6907 | test accuracy: 0.801\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.1954 | test accuracy: 0.801\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.9565 | test accuracy: 0.801\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0848 | test accuracy: 0.801\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1701 | test accuracy: 0.801\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5591 | test accuracy: 0.801\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2454 | test accuracy: 0.801\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1219 | test accuracy: 0.801\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3408 | test accuracy: 0.801\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1297 | test accuracy: 0.801\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.9666 | test accuracy: 0.801\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8054 | test accuracy: 0.801\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1780 | test accuracy: 0.801\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1459 | test accuracy: 0.801\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1903 | test accuracy: 0.801\n",
            "Epoch:  45 Iteration:  3220 | train loss: 1.4231 | test accuracy: 0.798\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1012 | test accuracy: 0.801\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6029 | test accuracy: 0.801\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0635 | test accuracy: 0.798\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.1183 | test accuracy: 0.798\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2535 | test accuracy: 0.798\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1323 | test accuracy: 0.798\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0731 | test accuracy: 0.798\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6006 | test accuracy: 0.801\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1258 | test accuracy: 0.801\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1337 | test accuracy: 0.801\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.8254 | test accuracy: 0.801\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1237 | test accuracy: 0.801\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.4079 | test accuracy: 0.801\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4110 | test accuracy: 0.801\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2055 | test accuracy: 0.801\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7277 | test accuracy: 0.798\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1048 | test accuracy: 0.801\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3997 | test accuracy: 0.801\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1383 | test accuracy: 0.801\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2950 | test accuracy: 0.801\n",
            "Epoch:  66 Iteration:  4690 | train loss: 1.7349 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.8283 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5549 | test accuracy: 0.795\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1820 | test accuracy: 0.798\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0859 | test accuracy: 0.801\n",
            "Epoch:  71 Iteration:  5040 | train loss: 1.2226 | test accuracy: 0.801\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1920 | test accuracy: 0.795\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1170 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.8401 | test accuracy: 0.801\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.4851 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8824 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1002 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3113 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1525 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3499 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6568 | test accuracy: 0.801\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2559 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2357 | test accuracy: 0.795\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.8843 | test accuracy: 0.795\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6062 | test accuracy: 0.798\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2451 | test accuracy: 0.801\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5643 | test accuracy: 0.798\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.3302 | test accuracy: 0.798\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1674 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0552 | test accuracy: 0.798\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1592 | test accuracy: 0.801\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2420 | test accuracy: 0.801\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.9331 | test accuracy: 0.801\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7055 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 1.1575 | test accuracy: 0.801\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.8328 | test accuracy: 0.801\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1713 | test accuracy: 0.798\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2202 | test accuracy: 0.795\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2439 | test accuracy: 0.798\n",
            "total time:  78.17437463700003\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25736069679260254.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.4048128128051758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5694121871675764 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1979532241821289.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.34821081161499023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.47049960238592964 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2114112377166748.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.35455918312072754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4150861029114042 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21300363540649414.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.358597993850708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3891689888068608 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20390033721923828.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.34572362899780273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3686848338161196 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22054481506347656.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3668954372406006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.359047657251358 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20090889930725098.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3428316116333008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34980996676853726 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21165704727172852.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.36620545387268066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34391377525670186 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2179713249206543.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3588571548461914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3384605475834438 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20499682426452637.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34912800788879395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33418025161538806 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20429325103759766.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35793423652648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33183418129171643 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2232682704925537.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3638625144958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3289042911359242 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20058274269104004.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.33861684799194336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3273286759853363 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21270537376403809.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36656761169433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32564946455614907 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20546603202819824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34999656677246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32398336359432767 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21335673332214355.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36928868293762207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3231371841260365 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21646332740783691.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.39695024490356445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3225364144359316 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22476434707641602.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36633896827697754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3210012755223683 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20397734642028809.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3479461669921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3208113674606596 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21576428413391113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3660402297973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3201082834175655 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20075201988220215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3395822048187256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31922503965241567 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20129656791687012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3476831912994385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3186423420906067 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148292064666748.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3618943691253662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31850250491074156 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20316290855407715.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34151625633239746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3180113068648747 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20438385009765625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35898327827453613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3175867625645229 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21052956581115723.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3554043769836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3175199308565685 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20828580856323242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3544502258300781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.316812869480678 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20619821548461914.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3495900630950928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31687173332486834 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21154284477233887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3534970283508301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3162638374737331 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20699834823608398.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35361409187316895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31648469013827185 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20251703262329102.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34819841384887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3161413822855268 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20767974853515625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35252857208251953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.316028265442167 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20503878593444824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35259509086608887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3156282846416746 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21142148971557617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35936427116394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31566669387476787 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20040225982666016.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34146976470947266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31539432534149714 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20562958717346191.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35706043243408203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3152418805020196 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20993733406066895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3584556579589844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31519711826528823 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20260262489318848.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3500075340270996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3152513171945299 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19852900505065918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34174513816833496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31501838394573756 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20521211624145508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35097336769104004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3149351303066526 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20172643661499023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428030014038086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31482195471014296 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20965933799743652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4137599468231201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.314987205181803 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2192537784576416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4394040107727051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3147119645561491 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3471379280090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6171112060546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3147049469607217 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.41579222679138184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6758837699890137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31476508208683557 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3213214874267578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5704677104949951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3144746571779251 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3142554759979248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5863096714019775\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31447081651006425 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34165334701538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5747778415679932\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31453034792627604 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3673839569091797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6310293674468994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31433180698326657 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3484971523284912.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6240606307983398\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31439334281853265 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34947967529296875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6082558631896973\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31420783145087106 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34447431564331055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.588780403137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3143048580203738 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3158895969390869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47048091888427734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141981929540634 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20368719100952148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486354351043701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141265822308404 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20687150955200195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470587730407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3141160160303116 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2101726531982422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540794849395752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3141343985285078 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2167675495147705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3652505874633789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3141106916325433 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21440863609313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356717586517334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3140300439936774 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21219396591186523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35300755500793457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31404697724751063 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20670342445373535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35658955574035645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3140338480472565 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20611023902893066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3511772155761719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140304386615753 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21737027168273926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3617126941680908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31401763473238264 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2146587371826172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.373798131942749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140200525522232 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20961642265319824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3522918224334717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139146076781409 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2253103256225586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37598752975463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31385673923151836 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20094609260559082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446493148803711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138271685157503 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21566987037658691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4011518955230713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31393135147435325 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20624828338623047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583838939666748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3138939427477973 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20886540412902832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556694984436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31386219944272725 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21213936805725098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35585856437683105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31380415473665507 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21666598320007324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36185669898986816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138187314782824 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20433402061462402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436918258666992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137672756399427 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2093193531036377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35954952239990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31378938938890183 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2005937099456787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508615493774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31372786334582736 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20088505744934082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389933109283447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31376207726342337 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20458078384399414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34404850006103516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137048491409847 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20647072792053223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35614895820617676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137697036777224 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20773649215698242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34588003158569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137134211403983 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20972681045532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35547852516174316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136607527732849 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20861577987670898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35204434394836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31367835530212945 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20666885375976562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34368896484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136642851999828 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20913314819335938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36217427253723145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31368614222322194 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21474146842956543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3592417240142822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137042624609811 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20407509803771973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37674379348754883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31368160034929005 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2158946990966797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3571484088897705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31367678003651756 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2150287628173828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3574869632720947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135849505662918 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20069551467895508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34854650497436523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136207354920251 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2116243839263916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3584139347076416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31362576910427636 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20679974555969238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478367328643799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136011672871453 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20696091651916504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36876463890075684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136194722993033 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2028357982635498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457803726196289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31358086126191276 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20088911056518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3434159755706787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135682331664222 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21448326110839844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36180901527404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31358895812715804 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2059633731842041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35525012016296387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136113720280784 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20167160034179688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457329273223877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135239945990699 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23664188385009766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38904404640197754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31354045442172457 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20945405960083008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3490469455718994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135445782116481 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2124485969543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35431671142578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31353204888956887 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21838974952697754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3638453483581543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135541430541447 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20303082466125488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34360384941101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135425899709974 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19796133041381836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442215919494629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31353796805654255 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22031521797180176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36084771156311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135206218276705 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20491290092468262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478109836578369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354190238884516 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2012953758239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34555983543395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31349499438490186 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106003761291504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.359133243560791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31353055025850024 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20019888877868652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34096837043762207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31348812920706615 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1980600357055664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35190534591674805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135462943996702 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053985595703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3441579341888428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135216704436711 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20572733879089355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34621596336364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134968497923442 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21309256553649902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3686330318450928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31349134147167207 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20704412460327148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34397220611572266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134951306240899 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20008468627929688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34132933616638184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134936937264034 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21123003959655762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37818455696105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134431277002607 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20766997337341309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487083911895752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.313466231737818 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20325112342834473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449845314025879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134609307561602 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22222685813903809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36773085594177246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31349598552499497 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2070469856262207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35291576385498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31346117428370884 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20455574989318848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35461950302124023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.313475358911923 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21381330490112305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556032180786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31346585963453566 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2037053108215332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3451406955718994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134449009384428 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2052156925201416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4227142333984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134481276784624 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23026680946350098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45322227478027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31342770372118267 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3688335418701172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.692873477935791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134303769895009 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3341684341430664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5778594017028809\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134458520582744 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34415698051452637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5989959239959717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134450797523771 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.31342077255249023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5477910041809082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31342433520725793 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.35993003845214844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5989668369293213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342688245432715 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3435628414154053.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5878591537475586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31343815454414914 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVyVdd7/8dfFOewggoDmrlQuuGVlC2mlkqj3mDNTpgntv7J0bLnNBfXGcrJUWkxtGce7rGzGSnJsXHDSUdNIsrzdqlE09w1QEJCd6/cHcgIFReVwwOv9fDx6eK5zbZ+Ded58v9/r+l6GaZomIiJiWW6uLkBERFxLQSAiYnEKAhERi1MQiIhYnIJARMTiFAQiIhZnd3UBcvVq164d69ato0mTJuet++ijj/j8888pLCyksLCQm266icmTJ3P06FH+9Kc/AZCdnU12drZj/9///vcMHDiQPn368NhjjzFu3LgKx3zkkUc4cOAAa9asqbKmDRs28Oc//xmAU6dOUVxcTHBwMAAjRoxg8ODB1fpsx48f5/HHH+ef//znBbcbO3YsUVFR9O7du1rHvZiCggLmzp1LYmIiZVd+R0VFMXLkSDw8PGrkHGI9hu4jEGepKgjWr1/Pq6++ysKFCwkKCqKgoIAXX3yRgIAAXn75Zcd2CQkJLF26lA8//NDx3qFDhxgyZAi+vr4kJibi5lbaqE1LS2PIkCEAFwyC8mbPns2xY8d45ZVXrvCT1p7nnnuO3NxcZs6cSYMGDcjIyGDcuHH4+fnx+uuvu7o8qafUNSS1bteuXbRq1YqgoCAAPDw8eOWVVxg7dmy19vfy8qJly5Zs3rzZ8d6KFSu47bbbrri23r17M2fOHPr168eRI0fYu3cvw4YNo3///kRGRjpaAIcOHaJjx45AaWCNHj2a2NhY+vXrx4ABA9i9ezcAMTEx/OMf/wBKg3HJkiUMHjyYO+64wxFwJSUlTJ06lYiICIYNG8Zf/vIXYmJizqtt9+7drFu3junTp9OgQQMAGjZsyLRp07jvvvvOO19l53///ffp168f06dPZ+rUqY7tTp48Sbdu3cjKyiIlJYXo6Gj69evH7373O7Zv3w5ATk4OI0eOpH///vTp04dJkyZRWFh4xT9zcT0FgdS622+/nQ0bNjBu3DjWrVtHdnY2fn5++Pn5VfsYUVFRFbplli1bRlRUVI3Ud/z4cRITE2natCkzZszg7rvvZsWKFUybNo2JEydW+uW3fv16HnzwQRITE7nllltYsGBBpcdOSUlhyZIlvPPOO7zxxhsUFxezbt061q9fz6pVq3j33Xf58ssvK903OTmZbt260bBhwwrvN2rUqNohaJomiYmJ9O/fn3//+9+O9//9739z66234uvry8iRI7n33ntJTExkypQpPPPMMxQVFbFkyRIaNGjAihUrSExMxGazkZKSUq3zSt2mIJBa17FjR/72t79RUlLC+PHjufXWWxk5ciRHjhyp9jHuuece1qxZQ2FhIYcPHyYvL482bdrUSH133XWX4/U777zD448/DsCNN95Ifn4+qamp5+0TFhZGp06dgNLPd/To0UqPfe+99wIQHh5Ofn4+6enpbN68mbvuugtfX18aNmzIwIEDK903MzOTRo0aXclHc3y2Ll26YJomv/zyCwD/+te/6N+/P3v37iU9Pd3RwrjxxhsJCgpiy5Ytjj83bNhASUkJL730Eh06dLiieqRu0GCxuETnzp2ZOXMmpmmyc+dOZs2axfPPP8+iRYuqtX9AQACdOnViw4YNpKSk0L9//xqrLSAgwPH6m2++4d133+XUqVMYhoFpmpSUlJy3j7+/v+O1zWajuLi40mOXbWez2YDSbqHTp0/TuHFjxzblX5cXGBjI8ePHL/0DlVO+NXHPPfewevVqWrZsyY8//kh8fDy7du0iLy+vws8zOzubjIwM+vfvT2ZmJrNmzWLv3r0MGjSICRMmaJD6KqAWgdS6zZs3O77QDMOgU6dOjBkzhl27dl3ScQYOHEhiYiIrV65kwIABNV5nYWEhzz33HE8//TSJiYksXboUwzBq/Dx+fn6cOXPGsVxZiwOgR48ebN269bwwOH36NLNmzcI0Tdzc3CoEVWZmZpXn7devH2vWrGHDhg3cfPPN+Pn5ERoaiq+vLytXrnT8t2HDBiIjIwEYOnQon3/+OcuXL2fnzp0sWbLkSj661BEKAql1X331FXFxcWRnZwNQVFTEsmXLuPnmmy/pOH369CE5ORmbzUaLFi1qvM7c3FzOnDnj6PJZsGAB7u7uFb60a0Lnzp1Zu3YteXl5nD59mhUrVlS6XVhYGAMGDOCFF14gLS0NgIyMDF544QVHiyUkJMTR3bNlyxb27dtX5XlvuOEG0tPTSUhIcLQAmjVrRpMmTVi5ciVQOoj8wgsvcObMGebOncsXX3wBlLZamjdv7pRglNqnriFxqpiYGEc3CMCf//xnJk6cyJtvvskf//hHoDQIbrnlFl599dVLOraPjw9du3alc+fONVpzmQYNGvDEE08wePBgGjVqxNNPP03fvn0ZMWIE77//fo2dJzIykrVr1xIVFUWrVq3o378/SUlJlW47depU3n33XYYPH45hGLi7uzNo0CDHOMajjz7KCy+8wPr16+nRowcRERFVntcwDPr27cvnn3/uuPTUMAzeeOMNpkyZwltvvYWbmxuPPvooPj4+3HvvvUyYMIF58+ZhGAZdu3Z1jHlI/ab7CETqANM0Hb9dL1y4kG+//Za5c+e6uCqxCnUNibjYzz//TJ8+fcjMzKSoqIhVq1bRrVs3V5clFqKuIREX69ChA4MHD+YPf/gDNpuNbt26ER0d7eqyxELUNSQiYnHqGhIRsbh61TWUl5fHjh07CAkJqXAlioiIVK24uJjU1FQ6deqEl5fXeevrVRDs2LGD4cOHu7oMEZF6aeHChdx0003nvV+vgiAkJAQo/TCVzXEvIiLnO3bsGMOHD3d8h56rXgVBWXdQkyZNaN68uYurERGpX6rqUtdgsYiIxSkIREQsTkEgImJxCgIREYtTEIiIWJyCQETE4iwTBKlZ+US8toaUE9muLkVELOq1114jJiaGqKgo7rzzTmJiYhg1atRF93v++efJy8tzWl316j6CK3EiK4/DGbmknMjm2lA/V5cjIhY0fvx4ABISEti9ezfjxo2r1n5vvvmmM8uyThB42ktvpMgvqvyh4iIirjB+/Hjc3d3JyMjg1Vdf5b//+785c+YMeXl5TJ48mS5dutC7d2+++uorpk6dSmhoKDt37uTIkSPEx8cTHh5+xTVYKAhKe8Hyi0ousqWIWMHiHw7x2eaDNXrMITe14I83XvqsBwEBAUydOpVff/2V+++/n759+5KUlMS8efOYPXt2hW0LCgqYP38+f/vb31iyZImC4FJ4uisIRKRu6tKlCwDBwcG88847zJ8/n4KCAnx8fM7btmzSuCZNmrBt27YaOb9lgsDL/WzXUKG6hkQE/nhj88v67d0Z3N3dAViwYAGNGzdm5syZbN++nRkzZpy3bfn5gmrquWKWuWpIXUMiUtedOnWKli1bAvD1119TWFhYK+e1TBB42BQEIlK33XvvvXzwwQc89thjdOnShdTUVBYvXuz089arZxYfOnSIPn36sHr16suahrrdpBU8EtGaCf07OKE6EZG66WLfnZZpEUBp91B+oVoEIiLlWSsI3G3qGhIROYe1gsDuphvKRETO4dTLR6dNm8bWrVsxDIPY2FjHtbIAvXv3pkmTJo5LoeLj49m3bx/PPvss1113HQDXX389kydPrrF6SoNALQIRkfKcFgTJycns37+fRYsWsWfPHmJjY1m0aFGFbebNm4evr69jed++ffTo0YO3337bKTV52m0aIxAROYfTuoaSkpLo27cvAGFhYWRmZpKd7dqZPz3d1TUkInIupwVBWloagYGBjuWgoCBSU1MrbBMXF8ewYcOIj4933CGXkpLCiBEjGDZsGBs3bqzRmtQ1JCJyvlqbYuLc2xVGjx5Nz549CQgIYOTIkSQmJnLDDTcwatQo+vfvz8GDB3nooYdYtWoVHh4eNVKDp91GRm7t3KknIlJfOK1FEBoaSlpammP5xIkThISEOJYHDx5Mo0aNsNvt9OrVi127dtG4cWMGDBiAYRi0bNmS4OBgjh8/XmM1ld5HoK4hEZHynBYEERERJCYmArBz505CQ0Px8yt9IExWVhaPP/44BQUFAHz//fdcd911LF26lPnz5wOQmppKeno6jRs3rrGaPN1tFKhrSESkAqd1DXXv3p3w8HCGDh2KYRjExcWRkJCAv78/kZGR9OrViwceeABPT086duxIVFQUOTk5jBkzhtWrV1NYWMiUKVNqrFsINEYgIlIZp44RjBkzpsJy+/btHa8ffvhhHn744Qrr/fz8eO+995xWj24oExE5n8XuLNZ9BCIi57JWELira0hE5FzWCgK7GwXFJZSU1JuZt0VEnM5iQVA6r1FBsVoFIiJlLBYEZ59SpnECEREHawWBe9njKnXlkIhIGWsFwdmuIQ0Yi4j8xmJBUPpx8zTNhIiIgyWDQC0CEZHfWCsI3Mu6htQiEBEpY6kg8NJVQyIi57FUEPzWIlAQiIiUsVYQ2HX5qIjIuSwaBGoRiIiUsVYQlHUNaYxARMTBWkGgriERkfNYNAjUIhARKWOxINBVQyIi57JUELjbDAwD8jXFhIiIg6WCwDAMPcBeROQclgoCOPvcYgWBiIiDBYPATVcNiYiUY70gcHfTfQQiIuVYLwjUNSQiUoEFg0BdQyIi5Vk0CNQiEBEpY8EgsGmMQESkHOsFgbu6hkREyrNeEKhrSESkAgsGga4aEhEpz+7Mg0+bNo2tW7diGAaxsbF06dLFsa537940adIEm610Irj4+HgaN258wX1qgqfdTXMNiYiU47QgSE5OZv/+/SxatIg9e/YQGxvLokWLKmwzb948fH19L2mfK1U6RqAWgYhIGad1DSUlJdG3b18AwsLCyMzMJDs7u8b3uVTqGhIRqchpQZCWlkZgYKBjOSgoiNTU1ArbxMXFMWzYMOLj4zFNs1r7XCndUCYiUpFTxwjKM02zwvLo0aPp2bMnAQEBjBw5ksTExIvuUxM87TYKi02KS0xsbkaNH19EpL5xWhCEhoaSlpbmWD5x4gQhISGO5cGDBzte9+rVi127dl10n5rg6V7aCCooKsHbw1ajxxYRqY+c1jUUERHh+C1/586dhIaG4ufnB0BWVhaPP/44BQUFAHz//fdcd911F9ynpugB9iIiFTmtRdC9e3fCw8MZOnQohmEQFxdHQkIC/v7+REZG0qtXLx544AE8PT3p2LEjUVFRGIZx3j41zctdzy0WESnPqWMEY8aMqbDcvn17x+uHH36Yhx9++KL71DRHi0DzDYmIABa9sxjUNSQiUsaCQVA2RqAWgYgIWDEI3DVYLCJSnvWCoKxrSGMEIiKAJYNAXUMiIuVZLwjUNSQiUoH1guBs11CeuoZERABLBoFaBCIi5Vk4CNQiEBEBKwaBu64aEhEpz3pBoK4hEZEKLBcEdjcDN0NdQyIiZSwXBIZh6HGVIiLlWC4I4OwD7AvVNSQiAlYNArubWgQiImdZNAjUNSQiUsaiQeCmq4ZERM6yZBB4e9g4U6AgEBEBiwaBv5ed7LwiV5chIlInWDII/DztZOcrCEREwLJB4E6WWgQiIoBFg8Dfy05WXqGryxARqRMuGgTZ2dn8+uuvACQnJ/Phhx9y8uRJpxfmTP5epV1Dpmm6uhQREZe7aBA899xznDhxgt27dzN9+nSCgoKYMGFCbdTmNH6edkpMdOWQiAjVCIKCggJuueUWVqxYwSOPPMKgQYPIz8+vjdqcxt/LHUADxiIiVDMIli5dyrJly7j77rs5dOgQWVlZtVGb0/h52QE0YCwiQjWCIC4ujm3btjFlyhT8/PxYt24dzz33XG3U5jT+nmVBoAFjERH7xTZo0aIFDz74IG3btiU5OZnCwkLCw8Nrozan8T/bIlDXkIhINQeLU1NTr67B4rIgUNeQiIhzB4unTZvGAw88wNChQ9m2bVul27z++uvExMQAsGnTJm699VZiYmKIiYlh6tSpl/BRqs/PU2MEIiJlLto1VH6wePHixdUeLE5OTmb//v0sWrSIPXv2EBsby6JFiypsk5KSwvfff4+7u7vjvR49evD2229fxkepvrKrhrLUNSQiUv3B4pdeeumSBouTkpLo27cvAGFhYWRmZpKdnV1hm9dee43nn3/+Mku/fGUtAnUNiYhUIwg6dOhAZGQkv/zyCx999BEdOnSgZ8+eFz1wWloagYGBjuWgoCBSU1MdywkJCfTo0YNmzZpV2C8lJYURI0YwbNgwNm7ceCmfpdpsbgY+HjZdNSQiQjW6hqZNm8bBgwfp0aMHeXl5vPPOO4SHh1/yb/Llp3PIyMggISGBDz74gOPHjzveb926NaNGjaJ///4cPHiQhx56iFWrVuHh4XFJ56qOsmkmRESs7qJBsHPnThYuXOhYfvLJJ4mOjr7ogUNDQ0lLS3MsnzhxgpCQEAC+++47Tp48yfDhwykoKODAgQNMmzaN2NhYBgwYAEDLli0JDg7m+PHjtGjR4pI/2MX4edo1RiAiQjW6hoqKisjLy3MsnzlzhuLii8/RExERQWJiIlAaJqGhofj5+QEQFRXF8uXL+eyzz5gzZw7h4eHExsaydOlS5s+fD0Bqairp6ek0btz4sj7Yxfh5aSpqERGoRovg4YcfZtCgQbRu3ZqSkhIOHDjA2LFjL3rg7t27Ex4eztChQzEMg7i4OBISEvD39ycyMrLSfXr37s2YMWNYvXo1hYWFTJkyxSndQgANvOxka4xAROTiQTBgwADuuusu9u3bh2EYtG7dusLlnhcyZsyYCsvt27c/b5vmzZvz8ccfA+Dn58d7771XrWNfKT9PO8dP5118QxGRq1y1Hkzj4+NDx44d6dChA97e3jz22GPOrsvp/Dzt6hoSEeEyn1B2NTzQxd/LXfcRiIhwmUFgGEZN11Hr/LzsZBcUUVJS/0NNRORKVDlGMH369Eq/8E3T5ODBg04tqjb4e9oxTcgpKHJMOSEiYkVVBsH1119f5U4XWldflJ+KWkEgIlZWZRD8/ve/r806al2FqagDXFyMiIgLXdYYwdWgbOK50xowFhGLs2wQ6AH2IiKlqgyCTZs2VVguKChwvP7888+dV1Et8ddTykREgAsEwdy5cyssP/HEE47XX331lfMqqiV+eoC9iAhwgSA496ax8stXxw1leoC9iAhcIAjOvYeg/PLVcEOZr4eeWywiAhe4fLSkpIS8vDzHb/9lyyUlJZSUlNRagc7i5mZoviERES4QBEeOHGHgwIEVuoHKHhpzNbQIoOwpZRojEBFrqzII1qxZU5t1uISfpx5XKSJS5RhBYWEhb731FoWFv/3GvHv3bt5+++1aKaw2+Hmpa0hEpMogmD59OtnZ2RW6hlq1akV2djZz5sypleKczV+PqxQRqToItmzZwqRJkyo8KtLDw4Px48ezcePGWinO2fzVNSQiUnUQ2Gy2yndwc6vQXVSflV41dHV8FhGRy1VlEAQGBrJ58+bz3l+7di3BwcFOLaq2+HvZNcWEiFhelVcNxcbG8qc//YmwsDA6dOhAcXExW7du5ejRo8yfP782a3QaPy87OQXFFJeY2NyujktiRUQuVZVB0KpVK5YsWcLGjRvZu3cvhmEQHR1NRETEVXMfQdl8Q9n5RQR46+E0ImJNVQYBlI4H9OzZk549e9ZWPbUqxN8TgNSsPAWBiFiWZZ9HANCsoTcAh07lurgSERHXsXQQND0bBEcy8lxciYiI61g6CBo38MLuZnA444yrSxERcRlLB4HNzaBJgBeH1TUkIhZm6SCA0u4hdQ2JiJVZPgiaN/TmcIZaBCJiXZYPgqYNvTl2Oo+i4vr/sB0Rkcvh1CCYNm0aDzzwAEOHDmXbtm2VbvP6668TExNzSfvUpGaB3hSXmBzPynf6uURE6iKnBUFycjL79+9n0aJFvPLKK7zyyivnbZOSksL3339/SfvUtLJ7CTRgLCJW5bQgSEpKom/fvgCEhYWRmZlJdnZ2hW1ee+01nn/++Uvap6aV3UugS0hFxKqcFgRpaWkEBgY6loOCgkhNTXUsJyQk0KNHD5o1a1btfZyhmW4qExGLq7XB4vJPOsvIyCAhIYFHH3202vs4i7eHjUa+HppmQkQs64KTzl2J0NBQ0tLSHMsnTpwgJCQEgO+++46TJ08yfPhwCgoKOHDgANOmTbvgPs7UVJeQioiFOa1FEBERQWJiIgA7d+4kNDQUPz8/AKKioli+fDmfffYZc+bMITw8nNjY2Avu40zNGnpzREEgIhbltBZB9+7dCQ8PZ+jQoRiGQVxcHAkJCfj7+xMZGVntfWpDs0Bv1u1KxTTNq+ZZCyIi1eW0IAAYM2ZMheX27duft03z5s35+OOPq9ynNjRt6E1uYTGnzhQS5OtR6+cXEXEly99ZDLqXQESsTUFAuSDQOIGIWJCCgNIxAlAQiIg1KQiAQB93/D3t7E/PcXUpIiK1TkEAGIZBmxBf9qYqCETEehQEZ7UN9mVvqnPnNRIRqYsUBGe1DfHjSGYeZwqKXF2KiEitUhCc1TbEF4B9aZqFVESsRUFwVpvg0iDYm6buIRGxFgXBWY4g0ICxiFiMguAsHw87TQO8NGAsIpajICinbYgfe9PUIhARa1EQlNM2xJdfU3Nq5YE4IiJ1hYKgnDbBvmTlF5Gane/qUkREao2CoJy2IaUPwdGAsYhYiYKgnLa6ckhELEhBUE6zht542t105ZCIWIqCoBw3N4M2wb78qiuHRMRCFATnaBviy3+OZ+nKIRGxDAXBOW5r24hDp3LZfULdQyJiDQqCc/Tr1ATDgOXbj7q6FBGRWqEgOEeovxc3tw5ixfZjri5FRKRWKAgqMaBTE/5zPIsUdQ+JiAUoCCoR1ekaAFaoe0hELEBBUIkmAV7c2CqQ5TvUPSQiVz8FQRX6d2rCz0dP654CEbnqKQiq0L/z2e6hHeoeEpGrm4KgCs0aetOtRUNdPSQiVz0FwQUM6NyE7YczOXhSD7QXkauXguAC+p+9ekg3l4nI1czuzINPmzaNrVu3YhgGsbGxdOnSxbHus88+44svvsDNzY327dsTFxdHcnIyzz77LNdddx0A119/PZMnT3ZmiRfUIsiHzs0CWL7jGE/dGeayOkREnMlpQZCcnMz+/ftZtGgRe/bsITY2lkWLFgGQm5vLsmXLWLhwIe7u7jz00ENs2bIFgB49evD22287q6xL1r9zE2as/A+HTp2heaCPq8sREalxTusaSkpKom/fvgCEhYWRmZlJdnbpnbre3t4sWLAAd3d3cnNzyc7OJiQkxFmlXJEBZ7uHVuqeAhG5SjktCNLS0ggMDHQsBwUFkZqaWmGbv/zlL0RGRhIVFUWLFi0ASElJYcSIEQwbNoyNGzc6q7xqax3sS+dmASzcdIDC4hJXlyMiUuNqbbC4svn9n3zySb7++mu++eYbfvjhB1q3bs2oUaN49913mT59OhMnTqSgoKC2SqzSn3pfy69pOSz+4ZCrSxERqXFOC4LQ0FDS0tIcyydOnHB0/2RkZPD9998D4OXlRa9evfjxxx9p3LgxAwYMwDAMWrZsSXBwMMePH3dWidUW2bExN7RsyKzVu8krLHZ1OSIiNcppQRAREUFiYiIAO3fuJDQ0FD8/PwCKiooYP348OTml0zds376dNm3asHTpUubPnw9Aamoq6enpNG7c2FklVpthGLzYrx1HM/P45Lv9ri5HRKRGOe2qoe7duxMeHs7QoUMxDIO4uDgSEhLw9/cnMjKSkSNH8tBDD2G322nXrh19+vQhJyeHMWPGsHr1agoLC5kyZQoeHh7OKvGS3B4WzB3XBjP33ykMubkFDbzcXV2SiEiNMMx69HDeQ4cO0adPH1avXk3z5s1r/fzbD2UyaO4GnuzZlgkDOtT6+UVELsfFvjt1Z/El6Nw8gD92b84HG/exP12zkorI1UFBcIle7NcOu83g1eW/uLoUEZEaoSC4RI0bePH0nWGs3HmMb1PSLr6DiEgdpyC4DP+vV1taNfJhzOdbyTxT6OpyRESuiILgMni523h76A2cyMpnwpfbKr1ZTkSkvlAQXKauLRoypl87lm8/xt+SD7q6HBGRy6YguAJP9mxLz+uCmbhkO2/+axfFJWoZiEj9oyC4Am5uBu/H3MgfbmjOrNW7iZm/iVM5rp8bSUTkUigIrpCPh53Xh3Rlxn1d2Lz/FEP/8h0nTue5uiwRkWpTENSQITe14MNHbubgqTPc/34SB9L1nGMRqR8UBDXo9muD+eSJWziVU0DfN9fxyrKfOKmuIhGp4xQENax7y0CWP9uTQV2bMn/Dr/ScvoY3/7WLrDzdbyAidZOCwAmaB/oQf39XVj3fi17XhzBr9W56zvg389bv1fMMRKTOURA40bWh/rwbfSNLR0XQuVkAryz/md7xa/lw468aQxCROsNpzyOQ33Rp3pCPH7+Fb1PSmL7yF6Z89RNTvvqJFkHedG3ekE7NAujbIZRrQ/1dXaqIWJCCoBbdfm0wS0ZG8GtaDt/sTuPbPWlsOZDBP7cdZfrKXxjQ6Roeu6M11wR4E+Dtjq+n/npExPn0TVPLDMOgbYgfbUP8ePj21gCcyMpjwbf7WPDtfpZtP+rYtllDb25sFUiPNkHceX0ILYJ8XFS1iFzNFAR1QKi/Fy/2a8//69mWpD3pZOYWcvJMATsPn2bTr+ks3XoEgLYhvnRt3pCO1zTgmoZe+HjYCPB2p3UjX4J8PTAMw8WfRETqIwVBHdLQx4P+na+p8J5pmuxJzWHtf07w7Z50kvak8+WWw+ft6+9lp22wL22CfQlt4IW7zcDHw0540wbc0DKQAG89Y1lEKqcgqOMMw+DaUD+uDfXjiZ5tATiZU0B6dj45BcWcying17Qcx3/f7ztFek4+RcUmRWcnwTMMcLe5UVxi4ml3o+M1DWh/jT+mCRlnCnFzM7gmwItQf098POx4e7gR7OdJi0AfQvw9KTFNSkzw9bBht+lCM5GrjYKgHiYSIUIAAA0PSURBVAry9SDI18OxfHcV2+XkF7H1YAY/HjhFVl4R7jY3svOL2Hkkk3/83xE8bG409HGnqMRk1c488otKLnpuf087Dbzdaejjjr+XHU+7DXebG552N9xtBu42NzzsbuXeK/3Ty92G3WZwpqCYnPwivN1tNPLzpIG3HdOEEtPENKG4xMTbw0aIvyeBPh7Y3QzcDAPDKA20stfl//S0u+HrYcfNrbRrrKTEPLu9uspEqkNBcBXz9bRz+7XB3H5t8EW3NU2T07lF5BYWk1tYzPHTeRw8eYb0nAJsZ790c/KLycgtIPNMIZm5hZzOKyTjTAEFxSaFxSUUFJU4/iwo/u31ubNz290MR2ulprgZ4O1uO3ve0mPb3IzScHJzw24zsNvc8LC54eYGBgZuZ8PCMMCgYrjA2XWUBhBn/zT4bXvOXX/OPgZlG1LhvbJQK3tdpuK+vwXZb+cwyq0rO27F41HFvuVr49x9K6nNsUUl68t/1nN/LhXOWe58la4/52dZ1bpzQ72y9eU/T6V/d2XvVfl3d/5n5dx9OV9Vv2tU+X4lR6nu7yuGYdDz2mACy/0SWFMUBAKU/k8W4ONOAKVjCW2Cfbm1baMaOXZhcQl5hcUUFpv4etrwtNvILyrmZE4BWXlFji9kN6P0y/lMQTGpWfmcOlNAcYlZocVgUtpNVbZcYprkF5aQlVdITkExHvbSL3sTKCouoaikNKRKu8pKKCgyz+5rYgIlZmkIOo5d8ts5Sh88Vxoqpetx7Fd+uUzZMUzzt9cV9i0Bk5Lz9i1/PModv3Rd+eNd4HzlasOx77n1mr8d1/FnFesd25hV11vufGVVVXa8ijWdX68e8Fd9o+6+ljH92tX4cRUE4nTuttIuovI87TauCfDmmoDK9+lwTeXvy9Xtt1A+PyjODbLzg6vc9pUF8TkhVTEwzw8pyp+7krAyqTzBqgq2yt6u6jG3VWVj60a+Vay5MgoCEakzyrpvzi65shRL0SUgIiIWpyAQEbE4BYGIiMUpCERELE5BICJicQoCERGLq1eXjxYXlz7m8dixYy6uRESk/ij7ziz7Dj1XvQqC1NRUAIYPH+7iSkRE6p/U1FRatWp13vuGWdWtbXVQXl4eO3bsICQkBJvN5upyRETqheLiYlJTU+nUqRNeXl7nra9XQSAiIjVPg8UiIhZXr8YIrsS0adPYunUrhmEQGxtLly5dXF3SRc2YMYMffviBoqIinnrqKTp37szYsWMpLi4mJCSEmTNn4uFR81PS1qS8vDz+67/+i2eeeYbbbrutXtW/dOlS/vrXv2K32xk9ejTt2rWrN/Xn5OQwbtw4MjMzKSwsZOTIkYSEhDBlyhQA2rVrx0svveTaIiuxa9cunnnmGR555BGio6M5evRopT/zpUuXsmDBAtzc3BgyZAj333+/q0sHKq9/woQJFBUVYbfbmTlzJiEhIXWvftMCNm3aZD755JOmaZpmSkqKOWTIEBdXdHFJSUnmE088YZqmaZ48edK88847zfHjx5vLly83TdM0X3/9dXPhwoWuLLFa3njjDfMPf/iDuXjx4npV/8mTJ8177rnHzMrKMo8fP25OmjSpXtX/8ccfm/Hx8aZpmuaxY8fMfv36mdHR0ebWrVtN0zTNF154wVy7dq0rSzxPTk6OGR0dbU6aNMn8+OOPTdM0K/2Z5+TkmPfcc495+vRpMzc31xw4cKB56tQpV5Zummbl9Y8dO9ZctmyZaZqm+cknn5jTp0+vk/VbomsoKSmJvn37AhAWFkZmZibZ2dkururCbr75ZmbNmgVAgwYNyM3NZdOmTfTp0weAu+++m6SkJFeWeFF79uwhJSWFu+66C6Be1Z+UlMRtt92Gn58foaGhTJ06tV7VHxgYSEZGBgCnT5+mYcOGHD582NESrov1e3h4MG/ePEJDQx3vVfYz37p1K507d8bf3x8vLy+6d+/Ojz/+6KqyHSqrPy4ujn79+gG//Z3UxfotEQRpaWkEBgY6loOCghyXotZVNpsNHx8fAL744gt69epFbm6uoyuiUaNGdf4zTJ8+nfHjxzuW61P9hw4dIi8vjxEjRvDggw+SlJRUr+ofOHAgR44cITIykujoaMaOHUuDBg0c6+ti/Xa7/bwrWir7maelpREUFOTYpq78e66sfh8fH2w2G8XFxXz66af87ne/q5P1W2aMoDyzHl0o9fXXX/PFF1/wv//7v9xzzz2O9+v6Z1iyZAndunWjRYsWla6v6/UDZGRkMGfOHI4cOcJDDz10ztPB6nb9//jHP2jatCnz58/nl19+YeTIkfj7+zvW1/X6K1NVzXX9sxQXFzN27FhuvfVWbrvtNr766qsK6+tC/ZYIgtDQUNLS0hzLJ06cICQkxIUVVc8333zDe++9x1//+lf8/f3x8fEhLy8PLy8vjh8/XqEJWtesXbuWgwcPsnbtWo4dO4aHh0e9qr9Ro0bccMMN2O12WrZsia+vLzabrd7U/+OPP3LHHXcA0L59e/Lz8ykqKnKsr+v1l6ns/5nK/j1369bNhVVe2IQJE2jVqhWjRo0CKv8+cnX9lugaioiIIDExEYCdO3cSGhqKn5+fi6u6sKysLGbMmMH7779Pw4YNAbj99tsdn2PVqlX07NnTlSVe0FtvvcXixYv57LPPuP/++3nmmWfqVf133HEH3333HSUlJZw6dYozZ87Uq/pbtWrF1q1bATh8+DC+vr6EhYWxefNmoO7XX6ayn3nXrl3Zvn07p0+fJicnhx9//JGbbrrJxZVWbunSpbi7uzN69GjHe3WxfsvcUBYfH8/mzZsxDIO4uDjat2/v6pIuaNGiRcyePZs2bdo43nvttdeYNGkS+fn5NG3alFdffRV3d3cXVlk9s2fPplmzZtxxxx2MGzeu3tT/97//nS+++AKAp59+ms6dO9eb+nNycoiNjSU9PZ2ioiKeffZZQkJC+J//+R9KSkro2rUrEyZMcHWZFezYsYPp06dz+PBh7HY7jRs3Jj4+nvHjx5/3M1+5ciXz58/HMAyio6MZNGiQq8uvtP709HQ8PT0dv3iGhYUxZcqUOle/ZYJAREQqZ4muIRERqZqCQETE4hQEIiIWpyAQEbE4BYGIiMUpCOSqcOjQIW644QZiYmIq/Fc2386VmD17Np988skFt2nXrh1r1qxxLG/atInZs2df9jk3bdpU4dpzEWeyxJ3FYg1t2rTh448/dsm5W7duzZw5c7jzzjv19DypdxQEctUbP348Pj4+7N27l1OnTvHqq6/SsWNHFixYwPLlywHo06cPTz75JIcPH2b8+PEUFxfTtGlTpk+fDpTOM//UU0+xb98+Jk6cSK9evSqcIzQ0lM6dO/Pll19y3333VVh3yy23sGnTJgBGjx7N8OHDSU5O5tSpU+zfv59Dhw7x7LPPsnjxYg4fPsy8efMAyMzMZOTIkRw+fJjIyEhGjhxJSkoKL7/8MoZh4Ovry2uvvcbp06d58cUX8fHxITo6mrvvvtvZP1K5yqhrSCyhqKiIDz/8kGeffZa5c+dy8OBBvvzySxYuXMjChQtZsWIFBw4c4M033+SRRx7h008/JTQ0lB07dgClE9C9//77TJo0ib///e+VnuOpp55iwYIF5OXlVaumzMxM5s+fT1RUFEuWLHG8Xr16NQD/+c9/mDFjBp999hmLFy8mIyODqVOn8vLLL7NgwQIiIiJYuHAhAD///DPx8fEKAbksahHIVePXX38lJibGsdymTRtefvlloHTOGoBu3boRHx/Pzz//TNeuXbHbS/8JdO/enV9++YWffvqJiRMnAjB27FgA1q9fT/fu3QFo3LgxWVlZlZ4/ICCAe++9l48++oiuXbtetN7OnTsDVJgAMTg42DGu0alTJ3x9fYHSqQkOHjzItm3bmDx5MgAFBQWOY7Ro0aLCVOsil0JBIFeNC40RlJSUOF4bhoFhGBWm/y0sLMTNzQ2bzVbptMBlgXExMTEx3HfffbRu3brS9YWFhZUes/zrsvMbhlFhX8Mw8Pb25qOPPqqw7tChQ3V2ziOpH9Q1JJbwww8/ALBlyxbCwsLo0KED//d//0dRURFFRUVs3bqVDh060KlTJ7777jsAZs2axbfffntJ5/H09OTRRx/lvffec7xnGAa5ubnk5uby888/V/tYP/30E7m5ueTn57Nnzx5atmxJ+/btWb9+PQDLli2rc08Zk/pJLQK5apzbNQTw4osvApCfn89TTz3F0aNHmTlzJs2bN+eBBx4gOjoa0zS5//77adasGaNHj2bChAl8+umnXHPNNYwaNcoRItU1ePBgPvjgA8fysGHDGDJkCGFhYYSHh1f7OB07diQ2NpZ9+/YxdOhQGjRowMSJE5k8eTLz5s3D09OT119/vc4/dlXqPs0+Kle98ePH069fPw2kilRBXUMiIhanFoGIiMWpRSAiYnEKAhERi1MQiIhYnIJARMTiFAQiIhanIBARsbj/D6CcVCgxA5vCAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3592 | test accuracy: 0.576\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3847 | test accuracy: 0.576\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4792 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6734 | test accuracy: 0.572\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9147 | test accuracy: 0.572\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8004 | test accuracy: 0.576\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5548 | test accuracy: 0.576\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5023 | test accuracy: 0.667\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4963 | test accuracy: 0.623\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6928 | test accuracy: 0.680\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3168 | test accuracy: 0.616\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5268 | test accuracy: 0.589\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.3263 | test accuracy: 0.697\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3965 | test accuracy: 0.650\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2410 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6998 | test accuracy: 0.724\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4321 | test accuracy: 0.710\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2289 | test accuracy: 0.744\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5635 | test accuracy: 0.727\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.3835 | test accuracy: 0.754\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0750 | test accuracy: 0.764\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.0927 | test accuracy: 0.764\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.3047 | test accuracy: 0.747\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.3699 | test accuracy: 0.764\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3189 | test accuracy: 0.747\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1206 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.5174 | test accuracy: 0.741\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.4963 | test accuracy: 0.788\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0994 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0933 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.4601 | test accuracy: 0.798\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2056 | test accuracy: 0.801\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5826 | test accuracy: 0.795\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1827 | test accuracy: 0.798\n",
            "Epoch:  34 Iteration:  2450 | train loss: 1.0880 | test accuracy: 0.801\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.4806 | test accuracy: 0.801\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1887 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0828 | test accuracy: 0.801\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5381 | test accuracy: 0.801\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1967 | test accuracy: 0.801\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.1112 | test accuracy: 0.801\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3938 | test accuracy: 0.801\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1111 | test accuracy: 0.801\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.9905 | test accuracy: 0.801\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.4452 | test accuracy: 0.801\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2184 | test accuracy: 0.801\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3974 | test accuracy: 0.801\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6343 | test accuracy: 0.801\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1202 | test accuracy: 0.801\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.4613 | test accuracy: 0.801\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2710 | test accuracy: 0.801\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.9205 | test accuracy: 0.801\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1232 | test accuracy: 0.798\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.0245 | test accuracy: 0.798\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.9925 | test accuracy: 0.801\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.8618 | test accuracy: 0.801\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.1118 | test accuracy: 0.801\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.0491 | test accuracy: 0.798\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.9821 | test accuracy: 0.795\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.3694 | test accuracy: 0.798\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2389 | test accuracy: 0.798\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2427 | test accuracy: 0.801\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.9150 | test accuracy: 0.801\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2302 | test accuracy: 0.801\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1688 | test accuracy: 0.795\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6645 | test accuracy: 0.801\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1088 | test accuracy: 0.798\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3217 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2256 | test accuracy: 0.801\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.9682 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5280 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3294 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1543 | test accuracy: 0.801\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7172 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6398 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.3005 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1522 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2028 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2038 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3940 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1160 | test accuracy: 0.795\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5733 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3434 | test accuracy: 0.795\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2388 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1249 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1502 | test accuracy: 0.795\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1391 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.9821 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.6880 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1562 | test accuracy: 0.795\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.8310 | test accuracy: 0.795\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.0844 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6059 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6177 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 1.2163 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7198 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1069 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.4099 | test accuracy: 0.795\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7580 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.5940 | test accuracy: 0.791\n",
            "total time:  77.37766403500007\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3482 | test accuracy: 0.576\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4024 | test accuracy: 0.576\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4873 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7094 | test accuracy: 0.576\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8739 | test accuracy: 0.576\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8513 | test accuracy: 0.576\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5858 | test accuracy: 0.576\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5551 | test accuracy: 0.623\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5515 | test accuracy: 0.609\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7226 | test accuracy: 0.650\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3083 | test accuracy: 0.582\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5446 | test accuracy: 0.576\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.3777 | test accuracy: 0.673\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.4072 | test accuracy: 0.616\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2789 | test accuracy: 0.599\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7085 | test accuracy: 0.710\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4259 | test accuracy: 0.710\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2531 | test accuracy: 0.724\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5721 | test accuracy: 0.717\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4039 | test accuracy: 0.727\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1094 | test accuracy: 0.747\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.9742 | test accuracy: 0.751\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.3191 | test accuracy: 0.731\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.3953 | test accuracy: 0.758\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3816 | test accuracy: 0.758\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1409 | test accuracy: 0.781\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6291 | test accuracy: 0.754\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5486 | test accuracy: 0.788\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0922 | test accuracy: 0.781\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0936 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.3992 | test accuracy: 0.798\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.1784 | test accuracy: 0.795\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5835 | test accuracy: 0.788\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1926 | test accuracy: 0.795\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.9837 | test accuracy: 0.795\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.4661 | test accuracy: 0.795\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2359 | test accuracy: 0.795\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0963 | test accuracy: 0.795\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4733 | test accuracy: 0.795\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2490 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.1251 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3715 | test accuracy: 0.795\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1121 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.9480 | test accuracy: 0.795\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.3423 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1767 | test accuracy: 0.788\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3886 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6337 | test accuracy: 0.788\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1359 | test accuracy: 0.788\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.3991 | test accuracy: 0.788\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2886 | test accuracy: 0.785\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.9328 | test accuracy: 0.788\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1613 | test accuracy: 0.785\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.0675 | test accuracy: 0.785\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.0723 | test accuracy: 0.785\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.8824 | test accuracy: 0.785\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.1041 | test accuracy: 0.785\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.0141 | test accuracy: 0.785\n",
            "Epoch:  58 Iteration:  4130 | train loss: 1.0238 | test accuracy: 0.788\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4014 | test accuracy: 0.785\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2527 | test accuracy: 0.785\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1859 | test accuracy: 0.788\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.9544 | test accuracy: 0.788\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2583 | test accuracy: 0.785\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1927 | test accuracy: 0.788\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6603 | test accuracy: 0.785\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1132 | test accuracy: 0.785\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3103 | test accuracy: 0.788\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2736 | test accuracy: 0.785\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.9261 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5155 | test accuracy: 0.788\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2797 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1450 | test accuracy: 0.785\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7178 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5784 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2972 | test accuracy: 0.788\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1404 | test accuracy: 0.785\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2567 | test accuracy: 0.785\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1854 | test accuracy: 0.795\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3856 | test accuracy: 0.785\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1175 | test accuracy: 0.788\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5907 | test accuracy: 0.785\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3705 | test accuracy: 0.785\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1718 | test accuracy: 0.785\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1472 | test accuracy: 0.785\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1704 | test accuracy: 0.785\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1245 | test accuracy: 0.788\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.9690 | test accuracy: 0.785\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.7103 | test accuracy: 0.785\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1343 | test accuracy: 0.785\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7697 | test accuracy: 0.785\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.1455 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.5980 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.5710 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 1.1614 | test accuracy: 0.795\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6108 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1008 | test accuracy: 0.795\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.4227 | test accuracy: 0.785\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6667 | test accuracy: 0.785\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6096 | test accuracy: 0.785\n",
            "total time:  73.82933293200006\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3341789245605469.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.5870389938354492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6872023369584764 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33806276321411133.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.6110959053039551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5386756249836513 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.347062349319458.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.5979537963867188\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4627894993339266 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.373532772064209.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.6304030418395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41643049589225223 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3511786460876465.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.6047942638397217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38966879418918066 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.5902180671691895.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 1.0226588249206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3703525534697941 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3377094268798828.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.5752980709075928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.35895012191363745 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3318202495574951.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.5950729846954346\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3537429081542151 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20114588737487793.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35810184478759766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3463421127625874 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1998739242553711.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34007740020751953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34061253028256555 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2252647876739502.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.37557339668273926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3376893026488168 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21454358100891113.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3576664924621582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3337190760033471 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20044803619384766.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3377833366394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3330345792429788 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2215747833251953.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36816978454589844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3289213606289455 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20481348037719727.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36103010177612305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32769518664905 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20456552505493164.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35349392890930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3261499319757734 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22933387756347656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3729879856109619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32488796753542765 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2052314281463623.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3485085964202881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3241346099546977 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21776819229125977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.37352418899536133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3224521062203816 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103562355041504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3557701110839844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32291721531323025 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21362900733947754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35395312309265137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32094516498701914 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20535874366760254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36908936500549316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3200260400772095 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21732711791992188.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36182475090026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3201593101024628 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20784687995910645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35059452056884766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3193934751408441 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21366214752197266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36737990379333496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3187820281301226 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20824408531188965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3517313003540039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3184757160288947 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20045256614685059.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3438434600830078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3183919974735805 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21268653869628906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35002589225769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.318153133562633 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19535088539123535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33765268325805664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31776466965675354 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20979952812194824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35283851623535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3173254349402019 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21918487548828125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3698585033416748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31678536236286164 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19789862632751465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3366587162017822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3167555161884853 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328283309936523.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3428170680999756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3165184812886374 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20585083961486816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3462831974029541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31625064057963237 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20388340950012207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34651637077331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3162520796060562 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21547770500183105.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3626127243041992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31582883638995035 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21212053298950195.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3551645278930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31599013464791437 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078406810760498.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34882473945617676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31582713169710974 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20201659202575684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35065793991088867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.315665727853775 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21466350555419922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3525850772857666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31536986018930163 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2033085823059082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3449442386627197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3153038374015263 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22036480903625488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36718153953552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3152202844619751 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21443653106689453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3588106632232666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31517491638660433 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23269438743591309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38898205757141113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31494188436440057 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2105717658996582.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3836381435394287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31503768989018033 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20608305931091309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.344285249710083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31495489180088043 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20482921600341797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3448612689971924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3146791189908981 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21445918083190918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37932538986206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31482075239930835 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21103882789611816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35911107063293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31472679036004203 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21309185028076172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36028409004211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31456641938005175 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21210408210754395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3493783473968506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31467710477965216 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2026371955871582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34023022651672363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3144085168838501 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2194817066192627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3685910701751709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31442557530743737 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2081310749053955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34522461891174316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143510737589427 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014000415802002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445577621459961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143219913755144 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035265922546387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508288860321045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143115861075265 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21112346649169922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34821534156799316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31435484119824003 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21082592010498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3504512310028076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3141941853931972 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.208526611328125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35726261138916016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141579713140215 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22242474555969238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36888766288757324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31429699531623295 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22211050987243652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3730192184448242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3142055694546018 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21072125434875488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36683011054992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31406574164118084 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2026209831237793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34714508056640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140235206910542 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2089087963104248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3562736511230469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31400990869317735 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2145674228668213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3720884323120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31398965205465046 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21586179733276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3626899719238281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.313996673481805 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21889448165893555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36779308319091797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139036987509046 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21610403060913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35761213302612305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139762158904757 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2091841697692871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3631627559661865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139463428940092 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21610641479492188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36346888542175293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31396527928965434 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21184730529785156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35126471519470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139238945075444 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22661375999450684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3738431930541992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31388711418424337 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20687174797058105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36888933181762695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138519342456545 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20861554145812988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350538969039917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31380212690149034 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2795538902282715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4289708137512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31378010298524583 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.338115930557251.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5864214897155762\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31380820402077264 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30994725227355957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5638878345489502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3138190184320722 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.32442426681518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5728545188903809\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31374111005238126 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33800458908081055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5770249366760254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3138032572610038 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34174609184265137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5737571716308594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137413165399006 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.35202503204345703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.618541955947876\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137709992272513 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3518362045288086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6014814376831055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137903950044087 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36947107315063477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6270973682403564\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136885621717998 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3508331775665283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.59328293800354\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136893425668989 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3313021659851074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5715973377227783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137147733143398 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2764739990234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4267892837524414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3137111706393106 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21719121932983398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3650343418121338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136585035494396 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21639585494995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3717916011810303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.313666946547372 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.212691068649292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355086088180542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136317074298859 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21944928169250488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36751222610473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31368662416934967 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20237517356872559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432948589324951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136514851025173 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017228603363037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34327006340026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136598182576043 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22808122634887695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3723030090332031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136191036020006 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2029736042022705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520662784576416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31362114250659945 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.222520112991333.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3730506896972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136117862803595 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2204151153564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3726682662963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31360103275094714 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21280527114868164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35410308837890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358943453856875 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2013099193572998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3552417755126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31355870153222765 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21150636672973633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520522117614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135641634464264 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20284104347229004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3397500514984131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.313533405320985 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2139110565185547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35648393630981445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135727775948388 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21865105628967285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37422752380371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135742655822209 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21034026145935059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3513832092285156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354105642863683 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21871304512023926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37596607208251953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31355004012584686 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2084493637084961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35388946533203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135248873914991 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126026153564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3542745113372803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135319867304393 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22026515007019043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37024617195129395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31351423944745743 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2131197452545166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3576791286468506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31353288122585843 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20181655883789062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458681106567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135537087917328 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22137856483459473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36945319175720215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135209488017218 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21227741241455078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35523033142089844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31349966398307255 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20128893852233887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3466458320617676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135233176606042 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20756125450134277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515744209289551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135032057762146 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20635986328125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34467315673828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31346939291272846 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21069860458374023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.373457670211792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134991709675108 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2072763442993164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3524773120880127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31348725855350495 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20886850357055664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3513329029083252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3135114239794867 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20797514915466309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3599364757537842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31348524136202677 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21071338653564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.362720251083374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134909084865025 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2202010154724121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598597049713135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31347543469497136 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22131729125976562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37543368339538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134677631514413 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2063732147216797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3561420440673828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344846061297826 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22376060485839844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37523818016052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31347895562648775 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2236957550048828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37476515769958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.313461001430239 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21438026428222656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3560447692871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134469607046672 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19841957092285156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454468250274658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134810724428722 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22228407859802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.371121883392334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134641494069781 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20873117446899414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470609188079834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31345417414392746 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.0390 | test accuracy: 0.576\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6515 | test accuracy: 0.576\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4559 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6012 | test accuracy: 0.576\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.4092 | test accuracy: 0.576\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4764 | test accuracy: 0.586\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4046 | test accuracy: 0.576\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5126 | test accuracy: 0.572\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6285 | test accuracy: 0.697\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3265 | test accuracy: 0.576\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5680 | test accuracy: 0.586\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8067 | test accuracy: 0.707\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7037 | test accuracy: 0.714\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3738 | test accuracy: 0.616\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4970 | test accuracy: 0.714\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3234 | test accuracy: 0.737\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3220 | test accuracy: 0.747\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6513 | test accuracy: 0.741\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5492 | test accuracy: 0.734\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5613 | test accuracy: 0.771\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2819 | test accuracy: 0.774\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7901 | test accuracy: 0.781\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2629 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6978 | test accuracy: 0.801\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1837 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 1.0620 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1432 | test accuracy: 0.795\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2275 | test accuracy: 0.801\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1602 | test accuracy: 0.798\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1527 | test accuracy: 0.801\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2456 | test accuracy: 0.801\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.3148 | test accuracy: 0.798\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5748 | test accuracy: 0.801\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.9370 | test accuracy: 0.801\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2402 | test accuracy: 0.801\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0898 | test accuracy: 0.798\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.3675 | test accuracy: 0.798\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1367 | test accuracy: 0.798\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7730 | test accuracy: 0.798\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1728 | test accuracy: 0.798\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2085 | test accuracy: 0.798\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4051 | test accuracy: 0.798\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1561 | test accuracy: 0.798\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2075 | test accuracy: 0.798\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.9692 | test accuracy: 0.798\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.8845 | test accuracy: 0.798\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.8359 | test accuracy: 0.798\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.9741 | test accuracy: 0.798\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1178 | test accuracy: 0.798\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2677 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 1.2609 | test accuracy: 0.798\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1135 | test accuracy: 0.798\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1243 | test accuracy: 0.798\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.9309 | test accuracy: 0.798\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.4928 | test accuracy: 0.795\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1527 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5078 | test accuracy: 0.798\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1379 | test accuracy: 0.795\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1095 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1289 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.0754 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3904 | test accuracy: 0.795\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.9457 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.1239 | test accuracy: 0.795\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1420 | test accuracy: 0.788\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.9422 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.8081 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1024 | test accuracy: 0.785\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1681 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1816 | test accuracy: 0.788\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0711 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1921 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2133 | test accuracy: 0.785\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1765 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5207 | test accuracy: 0.795\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2219 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2223 | test accuracy: 0.795\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3396 | test accuracy: 0.788\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8933 | test accuracy: 0.788\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1846 | test accuracy: 0.788\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1790 | test accuracy: 0.788\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2273 | test accuracy: 0.788\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1951 | test accuracy: 0.785\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7014 | test accuracy: 0.785\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2181 | test accuracy: 0.785\n",
            "Epoch:  85 Iteration:  6020 | train loss: 1.1317 | test accuracy: 0.785\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6069 | test accuracy: 0.785\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1596 | test accuracy: 0.788\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1996 | test accuracy: 0.788\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2952 | test accuracy: 0.788\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9847 | test accuracy: 0.785\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1624 | test accuracy: 0.788\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2388 | test accuracy: 0.788\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1921 | test accuracy: 0.788\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6670 | test accuracy: 0.788\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2434 | test accuracy: 0.788\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4687 | test accuracy: 0.788\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1585 | test accuracy: 0.788\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1813 | test accuracy: 0.788\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.4383 | test accuracy: 0.788\n",
            "total time:  74.96963797300009\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.0016 | test accuracy: 0.576\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6341 | test accuracy: 0.576\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4662 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5624 | test accuracy: 0.576\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.4490 | test accuracy: 0.576\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4690 | test accuracy: 0.586\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4117 | test accuracy: 0.572\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5126 | test accuracy: 0.572\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6039 | test accuracy: 0.704\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3373 | test accuracy: 0.572\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5367 | test accuracy: 0.606\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8753 | test accuracy: 0.707\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7764 | test accuracy: 0.704\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3440 | test accuracy: 0.620\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5229 | test accuracy: 0.714\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3458 | test accuracy: 0.724\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2737 | test accuracy: 0.747\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6517 | test accuracy: 0.747\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5627 | test accuracy: 0.714\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5154 | test accuracy: 0.754\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3367 | test accuracy: 0.744\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6852 | test accuracy: 0.741\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4486 | test accuracy: 0.768\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7894 | test accuracy: 0.781\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3256 | test accuracy: 0.781\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.9980 | test accuracy: 0.768\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1698 | test accuracy: 0.788\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2469 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0993 | test accuracy: 0.795\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1496 | test accuracy: 0.781\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2659 | test accuracy: 0.785\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5435 | test accuracy: 0.785\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5596 | test accuracy: 0.788\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7913 | test accuracy: 0.788\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2803 | test accuracy: 0.788\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1099 | test accuracy: 0.788\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.4422 | test accuracy: 0.785\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1465 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7944 | test accuracy: 0.788\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1569 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2434 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3302 | test accuracy: 0.795\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1608 | test accuracy: 0.795\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3956 | test accuracy: 0.795\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8708 | test accuracy: 0.795\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.8211 | test accuracy: 0.795\n",
            "Epoch:  46 Iteration:  3290 | train loss: 1.0086 | test accuracy: 0.795\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.9834 | test accuracy: 0.798\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1525 | test accuracy: 0.795\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2472 | test accuracy: 0.795\n",
            "Epoch:  50 Iteration:  3570 | train loss: 1.1225 | test accuracy: 0.795\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1037 | test accuracy: 0.798\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1241 | test accuracy: 0.795\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.0955 | test accuracy: 0.795\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.3858 | test accuracy: 0.798\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1160 | test accuracy: 0.798\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5822 | test accuracy: 0.795\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1393 | test accuracy: 0.798\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1297 | test accuracy: 0.798\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0814 | test accuracy: 0.798\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.1562 | test accuracy: 0.798\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3333 | test accuracy: 0.801\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.8415 | test accuracy: 0.798\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.1140 | test accuracy: 0.798\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1400 | test accuracy: 0.798\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.0399 | test accuracy: 0.798\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7159 | test accuracy: 0.801\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1073 | test accuracy: 0.798\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1876 | test accuracy: 0.798\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1663 | test accuracy: 0.798\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0731 | test accuracy: 0.798\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1751 | test accuracy: 0.798\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1984 | test accuracy: 0.798\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1979 | test accuracy: 0.798\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7106 | test accuracy: 0.795\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2823 | test accuracy: 0.798\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1935 | test accuracy: 0.795\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.4114 | test accuracy: 0.795\n",
            "Epoch:  78 Iteration:  5530 | train loss: 1.0538 | test accuracy: 0.798\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2043 | test accuracy: 0.798\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1915 | test accuracy: 0.798\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.3036 | test accuracy: 0.798\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1904 | test accuracy: 0.798\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7398 | test accuracy: 0.798\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2850 | test accuracy: 0.798\n",
            "Epoch:  85 Iteration:  6020 | train loss: 1.2349 | test accuracy: 0.795\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.5388 | test accuracy: 0.798\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1774 | test accuracy: 0.795\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2254 | test accuracy: 0.795\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.3434 | test accuracy: 0.798\n",
            "Epoch:  90 Iteration:  6370 | train loss: 1.0761 | test accuracy: 0.798\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1591 | test accuracy: 0.798\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2791 | test accuracy: 0.798\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1680 | test accuracy: 0.798\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7730 | test accuracy: 0.798\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.3105 | test accuracy: 0.798\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4286 | test accuracy: 0.795\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2148 | test accuracy: 0.798\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1960 | test accuracy: 0.798\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.4029 | test accuracy: 0.798\n",
            "total time:  74.69565026600003\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1944727897644043.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.3366522789001465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.48676869102886744 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21546268463134766.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3628499507904053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.41841532843453544 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062995433807373.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.342970609664917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.38244215079716276 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20571374893188477.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.35668468475341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3657175132206508 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.208693265914917.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3557775020599365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3520727378981454 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19457602500915527.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.33706068992614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3440570946250643 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20803427696228027.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3474605083465576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3372051588126591 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21207356452941895.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3585038185119629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33470619320869444 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20224595069885254.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3417065143585205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3315051623753139 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21105384826660156.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36574268341064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3272901965039117 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095797061920166.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35060811042785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32583407759666444 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048330307006836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34673380851745605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32422026438372475 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2124929428100586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3597290515899658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3225092036383493 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106790542602539.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3512308597564697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3219683012792042 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2132871150970459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36783456802368164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32142343989440375 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20948314666748047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3513979911804199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3199210264853069 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20751023292541504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34923768043518066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31922805053847175 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2081298828125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3583395481109619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.31899742994989666 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20499038696289062.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3563868999481201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31812699905463626 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21312212944030762.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3575887680053711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31797128745487757 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20280122756958008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3546316623687744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3175129818064826 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20747613906860352.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35069823265075684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31697483914239066 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20004010200500488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3399658203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3168030240706035 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20974326133728027.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36457014083862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3166761325938361 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20754790306091309.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3470742702484131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3164499487195696 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095487117767334.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35492539405822754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31626381533486503 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2192215919494629.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3714287281036377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31580900251865385 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21103310585021973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36852312088012695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31566274549279894 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22476649284362793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.37093281745910645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31543055857930863 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20977067947387695.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3580029010772705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31542055266244073 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21181535720825195.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.350039005279541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3153939664363861 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20536518096923828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3538353443145752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3151985675096512 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.212005615234375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3575015068054199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31512009671756197 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.204315185546875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3424050807952881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3150932567460196 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20249342918395996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3625025749206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3149119219609669 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21507883071899414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553340435028076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31492085116250174 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.208221435546875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508152961730957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3147671882595335 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2730720043182373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42500782012939453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31464154890605384 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.31159496307373047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5558257102966309\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31452219656535557 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.40035057067871094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6526908874511719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31460404225758143 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33044958114624023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5732710361480713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31454354013715474 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3462646007537842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5954110622406006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31451406095709117 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3377213478088379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6282479763031006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3144272195441382 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.41660380363464355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.66776442527771\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3144031609807696 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.35312819480895996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6032581329345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3143866607121059 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33959054946899414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5901012420654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3141957380941936 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33210110664367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5751895904541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31431279395307815 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3519446849822998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6165990829467773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3141055085829326 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20138049125671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35933899879455566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3140874377318791 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24160552024841309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3904273509979248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31404883520943777 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20495009422302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36520910263061523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.313998972092356 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20623159408569336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506789207458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31404376455715727 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20981144905090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36452150344848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31419951788016726 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22196626663208008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3661949634552002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3139654640640531 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20027709007263184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3392212390899658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3139581714357649 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20268988609313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34971046447753906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3138577342033386 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22068166732788086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36483263969421387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3139015227556229 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20950770378112793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35466718673706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31387699374130795 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2134089469909668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36699819564819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31394453772476744 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22052741050720215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36038994789123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31385329067707063 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1991746425628662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34359097480773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138697130339486 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20933771133422852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3582315444946289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138098044054849 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22482633590698242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36574268341064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138137034007481 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039649486541748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35160326957702637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31379049675805226 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20245671272277832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35576748847961426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31371888858931407 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2008655071258545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3427846431732178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.313762110045978 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2155170440673828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3670477867126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137780338525772 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21346616744995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3717520236968994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3136839440890721 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21454787254333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36090970039367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137565736259733 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2162795066833496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3579883575439453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137163175003869 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21444344520568848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35407519340515137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31368412588323863 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21370339393615723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35642457008361816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31368059047630853 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20661377906799316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35954904556274414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3136713032211576 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22131061553955078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3646116256713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31367276779242925 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20145034790039062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34949445724487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.313623046875 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2172999382019043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3653109073638916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136350555079324 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21352601051330566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35256528854370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31359620562621526 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2370917797088623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3830854892730713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31365313231945036 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20901799201965332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496718406677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.313603892496654 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21038532257080078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35829710960388184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31357715470450265 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2150437831878662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3614053726196289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136272260120937 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20894145965576172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3577747344970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31361880557877675 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20819854736328125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3519120216369629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3135815705571856 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21746182441711426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37572312355041504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31355591927255905 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21572256088256836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3679177761077881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31354820302554537 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21172356605529785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3617384433746338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31356133222579957 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20340633392333984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459646701812744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135348124163491 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21465802192687988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355607271194458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135650008916855 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1984877586364746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34214282035827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31353501762662617 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21234536170959473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3571627140045166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31354826390743257 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21185827255249023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35886096954345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31355442787919724 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2175445556640625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36740708351135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31354332225663323 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21280312538146973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35628414154052734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135312863758632 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21672391891479492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3619577884674072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31351364723273684 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2105269432067871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35132360458374023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135301798582077 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20379972457885742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.344268798828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135028941290719 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2137458324432373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35498499870300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3134946712425777 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21305227279663086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3626887798309326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31350493047918593 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20185375213623047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34987521171569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3134900676352637 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21111583709716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3616905212402344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31349604512963974 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2201848030090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3682081699371338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31348731475217 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21199321746826172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3605458736419678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134736912591117 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20736932754516602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3443772792816162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31350129331861226 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20647525787353516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34444332122802734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134757033416203 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972212791442871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3494997024536133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31347569142069137 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22180819511413574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38088369369506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31347950782094686 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21193790435791016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3563113212585449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134453079530171 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21706056594848633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549833297729492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31347013754504066 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20326638221740723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3426649570465088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134510278701782 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020726203918457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34227824211120605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134394828762327 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2157764434814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3646237850189209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134506472519466 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21118712425231934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565349578857422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134342338357653 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20966506004333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35228705406188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31345272617680686 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22036957740783691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38155245780944824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.313447613801275 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21254515647888184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37497401237487793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31344486900738305 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2210240364074707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4336850643157959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31344202288559503 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21008896827697754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4836888313293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31343652818884166 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.32141804695129395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5839004516601562\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134409763983318 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3449997901916504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5768685340881348\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134409308433533 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3386118412017822.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5816497802734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134300934416907 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.32864952087402344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5897505283355713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31341297115598404 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36493873596191406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6208522319793701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134150155952999 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3581686019897461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6208102703094482\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134158206837518 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34990859031677246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6003081798553467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134040402514594 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3119964599609375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5752220153808594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3133970081806183 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.35128021240234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5703737735748291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31341682629925866 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23718810081481934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3963146209716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134019144943782 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24280095100402832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3849070072174072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31341298733438766 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAESCAYAAAAWtRmOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVyVdd7/8dd1DvumoBxLcYvMBbesbLEcFVHUe9Jm0jG3sZpfNZml3o4iLVjO4F6TS5s5ZcbMYEiO3Wo42phahKZGSTWKlooLgrLIvp3fH8gRBBWTw0F5Px+PeXiuc77XdX0O9x1vvst1XYbVarUiIiJSicnRBYiISMOjcBARkWoUDiIiUo3CQUREqlE4iIhINQoHERGpxsnRBUjj07FjRz7//HNuuummap998MEHfPTRRxQXF1NcXMydd97Jiy++yMmTJ5k8eTIAOTk55OTk2PZ/6KGHGDZsGMHBwTz22GPMnDmzyjEnTpzI0aNH+eyzzy5Z086dO/nzn/8MQEZGBqWlpTRv3hyAp556ihEjRtTqu6WmpvL444/zf//3f5dtN2PGDEJDQxkwYECtjnslRUVFLF++nLi4OCpWp4eGhjJp0iRcXFzq5BzSuBi6zkHq26XCYfv27cydO5eoqCj8/PwoKiriT3/6E02aNOGVV16xtYuNjWX9+vW8//77tvdSUlIYNWoUnp6exMXFYTKVd4rT09MZNWoUwGXDobKlS5dy6tQp/vKXv1zjN60/U6ZMIT8/n4ULF+Lj40NmZiYzZ87Ey8uLxYsXO7o8uQ5pWEkajAMHDtC2bVv8/PwAcHFx4S9/+QszZsyo1f5ubm60adOGr7/+2vbepk2buPfee6+5tgEDBrBs2TIGDx7MiRMnOHz4MI888ghDhgwhJCTE1lNISUmhS5cuQHmIPfvss4SHhzN48GCGDh3KwYMHARg/fjz/+te/gPKwXLduHSNGjOD++++3hV5ZWRlz5syhT58+PPLII7zzzjuMHz++Wm0HDx7k888/Z/78+fj4+ADQtGlTIiMjefjhh6udr6bzv/322wwePJj58+czZ84cW7uzZ8/Ss2dPzp07R3JyMuPGjWPw4MH8+te/5rvvvgMgNzeXSZMmMWTIEIKDg3nhhRcoLi6+5p+5OJbCQRqM++67j507dzJz5kw+//xzcnJy8PLywsvLq9bHCA0NrTKks2HDBkJDQ+ukvtTUVOLi4mjZsiULFiygf//+bNq0icjISJ5//vkafyFu376dMWPGEBcXx913382qVatqPHZycjLr1q3jjTfe4NVXX6W0tJTPP/+c7du3s3nzZt58800+/vjjGvfdtWsXPXv2pGnTplXeb9asWa2D0Wq1EhcXx5AhQ/jPf/5je/8///kP99xzD56enkyaNInhw4cTFxfH7NmzefrppykpKWHdunX4+PiwadMm4uLiMJvNJCcn1+q80nApHKTB6NKlC//4xz8oKysjLCyMe+65h0mTJnHixIlaH2PQoEF89tlnFBcXc/z4cQoKCmjfvn2d1NevXz/b6zfeeIPHH38cgDvuuIPCwkLS0tKq7RMYGEjXrl2B8u938uTJGo89fPhwAIKCgigsLOTMmTN8/fXX9OvXD09PT5o2bcqwYcNq3DcrK4tmzZpdy1ezfbfu3btjtVr58ccfAfj3v//NkCFDOHz4MGfOnLH1RO644w78/PzYt2+f7d+dO3dSVlbGyy+/TOfOna+pHnE8TUhLg9KtWzcWLlyI1WolKSmJ119/nalTpxIdHV2r/Zs0aULXrl3ZuXMnycnJDBkypM5qa9Kkie31jh07ePPNN8nIyMAwDKxWK2VlZdX28fb2tr02m82UlpbWeOyKdmazGSgfUsrOzqZFixa2NpVfV+br60tqaurVf6FKKvc6Bg0axNatW2nTpg179+5l0aJFHDhwgIKCgio/z5ycHDIzMxkyZAhZWVm8/vrrHD58mAcffJBZs2ZpIvw6p56DNBhff/217ZecYRh07dqV6dOnc+DAgas6zrBhw4iLi+PTTz9l6NChdV5ncXExU6ZM4Y9//CNxcXGsX78ewzDq/DxeXl7k5eXZtmvqmQD07t2bxMTEagGRnZ3N66+/jtVqxWQyVQmvrKysS5538ODBfPbZZ+zcuZO77roLLy8vLBYLnp6efPrpp7b/7dy5k5CQEABGjx7NRx99xMaNG0lKSmLdunXX8tWlAVA4SIPxySefEBERQU5ODgAlJSVs2LCBu+6666qOExwczK5duzCbzbRu3brO68zPzycvL882XLRq1SqcnZ2r/CKvC926dWPbtm0UFBSQnZ3Npk2bamwXGBjI0KFDmTZtGunp6QBkZmYybdo0W8/G39/fNlS0b98+fv7550ue9/bbb+fMmTPExsbaegqtWrXipptu4tNPPwXKJ6qnTZtGXl4ey5cvJyYmBijv3QQEBNglLKV+aVhJHGL8+PG2IRSAP//5zzz//PO89tpr/Pa3vwXKw+Huu+9m7ty5V3VsDw8PevToQbdu3eq05go+Pj784Q9/YMSIETRr1ow//vGPDBw4kKeeeoq33367zs4TEhLCtm3bCA0NpW3btgwZMoT4+Pga286ZM4c333yTsWPHYhgGzs7OPPjgg7Z5kUcffZRp06axfft2evfuTZ8+fS55XsMwGDhwIB999JFtGaxhGLz66qvMnj2bv/71r5hMJh599FE8PDwYPnw4s2bNYsWKFRiGQY8ePWxzKHL90nUOIg2Y1Wq1/RUeFRXFl19+yfLlyx1clTQGGlYSaaB++OEHgoODycrKoqSkhM2bN9OzZ09HlyWNhIaVRBqozp07M2LECH7zm99gNpvp2bMn48aNc3RZ0khoWElERKrRsJKIiFRzQwwrFRQUsH//fvz9/ausgBERkUsrLS0lLS2Nrl274ubmVuWzGyIc9u/fz9ixYx1dhojIdSkqKoo777yzyns3RDj4+/sD5V+wpmcEiIhIdadOnWLs2LG236GV3RDhUDGUdNNNNxEQEODgakREri81DcdrQlpERKpROIiISDUKBxERqUbhICIi1SgcRESkGoWDiIhUc0MsZb0WH351hH9/n8qqx3o7uhQRaaTmzZtHUlISaWlp5Ofn06ZNG5o0acKyZcsuu9/UqVOZO3dutaub60KjD4dDaTnsPZLh6DJEpBELCwsDIDY2loMHDzJz5sxa7ffaa6/ZraZGHw6uTmYKS6o/GF5ExJHCwsJwdnYmMzOTuXPn8r//+7/k5eVRUFDAiy++SPfu3RkwYACffPIJc+bMwWKxkJSUxIkTJ1i0aBFBQUHXdH6Fg5OJotKyKk/cEpHGa+2eFNZ8faxOjznqztb89o6rv3tDkyZNmDNnDj/99BMjR45k4MCBxMfHs2LFCpYuXVqlbVFREStXruQf//gH69atUzhcK1fn8jn5wpIy3Jx1R1cRaTi6d+8OQPPmzXnjjTdYuXIlRUVFeHh4VGtbceO8m266iW+//faaz61wcCoPBIWDiAD89o6AX/RXvj04OzsDsGrVKlq0aMHChQv57rvvWLBgQbW2le+PVBfPcGv0S1ldnSp6DqUOrkREpGYZGRm0adMGgC1btlBcXGz3czb6cHCpCIdiTUqLSMM0fPhw3nvvPR577DG6d+9OWloaa9eutes5b4hnSKekpBAcHMzWrVuv+pbd//rmOM/98xu2TPsVt1q87FShiEjDc7nfnY2+53BhzkHDSiIiFRQOlVYriYhIObuuVoqMjCQxMRHDMAgPD7cty6ps8eLFfPPNN6xevZqPPvqI9evX2z7bv38/+/btY/z48eTl5dmWb82cOZOuXbvWSY0VE9JFCgcRERu7hcOuXbs4cuQI0dHRHDp0iPDwcKKjo6u0SU5OZvfu3bblWiNHjmTkyJG2/Tdt2mRrO3fuXG677bY6r7PyUlYRESlnt2Gl+Ph4Bg4cCEBgYCBZWVnk5ORUaTNv3jymTp1a4/7Lly/n6aeftld5NralrMWacxARqWC3nkN6enqVy7f9/PxIS0vDy6t8RVBsbCy9e/emVatW1fb99ttvufnmm/H397e9t2TJEjIyMggMDCQ8PLzO7kLopjkHEZFq6m1CuvKK2czMTGJjY3n00UdrbBsTE8NDDz1k254wYQIzZswgKioKwzCIioqqs7o0rCQiUp3dwsFisZCenm7bPn36tK0n8NVXX3H27FnGjh3LM888Q1JSEpGRkba2CQkJ3H777bbtkJAQ29WBAwYM4MCBA3VWp4uukBYRqcZu4dCnTx/i4uIASEpKwmKx2IaUQkND2bhxI2vWrGHZsmUEBQURHh4OQGpqKp6enri4uADlPY6JEyeSnZ0NlAdHhw4d6qxOV10hLSJSjd3mHHr16kVQUBCjR4/GMAwiIiKIjY3F29ubkJCQS+6XlpaGn5+fbdswDEaNGsXEiRNxd3enRYsWTJ48uc7qrBhWKipVOIiIVLDrdQ7Tp0+vst2pU6dqbQICAli9erVtu2vXrrz77rtV2gwdOpShQ4fapUbdW0lEpLpGf4W02WTgbDY05yAiUkmjDwfQo0JFRC6mcKB8Ulo9BxGRCxQOnA8HzTmIiNgoHCiflNawkojIBQoHKuYcNKwkIlJB4UD5Mx10y24RkQsUDlRMSCscREQqKBzQUlYRkYspHNBSVhGRiykcKJ9z0FJWEZELFA5oWElE5GIKB8DFrGElEZHKFA5oKauIyMUUDmgpq4jIxRQOaM5BRORiCgfKew6lZVZK9DQ4ERFA4QCUzzkA6j2IiJyncODCc6QVDiIi5ez6DOnIyEgSExMxDIPw8HC6d+9erc3ixYv55ptvWL16NQkJCTz33HN06NABgNtuu40XX3yRkydPMmPGDEpLS/H392fhwoW4uLjUWZ2uFc+R1nJWERHAjuGwa9cujhw5QnR0NIcOHSI8PJzo6OgqbZKTk9m9ezfOzs6293r37s2SJUuqtFuyZAljxoxhyJAhvPrqq8TExDBmzJg6q9WlIhx0lbSICGDHYaX4+HgGDhwIQGBgIFlZWeTk5FRpM2/ePKZOnXrFYyUkJBAcHAxA//79iY+Pr9NaK4aVijQhLSIC2DEc0tPT8fX1tW37+fmRlpZm246NjaV37960atWqyn7Jyck89dRTPPLII3zxxRcA5Ofn24aRmjVrVuU4dcFVPQcRkSrsOudQmdVqtb3OzMwkNjaW9957j9TUVNv77dq145lnnmHIkCEcO3aMCRMmsHnz5ksep65cWK2kOQcREbBjOFgsFtLT023bp0+fxt/fH4CvvvqKs2fPMnbsWIqKijh69CiRkZGEh4czdOhQANq0aUPz5s1JTU3Fw8ODgoIC3NzcSE1NxWKx1GmtWq0kIlKV3YaV+vTpQ1xcHABJSUlYLBa8vLwACA0NZePGjaxZs4Zly5YRFBREeHg469evZ+XKlQCkpaVx5swZWrRowX333Wc71ubNm3nggQfqtFatVhIRqcpuPYdevXoRFBTE6NGjMQyDiIgIYmNj8fb2JiQkpMZ9BgwYwPTp09m6dSvFxcXMnj0bFxcXJk+ezMyZM4mOjqZly5aMGDGiTmu1DStpzkFEBLDznMP06dOrbHfq1Klam4CAAFavXg2Al5cXb731VrU2FouF9957zz5FomElEZGL6QppLlznoNt2i4iUUzigOQcRkYspHKgcDuo5iIiAwgHQnIOIyMUUDoCz2cAwoLBYw0oiIqBwAMAwDD0qVESkEoXDeXpUqIjIBQqH88p7DhpWEhEBhYONi4aVRERsFA7nac5BROQChcN5rk5m3VtJROQ8hcN5rs6acxARqaBwOE/DSiIiFygcztNSVhGRCxQO57k6mXSFtIjIeQqH81ydzRSVqucgIgIKBxsXs0mrlUREzlM4nFe+WknhICICCgcb3T5DROQChcN5Wq0kInKBkz0PHhkZSWJiIoZhEB4eTvfu3au1Wbx4Md988w2rV68GYMGCBezZs4eSkhKefPJJBg0aRFhYGElJSTRt2hSAxx9/nH79+tVpra5OJopKyrBarRiGUafHFhG53tgtHHbt2sWRI0eIjo7m0KFDhIeHEx0dXaVNcnIyu3fvxtnZGYCvvvqKgwcPEh0dTUZGBg899BCDBg0CYNq0afTv399e5eLqfOFRoW7OZrudR0TkemC3YaX4+HgGDhwIQGBgIFlZWeTk5FRpM2/ePKZOnWrbvuuuu3j99dcB8PHxIT8/n9LS+pkHqHhUqJaziojYMRzS09Px9fW1bfv5+ZGWlmbbjo2NpXfv3rRq1cr2ntlsxsPDA4CYmBj69u2L2Vz+S/vDDz9kwoQJTJ06lbNnz9Z5va5O53sOWs4qIlJ/E9JWq9X2OjMzk9jYWB599NEa227ZsoWYmBheeuklAIYPH8706dP54IMP6Ny5M8uWLavz+mzhoBVLIiL2CweLxUJ6erpt+/Tp0/j7+wPlcwtnz55l7NixPPPMMyQlJREZGQnAjh07eOutt1ixYgXe3t4A3HvvvXTu3BmAAQMGcODAgTqv18XpwpyDiEhjZ7dw6NOnD3FxcQAkJSVhsVjw8vICIDQ0lI0bN7JmzRqWLVtGUFAQ4eHhnDt3jgULFvD222/bViYBTJ48mWPHjgGQkJBAhw4d6rzeijkHDSuJiNhxtVKvXr0ICgpi9OjRGIZBREQEsbGxeHt7ExISUuM+GzduJCMjgylTptjemz9/PmPHjmXKlCm4u7vj4eHB3Llz67zeC6uVNKwkImLX6xymT59eZbtTp07V2gQEBNiucfjd737H7373u2ptWrZsydq1a+1T5HmuGlYSEbHRFdLn2YaVFA4iIgqHCheWsmpYSURE4XCel2v5CFtOYYmDKxERcTyFw3nebuXhcK5A4SAionA4z9ut/P5O5wqKHVyJiIjjXTEccnJy+Omnn4Dym+m9//77drl9haO5OJlwdTKp5yAiQi3CYcqUKZw+fZqDBw8yf/58/Pz8mDVrVn3UVu+83ZzJVjiIiFw5HIqKirj77rvZtGkTEydO5MEHH6SwsLA+aqt3Pm5OGlYSEaGW4bB+/Xo2bNhA//79SUlJ4dy5c/VRW73zdnPSsJKICLUIh4iICL799ltmz56Nl5cXn3/+eZXbW9xIvN2ctZRVRIRa3D6jdevWjBkzhltuuYVdu3ZRXFxMUFBQfdRW77xcnTh9rsDRZYiIOFytJqTT0tIayYS0hpVEREAT0lV4uzkrHERE0IR0Fd5uTuQUllBaZr1yYxGRG1itJ6RffvnlRjAhrfsriYhALSakO3fuTEhICD/88AMHDhyga9eu9OrVqz5qq3c+lW6h0cTd2cHViIg4zhV7DpGRkbz//vtYrVYKCgp44403eO211+qjtnqnm++JiJS7Ys8hKSmJqKgo2/YTTzzBuHHj7FqUo1y4+Z7CQUQatyv2HEpKSigouLD2Py8vj9LSG/OBOBd6DrqFhog0blfsOfz+97/nwQcfpF27dpSVlXH06FFmzJhRq4NHRkaSmJiIYRiEh4fTvXv3am0WL17MN998Y3uOdE37nDx5khkzZlBaWoq/vz8LFy7ExcXlKr/qlWlYSUSk3BXDYejQofTr14+ff/4ZwzBo164dzs5XnqzdtWsXR44cITo6mkOHDhEeHk50dHSVNsnJyezevdt2vEvts2TJEsaMGcOQIUN49dVXiYmJYcyYMb/wK1+anukgIlKuVg/78fDwoEuXLnTu3Bl3d3cee+yxK+4THx/PwIEDAQgMDCQrK4ucnJwqbebNm8fUqVOvuE9CQgLBwcEA9O/fn/j4+Np9u6tU0XPQbbtFpLH7RU+Cs1qvfJFYeno6vr6+tm0/Pz/S0tJs27GxsfTu3ZtWrVpdcZ/8/HzbMFKzZs2qHKcuuTmbcTHrgT8iIr8oHAzDuOp9KgdKZmYmsbGxPProo7Xe53Lv1SVvPdNBROTScw7z58+vMQSsVivHjh274oEtFgvp6em27dOnT+Pv7w/AV199xdmzZxk7dixFRUUcPXqUyMjIS+7j4eFBQUEBbm5upKamYrFYrupLXg3dfE9E5DLhcNttt11yp8t9VqFPnz4sXbqU0aNHk5SUhMViwcvLC4DQ0FBCQ0MBSElJYdasWYSHh7N3794a97nvvvuIi4tj+PDhbN68mQceeOBqv2etld98Tz0HEWncLhkODz300DUduFevXgQFBTF69GgMwyAiIoLY2Fi8vb0JCQmp9T4AkydPZubMmURHR9OyZUtGjBhxTbVdjnoOIiK1WMp6LaZPn15lu1OnTtXaBAQE2K5xqGkfKB+ieu+99+q+wBp4uznxc3pevZxLRKSh+kUT0jcyDSuJiFwmHBISEqpsFxUV2V5/9NFH9qvIwTSsJCJymXBYvnx5le0//OEPtteffPKJ/SpyMG83Z3KKSijTA39EpBG7ZDhcfD1B5W17X2vgSD5uTlitkFOk3oOINF6XDIeLr3GovP1LLoK7XujmeyIil1mtVFZWRkFBga2XULFdVlZGWVlZvRVY37xcy2++l6NwEJFG7JLhcOLECYYNG1ZlCGno0KFAY+k5aMWSiDRelwyHzz77rD7raDA0rCQicpk5h+LiYv76179SXHzhL+iDBw+yZMmSeinMUSqe6ZCtnoOINGKXDIf58+eTk5NTZVipbdu25OTksGzZsnopzhF81HMQEbl0OOzbt48XXnihyuM4XVxcCAsL44svvqiX4hzhwtPgFA4i0nhdMhzMZnPNO5hMVYaabjRuziacTIYmpEWkUbtkOPj6+vL1119Xe3/btm00b97crkU5kmEYuoWGiDR6l1ytFB4ezuTJkwkMDKRz586UlpaSmJjIyZMnWblyZX3WWO908z0RaewuGQ5t27Zl3bp1fPHFFxw+fBjDMBg3bhx9+vS5oa9zAPBxdyIrX+EgIo3XZZ/nYDKZeOCBB+z65LWG6CYfN1Iy8h1dhoiIw+h5DjUI8PXgeEb+DX2DQRGRy1E41CDA151zhSVk52tSWkQaJ4VDDQJ8PQA4lqHHhYpI46RwqEGArzsAKQoHEWmkLjshfa0iIyNJTEzEMAzCw8Pp3r277bM1a9YQExODyWSiU6dOREREEBMTw/r1621t9u/fz759+xg/fjx5eXl4eJT/RT9z5ky6du1qt7pbn+85aFJaRBoru4XDrl27OHLkCNHR0Rw6dIjw8HCio6MByM/PZ8OGDURFReHs7MyECRPYt28fI0eOZOTIkbb9N23aZDve3Llzue222+xVbhU+7k54uzopHESk0bLbsFJ8fDwDBw4EIDAwkKysLHJycgBwd3dn1apVODs7k5+fT05ODv7+/lX2X758OU8//bS9yrsswzBo5euuYSURabTsFg7p6en4+vratv38/EhLS6vS5p133iEkJITQ0FBat25te//bb7/l5ptvrhIYS5YsYezYsbz00ksUFBTYq2ybAF8P9RxEpNGqtwnpmq4ZeOKJJ9iyZQs7duxgz549tvdjYmJ46KGHbNsTJkxgxowZREVFYRgGUVFRdq83wNedY2fzdK2DiDRKdgsHi8VCenq6bfv06dO2nkBmZia7d+8GwM3Njb59+7J3715b24SEBG6//XbbdkhICG3atAFgwIABHDhwwF5l27T28yC3qJTMPN1GQ0QaH7uFQ58+fYiLiwMgKSkJi8WCl5cXACUlJYSFhZGbmwvAd999R/v27QFITU3F09PT9hwJq9XKxIkTyc7OBsqDo0OHDvYq2+bCclYNLYlI42O31Uq9evUiKCiI0aNHYxgGERERxMbG4u3tTUhICJMmTWLChAk4OTnRsWNHgoODAUhLS8PPz892HMMwGDVqFBMnTsTd3Z0WLVowefJke5VtU/lah24BTex+PhGRhsSw3gCD6ikpKQQHB7N161YCAgLq5JhZ+cX0eHkzzw/tzP/re0udHFNEpCG53O9OXSF9CU3cnfF2c9ItNESkUVI4XIaWs4pIY6VwuIwAXQgnIo2UwuEyysNBz3UQkcZH4XAZrX09yCsq5WxukaNLERGpVwqHy2jXvPzurD+l5zq4EhGR+qVwuIxA//KL9g6nKRxEpHFROFxGgK8HLmYTh9JyHF2KiEi9Ujhchtlk0K65B4fUcxCRRkbhcAW3NPficLp6DiLSuCgcruAWf0+OnsmjuLTM0aWIiNQbhcMVBPp7UVJm5ehZXQwnIo2HwuEKbvH3BODQaQ0tiUjjoXC4glsqlrPqWgcRaUQUDlfQxN2Z5l6uHNZyVhFpRBQOtRDo76nlrCLSqCgcauEWfy/1HESkUVE41EKgvycZecW6AZ+INBoKh1q4cI8l9R5EpHFwsufBIyMjSUxMxDAMwsPD6d69u+2zNWvWEBMTg8lkolOnTkRERLBr1y6ee+45OnToAMBtt93Giy++yMmTJ5kxYwalpaX4+/uzcOFCXFxc7Fl6FRXLWQ+n5XJnO796O6+IiKPYLRx27drFkSNHiI6O5tChQ4SHhxMdHQ1Afn4+GzZsICoqCmdnZyZMmMC+ffsA6N27N0uWLKlyrCVLljBmzBiGDBnCq6++SkxMDGPGjLFX6dUE+Hrg7mxm/4ksRtG63s4rIuIodhtWio+PZ+DAgQAEBgaSlZVFTk75sIy7uzurVq3C2dmZ/Px8cnJy8Pf3v+SxEhISCA4OBqB///7Ex8fbq+wamU0Gvdv78eWhM/V6XhERR7FbOKSnp+Pr62vb9vPzIy0trUqbd955h5CQEEJDQ2nduvwv8uTkZJ566ikeeeQRvvjiC6C8p1ExjNSsWbNqx6kPfW5tRvLpHFKzC+r93CIi9a3eJqRreg7zE088wZYtW9ixYwd79uyhXbt2PPPMM7z55pvMnz+f559/nqKioisepz7cF9gcgC+S0x1yfhGR+mS3cLBYLKSnX/hFevr0advQUWZmJrt37wbAzc2Nvn37snfvXlq0aMHQoUMxDIM2bdrQvHlzUlNT8fDwoKCg/C/21NRULBaLvcq+pC43++Dr4cwXyRpaEpEbn93CoU+fPsTFxQGQlJSExWLBy6t8SWhJSQlhYWHk5pZfdfzdd9/Rvn171q9fz8qVKwFIS0vjzJkztGjRgvvuu892rM2bN/PAAw/Yq+xLMpkM7g1sxpeH0r/JY3oAABQ1SURBVB3WexERqS92W63Uq1cvgoKCGD16NIZhEBERQWxsLN7e3oSEhDBp0iQmTJiAk5MTHTt2JDg4mNzcXKZPn87WrVspLi5m9uzZuLi4MHnyZGbOnEl0dDQtW7ZkxIgR9ir7su4LbM7G707xU3qu7YZ8IiI3IsN6A/wZnJKSQnBwMFu3biUgIMBu5/k5PZd+i7YxZ0RXxt/T1m7nERGpD5f73akrpK9C22YetGrqzpealBaRG5zC4SoYhkGfW5vxRXK6HhsqIjc0hcNVGtCpBdkFJez++ayjSxERsRuFw1V6oENzXJxMbPn+tKNLERGxG4XDVfJ0deL+W5vz7x9OaUmriNywFA6/QEiXFhw7m8+BVN3CW0RuTAqHXyC4U/kV2v/+/pSDKxERsQ+Fwy9g8XGjZ+um/PsHzTuIyI1J4fALhXRpQeKxTN2lVURuSAqHX2hI15swmwzC1n5LaZkmpkXkxqJw+IVu8ffi5QeD+M9/0/jLhh8cXY6ISJ2y6zOkb3Tj7mnLobQc/vbFT3Ro4cUjvds4uiQRkTqhnsM1emFYFx7o0JyXP0niyJlcR5cjIlInFA7XyGwyWPBwd5xNJsLWfkeZ5h9E5AagcKgDNzdxJ3xYZ+IPn+Efu486uhwRkWumcKgjo+9qTZ9bmzF344+knSt0dDkiItdE4VBHDMPgleFdySsqYcWOw44uR0Tkmigc6lCgvxfDe7ZidfwR0nPUexCR65fCoY49M+BWCktKWbFdvQcRuX4pHOpYoL8XD/ZoyQfqPYjIdcyuF8FFRkaSmJiIYRiEh4fTvXt322dr1qwhJiYGk8lEp06diIiIwDAMFixYwJ49eygpKeHJJ59k0KBBhIWFkZSURNOmTQF4/PHH6devnz1LvyaTgzuwPvEEj72/m1dH9eRWi5ejSxIRuSp2C4ddu3Zx5MgRoqOjOXToEOHh4URHRwOQn5/Phg0biIqKwtnZmQkTJrBv3z6Kioo4ePAg0dHRZGRk8NBDDzFo0CAApk2bRv/+/e1Vbp0K9PfijbF3MCv2W4Yt2cHzwzoz/p62GIbh6NJERGrFbuEQHx/PwIEDAQgMDCQrK4ucnBy8vLxwd3dn1apVQHlQ5OTk4O/vT8uWLW29Cx8fH/Lz8yktLbVXiXYV2vUmerVpyoy13/LSv5LYeySDub/pjruL2dGliYhckd3mHNLT0/H19bVt+/n5kZaWVqXNO++8Q0hICKGhobRu3Rqz2YyHhwcAMTEx9O3bF7O5/Jfphx9+yIQJE5g6dSpnz561V9l1yuLjxt9+fxf/G3Ib/0o8wW/e/JKjZ/IcXZaIyBXV24R0Tc9bfuKJJ9iyZQs7duxgz549tve3bNlCTEwML730EgDDhw9n+vTpfPDBB3Tu3Jlly5bVV9nXzGQymBzcgb9NvIvjGXn8etlOtv1XDwkSkYbNbuFgsVhIT0+3bZ8+fRp/f38AMjMz2b17NwBubm707duXvXv3ArBjxw7eeustVqxYgbe3NwD33nsvnTt3BmDAgAEcOHDAXmXbTf+OFj6ZfD83N3Hj0fd3M/2jRNYnntDV1CLSINktHPr06UNcXBwASUlJWCwWvLzKV+2UlJQQFhZGbm75XUy/++472rdvz7lz51iwYAFvv/22bWUSwOTJkzl27BgACQkJdOjQwV5l21XbZp58/HQfHundhrj9p3j2H/u46y9bGPTa58xen8RP6bqrq4g0DHabkO7VqxdBQUGMHj0awzCIiIggNjYWb29vQkJCmDRpEhMmTMDJyYmOHTsSHBzMmjVryMjIYMqUKbbjzJ8/n7FjxzJlyhTc3d3x8PBg7ty59irb7txdzEQ+1I1XHgwi6UQ2Xx46w5eH0vnn7qNE7z7G7Ae7MOrO1lrZJCIOZVhrmgy4zqSkpBAcHMzWrVsJCAhwdDm/yKmsAqat+YYvD53h/lub89s7WhHcuQU+bs6OLk1EblCX+92pJ8E1EDc1cePDx+9m5c6fWLnzJ6ZGJ2I2Gdzk48ZNTdy4s50vY3u3pU0zD0eXKiKNgMKhATGZDP5f31t4/P727DuWyef/PU1KZj4pGfm8u+Mn3tl+mLva+XFzEzd8PVzocrMPd7X3o10zDw1DiUidUjg0QCaTwR1tfbmj7YXrRE5lFfCPXUfZdiCNfUczOZNTSG5R+QWCzb1cuaudL73a+NKyqTt+ni4093KhmZcrTd2dMZkUHCJydRQO14mbmrgxNeQ2pobcBpRfN3IoLYddP2Xw9c9n2fXzWTbtP1VtP5MBfp4uNPN0pZmXC36eLrT286BHQFO6BzShuZcrLk66/6KIVKVwuE4ZhsGtFm9utXgz5u42AJzJKSQtp5CzOUWk5xZxJqeQs7lFpOcUcTa3kDM5RSSdyCYu6RTFpRfWIbg5m/B2c8bHzQkfd2d83JzxdnPCxWzCZDLwdDHT3MuVZl6ueLqa8XRxwsPFjLuLGU9XJ9ydzXicf+3qZNIQl8gNQOFwA2l2/hf4lRQUl/L9yWy+P5FNVn4x2fnFZBcUk51fQnZBMZl5RRw7m0dRaRllZVZyCkvILiipVQ0mAzxcnHB3KQ8Mj/NB4unqhLebE65mEwUlpRQUl+FkMnB1NuPqZMLVyYSb7bUZN+fy9yo+r/xZmbW8psKSUvw8XbF4u+LhYsZ0PpRMJgOTASbDwDj/r8kwKC2zkpVfRFZ+MZ6uTtzcxB0fNyeFmUgNFA6NkJuzmV5tyucoaquguJSMvCJyC0vJKyohr6iU/KJS8opKyS0qsb2u+Kzq6xKy8otJycijqKQMd2czrs4mSkqtFJWUUVBcSmFJGYXnX5eU1d/qarPJwFwlRCqFismwvWcY5e0qXptMF0Kn+r4V7S7apuo+lYPLqDhupWNBpW0TGBiVjnn+3/JmGFw4p0F5e2ztq+4LF9oZFXWZqh6r4v3K7c5/TMUBjAsvz39W+dgXfVYpgGvVvtJnF/YzbDVcXI9hVD8WldpX/s4XH/dqVd714uMYVHujxv3K9zUu1bTasS93Hh93J/rdZqnzuUWFg9SKm7OZm5u418u5SkrLKCoto6C4jMKSUgqLyyg4/29hSRkmA7zdnHE2G5zJLeJ0diGFJaWUWaHMaoXz/1ZsW8+/Ngxo4u5ME3dncgtLOZmVT0ZeUaV2UFZWfb/SitdlF45b/pm16r7n3ystA6jc7kIbK5WOUwallNk+L7MCldpX7F/52FbKazzf1HZcuHAuKxX7lNdha1dpX8p/TOXHr3ws22cV9V64L1pFZF//V0bdWEwG/Hvarwj0r9vnxigcpMFxMptwMpvwcLly21v87V+PXJotOCoCikuHSUXgVN2//P2Lj1Fx7GqBdJXtbe9Ya66n2l/6V3DhDNVD8uLMrHx98ZUCtfqxan8eTxczFh+3y5/gF1A4iMgvVnk4p9K7DqlF6pbWMIqISDUKBxERqUbhICIi1SgcRESkGoWDiIhUo3AQEZFqboilrKWl5XcnPXWq+o3nRESkZhW/Myt+h1Z2Q4RDWloaAGPHjnVwJSIi15+0tDTatm1b5b0b4jGhBQUF7N+/H39/f8xms6PLERG5LpSWlpKWlkbXrl1xc6t6lfUNEQ4iIlK3NCEtIiLV3BBzDtciMjKSxMREDMMgPDyc7t27O7qkK1qwYAF79uyhpKSEJ598km7dujFjxgxKS0vx9/dn4cKFuLjU4q51DlRQUMD//M//8PTTT3PvvfdeV/WvX7+ed999FycnJ5599lk6dux43dSfm5vLzJkzycrKori4mEmTJuHv78/s2bMB6NixIy+//LJji6zBgQMHePrpp5k4cSLjxo3j5MmTNf7M169fz6pVqzCZTIwaNYqRI0c6unSg5vpnzZpFSUkJTk5OLFy4EH9//4ZVv7URS0hIsD7xxBNWq9VqTU5Oto4aNcrBFV1ZfHy89Q9/+IPVarVaz549a/3Vr35lDQsLs27cuNFqtVqtixcvtkZFRTmyxFp59dVXrb/5zW+sa9euva7qP3v2rHXQoEHWc+fOWVNTU60vvPDCdVX/6tWrrYsWLbJarVbrqVOnrIMHD7aOGzfOmpiYaLVardZp06ZZt23b5sgSq8nNzbWOGzfO+sILL1hXr15ttVqtNf7Mc3NzrYMGDbJmZ2db8/PzrcOGDbNmZGQ4snSr1Vpz/TNmzLBu2LDBarVarR9++KF1/vz5Da7+Rj2sFB8fz8CBAwEIDAwkKyuLnJwcB1d1eXfddRevv/46AD4+PuTn55OQkEBwcDAA/fv3Jz4+3pElXtGhQ4dITk6mX79+ANdV/fHx8dx77714eXlhsViYM2fOdVW/r68vmZmZAGRnZ9O0aVOOHz9u6zE3xPpdXFxYsWIFFovF9l5NP/PExES6deuGt7c3bm5u9OrVi7179zqqbJua6o+IiGDw4MHAhf+bNLT6G3U4pKen4+t74Wlofn5+tmWxDZXZbMbDwwOAmJgY+vbtS35+vm0Yo1mzZg3+O8yfP5+wsDDb9vVUf0pKCgUFBTz11FOMGTOG+Pj466r+YcOGceLECUJCQhg3bhwzZszAx8fH9nlDrN/JyanaSpqafubp6en4+fnZ2jSU/55rqt/DwwOz2UxpaSl///vf+fWvf93g6m/0cw6VWa+jhVtbtmwhJiaGv/3tbwwaNMj2fkP/DuvWraNnz560bt26xs8bev0AmZmZLFu2jBMnTjBhwoSLHurSsOv/17/+RcuWLVm5ciU//vgjkyZNwtvb2/Z5Q6+/JpequaF/l9LSUmbMmME999zDvffeyyeffFLlc0fX36jDwWKxkJ6ebts+ffo0/v4N/9FiO3bs4K233uLdd9/F29sbDw8PCgoKcHNzIzU1tUr3taHZtm0bx44dY9u2bZw6dQoXF5frqv5mzZpx++234+TkRJs2bfD09MRsNl839e/du5f7778fgE6dOlFYWEhJSYnt84Zef4Wa/n+mpv+ee/bs6cAqL2/WrFm0bduWZ555Bqj595Ej62/Uw0p9+vQhLi4OgKSkJCwWC15edfsc1rp27tw5FixYwNtvv03Tpk0BuO+++2zfY/PmzTzwwAOOLPGy/vrXv7J27VrWrFnDyJEjefrpp6+r+u+//36++uorysrKyMjIIC8v77qqv23btiQmJgJw/PhxPD09CQwM5OuvvwYafv0VavqZ9+jRg++++47s7Gxyc3PZu3cvd955p4Mrrdn69etxdnbm2Weftb3X0Opv9BfBLVq0iK+//hrDMIiIiKBTp06OLumyoqOjWbp0Ke3bt7e9N2/ePF544QUKCwtp2bIlc+fOxdnZ2YFV1s7SpUtp1aoV999/PzNnzrxu6v/nP/9JTEwMAH/84x/p1q3bdVN/bm4u4eHhnDlzhpKSEp577jn8/f156aWXKCsro0ePHsyaNcvRZVaxf/9+5s+fz/Hjx3FycqJFixYsWrSIsLCwaj/zTz/9lJUrV2IYBuPGjePBBx90dPk11n/mzBlcXV1tf4wGBgYye/bsBlV/ow8HERGprlEPK4mISM0UDiIiUo3CQUREqlE4iIhINQoHERGpRuEgN7SUlBRuv/12xo8fX+V/FfcXuhZLly7lww8/vGybjh078tlnn9m2ExISWLp06S8+Z0JCQpW18SL20qivkJbGoX379qxevdoh527Xrh3Lli3jV7/6lZ5SKNcVhYM0WmFhYXh4eHD48GEyMjKYO3cuXbp0YdWqVWzcuBGA4OBgnnjiCY4fP05YWBilpaW0bNmS+fPnA+X36X/yySf5+eefef755+nbt2+Vc1gsFrp168bHH3/Mww8/XOWzu+++m4SEBACeffZZxo4dy65du8jIyODIkSOkpKTw3HPPsXbtWo4fP86KFSsAyMrKYtKkSRw/fpyQkBAmTZpEcnIyr7zyCoZh4Onpybx588jOzuZPf/oTHh4ejBs3jv79+9v7Ryo3EA0rSaNWUlLC+++/z3PPPcfy5cs5duwYH3/8MVFRUURFRbFp0yaOHj3Ka6+9xsSJE/n73/+OxWJh//79QPlN+N5++21eeOEF/vnPf9Z4jieffJJVq1ZRUFBQq5qysrJYuXIloaGhrFu3zvZ669atAPz3v/9lwYIFrFmzhrVr15KZmcmcOXN45ZVXWLVqFX369CEqKgqAH374gUWLFikY5Kqp5yA3vJ9++onx48fbttu3b88rr7wClN+jB6Bnz54sWrSIH374gR49euDkVP6fRq9evfjxxx/5/vvvef755wGYMWMGANu3b6dXr14AtGjRgnPnztV4/iZNmjB8+HA++OADevToccV6u3XrBlDlJpDNmze3zZN07doVT09PoPy2C8eOHePbb7/lxRdfBKCoqMh2jNatW1e5Lb1IbSkc5IZ3uTmHsrIy22vDMDAMo8qtkouLizGZTJjN5hpvoVwRIlcyfvx4Hn74Ydq1a1fj58XFxTUes/LrivMbhlFlX8MwcHd354MPPqjyWUpKSoO9x5M0fBpWkkZtz549AOzbt4/AwEA6d+7MN998Q0lJCSUlJSQmJtK5c2e6du3KV199BcDrr7/Ol19+eVXncXV15dFHH+Wtt96yvWcYBvn5+eTn5/PDDz/U+ljff/89+fn5FBYWcujQIdq0aUOnTp3Yvn07ABs2bGhwT3OT6496DnLDu3hYCeBPf/oTAIWFhTz55JOcPHmShQsXEhAQwO9+9zvGjRuH1Wpl5MiRtGrVimeffZZZs2bx97//nZtvvplnnnnGFiy1NWLECN577z3b9iOPPMKoUaMIDAwkKCio1sfp0qUL4eHh/Pzzz4wePRofHx+ef/55XnzxRVasWIGrqyuLFy9u8I+8lYZNd2WVRissLIzBgwdrslakBhpWEhGRatRzEBGRatRzEBGRahQOIiJSjcJBRESqUTiIiEg1CgcREalG4SAiItX8f9bsG/fOO8YrAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"basic\"\n",
        "flip_probabilities = [0.1, 0.5, 0.9]\n",
        "rnn_arr2 = []\n",
        "nrnn_arr2 = []\n",
        "lstm_arr2 = []\n",
        "seed_arr2 = []\n",
        "method_arr2 = []\n",
        "specify_arr2 = []\n",
        "\n",
        "\n",
        "for flip_probability in flip_probabilities:\n",
        "\n",
        "  x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped= generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                      flip_probability, flip_probability_0=None, flip_probability_1=None,\n",
        "                      startprob=None, transmat=None)\n",
        "  \n",
        "  basic_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "  basic_train_flipped_loader = data_utils.DataLoader(basic_train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "  basic_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "  basic_test_flipped_loader = data_utils.DataLoader(basic_test_flipped, batch_size=25, shuffle=True)\n",
        "\n",
        "  for seed in seeds:\n",
        "    acc = driver(seed, basic_train_flipped_loader, basic_test_flipped_loader)\n",
        "    acc2 = driver(seed, basic_train_flipped_loader, basic_test_flipped_loader, nrnn = True)\n",
        "    acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
        "    rnn_arr2.append(acc)\n",
        "    nrnn_arr2.append(acc2)\n",
        "    lstm_arr2.append(acc3)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(flip_probability)"
      ],
      "metadata": {
        "id": "8Wy7bjBuWxp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "57541d1b-d888-4eb5-f720-30742c98e95d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6442 | test accuracy: 0.438\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7346 | test accuracy: 0.525\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7122 | test accuracy: 0.455\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6686 | test accuracy: 0.508\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6388 | test accuracy: 0.434\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7357 | test accuracy: 0.505\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8165 | test accuracy: 0.431\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6875 | test accuracy: 0.468\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7507 | test accuracy: 0.545\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7217 | test accuracy: 0.492\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6202 | test accuracy: 0.522\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6864 | test accuracy: 0.485\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6960 | test accuracy: 0.468\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7000 | test accuracy: 0.468\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7331 | test accuracy: 0.542\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6863 | test accuracy: 0.478\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7160 | test accuracy: 0.522\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6533 | test accuracy: 0.468\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7108 | test accuracy: 0.448\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7788 | test accuracy: 0.559\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7059 | test accuracy: 0.478\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6504 | test accuracy: 0.478\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7871 | test accuracy: 0.485\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7029 | test accuracy: 0.468\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6854 | test accuracy: 0.475\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6680 | test accuracy: 0.458\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6790 | test accuracy: 0.434\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7226 | test accuracy: 0.502\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6645 | test accuracy: 0.458\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6593 | test accuracy: 0.465\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.5662 | test accuracy: 0.498\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6852 | test accuracy: 0.495\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6166 | test accuracy: 0.471\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6442 | test accuracy: 0.458\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6624 | test accuracy: 0.468\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7020 | test accuracy: 0.458\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7007 | test accuracy: 0.461\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6974 | test accuracy: 0.461\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7075 | test accuracy: 0.468\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6666 | test accuracy: 0.461\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7358 | test accuracy: 0.458\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6693 | test accuracy: 0.465\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6857 | test accuracy: 0.468\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7254 | test accuracy: 0.458\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7065 | test accuracy: 0.465\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6390 | test accuracy: 0.461\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7565 | test accuracy: 0.458\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7567 | test accuracy: 0.461\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7000 | test accuracy: 0.461\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6513 | test accuracy: 0.465\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6817 | test accuracy: 0.465\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6553 | test accuracy: 0.461\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6941 | test accuracy: 0.458\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7371 | test accuracy: 0.465\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7299 | test accuracy: 0.458\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7549 | test accuracy: 0.468\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7036 | test accuracy: 0.458\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6196 | test accuracy: 0.471\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6522 | test accuracy: 0.455\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6780 | test accuracy: 0.461\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6241 | test accuracy: 0.465\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6611 | test accuracy: 0.461\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6739 | test accuracy: 0.461\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6750 | test accuracy: 0.465\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6509 | test accuracy: 0.468\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6220 | test accuracy: 0.465\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7387 | test accuracy: 0.458\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7176 | test accuracy: 0.468\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6730 | test accuracy: 0.461\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7461 | test accuracy: 0.465\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6848 | test accuracy: 0.455\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7344 | test accuracy: 0.458\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7277 | test accuracy: 0.468\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7063 | test accuracy: 0.468\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6188 | test accuracy: 0.458\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7018 | test accuracy: 0.468\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6008 | test accuracy: 0.458\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6273 | test accuracy: 0.468\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7059 | test accuracy: 0.468\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6908 | test accuracy: 0.465\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7013 | test accuracy: 0.451\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6022 | test accuracy: 0.468\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7024 | test accuracy: 0.458\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7438 | test accuracy: 0.468\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7011 | test accuracy: 0.458\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7292 | test accuracy: 0.461\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7252 | test accuracy: 0.465\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6438 | test accuracy: 0.468\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6592 | test accuracy: 0.471\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6378 | test accuracy: 0.468\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6628 | test accuracy: 0.461\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6540 | test accuracy: 0.458\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7613 | test accuracy: 0.468\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6924 | test accuracy: 0.461\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7450 | test accuracy: 0.468\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6333 | test accuracy: 0.458\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6963 | test accuracy: 0.471\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7148 | test accuracy: 0.475\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6762 | test accuracy: 0.468\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6872 | test accuracy: 0.465\n",
            "total time:  77.17408172799992\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6311 | test accuracy: 0.468\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7482 | test accuracy: 0.559\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7211 | test accuracy: 0.438\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6884 | test accuracy: 0.515\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6487 | test accuracy: 0.438\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7203 | test accuracy: 0.495\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7995 | test accuracy: 0.444\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6875 | test accuracy: 0.478\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7647 | test accuracy: 0.556\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7242 | test accuracy: 0.492\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6187 | test accuracy: 0.529\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6755 | test accuracy: 0.505\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7161 | test accuracy: 0.451\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6763 | test accuracy: 0.458\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7393 | test accuracy: 0.542\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6697 | test accuracy: 0.461\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7177 | test accuracy: 0.508\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6424 | test accuracy: 0.451\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6941 | test accuracy: 0.438\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7782 | test accuracy: 0.556\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6955 | test accuracy: 0.485\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6386 | test accuracy: 0.441\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.8131 | test accuracy: 0.508\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6958 | test accuracy: 0.448\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6880 | test accuracy: 0.468\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6865 | test accuracy: 0.438\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6741 | test accuracy: 0.434\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7371 | test accuracy: 0.519\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6831 | test accuracy: 0.478\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6618 | test accuracy: 0.481\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.5793 | test accuracy: 0.492\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6983 | test accuracy: 0.475\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6190 | test accuracy: 0.478\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6318 | test accuracy: 0.471\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6662 | test accuracy: 0.478\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7162 | test accuracy: 0.478\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6980 | test accuracy: 0.471\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6905 | test accuracy: 0.471\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7006 | test accuracy: 0.478\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6691 | test accuracy: 0.471\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7405 | test accuracy: 0.475\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6642 | test accuracy: 0.475\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6793 | test accuracy: 0.478\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7256 | test accuracy: 0.471\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7183 | test accuracy: 0.478\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6353 | test accuracy: 0.468\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7509 | test accuracy: 0.468\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7664 | test accuracy: 0.468\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7183 | test accuracy: 0.468\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6512 | test accuracy: 0.475\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6898 | test accuracy: 0.475\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6556 | test accuracy: 0.471\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6869 | test accuracy: 0.468\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7539 | test accuracy: 0.475\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7285 | test accuracy: 0.468\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7502 | test accuracy: 0.481\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7208 | test accuracy: 0.468\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6162 | test accuracy: 0.478\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6537 | test accuracy: 0.465\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6829 | test accuracy: 0.468\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6323 | test accuracy: 0.485\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6448 | test accuracy: 0.468\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6759 | test accuracy: 0.471\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6834 | test accuracy: 0.468\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6530 | test accuracy: 0.478\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6177 | test accuracy: 0.471\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7442 | test accuracy: 0.465\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7225 | test accuracy: 0.478\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6529 | test accuracy: 0.468\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7527 | test accuracy: 0.468\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6672 | test accuracy: 0.468\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7449 | test accuracy: 0.468\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7233 | test accuracy: 0.475\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7193 | test accuracy: 0.471\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6212 | test accuracy: 0.471\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7026 | test accuracy: 0.471\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6083 | test accuracy: 0.471\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6303 | test accuracy: 0.468\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6988 | test accuracy: 0.471\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6939 | test accuracy: 0.468\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6869 | test accuracy: 0.471\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6004 | test accuracy: 0.481\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7042 | test accuracy: 0.481\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7333 | test accuracy: 0.471\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6983 | test accuracy: 0.468\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7242 | test accuracy: 0.468\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7287 | test accuracy: 0.468\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6412 | test accuracy: 0.471\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6562 | test accuracy: 0.465\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6519 | test accuracy: 0.465\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6617 | test accuracy: 0.471\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6472 | test accuracy: 0.461\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7551 | test accuracy: 0.458\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6960 | test accuracy: 0.468\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7384 | test accuracy: 0.478\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6407 | test accuracy: 0.468\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6838 | test accuracy: 0.478\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7189 | test accuracy: 0.468\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6775 | test accuracy: 0.465\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6824 | test accuracy: 0.465\n",
            "total time:  78.492054199\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19820499420166016.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.3419649600982666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5697905889579228 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20241045951843262.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.34751319885253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4749341424022402 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20710158348083496.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3450801372528076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4177906279053007 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068321704864502.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.35509538650512695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.391172422681536 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19955992698669434.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.34598326683044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3715885864836829 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20935988426208496.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.36115288734436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.36100745584283556 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046365737915039.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35024499893188477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.35134538752692085 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20860624313354492.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35806727409362793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3446619344609124 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19804859161376953.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.354367733001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3392230778932571 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21196198463439941.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3556694984436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33534476161003113 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20836663246154785.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3475348949432373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3324363338095801 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20189118385314941.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3503258228302002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3305805483034679 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2041158676147461.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34545230865478516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.328087061217853 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20378971099853516.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34386706352233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32662836738995144 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21077394485473633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3518552780151367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.324501063142504 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20830368995666504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3719213008880615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32397508493491584 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21514320373535156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3630349636077881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32240160277911595 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2153613567352295.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3624577522277832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3218062860625131 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20629215240478516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3558056354522705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32119489908218385 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20614862442016602.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34827375411987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3200970185654504 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21477246284484863.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35978055000305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.319571202141898 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046496868133545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3493671417236328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31892884203365873 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20168256759643555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34420228004455566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3188938042947224 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20951509475708008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3558475971221924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31807300959314616 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21255254745483398.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3585700988769531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3180765969412667 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20946288108825684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3632230758666992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.318161107812609 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21190905570983887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3619813919067383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31720485176358904 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20720815658569336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35011792182922363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31711476998669763 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20717501640319824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.370455265045166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3167469369513648 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062358856201172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3569486141204834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31648441425391605 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21256136894226074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35573434829711914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31627967485359737 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21653294563293457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3629589080810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31603210142680577 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21348977088928223.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36450648307800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31585786598069326 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2080364227294922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35138654708862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31565041542053224 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21014904975891113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3535282611846924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31572039638246807 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20604896545410156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.349200963973999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31561548624719893 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20456767082214355.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3476088047027588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31532845795154574 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21611261367797852.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36270737648010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3152281688792365 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20831060409545898.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35172581672668457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31530745114598957 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062242031097412.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3558638095855713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3152241493974413 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21643447875976562.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3605656623840332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31504412846905844 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21293854713439941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35577988624572754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31505441963672637 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20053362846374512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3417975902557373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31480494950498855 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21370625495910645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36801743507385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3148924154894693 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20433473587036133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36596035957336426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3146779639380319 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20077729225158691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540639877319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3148033299616405 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035961151123047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35108089447021484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3147349042551858 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20133066177368164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3421952724456787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31456291122095925 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2121896743774414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3639249801635742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3145481220313481 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20812392234802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34691381454467773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31446068925516946 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19792962074279785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33516621589660645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31445566543510983 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1960000991821289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3502194881439209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31431132810456414 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20570707321166992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3414762020111084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3143307306936809 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20200657844543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33937644958496094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141969497714724 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20555543899536133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3622307777404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3142019484724317 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20358657836914062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34440112113952637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3142206941332136 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19969534873962402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34653449058532715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3141814819404057 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20993900299072266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35243725776672363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142264412982123 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027742862701416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34336256980895996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140718438795635 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20159292221069336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345916748046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31413211226463317 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22091126441955566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3667268753051758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141034607376371 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20806503295898438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35260462760925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31398954944951196 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21431803703308105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44681406021118164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139918782881328 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23627591133117676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46298766136169434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139654913118907 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3260798454284668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5879061222076416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31392675169876644 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3424794673919678.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5835340023040771\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138937592506409 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.4504823684692383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.888096809387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3138663675103869 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33484482765197754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5816512107849121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3138715595006943 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3567161560058594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6200251579284668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138848355838231 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.37772536277770996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6428344249725342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138589556728091 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.336273193359375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5862383842468262\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138224908283779 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.47223997116088867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7348380088806152\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138150487627302 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.38517117500305176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6382315158843994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31381105269704546 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3385934829711914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5773732662200928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31378729726587024 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3629288673400879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7455539703369141\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31373652602945057 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6285316944122314.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7683990001678467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137557830129351 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20233440399169922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3407108783721924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31375434058053153 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20940232276916504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3568577766418457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137074568441936 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21570396423339844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356581449508667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137494764157704 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20130157470703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452911376953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31372821714196886 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20378756523132324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34729433059692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136843613215855 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148439884185791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534538745880127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31368288482938494 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999979019165039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3410525321960449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137350359133312 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20749878883361816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3557274341583252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31365591543061394 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21052980422973633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35445618629455566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136698152337756 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20923566818237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3591017723083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136640876531601 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21321725845336914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3592519760131836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136324294975826 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126760482788086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3529984951019287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136856836932046 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21236920356750488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36467456817626953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31363884678908754 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20702457427978516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36902761459350586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31365732082298825 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21211791038513184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35982203483581543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136268241064889 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21744513511657715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3635437488555908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31356885646070753 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20548605918884277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35344696044921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135640182665416 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21022748947143555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35221147537231445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31358040273189547 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20933079719543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583853244781494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135740752731051 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20996904373168945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36223340034484863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135654887982777 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20236825942993164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458077907562256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358528009482795 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20217370986938477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454763889312744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31356544920376367 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21956777572631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36428093910217285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357322122369496 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20699286460876465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35012388229370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.313519338624818 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20735383033752441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35378575325012207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135283214705331 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21570825576782227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3542661666870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31355383481298177 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19612741470336914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343951940536499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135189848286765 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2009899616241455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3396260738372803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135104975530079 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21109414100646973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34801626205444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135367240224566 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20651459693908691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486475944519043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135126556668963 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20294833183288574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35124969482421875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135124547140939 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2180933952331543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3719644546508789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31351854119982037 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2165052890777588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3639554977416992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31346900590828486 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21082067489624023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35556817054748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135004350117275 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21056747436523438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3519167900085449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348105796745845 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20818781852722168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583414554595947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31348408673490796 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2121570110321045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36136913299560547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31347402163914273 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20301413536071777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34663987159729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347078042370935 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19914913177490234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3383982181549072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134672786508288 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20269107818603516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521728515625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134617503200259 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21008086204528809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35839056968688965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134784417493003 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055826187133789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3463468551635742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134854704141617 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21975469589233398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3694641590118408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134723335504532 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2224116325378418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36847758293151855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134677290916443 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22226476669311523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3660118579864502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31344154008797237 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21610331535339355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37310147285461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344640510422844 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21041417121887207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3636796474456787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31347558540957315 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20001697540283203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3387265205383301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343279693807874 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21903657913208008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3595438003540039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134268773453576 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19643855094909668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3419818878173828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134491558585848 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982133388519287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33890414237976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342889964580534 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2135777473449707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3665885925292969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134329446724483 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7246 | test accuracy: 0.455\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7023 | test accuracy: 0.465\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6682 | test accuracy: 0.438\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6974 | test accuracy: 0.492\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6467 | test accuracy: 0.461\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6427 | test accuracy: 0.441\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6741 | test accuracy: 0.552\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6812 | test accuracy: 0.471\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6169 | test accuracy: 0.519\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6288 | test accuracy: 0.444\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6891 | test accuracy: 0.481\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6655 | test accuracy: 0.478\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6559 | test accuracy: 0.481\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5136 | test accuracy: 0.542\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6994 | test accuracy: 0.434\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6749 | test accuracy: 0.471\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7091 | test accuracy: 0.481\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6647 | test accuracy: 0.478\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7234 | test accuracy: 0.495\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7227 | test accuracy: 0.529\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7368 | test accuracy: 0.498\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6243 | test accuracy: 0.478\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6871 | test accuracy: 0.556\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7093 | test accuracy: 0.471\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7407 | test accuracy: 0.441\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6862 | test accuracy: 0.478\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7874 | test accuracy: 0.515\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6762 | test accuracy: 0.488\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7317 | test accuracy: 0.505\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7164 | test accuracy: 0.481\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6639 | test accuracy: 0.529\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7448 | test accuracy: 0.508\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6599 | test accuracy: 0.481\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7304 | test accuracy: 0.468\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6749 | test accuracy: 0.468\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6981 | test accuracy: 0.458\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7307 | test accuracy: 0.471\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7184 | test accuracy: 0.465\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7125 | test accuracy: 0.468\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7202 | test accuracy: 0.465\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7204 | test accuracy: 0.465\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6576 | test accuracy: 0.471\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7270 | test accuracy: 0.471\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7404 | test accuracy: 0.468\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6980 | test accuracy: 0.481\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6575 | test accuracy: 0.478\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6661 | test accuracy: 0.471\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6690 | test accuracy: 0.461\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6590 | test accuracy: 0.481\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7402 | test accuracy: 0.465\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6346 | test accuracy: 0.465\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6968 | test accuracy: 0.485\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7004 | test accuracy: 0.471\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6861 | test accuracy: 0.475\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7070 | test accuracy: 0.481\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6375 | test accuracy: 0.458\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7035 | test accuracy: 0.458\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6704 | test accuracy: 0.475\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7500 | test accuracy: 0.458\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6788 | test accuracy: 0.481\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6692 | test accuracy: 0.478\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6506 | test accuracy: 0.481\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6925 | test accuracy: 0.461\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6578 | test accuracy: 0.465\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6986 | test accuracy: 0.475\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6387 | test accuracy: 0.475\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6326 | test accuracy: 0.481\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6857 | test accuracy: 0.481\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7161 | test accuracy: 0.485\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6674 | test accuracy: 0.468\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7583 | test accuracy: 0.481\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6747 | test accuracy: 0.481\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7103 | test accuracy: 0.478\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6477 | test accuracy: 0.485\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6714 | test accuracy: 0.475\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6868 | test accuracy: 0.471\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6403 | test accuracy: 0.478\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7507 | test accuracy: 0.478\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6588 | test accuracy: 0.461\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7304 | test accuracy: 0.465\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6668 | test accuracy: 0.485\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6455 | test accuracy: 0.485\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6730 | test accuracy: 0.478\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6174 | test accuracy: 0.478\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7406 | test accuracy: 0.471\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6465 | test accuracy: 0.485\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7297 | test accuracy: 0.485\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7054 | test accuracy: 0.485\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7327 | test accuracy: 0.481\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7167 | test accuracy: 0.475\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6223 | test accuracy: 0.478\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6311 | test accuracy: 0.471\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7100 | test accuracy: 0.461\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6519 | test accuracy: 0.465\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6296 | test accuracy: 0.488\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6904 | test accuracy: 0.471\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6724 | test accuracy: 0.488\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7045 | test accuracy: 0.492\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6600 | test accuracy: 0.478\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6756 | test accuracy: 0.488\n",
            "total time:  78.00321116400005\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7561 | test accuracy: 0.438\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6651 | test accuracy: 0.485\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6905 | test accuracy: 0.418\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6753 | test accuracy: 0.502\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6486 | test accuracy: 0.444\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6007 | test accuracy: 0.448\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6573 | test accuracy: 0.542\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6716 | test accuracy: 0.492\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6275 | test accuracy: 0.512\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6285 | test accuracy: 0.448\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6862 | test accuracy: 0.485\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6599 | test accuracy: 0.468\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6553 | test accuracy: 0.485\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5210 | test accuracy: 0.535\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6801 | test accuracy: 0.438\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6731 | test accuracy: 0.475\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6908 | test accuracy: 0.488\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6711 | test accuracy: 0.471\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7126 | test accuracy: 0.485\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7276 | test accuracy: 0.522\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7177 | test accuracy: 0.512\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6090 | test accuracy: 0.492\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6855 | test accuracy: 0.559\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7142 | test accuracy: 0.475\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7466 | test accuracy: 0.441\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6790 | test accuracy: 0.475\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7924 | test accuracy: 0.498\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6744 | test accuracy: 0.488\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7418 | test accuracy: 0.505\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6954 | test accuracy: 0.471\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6538 | test accuracy: 0.539\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7413 | test accuracy: 0.502\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6557 | test accuracy: 0.478\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7229 | test accuracy: 0.471\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6896 | test accuracy: 0.478\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6981 | test accuracy: 0.478\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7208 | test accuracy: 0.478\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7128 | test accuracy: 0.478\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6982 | test accuracy: 0.475\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7168 | test accuracy: 0.481\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7215 | test accuracy: 0.478\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6608 | test accuracy: 0.485\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7232 | test accuracy: 0.481\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7318 | test accuracy: 0.481\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7064 | test accuracy: 0.461\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6582 | test accuracy: 0.471\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6557 | test accuracy: 0.478\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6655 | test accuracy: 0.485\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6648 | test accuracy: 0.468\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7259 | test accuracy: 0.478\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6360 | test accuracy: 0.475\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6855 | test accuracy: 0.468\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7102 | test accuracy: 0.478\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6907 | test accuracy: 0.471\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7169 | test accuracy: 0.468\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6479 | test accuracy: 0.485\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7147 | test accuracy: 0.485\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6682 | test accuracy: 0.478\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7482 | test accuracy: 0.485\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6834 | test accuracy: 0.468\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6767 | test accuracy: 0.465\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6621 | test accuracy: 0.478\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6990 | test accuracy: 0.481\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6602 | test accuracy: 0.481\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7094 | test accuracy: 0.478\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6490 | test accuracy: 0.468\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6264 | test accuracy: 0.465\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6774 | test accuracy: 0.471\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7234 | test accuracy: 0.471\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6883 | test accuracy: 0.481\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7503 | test accuracy: 0.468\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6828 | test accuracy: 0.471\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7036 | test accuracy: 0.468\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6664 | test accuracy: 0.478\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6819 | test accuracy: 0.475\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6926 | test accuracy: 0.471\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6424 | test accuracy: 0.468\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7549 | test accuracy: 0.475\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6705 | test accuracy: 0.478\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7276 | test accuracy: 0.478\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6706 | test accuracy: 0.468\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6481 | test accuracy: 0.468\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6682 | test accuracy: 0.471\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6245 | test accuracy: 0.471\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7464 | test accuracy: 0.471\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6535 | test accuracy: 0.475\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7452 | test accuracy: 0.475\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7049 | test accuracy: 0.471\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7266 | test accuracy: 0.475\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7193 | test accuracy: 0.475\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6391 | test accuracy: 0.475\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6389 | test accuracy: 0.468\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6908 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6554 | test accuracy: 0.475\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6388 | test accuracy: 0.471\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6888 | test accuracy: 0.471\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6765 | test accuracy: 0.475\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7050 | test accuracy: 0.471\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6524 | test accuracy: 0.471\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6824 | test accuracy: 0.478\n",
            "total time:  74.57238163800002\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2108902931213379.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.36319470405578613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6887601980141231 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22043800354003906.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.36931633949279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5425699097769601 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20066142082214355.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.3398113250732422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46235226009573255 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21061921119689941.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.357623815536499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4157077469996044 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20855236053466797.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3480074405670166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39300224568162645 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20574569702148438.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.35286450386047363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37460472072873796 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20083284378051758.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.34609508514404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3606276886803763 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21568775177001953.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3566102981567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35333350130489893 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2011394500732422.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3430306911468506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3474404547895704 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2121725082397461.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.36177945137023926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3431451984814235 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21371245384216309.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3544018268585205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33836999109813143 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20613408088684082.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35253310203552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3349118364708764 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2118525505065918.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3516817092895508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3327053240367344 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.214219331741333.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35566186904907227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.33009563301290784 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22067689895629883.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3642892837524414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32812815862042566 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682480812072754.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43001842498779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32677415311336516 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2600541114807129.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.5218086242675781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3256895588976996 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3701159954071045.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.643395185470581\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3248368229184832 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.32535266876220703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.7439072132110596\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3231647346700941 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.5802278518676758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.8432614803314209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3224976475749697 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3444099426269531.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.6179251670837402\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32191756239959174 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3407280445098877.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5706708431243896\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3210741141012737 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36190176010131836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.6313071250915527\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3200891635247639 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36925244331359863.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.6397719383239746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3197974724428994 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.39434218406677246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.664146900177002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3194319891078132 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.39277052879333496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.6434941291809082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31890155630452294 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3418247699737549.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5811011791229248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31853347378117697 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6317191123962402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 1.0590310096740723\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3180016130208969 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21379971504211426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3613312244415283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3179066892181124 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20764684677124023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34804391860961914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3175635874271393 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20702195167541504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36574530601501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31730759399277825 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21738409996032715.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36159253120422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31687466204166415 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20773696899414062.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35398316383361816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3165895457778658 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23157072067260742.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3951847553253174\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3165069316114698 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21401286125183105.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.365527868270874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3160116059439523 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22322821617126465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3695085048675537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31635696717670986 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22860002517700195.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3789350986480713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31623865365982057 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21591830253601074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35684633255004883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31580924221447537 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20264101028442383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.357480525970459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3155582594020026 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21927404403686523.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3686177730560303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31549747799124034 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2125256061553955.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35732054710388184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3154419200760978 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20524072647094727.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3527400493621826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31535216782774245 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2031247615814209.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35461926460266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3152136764356068 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20635223388671875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3447606563568115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31500027222292765 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20769524574279785.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36626124382019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31508249597890037 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21870040893554688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36398911476135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149834909609386 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20470762252807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34586310386657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31489143329007285 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21626520156860352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3660547733306885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3148916448865618 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20501708984375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3561818599700928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3146184802055359 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1975250244140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3443751335144043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3146864312035697 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035337448120117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3619687557220459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31467236280441285 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2112870216369629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3505673408508301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31453877389431 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2002243995666504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34612607955932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31469486228057314 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21858739852905273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3644216060638428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31443873729024613 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21283888816833496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35356736183166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3144470423460007 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19847321510314941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478817939758301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31434106741632734 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21243023872375488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3587672710418701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3145055736814226 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20528912544250488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35840535163879395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3143811796392713 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21112394332885742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35611391067504883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142727464437485 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2295072078704834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3914456367492676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31418844759464265 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21326518058776855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3544178009033203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31416024608271464 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22028636932373047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3967571258544922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141633804355349 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21961212158203125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36574435234069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141201364142554 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21497869491577148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3735485076904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.314097854920796 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21855878829956055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.363156795501709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31406732755047934 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.220473051071167.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.371854305267334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31398160670484815 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2113962173461914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3547675609588623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139714334692274 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22754836082458496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38242340087890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.313983548113278 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039186954498291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34491705894470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31403854957648686 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20788264274597168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36037588119506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31389098295143675 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143101692199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35700464248657227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138925475733621 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2084038257598877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35068297386169434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31392139451844353 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21251416206359863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37241125106811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31391154101916724 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21300053596496582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3572232723236084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138241448572704 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.200486421585083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475148677825928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138524476970945 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066178321838379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356795072555542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137798509427479 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21184778213500977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35082125663757324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31383044975144525 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20104217529296875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34213757514953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31378445497580937 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2118089199066162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.370591402053833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3138297723872321 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20431733131408691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345365047454834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31383395961352756 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20829463005065918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3656618595123291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137136540242604 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22228431701660156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36975932121276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31378610815320696 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20833897590637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483316898345947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137502798012325 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20886921882629395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565354347229004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137070459978921 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21238088607788086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3590199947357178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137244667325701 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20589280128479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3476111888885498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3137080413954599 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.209869384765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3580050468444824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31366672345570157 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2151172161102295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3654215335845947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31366812331335886 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20153141021728516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34383726119995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136776498385838 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21057510375976562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3677198886871338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31368700436183383 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27522969245910645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45736145973205566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136267410857337 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36002254486083984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6061673164367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136507030044283 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3296501636505127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5824308395385742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136305230004447 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3302958011627197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5936436653137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31369401812553405 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34441304206848145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6188418865203857\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136207567793982 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3581817150115967.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6105096340179443\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31358974277973173 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36148715019226074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6080222129821777\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.313565508382661 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3608741760253906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6209022998809814\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135644146374294 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.37436437606811523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6238458156585693\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31358457931450434 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3331582546234131.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7953143119812012\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135567537375859 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.508446455001831.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7592782974243164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135675655944007 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3474283218383789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48979687690734863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135490779365812 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20063233375549316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34085679054260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354563789708273 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2089552879333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36335039138793945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31356106272765566 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2212662696838379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3682575225830078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.313588615826198 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2212209701538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37950825691223145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31355018062250956 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22089195251464844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3684830665588379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31354437725884576 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21421337127685547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3586115837097168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135256805590221 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21837544441223145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36571717262268066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.313524101461683 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20691728591918945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37643909454345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135536415236337 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20493721961975098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34699058532714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31349184044769834 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22974872589111328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3804299831390381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351562312671116 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2121748924255371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3623781204223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31349391085760936 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20921850204467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3611867427825928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135096217904772 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21657466888427734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585019111633301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31348400924886977 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21826386451721191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3579235076904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31351278424263 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090129852294922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521857261657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134972448859896 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21225857734680176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3541276454925537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134983701365335 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21321606636047363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3633134365081787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.313472946201052 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2105998992919922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508129119873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.313490891456604 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20235013961791992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3414764404296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134680667093822 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21281075477600098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3617980480194092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134647795132228 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21398258209228516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36568593978881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31345885140555246 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21290969848632812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549225330352783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31347865164279937 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21572065353393555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3709261417388916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134739748069218 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20453143119812012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484823703765869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134561470576695 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20482993125915527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3522374629974365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31345200751508984 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21499276161193848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36371755599975586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134554501090731 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7469 | test accuracy: 0.539\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6692 | test accuracy: 0.522\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6236 | test accuracy: 0.525\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6653 | test accuracy: 0.481\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7035 | test accuracy: 0.488\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6902 | test accuracy: 0.525\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5876 | test accuracy: 0.441\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5480 | test accuracy: 0.438\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7642 | test accuracy: 0.434\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6830 | test accuracy: 0.461\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6899 | test accuracy: 0.481\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6679 | test accuracy: 0.545\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6742 | test accuracy: 0.505\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6999 | test accuracy: 0.492\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6337 | test accuracy: 0.549\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7094 | test accuracy: 0.492\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5520 | test accuracy: 0.542\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6279 | test accuracy: 0.515\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7729 | test accuracy: 0.434\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7064 | test accuracy: 0.505\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7206 | test accuracy: 0.505\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6387 | test accuracy: 0.559\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6456 | test accuracy: 0.492\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7238 | test accuracy: 0.444\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6643 | test accuracy: 0.451\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6890 | test accuracy: 0.481\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6744 | test accuracy: 0.441\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7288 | test accuracy: 0.498\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7456 | test accuracy: 0.488\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6840 | test accuracy: 0.468\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.8458 | test accuracy: 0.505\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6875 | test accuracy: 0.502\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7300 | test accuracy: 0.498\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6745 | test accuracy: 0.495\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7034 | test accuracy: 0.492\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6741 | test accuracy: 0.488\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6999 | test accuracy: 0.481\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7065 | test accuracy: 0.498\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7109 | test accuracy: 0.498\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6277 | test accuracy: 0.498\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6606 | test accuracy: 0.492\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7274 | test accuracy: 0.498\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7280 | test accuracy: 0.492\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7128 | test accuracy: 0.488\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6535 | test accuracy: 0.478\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7325 | test accuracy: 0.481\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6711 | test accuracy: 0.498\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7238 | test accuracy: 0.485\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6820 | test accuracy: 0.485\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7132 | test accuracy: 0.478\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6529 | test accuracy: 0.481\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6939 | test accuracy: 0.495\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6618 | test accuracy: 0.498\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6828 | test accuracy: 0.488\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6634 | test accuracy: 0.492\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7140 | test accuracy: 0.481\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6520 | test accuracy: 0.488\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6370 | test accuracy: 0.485\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7023 | test accuracy: 0.488\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7074 | test accuracy: 0.478\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7384 | test accuracy: 0.488\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7629 | test accuracy: 0.488\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6665 | test accuracy: 0.492\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6777 | test accuracy: 0.488\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6802 | test accuracy: 0.485\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6773 | test accuracy: 0.481\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5822 | test accuracy: 0.481\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6709 | test accuracy: 0.481\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6658 | test accuracy: 0.485\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7056 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6798 | test accuracy: 0.478\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6528 | test accuracy: 0.488\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7088 | test accuracy: 0.492\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6456 | test accuracy: 0.478\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6664 | test accuracy: 0.488\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6408 | test accuracy: 0.485\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6691 | test accuracy: 0.488\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7309 | test accuracy: 0.488\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6288 | test accuracy: 0.478\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6484 | test accuracy: 0.488\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6285 | test accuracy: 0.492\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6901 | test accuracy: 0.488\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6213 | test accuracy: 0.485\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6961 | test accuracy: 0.478\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6312 | test accuracy: 0.485\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7025 | test accuracy: 0.481\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6613 | test accuracy: 0.485\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6991 | test accuracy: 0.481\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7307 | test accuracy: 0.492\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7102 | test accuracy: 0.485\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6619 | test accuracy: 0.475\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6976 | test accuracy: 0.475\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6753 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6648 | test accuracy: 0.488\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6453 | test accuracy: 0.481\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6108 | test accuracy: 0.485\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7419 | test accuracy: 0.475\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7552 | test accuracy: 0.488\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6972 | test accuracy: 0.488\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6879 | test accuracy: 0.478\n",
            "total time:  74.33727394899984\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7398 | test accuracy: 0.498\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6797 | test accuracy: 0.505\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6304 | test accuracy: 0.478\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6729 | test accuracy: 0.485\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7289 | test accuracy: 0.468\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7029 | test accuracy: 0.481\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5803 | test accuracy: 0.438\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5520 | test accuracy: 0.434\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7614 | test accuracy: 0.438\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6852 | test accuracy: 0.451\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6929 | test accuracy: 0.475\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6943 | test accuracy: 0.535\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6832 | test accuracy: 0.471\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6822 | test accuracy: 0.481\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6228 | test accuracy: 0.519\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6897 | test accuracy: 0.475\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5589 | test accuracy: 0.519\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6371 | test accuracy: 0.475\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7701 | test accuracy: 0.438\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7008 | test accuracy: 0.478\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7538 | test accuracy: 0.471\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6473 | test accuracy: 0.552\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6453 | test accuracy: 0.478\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7278 | test accuracy: 0.431\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6567 | test accuracy: 0.444\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6906 | test accuracy: 0.468\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6857 | test accuracy: 0.451\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7347 | test accuracy: 0.488\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7352 | test accuracy: 0.468\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6721 | test accuracy: 0.478\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.8721 | test accuracy: 0.488\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6796 | test accuracy: 0.465\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7321 | test accuracy: 0.455\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6912 | test accuracy: 0.465\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7144 | test accuracy: 0.465\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6549 | test accuracy: 0.461\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6820 | test accuracy: 0.468\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7045 | test accuracy: 0.455\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6960 | test accuracy: 0.461\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6210 | test accuracy: 0.461\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6365 | test accuracy: 0.468\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7393 | test accuracy: 0.451\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7267 | test accuracy: 0.468\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7103 | test accuracy: 0.465\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6199 | test accuracy: 0.458\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7224 | test accuracy: 0.461\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6714 | test accuracy: 0.451\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7390 | test accuracy: 0.468\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6687 | test accuracy: 0.461\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7221 | test accuracy: 0.461\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6511 | test accuracy: 0.465\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6982 | test accuracy: 0.455\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6972 | test accuracy: 0.458\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6939 | test accuracy: 0.455\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6587 | test accuracy: 0.455\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7245 | test accuracy: 0.465\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6440 | test accuracy: 0.461\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6352 | test accuracy: 0.461\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7098 | test accuracy: 0.461\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7083 | test accuracy: 0.461\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7406 | test accuracy: 0.455\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7495 | test accuracy: 0.458\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6526 | test accuracy: 0.461\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6566 | test accuracy: 0.461\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6764 | test accuracy: 0.461\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6645 | test accuracy: 0.465\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5727 | test accuracy: 0.461\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6755 | test accuracy: 0.461\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6617 | test accuracy: 0.458\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7109 | test accuracy: 0.461\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6761 | test accuracy: 0.461\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6534 | test accuracy: 0.455\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7058 | test accuracy: 0.451\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6313 | test accuracy: 0.461\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6451 | test accuracy: 0.455\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6220 | test accuracy: 0.451\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6609 | test accuracy: 0.461\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7243 | test accuracy: 0.458\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6100 | test accuracy: 0.465\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6548 | test accuracy: 0.458\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6328 | test accuracy: 0.451\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6789 | test accuracy: 0.458\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6153 | test accuracy: 0.461\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6861 | test accuracy: 0.465\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6013 | test accuracy: 0.461\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7239 | test accuracy: 0.461\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6625 | test accuracy: 0.458\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7023 | test accuracy: 0.461\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7437 | test accuracy: 0.461\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7018 | test accuracy: 0.458\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6590 | test accuracy: 0.468\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7060 | test accuracy: 0.465\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6470 | test accuracy: 0.465\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6496 | test accuracy: 0.461\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6430 | test accuracy: 0.465\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.5900 | test accuracy: 0.471\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7388 | test accuracy: 0.465\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7474 | test accuracy: 0.461\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6910 | test accuracy: 0.455\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6830 | test accuracy: 0.461\n",
            "total time:  79.73114302199997\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19132471084594727.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.34401392936706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.4905148616858891 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21294760704040527.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.36744117736816406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.42018994987010955 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21070003509521484.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3606603145599365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.3879036511693682 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111802101135254.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.35277676582336426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.36655929642064233 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2261059284210205.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3730013370513916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.35439853285040174 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23259449005126953.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3767666816711426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34736932814121246 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20398926734924316.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34682726860046387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34024682343006135 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2120065689086914.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3609464168548584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33448230794497896 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2098829746246338.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3557436466217041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3329971194267273 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021477222442627.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3467066287994385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.32913602846009393 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21318507194519043.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35767483711242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32667757613318305 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2124953269958496.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35434794425964355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3251414558717183 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2120063304901123.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3577301502227783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3234741304601942 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2018883228302002.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34816670417785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3225502780505589 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2173619270324707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3641703128814697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3219582770551954 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067887783050537.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.356891393661499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3210514579500471 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2233583927154541.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.379256010055542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32033537796565464 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046058177947998.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3429713249206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3193554060799735 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2109830379486084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35605883598327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31893518226487294 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21060395240783691.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36480140686035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3181092619895935 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20496106147766113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.354633092880249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3178474413497107 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21139931678771973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35654473304748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3173434853553772 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22000432014465332.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36020541191101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31714741374765126 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19931769371032715.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34049344062805176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31703434075628006 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21373796463012695.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35780978202819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3168627108846392 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2189042568206787.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3640632629394531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31615338836397444 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2101285457611084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36855125427246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3160400858947209 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21404242515563965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35977745056152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31582039211477553 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2165236473083496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3557901382446289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3156049243041447 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2089078426361084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35144639015197754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3157936875309263 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.205810546875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34743213653564453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31578588528292517 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21066737174987793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3496279716491699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3152615819658552 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22265839576721191.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3865818977355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31539569667407447 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22420477867126465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3719320297241211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3151469341346196 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20356965065002441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506431579589844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31498910784721373 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21343612670898438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3619222640991211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3151096488748278 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20792174339294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.365037202835083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3148129403591156 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21643400192260742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3635988235473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3146141848393849 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2076270580291748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34702014923095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31477431356906893 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22252750396728516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4081108570098877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3147063493728638 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2449779510498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42185258865356445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3146624628986631 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.32996630668640137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6253869533538818\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3145321445805686 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.36188721656799316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6174671649932861\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31447865068912506 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34773874282836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6187779903411865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31443866193294523 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.441133975982666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7907960414886475\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3144331161464964 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.701448917388916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.1797966957092285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31432636295046124 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6769893169403076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.1058928966522217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3142722955771855 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.672368049621582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.1008248329162598\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3142021166426795 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.7149686813354492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.2719309329986572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3141980115856443 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.5241601467132568.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.3807508945465088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31414631136826104 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6190266609191895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.1265544891357422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31413799822330474 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.5881373882293701.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.9007670879364014\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3141273528337479 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33432769775390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48131561279296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31419701874256134 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20893383026123047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3557462692260742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3140793583222798 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2107398509979248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556404113769531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3139354399272374 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21309614181518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583865165710449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31396195335047583 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20253682136535645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35205817222595215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3139271974563599 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23464155197143555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3848733901977539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3139100321701595 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2160811424255371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3603990077972412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139680355787277 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20378541946411133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3701963424682617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31388860046863554 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2099134922027588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35391807556152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31389519316809517 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20559978485107422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481626510620117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138356370585305 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21753573417663574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3766613006591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138597786426544 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.209197998046875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534080982208252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138973346778325 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20911622047424316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36081790924072266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31383831330708095 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21773314476013184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3578307628631592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137976522956576 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20417118072509766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420250415802002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137840437037604 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20055174827575684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34432268142700195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31376419152532303 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21939778327941895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3785419464111328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138125368527004 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21068787574768066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35355067253112793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31371018843991416 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2327404022216797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3790311813354492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31367591066019873 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21254181861877441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35033416748046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31371705872671946 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20702385902404785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345782995223999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137011736631393 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20818877220153809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3616456985473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31369164415768214 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2158052921295166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3596658706665039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136642622096198 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21264386177062988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.359466552734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136442175933293 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22018122673034668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3773157596588135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136273115873337 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21162652969360352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3573179244995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31361016631126404 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20777344703674316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3597874641418457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136966620172773 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20578217506408691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36536192893981934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3135841075863157 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20943760871887207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3538045883178711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31364407667091915 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21383237838745117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36265063285827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136697122028896 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21651172637939453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3621039390563965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31365199855395726 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21383953094482422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549048900604248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136283427476883 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20938658714294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3675577640533447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135626835482461 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21912431716918945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3631114959716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31356274017265867 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20282769203186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34108996391296387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.313573939033917 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20751714706420898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35533690452575684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135608434677124 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21067214012145996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559298515319824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31353539526462554 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2115161418914795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3664286136627197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31352769264153074 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21331238746643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3597598075866699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135352781840733 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22266650199890137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3622889518737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31351193487644197 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2007734775543213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389618396759033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31354258741651264 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21495389938354492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3732728958129883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31351794600486754 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20246267318725586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35386157035827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31351437738963533 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21913433074951172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36788320541381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135069617203304 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21903562545776367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37972354888916016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31350878987993513 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2149815559387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3590247631072998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135005614587239 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23259496688842773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3825242519378662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31348228539739337 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22256040573120117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36690497398376465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31349254080227446 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078859806060791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496532440185547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31350165477820807 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20940351486206055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3624897003173828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134880270276751 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2236628532409668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36995625495910645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31350335563932147 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21186518669128418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36321496963500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135187327861786 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22166180610656738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3856394290924072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135037945849555 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2105712890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.436434268951416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31346672475337983 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22342300415039062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46957898139953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31345967437539785 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33754754066467285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.613837718963623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134660112006324 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.34815502166748047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5947229862213135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134630445923124 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6182971000671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.9320402145385742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134675839117595 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.31225132942199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5517573356628418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31345122584274837 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30036497116088867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5454773902893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31344092530863626 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.31865596771240234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5975959300994873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31343429556914737 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3542642593383789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6187617778778076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134278523070472 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.47281503677368164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7775185108184814\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31345515251159667 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.39625096321105957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6761050224304199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134263059922627 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3477146625518799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.6106507778167725\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134211084672383 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3753190040588379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7777254581451416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.313429012468883 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6271545886993408.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.7904212474822998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31341293922492436 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21796679496765137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36638593673706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31344211655003684 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2172400951385498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3693423271179199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343407928943634 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21486949920654297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3639054298400879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134131895644324 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22422552108764648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37847423553466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31342524715832304 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.211411714553833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554494380950928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134013716663633 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20809507369995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.347625732421875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313414762701307 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20619750022888184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35233187675476074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134211527449744 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20128250122070312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34742212295532227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134035983255931 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042539119720459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34278345108032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3133963223014559 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6178 | test accuracy: 0.512\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8597 | test accuracy: 0.481\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6949 | test accuracy: 0.478\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7550 | test accuracy: 0.502\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6471 | test accuracy: 0.465\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7064 | test accuracy: 0.495\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6876 | test accuracy: 0.522\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6818 | test accuracy: 0.498\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7517 | test accuracy: 0.475\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6932 | test accuracy: 0.515\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7529 | test accuracy: 0.502\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7192 | test accuracy: 0.508\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7977 | test accuracy: 0.502\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7452 | test accuracy: 0.522\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6878 | test accuracy: 0.512\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8008 | test accuracy: 0.471\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6593 | test accuracy: 0.478\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6483 | test accuracy: 0.502\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7972 | test accuracy: 0.522\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6841 | test accuracy: 0.522\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7207 | test accuracy: 0.519\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6573 | test accuracy: 0.502\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7748 | test accuracy: 0.519\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7539 | test accuracy: 0.481\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7447 | test accuracy: 0.481\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6918 | test accuracy: 0.525\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6663 | test accuracy: 0.505\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6740 | test accuracy: 0.478\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7687 | test accuracy: 0.488\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7456 | test accuracy: 0.505\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7056 | test accuracy: 0.508\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6840 | test accuracy: 0.515\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6868 | test accuracy: 0.505\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6649 | test accuracy: 0.502\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7165 | test accuracy: 0.502\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7376 | test accuracy: 0.502\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6299 | test accuracy: 0.502\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7309 | test accuracy: 0.505\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6688 | test accuracy: 0.505\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7355 | test accuracy: 0.505\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6814 | test accuracy: 0.502\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6378 | test accuracy: 0.485\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7168 | test accuracy: 0.498\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6281 | test accuracy: 0.495\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6859 | test accuracy: 0.505\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7361 | test accuracy: 0.495\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6859 | test accuracy: 0.498\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6827 | test accuracy: 0.498\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6224 | test accuracy: 0.495\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6343 | test accuracy: 0.488\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7057 | test accuracy: 0.498\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6601 | test accuracy: 0.488\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7602 | test accuracy: 0.498\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6510 | test accuracy: 0.488\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7109 | test accuracy: 0.485\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.5935 | test accuracy: 0.485\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6628 | test accuracy: 0.492\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7550 | test accuracy: 0.488\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6929 | test accuracy: 0.485\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6449 | test accuracy: 0.488\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6789 | test accuracy: 0.492\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7071 | test accuracy: 0.485\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6919 | test accuracy: 0.485\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6829 | test accuracy: 0.485\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6935 | test accuracy: 0.488\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7293 | test accuracy: 0.485\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7389 | test accuracy: 0.488\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6907 | test accuracy: 0.488\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6201 | test accuracy: 0.502\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6840 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6591 | test accuracy: 0.492\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7104 | test accuracy: 0.488\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7178 | test accuracy: 0.488\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6577 | test accuracy: 0.492\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7465 | test accuracy: 0.485\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7201 | test accuracy: 0.488\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7311 | test accuracy: 0.498\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6709 | test accuracy: 0.488\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6725 | test accuracy: 0.488\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7108 | test accuracy: 0.488\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6135 | test accuracy: 0.488\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7079 | test accuracy: 0.488\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7446 | test accuracy: 0.488\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6261 | test accuracy: 0.488\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7418 | test accuracy: 0.492\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6230 | test accuracy: 0.492\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6475 | test accuracy: 0.492\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6517 | test accuracy: 0.495\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6521 | test accuracy: 0.488\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6835 | test accuracy: 0.502\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6586 | test accuracy: 0.488\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7087 | test accuracy: 0.488\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7630 | test accuracy: 0.481\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6758 | test accuracy: 0.492\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6855 | test accuracy: 0.492\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6967 | test accuracy: 0.498\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6952 | test accuracy: 0.498\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6220 | test accuracy: 0.488\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6687 | test accuracy: 0.492\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6929 | test accuracy: 0.492\n",
            "total time:  82.5280269079999\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5798 | test accuracy: 0.522\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8800 | test accuracy: 0.492\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6661 | test accuracy: 0.485\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7643 | test accuracy: 0.502\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6946 | test accuracy: 0.492\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6968 | test accuracy: 0.498\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6902 | test accuracy: 0.515\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6860 | test accuracy: 0.519\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7401 | test accuracy: 0.465\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7013 | test accuracy: 0.512\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7678 | test accuracy: 0.515\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7161 | test accuracy: 0.529\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7794 | test accuracy: 0.515\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7508 | test accuracy: 0.522\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6713 | test accuracy: 0.535\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8031 | test accuracy: 0.468\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6642 | test accuracy: 0.495\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6406 | test accuracy: 0.495\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7982 | test accuracy: 0.515\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6698 | test accuracy: 0.522\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7255 | test accuracy: 0.512\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6566 | test accuracy: 0.508\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7731 | test accuracy: 0.508\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7533 | test accuracy: 0.468\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7418 | test accuracy: 0.485\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6972 | test accuracy: 0.529\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6719 | test accuracy: 0.492\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6654 | test accuracy: 0.481\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7657 | test accuracy: 0.508\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7330 | test accuracy: 0.502\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7077 | test accuracy: 0.515\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6822 | test accuracy: 0.505\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6910 | test accuracy: 0.505\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6625 | test accuracy: 0.502\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7075 | test accuracy: 0.502\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7298 | test accuracy: 0.498\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6289 | test accuracy: 0.502\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7276 | test accuracy: 0.498\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6600 | test accuracy: 0.488\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7335 | test accuracy: 0.492\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6821 | test accuracy: 0.495\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6323 | test accuracy: 0.488\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7290 | test accuracy: 0.498\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6349 | test accuracy: 0.495\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6963 | test accuracy: 0.495\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7303 | test accuracy: 0.495\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6941 | test accuracy: 0.495\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6822 | test accuracy: 0.492\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6201 | test accuracy: 0.495\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6180 | test accuracy: 0.495\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7013 | test accuracy: 0.505\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6501 | test accuracy: 0.495\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7624 | test accuracy: 0.488\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6473 | test accuracy: 0.488\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7095 | test accuracy: 0.488\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.5885 | test accuracy: 0.492\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6727 | test accuracy: 0.495\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7582 | test accuracy: 0.495\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6981 | test accuracy: 0.492\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6457 | test accuracy: 0.492\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6727 | test accuracy: 0.492\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7191 | test accuracy: 0.495\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6857 | test accuracy: 0.488\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6894 | test accuracy: 0.495\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7048 | test accuracy: 0.498\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7450 | test accuracy: 0.498\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7289 | test accuracy: 0.492\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6980 | test accuracy: 0.492\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6185 | test accuracy: 0.495\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6976 | test accuracy: 0.495\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6463 | test accuracy: 0.498\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7226 | test accuracy: 0.495\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7272 | test accuracy: 0.492\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6473 | test accuracy: 0.492\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7514 | test accuracy: 0.495\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7218 | test accuracy: 0.488\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7487 | test accuracy: 0.498\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6639 | test accuracy: 0.492\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6769 | test accuracy: 0.498\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6976 | test accuracy: 0.488\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6030 | test accuracy: 0.495\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7018 | test accuracy: 0.495\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7438 | test accuracy: 0.488\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6308 | test accuracy: 0.492\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7499 | test accuracy: 0.498\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6207 | test accuracy: 0.495\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6334 | test accuracy: 0.502\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6537 | test accuracy: 0.495\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6528 | test accuracy: 0.492\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6838 | test accuracy: 0.492\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6521 | test accuracy: 0.492\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7039 | test accuracy: 0.495\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7560 | test accuracy: 0.495\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6784 | test accuracy: 0.492\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6903 | test accuracy: 0.495\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7002 | test accuracy: 0.492\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6928 | test accuracy: 0.492\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6123 | test accuracy: 0.488\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6647 | test accuracy: 0.492\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7144 | test accuracy: 0.495\n",
            "total time:  75.22034945999985\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19895362854003906.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.34296369552612305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5673210948705674 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21364188194274902.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.35431957244873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.46741397721426825 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2087099552154541.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.35788655281066895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.41262280259813583 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2072007656097412.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.35450077056884766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.38569012241704126 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21874666213989258.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.36224842071533203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3684024444648198 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067263126373291.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3522830009460449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3558538177183696 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21274328231811523.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.357227087020874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3468461607183729 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20899677276611328.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3517611026763916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34268116099493845 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20796513557434082.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3556532859802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33706350454262324 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20428943634033203.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34587883949279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3335017327751432 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21050643920898438.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35785675048828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3315768616540091 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20540833473205566.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.346055269241333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32765556488718306 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20760083198547363.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35891127586364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32661646817411694 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20690202713012695.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34867024421691895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32535465402262553 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20487737655639648.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3474001884460449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3236070100750242 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20285820960998535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35141825675964355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32324197547776357 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19906163215637207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3472158908843994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32228338633264814 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20358610153198242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34873390197753906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32099823355674745 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19620990753173828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3448464870452881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32020818676267354 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2116076946258545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3523411750793457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31947637498378756 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2074439525604248.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35141563415527344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3190684275967734 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20282721519470215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35204410552978516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3185500936848777 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20958805084228516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35325074195861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31803133274827683 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20572328567504883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34674835205078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31759779410702843 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2086341381072998.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34842467308044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3172682902642659 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20923089981079102.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35146117210388184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31732407680579594 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20107817649841309.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33722591400146484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3166351412023817 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2100820541381836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35111570358276367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31630674387727464 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111375331878662.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3546133041381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3163278358323233 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19841670989990234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3396873474121094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3160083596195493 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21270418167114258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36588311195373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3158689154045922 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062840461730957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3482177257537842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31575001052447726 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21101045608520508.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34979891777038574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31584794223308565 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20507168769836426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3449077606201172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3154160193034581 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20407533645629883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3478264808654785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3152666415487017 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19677305221557617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3414602279663086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31525074924741475 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2187821865081787.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.37140607833862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3151512375899724 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21460390090942383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3572084903717041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3152091703244618 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20630240440368652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35431408882141113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3147729000874928 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2209489345550537.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.37023186683654785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31501116454601286 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22078847885131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3664555549621582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31498496276991705 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20947599411010742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36224842071533203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31489433561052593 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035552024841309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3587534427642822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31469048176492964 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21112608909606934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554067611694336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31462178868906837 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2133350372314453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.371690034866333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3146134035927909 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20947766304016113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36028409004211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3145207613706589 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20717716217041016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461778163909912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3144512776817594 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21565842628479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36035633087158203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3143175423145294 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046797275543213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501594066619873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31425849710192 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1985468864440918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3441348075866699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31431189221995215 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22089099884033203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3781740665435791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31425667149680003 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.219343900680542.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36328577995300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3143341405051095 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20612645149230957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454101085662842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3142243653535843 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21190166473388672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565948009490967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31410111359187537 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20412898063659668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34912610054016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31418892443180085 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20006847381591797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34871435165405273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3141506246158055 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21719908714294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3634500503540039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140247677053724 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21350908279418945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35519909858703613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31404473653861453 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19806361198425293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461456298828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31401938157422205 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2052919864654541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349959135055542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31396500510828834 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20943021774291992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3551359176635742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3139741203614644 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071990966796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36753344535827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138929856675012 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20828747749328613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35296106338500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138825476169586 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20961785316467285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34996747970581055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138554798705237 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21288275718688965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3602004051208496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138207869870322 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20936274528503418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478665351867676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138523131608963 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21362686157226562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540515899658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31388462952205115 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21822643280029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37142038345336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137927268232618 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21645450592041016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36062002182006836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138263783284596 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010054588317871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33968210220336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31376666639532363 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20978116989135742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3513474464416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31378837185246605 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20174813270568848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3414130210876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31371820739337375 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20712065696716309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36153411865234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31379723378590174 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21578741073608398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3618621826171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138108892100198 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20945239067077637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.353132963180542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31373265598501476 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972637176513672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3366203308105469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136619512523924 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21344375610351562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3599412441253662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136740837778364 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19587326049804688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3374173641204834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137269743851253 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20001220703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34248781204223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136634111404419 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21354436874389648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35723257064819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31366186738014223 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20130634307861328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34441065788269043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136629624026162 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20659446716308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3492159843444824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136196030037744 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21213626861572266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498649597167969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136645087174007 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20483875274658203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34839367866516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31365224208150594 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20511555671691895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36068272590637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31365268911634175 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.209608793258667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35190725326538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31360615406717574 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021009922027588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34622979164123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136052885225841 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21238470077514648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36532092094421387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31366040451186045 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20803618431091309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34756922721862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31358408502170015 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2123706340789795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3648102283477783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31359533710139137 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21257376670837402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3687467575073242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31357143436159407 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20747923851013184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35293030738830566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135652124881744 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21184325218200684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35881924629211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135439544916153 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20285344123840332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34282517433166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31353375911712644 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20657110214233398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3434896469116211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31354070561272757 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19447636604309082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34329938888549805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31358205803803035 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20499205589294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3513491153717041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31352238016469136 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2013697624206543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420093059539795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135715901851654 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20573163032531738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35056161880493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135402428252356 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21328425407409668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3544597625732422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31357605840478625 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20046424865722656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34391212463378906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31354181000164577 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20198297500610352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470954895019531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135495173079627 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20894789695739746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34852075576782227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3134827656405313 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20102596282958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33846235275268555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31351872980594636 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20439910888671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449678421020508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135324516466686 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20608210563659668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34565019607543945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134765020438603 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19988703727722168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34599947929382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31351989550249915 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21717524528503418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3607673645019531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.313487726024219 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21246814727783203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35559844970703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31352617910930086 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20182013511657715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3468358516693115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31351211539336615 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19734406471252441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34018802642822266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.313510856458119 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090318202972412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35035276412963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31347273417881555 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21234917640686035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35946035385131836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134426423481533 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20356416702270508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35509777069091797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31346116406576974 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2007582187652588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33895325660705566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134476155042648 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.202040433883667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34726548194885254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31346866701330456 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19784879684448242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34704017639160156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31345731786319186 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20200586318969727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34452366828918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31344393874917714 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21466541290283203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3605306148529053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134542192731585 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20387625694274902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3557436466217041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134492929492678 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19864678382873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3396449089050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345830942903247 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19990158081054688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34551167488098145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134441533258983 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19986987113952637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35295963287353516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134851362024035 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20114374160766602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34458208084106445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343826012951986 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21140027046203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3541548252105713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134156427213124 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21184539794921875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35654497146606445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31345170651163373 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20674753189086914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35552072525024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31343086276735577 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032182216644287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483257293701172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31343095558030265 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6381 | test accuracy: 0.468\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6526 | test accuracy: 0.508\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7407 | test accuracy: 0.458\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5336 | test accuracy: 0.522\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7258 | test accuracy: 0.471\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7193 | test accuracy: 0.502\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7717 | test accuracy: 0.475\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6678 | test accuracy: 0.505\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7106 | test accuracy: 0.519\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7757 | test accuracy: 0.478\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7010 | test accuracy: 0.519\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7040 | test accuracy: 0.492\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7052 | test accuracy: 0.478\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7205 | test accuracy: 0.485\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7524 | test accuracy: 0.458\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6500 | test accuracy: 0.488\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7266 | test accuracy: 0.478\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7190 | test accuracy: 0.522\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7580 | test accuracy: 0.525\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7110 | test accuracy: 0.508\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6548 | test accuracy: 0.498\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.8087 | test accuracy: 0.495\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6607 | test accuracy: 0.508\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6190 | test accuracy: 0.478\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7311 | test accuracy: 0.478\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7589 | test accuracy: 0.519\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6627 | test accuracy: 0.468\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7000 | test accuracy: 0.485\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7352 | test accuracy: 0.508\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7096 | test accuracy: 0.492\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7117 | test accuracy: 0.475\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7694 | test accuracy: 0.461\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6186 | test accuracy: 0.492\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6553 | test accuracy: 0.505\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6171 | test accuracy: 0.495\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7691 | test accuracy: 0.485\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6421 | test accuracy: 0.478\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7581 | test accuracy: 0.485\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6867 | test accuracy: 0.488\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7372 | test accuracy: 0.481\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.5945 | test accuracy: 0.478\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7561 | test accuracy: 0.481\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6257 | test accuracy: 0.475\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6979 | test accuracy: 0.488\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6596 | test accuracy: 0.478\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6979 | test accuracy: 0.481\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7268 | test accuracy: 0.485\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6607 | test accuracy: 0.481\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6488 | test accuracy: 0.481\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6625 | test accuracy: 0.481\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7081 | test accuracy: 0.475\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7418 | test accuracy: 0.478\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7459 | test accuracy: 0.481\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7222 | test accuracy: 0.475\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6965 | test accuracy: 0.481\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6849 | test accuracy: 0.478\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7089 | test accuracy: 0.478\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6041 | test accuracy: 0.475\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6823 | test accuracy: 0.481\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6379 | test accuracy: 0.481\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7106 | test accuracy: 0.475\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6793 | test accuracy: 0.478\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6804 | test accuracy: 0.485\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7513 | test accuracy: 0.478\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5739 | test accuracy: 0.478\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6012 | test accuracy: 0.471\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5876 | test accuracy: 0.481\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7070 | test accuracy: 0.478\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6837 | test accuracy: 0.478\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6949 | test accuracy: 0.475\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6409 | test accuracy: 0.475\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7067 | test accuracy: 0.481\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6975 | test accuracy: 0.478\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6653 | test accuracy: 0.478\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6421 | test accuracy: 0.475\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6503 | test accuracy: 0.478\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6545 | test accuracy: 0.471\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7215 | test accuracy: 0.478\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7693 | test accuracy: 0.478\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7603 | test accuracy: 0.478\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7316 | test accuracy: 0.481\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6532 | test accuracy: 0.481\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7652 | test accuracy: 0.478\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7400 | test accuracy: 0.478\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7367 | test accuracy: 0.468\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6313 | test accuracy: 0.468\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6571 | test accuracy: 0.481\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6753 | test accuracy: 0.478\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6352 | test accuracy: 0.471\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7163 | test accuracy: 0.485\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7425 | test accuracy: 0.475\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6685 | test accuracy: 0.481\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7170 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6533 | test accuracy: 0.481\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7347 | test accuracy: 0.485\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7309 | test accuracy: 0.475\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7807 | test accuracy: 0.478\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.5849 | test accuracy: 0.465\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7161 | test accuracy: 0.481\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6560 | test accuracy: 0.471\n",
            "total time:  66.6003839709997\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7103 | test accuracy: 0.492\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6107 | test accuracy: 0.495\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7209 | test accuracy: 0.488\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5720 | test accuracy: 0.522\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7346 | test accuracy: 0.478\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7072 | test accuracy: 0.502\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7708 | test accuracy: 0.478\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6633 | test accuracy: 0.508\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7187 | test accuracy: 0.522\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7853 | test accuracy: 0.478\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6970 | test accuracy: 0.492\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6880 | test accuracy: 0.498\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7028 | test accuracy: 0.515\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7210 | test accuracy: 0.505\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7463 | test accuracy: 0.525\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6434 | test accuracy: 0.508\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7074 | test accuracy: 0.492\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7149 | test accuracy: 0.519\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7498 | test accuracy: 0.522\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6997 | test accuracy: 0.495\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6533 | test accuracy: 0.505\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7798 | test accuracy: 0.498\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6653 | test accuracy: 0.498\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6276 | test accuracy: 0.508\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7482 | test accuracy: 0.495\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7265 | test accuracy: 0.522\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6726 | test accuracy: 0.485\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7023 | test accuracy: 0.488\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7151 | test accuracy: 0.502\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6955 | test accuracy: 0.508\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7034 | test accuracy: 0.475\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7464 | test accuracy: 0.492\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6170 | test accuracy: 0.505\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6638 | test accuracy: 0.488\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6198 | test accuracy: 0.488\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7438 | test accuracy: 0.495\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6265 | test accuracy: 0.492\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7381 | test accuracy: 0.498\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7117 | test accuracy: 0.502\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7489 | test accuracy: 0.498\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6036 | test accuracy: 0.502\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7482 | test accuracy: 0.488\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6425 | test accuracy: 0.502\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7039 | test accuracy: 0.502\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6507 | test accuracy: 0.488\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7109 | test accuracy: 0.498\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7281 | test accuracy: 0.492\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6736 | test accuracy: 0.488\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6575 | test accuracy: 0.502\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6704 | test accuracy: 0.505\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7049 | test accuracy: 0.502\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7259 | test accuracy: 0.492\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7624 | test accuracy: 0.505\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7044 | test accuracy: 0.498\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7073 | test accuracy: 0.492\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6673 | test accuracy: 0.485\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7118 | test accuracy: 0.492\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6143 | test accuracy: 0.502\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6713 | test accuracy: 0.488\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6450 | test accuracy: 0.492\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6996 | test accuracy: 0.485\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6833 | test accuracy: 0.498\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6641 | test accuracy: 0.485\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7526 | test accuracy: 0.495\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5809 | test accuracy: 0.488\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6152 | test accuracy: 0.502\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5970 | test accuracy: 0.492\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7069 | test accuracy: 0.492\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6819 | test accuracy: 0.502\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7178 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6360 | test accuracy: 0.495\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7050 | test accuracy: 0.492\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7037 | test accuracy: 0.502\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6536 | test accuracy: 0.492\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6318 | test accuracy: 0.485\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6644 | test accuracy: 0.502\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6558 | test accuracy: 0.492\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7146 | test accuracy: 0.495\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7395 | test accuracy: 0.492\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7552 | test accuracy: 0.492\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7096 | test accuracy: 0.485\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6463 | test accuracy: 0.485\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7466 | test accuracy: 0.485\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7601 | test accuracy: 0.488\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7260 | test accuracy: 0.488\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6416 | test accuracy: 0.485\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6620 | test accuracy: 0.492\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6982 | test accuracy: 0.488\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6370 | test accuracy: 0.488\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7043 | test accuracy: 0.492\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7516 | test accuracy: 0.488\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6563 | test accuracy: 0.481\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7247 | test accuracy: 0.485\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6730 | test accuracy: 0.485\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7330 | test accuracy: 0.481\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7478 | test accuracy: 0.492\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7651 | test accuracy: 0.481\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6027 | test accuracy: 0.471\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7225 | test accuracy: 0.485\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6537 | test accuracy: 0.485\n",
            "total time:  66.76001612800019\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20410990715026855.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.34857869148254395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.688192207472665 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2105255126953125.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.3481769561767578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5393495065825326 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1965322494506836.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.3364832401275635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4555031486919948 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982567310333252.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.34195947647094727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41043383990015303 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21768403053283691.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3562748432159424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3866037662540163 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20097589492797852.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3417975902557373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.36766789172376907 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20753026008605957.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.35436582565307617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.35789929500647955 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2076122760772705.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35112762451171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35008186953408377 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19730329513549805.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3425476551055908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34274928016322 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19873571395874023.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.33864784240722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33913803015436444 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20286250114440918.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.349790096282959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33473766914435793 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21332025527954102.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3589465618133545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3339628074850355 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21776247024536133.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3834035396575928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33044728551592145 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20645356178283691.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3531177043914795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32808302555765423 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2100064754486084.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35844850540161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3272185883351735 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20997905731201172.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36349010467529297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32553636176245554 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20790362358093262.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34840869903564453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32429749156747545 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20601201057434082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34461545944213867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3231607194457735 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20290255546569824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34958791732788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32156825831958225 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20345664024353027.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3568110466003418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3213862018925803 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20035839080810547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.339601993560791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3205565150294985 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2079148292541504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3513143062591553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31983867841107505 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19750642776489258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3378732204437256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3191827727215631 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2052626609802246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3488306999206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3191256791353226 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20414304733276367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34169578552246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31849301627704074 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2008657455444336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3457491397857666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3183533983571189 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20309972763061523.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3472259044647217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3174752810171672 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20773601531982422.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3447136878967285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31739415568964824 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1998274326324463.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3412003517150879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3175799024956567 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020862102508545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34933972358703613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3169235438108444 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20663785934448242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35987186431884766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31682322110448563 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20315980911254883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3504185676574707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31628662305218835 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20857739448547363.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.351520299911499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31629423115934646 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21380257606506348.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.357480525970459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3163282777581896 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20537137985229492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.345916748046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3159977661711829 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020576000213623.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3427600860595703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31560860105923244 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21040749549865723.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35601377487182617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31571358697755 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20971083641052246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35262155532836914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31571159064769744 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2107715606689453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35480260848999023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31540910729340144 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20640850067138672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34646010398864746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3153394298894065 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20145153999328613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.342710018157959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3152815384524209 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20540094375610352.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3541219234466553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31515596636704035 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20180773735046387.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34882092475891113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3151712366512844 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1921851634979248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3354313373565674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31493450232914516 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19704127311706543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3411741256713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31488554647990635 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20032405853271484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3408076763153076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.314621610726629 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19763922691345215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34078121185302734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3145725918667657 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19907402992248535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3543727397918701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31473562376839775 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19731903076171875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3370630741119385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31460497209003996 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20444798469543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3529078960418701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.314510845712253 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20600080490112305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3541426658630371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3143976952348437 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19339203834533691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33360958099365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31435530952044893 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19488811492919922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3348045349121094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144354177372796 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20288610458374023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3519139289855957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3145130634307861 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2104790210723877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3581717014312744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31448064276150295 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20674824714660645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35168933868408203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31435330041817255 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20006489753723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35524845123291016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31429672156061444 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20969128608703613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36396026611328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31427480578422545 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20840668678283691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484365940093994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140750054802213 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20993542671203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35233592987060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142133512667247 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2102818489074707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36298298835754395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31419652444975715 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143099308013916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3580780029296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141024938651494 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21403002738952637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3627133369445801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31398476617676874 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19768548011779785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3451671600341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139339774847031 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21027398109436035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520843982696533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138717676912035 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21726536750793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3602867126464844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139940065996987 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21514129638671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3651435375213623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31398525067738126 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20017170906066895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515307903289795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31391573122569494 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21511340141296387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583352565765381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.313879046695573 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21480989456176758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35796332359313965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31386423621858867 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20258116722106934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549964427947998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31384879435811724 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20635414123535156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3456416130065918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138653320925576 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20314645767211914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420724868774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138283797672817 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20236706733703613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3468930721282959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3137667877333505 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20827865600585938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540074825286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137421590941293 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20771384239196777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3517923355102539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31374894295419964 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20729827880859375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3562464714050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3138329173837389 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2070026397705078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537778854370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3138051667383739 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21114039421081543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35994553565979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137055665254593 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22698044776916504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3776211738586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31375055483409336 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082045078277588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35364389419555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136959795440946 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20415425300598145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34892916679382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31365681886672975 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143688201904297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3623342514038086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31370735551629747 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21555280685424805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36478376388549805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137197217770985 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.205488920211792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34446096420288086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136350095272064 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21616387367248535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35959291458129883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31366928730692184 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2097320556640625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3513355255126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31362107736723766 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20226049423217773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34075212478637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136293607098716 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21269488334655762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35417628288269043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31356484719685146 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20926570892333984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34821534156799316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31367126532963346 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19802141189575195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457019329071045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136310258081981 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095351219177246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521430492401123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31361084921019416 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19896817207336426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3386673927307129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136373234646661 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19780445098876953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.337615966796875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31363178363868166 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20798349380493164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35027480125427246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136108555964061 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1979539394378662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33787035942077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136001352752958 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20357680320739746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3655056953430176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31359037586620875 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20612835884094238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3524489402770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135491354124887 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19777750968933105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3373253345489502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357744123254505 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2070465087890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3588871955871582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31352382813181195 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2007284164428711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3381221294403076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31355706793921334 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20372843742370605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432626724243164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135206835610526 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20049095153808594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35689735412597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135499532733645 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199784517288208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428916931152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135411913905825 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20032882690429688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428356647491455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134887827294213 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20307493209838867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35388660430908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135346395628793 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20923471450805664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34987330436706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31353826693126136 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20143580436706543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3466155529022217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135093135493142 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20616888999938965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549027442932129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135193075452532 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21070194244384766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35092592239379883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134987826858248 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20427417755126953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430325984954834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135085961648396 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2098240852355957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3528451919555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31349915393761224 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19864869117736816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33579182624816895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.313491433433124 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1980299949645996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33760643005371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134902162211282 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20454931259155273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3530113697052002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31350537027631487 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1950700283050537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.332599401473999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134987988642284 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20035648345947266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34435081481933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134701962981905 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21193695068359375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553473949432373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134635912520545 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2037665843963623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34549736976623535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31349668758256094 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20676279067993164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35206031799316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31349790479455675 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20934462547302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3602755069732666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134960970708302 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20303606986999512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3500690460205078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134628415107727 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21406936645507812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36252808570861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134554173265185 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21487832069396973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.360156774520874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134500814335687 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19808006286621094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34798622131347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31346615723201204 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20028448104858398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3450028896331787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.313474542754037 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21708273887634277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3579106330871582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31343686963830675 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2011425495147705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34197473526000977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31344219446182253 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVyVZf7/8dd9zmFRDigoB8cFNcYVt6w0o6wUEnWmnKXSXNrmZ046Wo4pYn6xnFBLp8V2x5kcxxrMyLExw282ZhZJLkNF9VUsd2VxQUGWA+f+/YEcQVDMOICe9/Px6OG5z31f9/05pOfNdV33YpimaSIiIl7L0tAFiIhIw1IQiIh4OQWBiIiXUxCIiHg5BYGIiJdTEIiIeDlbQxcgV64uXbrw8ccf06pVq2rr/v73v/P222/jdDpxOp1ce+21zJ49m8OHD/OHP/wBgPz8fPLz893tf/WrXzF8+HAGDx7MAw88wIwZM6rs87777mPfvn189NFH561p8+bN/OlPfwLg+PHjlJWV0bJlSwAmTJjAiBEjLuqzZWVl8eCDD/Lvf//7gttNnz6d2NhYBg0adFH7rU1JSQkvvfQSKSkpVJz5HRsby8SJE/H19a2TY4j3MXQdgXjK+YJg06ZNzJs3jxUrVhASEkJJSQmPPfYYzZo148knn3Rvl5yczJo1a3jjjTfc7x04cIC77rqLgIAAUlJSsFjKO7W5ubncddddABcMgsoWL17MkSNHeOqpp37iJ60/jzzyCIWFhTzzzDMEBQVx4sQJZsyYgd1uZ9GiRQ1dnlymNDQk9W7nzp20b9+ekJAQAHx9fXnqqaeYPn36RbX39/cnPDycrVu3ut9bt24dAwYM+Mm1DRo0iBdffJEhQ4Zw6NAhvv/+e0aNGsXQoUOJiYlx9wAOHDhA9+7dgfLAmjx5MvHx8QwZMoRhw4axa9cuAMaOHcu//vUvoDwYV69ezYgRI7jxxhvdAedyuZg7dy5RUVGMGjWK119/nbFjx1arbdeuXXz88ccsWLCAoKAgAJo3b05iYiK//e1vqx2vpuO/9tprDBkyhAULFjB37lz3dseOHaNPnz6cOnWKzMxMxowZw5AhQ/jlL3/JV199BUBBQQETJ05k6NChDB48mMcffxyn0/mTf+bS8BQEUu9uuOEGNm/ezIwZM/j444/Jz8/Hbrdjt9sveh+xsbFVhmXWrl1LbGxsndSXlZVFSkoKrVu35umnn+bWW29l3bp1JCYmMmvWrBq//DZt2sQ999xDSkoK/fv3Z9myZTXuOzMzk9WrV/Pyyy/z5z//mbKyMj7++GM2bdrE+vXreeWVV3j33XdrbJuWlkafPn1o3rx5lfdbtGhx0SFomiYpKSkMHTqU//znP+73//Of/3D99dcTEBDAxIkTueOOO0hJSWHOnDk8/PDDlJaWsnr1aoKCgli3bh0pKSlYrVYyMzMv6rjSuCkIpN51796dt956C5fLRVxcHNdffz0TJ07k0KFDF72P2267jY8++gin08nBgwcpKiqiY8eOdVLfLbfc4n798ssv8+CDDwJwzTXXUFxcTE5OTrU2ERER9OjRAyj/fIcPH65x33fccQcAkZGRFBcXc/ToUbZu3cott9xCQEAAzZs3Z/jw4TW2zcvLo0WLFj/lo7k/W69evTBNk++++w6A//3f/2Xo0KF8//33HD161N3DuOaaawgJCWHHjh3uPzdv3ozL5eKJJ56gW7duP6keaRw0WSwNomfPnjzzzDOYpklGRgbPP/88jz76KElJSRfVvlmzZvTo0YPNmzeTmZnJ0KFD66y2Zs2auV9/8sknvPLKKxw/fhzDMDBNE5fLVa1NYGCg+7XVaqWsrKzGfVdsZ7VagfJhoZMnTxIWFubepvLryoKDg8nKyvrxH6iSyr2J2267jQ0bNhAeHs727dtZuHAhO3fupKioqMrPMz8/nxMnTjB06FDy8vJ4/vnn+f7777n99tuZOXOmJqmvAOoRSL3bunWr+wvNMAx69OjBtGnT2Llz54/az/Dhw0lJSeGDDz5g2LBhdV6n0+nkkUce4fe//z0pKSmsWbMGwzDq/Dh2u53Tp0+7l2vqcQD069eP9PT0amFw8uRJnn/+eUzTxGKxVAmqvLy88x53yJAhfPTRR2zevJnrrrsOu92Ow+EgICCADz74wP3f5s2biYmJAWDkyJG8/fbbvP/++2RkZLB69eqf8tGlkVAQSL177733SEhIID8/H4DS0lLWrl3Ldddd96P2M3jwYNLS0rBarbRr167O6ywsLOT06dPuIZ9ly5bh4+NT5Uu7LvTs2ZONGzdSVFTEyZMnWbduXY3bRUREMGzYMKZOnUpubi4AJ06cYOrUqe4eS2hoqHu4Z8eOHezZs+e8x7366qs5evQoycnJ7h5AmzZtaNWqFR988AFQPok8depUTp8+zUsvvcSqVauA8l5L27ZtPRKMUv80NCQeNXbsWPcwCMCf/vQnZs2axbPPPstvfvMboDwI+vfvz7x5837Uvps2bUrv3r3p2bNnndZcISgoiN/97neMGDGCFi1a8Pvf/57o6GgmTJjAa6+9VmfHiYmJYePGjcTGxtK+fXuGDh1KampqjdvOnTuXV155hdGjR2MYBj4+Ptx+++3ueYz777+fqVOnsmnTJvr160dUVNR5j2sYBtHR0bz99tvuU08Nw+DPf/4zc+bM4bnnnsNisXD//ffTtGlT7rjjDmbOnMmSJUswDIPevXu75zzk8qbrCEQaAdM03b9dr1ixgs8++4yXXnqpgasSb6GhIZEG9u233zJ48GDy8vIoLS1l/fr19OnTp6HLEi+ioSGRBtatWzdGjBjBr3/9a6xWK3369GHMmDENXZZ4EQ0NiYh4OQ0NiYh4uctqaKioqIivv/6a0NDQKmeiiIjI+ZWVlZGTk0OPHj3w9/evtt6jQZCYmEh6ejqGYRAfH0+vXr2A8nu5TJs2zb3d/v37+eMf/0hsbCxxcXEcOnQIq9XKvHnzqpwf/vXXXzN69GhPliwicsVasWIF1157bbX3PRYEaWlp7N27l6SkJHbv3k18fLz79gFhYWEsX74cKD+HfOzYsQwaNIh///vfBAUFsWjRIjZv3syiRYt47rnn3PsMDQ11f5ia7nEvIiLVHTlyhNGjR7u/Q8/lsSBITU0lOjoaKL8iMi8vz32XycreffddhgwZQkBAAKmpqe4Hg9xwww3Ex8dX2bZiOKhVq1a0bdvWU6WLiFyRzjek7rHJ4tzcXIKDg93LISEhNd5D5e2333bf6TA3N9d9j3qLxYJhGJSUlHiqRBERoR7PGqrpLNUdO3Zw1VVXnfc+9DqzVUTE8zwWBA6Hw31jLIDs7Oxq41MbN26s8kANh8Ph7jU4nU5M09QtbkVEPMxjQRAVFUVKSgoAGRkZOByOar/5f/XVV3Tt2rVKm4q7Hv7nP/+hf//+nipPRETO8Nhkcd++fYmMjGTkyJEYhkFCQgLJyckEBga6722ek5NT5YlLw4YN47PPPmPUqFH4+voyf/58T5UnIiJnePQ6gsrXCgBVfvuH8vvSV1Zx7YCIiNQfr7nFRM6pYqLmf8TunPyGLkVEvNT8+fMZO3YssbGx3HzzzYwdO5ZJkybV2u7RRx+lqKjIY3VdVreY+CmyTxVx8EQhu7LyiQit+SwlERFPiouLAyA5OZldu3YxY8aMi2r37LPPerIs7wkCP1v5hRQlZdUfPC4i0lDi4uLw8fHhxIkTzJs3jz/+8Y+cPn2aoqIiZs+eTa9evRg0aBDvvfcec+fOxeFwkJGRwaFDh1i4cCGRkZE/uQYvCoLyUbBiZ1kDVyIijcE72w6wcuv+Ot3nXde24zfX/Pi7HjRr1oy5c+fyww8/cOeddxIdHU1qaipLlixh8eLFVbYtKSlh6dKlvPXWW6xevVpB8GP4+ZwJglL1CESkcam4IWfLli15+eWXWbp0KSUlJTRt2rTathU3jWvVqhVffvllnRzfe4LgzD02FAQiAvCba9pe0m/vnuDj4wPAsmXLCAsL45lnnuGrr77i6aefrrZt5fsF1dXdF7zmrKGzPQINDYlI43T8+HHCw8MB+PDDD3E6nfVyXK8JAl9rxRyBegQi0jjdcccd/O1vf+OBBx6gV69e5OTk8M4773j8uJfVM4sPHDjA4MGD2bBhwyXdhrrzrHU8eFNHZsR2rX1jEZErRG3fnV7TI4DyM4fUIxARqcq7gsDHojkCEZFzeFUQ+FotOmtIROQcXhUEfj5WBYGIyDm8KwhsFko0NCQiUoXXBYF6BCIiVXlZEFh11pCIyDm8Kgh8bTprSETkXB6911BiYiLp6ekYhkF8fLz7xkoAhw8fZurUqTidTrp3786TTz7Jli1bmDJlCp06dQKgc+fOzJ49u87q8bNZOH5aPQIRkco8FgRpaWns3buXpKQkdu/eTXx8PElJSe718+fP54EHHiAmJoYnnniCQ4cOAdCvXz9eeOEFj9Tk52OhRHMEIiJVeGxoKDU1lejoaAAiIiLIy8sjP7/8MZEul4tt27YxaNAgABISEmjdurWnSnHzs+n0URGRc3ksCHJzcwkODnYvh4SEkJOTA8CxY8cICAhg3rx5jBo1ikWLFrm3y8zMZMKECYwaNYpPP/20Tmvy0xyBiEg19fY8gsr3tjNNk6ysLMaNG0ebNm0YP348GzdupFu3bkyaNImhQ4eyf/9+xo0bx/r16/H19a2TGnT6qIhIdR7rETgcDnJzc93L2dnZhIaGAhAcHEzr1q0JDw/HarUyYMAAdu3aRVhYGMOGDcMwDMLDw2nZsiVZWVl1VpOvbjonIlKNx4IgKiqKlJQUADIyMnA4HNjtdgBsNhvt2rVjz5497vUdO3ZkzZo1LF26FICcnByOHj1KWFhYndXkZ7Pq4fUiIufw2NBQ3759iYyMZOTIkRiGQUJCAsnJyQQGBhITE0N8fDxxcXGYpknnzp0ZNGgQp0+fZtq0aWzYsAGn08mcOXPqbFgIyoeGylwmpWUubFavuoRCROS8PDpHMG3atCrLXbuefSBM+/bteeutt6qst9vtvPrqqx6rp/ID7BUEIiLlvOrb0M+mB9iLiJzLq4LA16YH2IuInMurgsDPpgfYi4icy8uCoHxoSGcOiYic5WVBoB6BiMi5vCsIfDRHICJyLu8KAp01JCJSjVcFgc4aEhGpzquCoGKOQM8kEBE5yyuDQENDIiJneVcQ+JyZI9BZQyIibt4VBJojEBGpxquCwFdDQyIi1XhVEGiOQESkOq8KAl+rgkBE5FxeFQSGYegB9iIi5/CqIIAzD7DXWUMiIm7eFwQ+Vg0NiYhU4tFHVSYmJpKeno5hGMTHx9OrVy/3usOHDzN16lScTifdu3fnySefrLVNXfC1WnRlsYhIJR7rEaSlpbF3716SkpJ46qmneOqpp6qsnz9/Pg888ACrVq3CarVy6NChWtvUBT8fzRGIiFTmsSBITU0lOjoagIiICPLy8sjPzwfA5XKxbds2Bg0aBEBCQgKtW7e+YJu64mfT0JCISGUeC4Lc3FyCg4PdyyEhIeTk5ABw7NgxAgICmDdvHqNGjWLRokW1tqkr5WcNKQhERCp4dI6gMtM0q7zOyspi3LhxtGnThvHjx7Nx48YLtqkr5WcNaWhIRKSCx3oEDoeD3Nxc93J2djahoaEABAcH07p1a8LDw7FarQwYMIBdu3ZdsE1d8bVZ9MxiEZFKPBYEUVFRpKSkAJCRkYHD4cButwNgs9lo164de/bsca/v2LHjBdvUFT+bVdcRiIhU4rGhob59+xIZGcnIkSMxDIOEhASSk5MJDAwkJiaG+Ph44uLiME2Tzp07M2jQICwWS7U2dU1nDYmIVOXROYJp06ZVWe7atav7dfv27XnrrbdqbVPXNFksIlKV911ZrNNHRUSq8MIg0FlDIiKVeWUQ6KwhEZGzvDIIiktdHrlGQUTkcuR9QeBjxTTBWaYgEBEBbwwCPcBeRKQKLw4CzROIiIBXBoEVQM8kEBE5w+uCwFc9AhGRKrwuCDRHICJSlfcFgc+ZINCN50REAG8MgjNzBBoaEhEp54VBUP6RNVksIlLO64LAV3MEIiJVeF0QaGhIRKQqLwwC9QhERCrzviDQWUMiIlV49AlliYmJpKenYxgG8fHx9OrVy71u0KBBtGrVCqu1fKhm4cKF7NmzhylTptCpUycAOnfuzOzZs+u0JveVxboVtYgI4MEgSEtLY+/evSQlJbF7927i4+NJSkqqss2SJUsICAhwL+/Zs4d+/frxwgsveKqss0ND6hGIiAAeHBpKTU0lOjoagIiICPLy8sjPz/fU4S6azhoSEanKY0GQm5tLcHCwezkkJIScnJwq2yQkJDBq1CgWLlzoflBMZmYmEyZMYNSoUXz66ad1XpfNYmAxdNaQiEgFj84RVHbuE8EmT57MTTfdRLNmzZg4cSIpKSlcffXVTJo0iaFDh7J//37GjRvH+vXr8fX1rbM6DMPQA+xFRCrxWI/A4XCQm5vrXs7OziY0NNS9PGLECFq0aIHNZmPgwIHs3LmTsLAwhg0bhmEYhIeH07JlS7Kysuq8Nj8fPcBeRKSCx4IgKiqKlJQUADIyMnA4HNjtdgBOnTrFgw8+SElJCQBffPEFnTp1Ys2aNSxduhSAnJwcjh49SlhYWJ3XpgfYi4ic5bGhob59+xIZGcnIkSMxDIOEhASSk5MJDAwkJiaGgQMHcvfdd+Pn50f37t2JjY2loKCAadOmsWHDBpxOJ3PmzKnTYaEKvjaLzhoSETnDo3ME06ZNq7LctWtX9+t7772Xe++9t8p6u93Oq6++6smSADRHICJSidddWQzg72OhUHMEIiKAlwaB3c9GfnFpQ5chItIoeGkQ+JBfpCAQEYGLCIL8/Hx++OEHoPy2EW+88QbHjh3zeGGeFOivHoGISIVag+CRRx4hOzubXbt2sWDBAkJCQpg5c2Z91OYxGhoSETmr1iAoKSmhf//+rFu3jvvuu4/bb7+d4uLi+qjNY+z+Ng0NiYiccVFBsGbNGtauXcutt97KgQMHOHXqVH3U5jF2PxslZS7deE5EhIsIgoSEBL788kvmzJmD3W7n448/5pFHHqmP2jwm0L/88gn1CkRELuKCsnbt2nHPPfdw1VVXkZaWhtPpJDIysj5q85gA3zNBUFxKC7tfA1cjItKwLmqyOCcn58qaLPY/GwQiIt7OKyeLA/00NCQiUsE7J4vVIxARcbvoyeInnnjiipkstvspCEREKtQ6WdytWzdiYmL49ttv2blzJz169KBv3771UZvHVPQITmloSESk9h5BYmIib7zxBqZpUlRUxMsvv8yzzz5bH7V5TKCfD6AegYgIXESPICMjgxUrVriXx48fz5gxYzxalKf5+1iwWgxNFouIcBE9gtLSUoqKitzLp0+fpqzs8r4i1zAMAnyt6hGIiHARPYJ7772X22+/nQ4dOuByudi3bx/Tp0+/qJ0nJiaSnp6OYRjEx8fTq1cv97pBgwbRqlUrrFYrAAsXLiQsLOyCbepSoL+P5ghERLiIIBg2bBi33HILe/bswTAMOnTogI+PT607TktLY+/evSQlJbF7927i4+NJSkqqss2SJUsICAj4UW3qit3PRoF6BCIiF/dgmqZNm9K9e3e6detGkyZNeOCBB2ptk5qaSnR0NAARERHk5eWRn59f520ulV3PJBARAS7xCWWmada6TW5uLsHBwe7lkJAQcnJyqmyTkJDAqFGjWLhwIaZpXlSbumL3s3FKQSAiUvvQUE0Mw/jRbc4Nj8mTJ3PTTTfRrFkzJk6cSEpKSq1t6pLd38aB46c9tn8RkcvFeYNgwYIFNX7hm6bJ/v37a92xw+EgNzfXvZydnU1oaKh7ecSIEe7XAwcOZOfOnbW2qUuBekqZiAhwgaGhzp0706lTp2r/de7cmcmTJ9e646ioKPdv+RkZGTgcDux2OwCnTp3iwQcfpKSkBIAvvviCTp06XbBNXbP76SllIiJwgR7Br371q5+04759+xIZGcnIkSMxDIOEhASSk5MJDAwkJiaGgQMHcvfdd+Pn50f37t2JjY3FMIxqbTwlwM9GQUkZZS4Tq+XHD3WJiFwpLmmO4GJNmzatynLXrl3dr++9917uvffeWtt4SsVTygpKSgnyr/10WBGRK9UlnTV0JbDrmQQiIsAFgmDLli1VlivG8wHefvttz1VUTyruQKqLykTE2503CF566aUqy7/73e/cr9977z3PVVRPKnoEupZARLzdeYPg3HP4Ky978vz++lIxR6ChIRHxducNgnOvIai8fCkXlDU2dj2TQEQEuMBZQy6Xi6KiIvdv/xXLLpcLl8tVbwV6il09AhER4AJBcOjQIYYPH15lGGjYsGHAFdIj8NUcgYgIXCAIPvroo/qso94F+JU/B0E9AhHxduedI3A6nTz33HM4nU73e7t27eKFF16ol8I8zWa10MTHSn6xs/aNRUSuYOcNggULFpCfn19laKh9+/bk5+fz4osv1ktxnqZnEoiIXCAIduzYweOPP46vr6/7PV9fX+Li4vj000/rpThPC/Sz6XGVIuL1zhsEFc8SrtbAYqkyXHQ5s/vrcZUiIucNguDgYLZu3Vrt/Y0bN9KyZUuPFlVf7HomgYjI+c8aio+P5w9/+AMRERF069aNsrIy0tPTOXz4MEuXLq3PGj3G7mdj3zE9pUxEvNt5g6B9+/asXr2aTz/9lO+//x7DMBgzZgxRUVFXxHUEoB6BiAjU8jwCi8XCTTfdxE033VRf9dQrnTUkIuLFzyOAs4+rvBJuoicicqm8Owj8bZS6TIpLL/97J4mIXCqPPqoyMTGR9PR0DMMgPj6eXr16Vdtm0aJF/Pe//2X58uVs2bKFKVOm0KlTJwA6d+7M7NmzPVZf4JlHVJ4sdOLvU/PpsiIiVzqPBUFaWhp79+4lKSmJ3bt3Ex8fT1JSUpVtMjMz+eKLL/DxOfvM4H79+tXbbSxC7X4AZJ8qxhHkXy/HFBFpbDw2NJSamkp0dDQAERER5OXlkZ+fX2Wb+fPn8+ijj3qqhFqFBVUEQVGD1SAi0tA8FgS5ubkEBwe7l0NCQsjJyXEvJycn069fP9q0aVOlXWZmJhMmTGDUqFEev5VF2JleQNbJYo8eR0SkMfPoHEFllc/MOXHiBMnJyfztb38jKyvL/X6HDh2YNGkSQ4cOZf/+/YwbN47169dXud9RXWp5Zmgo66R6BCLivTzWI3A4HOTm5rqXs7OzCQ0NBeDzzz/n2LFjjB49mkmTJpGRkUFiYiJhYWEMGzYMwzAIDw+nZcuWVYKirvnaLLQI8FWPQES8mseCICoqipSUFAAyMjJwOBzY7XYAYmNjef/991m5ciUvvvgikZGRxMfHs2bNGvftK3Jycjh69ChhYWGeKhEAR5A/OZojEBEv5rGhob59+xIZGcnIkSMxDIOEhASSk5MJDAwkJiamxjaDBg1i2rRpbNiwAafTyZw5czw2LFQhLMhPPQIR8WoenSOYNm1aleWuXbtW26Zt27YsX74cALvdzquvvurJkqoJC/Tnm0Mn6/WYIiKNiVdfWQzgCPIjN7+Y0jJdXSwi3klBEOSPy4SjBSUNXYqISIPw+iAICzxzUZnmCUTESykI3BeV6cwhEfFOXh8EjjO3mcjSKaQi4qW8Pgha2v0wDN1mQkS8l9cHgY/VQosAP7I1NCQiXsrrgwDKLyrLPqUegYh4JwUB5RPGmiwWEW+lIAAcgbrNhIh4LwUB5ReVHS0oxqmri0XECykIKJ8jME3IzVevQES8j4KA8hvPga4uFhHvpCCg0kVlmjAWES+kIKDSbSZ0CqmIeCEFAeVXF/vaLOw7WtDQpYiI1DsFAWC1GHQJC+Sbw3pAjYh4H48GQWJiInfffTcjR47kyy+/rHGbRYsWMXbs2B/VxhMiWweRcegkpmnW2zFFRBoDjwVBWloae/fuJSkpiaeeeoqnnnqq2jaZmZl88cUXP6qNp3RvHcSJ004O5WnCWES8i8eCIDU1lejoaAAiIiLIy8sjPz+/yjbz58/n0Ucf/VFtPCWydRCAnl8sIl7HY0GQm5tLcHCwezkkJIScnBz3cnJyMv369aNNmzYX3caTurYKwjAg41BevRxPRKSxqLfJ4spj7ydOnCA5OZn777//ott4WoCfjY4tA8hQj0BEvIzNUzt2OBzk5ua6l7OzswkNDQXg888/59ixY4wePZqSkhL27dtHYmLiBdvUh8jWzdi+93i9HU9EpDHwWI8gKiqKlJQUADIyMnA4HNjtdgBiY2N5//33WblyJS+++CKRkZHEx8dfsE196P6zIA6eKOR4QUm9HVNEpKF5rEfQt29fIiMjGTlyJIZhkJCQQHJyMoGBgcTExFx0m/pUMWH87eGT3PDzlvV6bBGRhuKxIACYNm1aleWuXbtW26Zt27YsX778vG3qU0UQZBxSEIiI99CVxZW0sPvRKshfZw6JiFdREJwjsnUQX+vMIRHxIgqCc/Rp15zM7HxOnNaEsYh4BwXBOfp1DAHgiz06jVREvIOC4By92zXH12rhiz3HGroUEZF6oSA4h7+Pld7tmpH2g4JARLyDgqAG13UI4euDeZwuKW3oUkREPE5BUIN+HUModZns2HeioUsREfE4BUENrmkfjMWALRoeEhEvoCCoQaC/D91bB/GFgkBEvICC4Dyu6xDC9n3HKSl1NXQpIiIepSA4j34dQigudbF9n64nEJErm4LgPG7qHEqQv403Pt3T0KWIiHiUguA87H42xg5oT8o3R9idUz/PTRYRaQgKggu474aO+FgtLNn0fUOXIiLiMQqCCwgN9OPOa9qSvP0g2SeLGrocERGPUBDUYvzAqyh1ufjL5h8auhQREY/w6BPKEhMTSU9PxzAM4uPj6dWrl3vdypUrWbVqFRaLha5du5KQkEBaWhpTpkyhU6dOAHTu3JnZs2d7ssRatW8RwIir2/DGZ3sY07894S2aNmg9IiJ1zWNBkJaWxt69e0lKSmL37t3Ex8eTlJQEQGFhIWvXrmXFihX4+Pgwbtw4duzYAUC/fv144YUXPFXWJZkR25UPvj7Cn9Z+w+vjrm3ockRE6pTHhoZSU1OJjjW5OzIAABKnSURBVI4GICIigry8PPLzy8++adKkCcuWLcPHx4fCwkLy8/MJDQ31VCk/WViQPxNv/Tnrv8li867chi5HRKROeSwIcnNzCQ4Odi+HhISQk5NTZZvXX3+dmJgYYmNjadeuHQCZmZlMmDCBUaNG8emnn3qqvB/twRs7Eh7SlCfey8BZpquNReTKUW+TxaZpVntv/PjxfPjhh3zyySds27aNDh06MGnSJF555RUWLFjArFmzKClpHI+M9PexMvsX3dmVnc9fNXEsIlcQjwWBw+EgN/fsMEp2drZ7+OfEiRN88cUXAPj7+zNw4EC2b99OWFgYw4YNwzAMwsPDadmyJVlZWZ4q8UeL6R5GdLcwnvtwFweOn27ockRE6oTHgiAqKoqUlBQAMjIycDgc2O12AEpLS4mLi6OgoACAr776io4dO7JmzRqWLl0KQE5ODkePHiUsLMxTJV6SJ+6IBCDhXxk19nJERC43HjtrqG/fvkRGRjJy5EgMwyAhIYHk5GQCAwOJiYlh4sSJjBs3DpvNRpcuXRg8eDAFBQVMmzaNDRs24HQ6mTNnDr6+vp4q8ZK0ad6ER2M6kfj+d/zj872MHdChoUsSEflJDPMy+rX2wIEDDB48mA0bNtC2bdsGq8NZ5uLBZVvZtDOHMdeH8z+/iMTXpmvzRKRxqu27U99el8DHauGv917LQwOv4h+f72PM0i2cKnI2dFkiIpdEQXCJbFYLM4d14/mRfdi+9zhjlqaRd1phICKXH4/eYsIb3NGnDQG+Nh5esZ27X09ldP9wOocF0rtdc/x9rA1dnohIrdQjqAPR3cNYcu+1ZJ8qZva/Mrj79c8Z9vwn/N+RUw1dmohIrRQEdeTmzqFsezya1JmDeOmevpwqLmXES5+SvP2ATjMVkUZNQVCHDMPgZ82aMLzXz1j7hxvp2aYZU1em8+tXPuOzzFwFgog0Spoj8BBHkD9v/r/+rNp2gBc27OKev2whJMCX7j8L4toOwYzo04YOLQMaukwREQWBJ9msFkb2C2fE1W1Y899DbN93nIxDJ3l+wy6e+3AX17QP5tYuodzw85b0atMMm1UdNBGpfwqCeuDvY+Wu69px13Xld1g9nFfIuzsO8u/0wyxcvxPW7yTQz0b/q0K4qVMoQ3u0whHk38BVi4i3UBA0gJ81a8LDt/ych2/5OUfzi0n9/iif7T7KZ5m5fPhtNk+8l8H1V7WgQ8sAmvhYaWH3pUtYIJ3DAmnTvAkWi9HQH0FEriAKggbWwu7HL3q15he9WgOQmX2KNf89REpGFjuzTnG6pIzTJWXu7Zv6WunksBMRaqdDywBaN2+Cj9XAajFoFeRP+xYBtLT7YhgKCxG5OAqCRubnjkCm3taFqbd1cb+XV+gkM/sU/3ckn51Zp9iZdYrPdh8lecfBGvdhMcDPZiXAz0onRyA92gQRGuiH1VI+B1HkLKPYWYYjyJ+OLQMIDfTDx2rB38dCqN1PcxUiXkZBcBlo1sSHa9qHcE37kCrvF5aUkX2qiFKXSWmZyaG8QvbkFnA0v4SSMhd5p518d+Qky1L3UlJ6cU9VsxjQKsgff18rmGBy9qFCgf4+hAT4EhLgS3BTX5o18cHExOUyKTNNylxgtZTXG+Tvg6/Ngs1qwcdiYLNasFkNfCxn/rQa2NyvLdgs5X9aLQZNfKwE+tuqBZJpmpS5TKwWQz0ekTqkILiMNfG10r7F2VNQu7QKhC7VtytzmRQ5yyh1lX+z+/ta8LFYOHKyqDw4CkoodbkoLHFxJK+QgyeKKCotw6D82giLAaYJJ4ucHC8oYXdOPscLSiioNGRltRhYDeNMINTN9RJ+tvKAsBgGJWUuiiuFma/1bIhUhIqP7WzQGJTX4jJNTLP8Z+Ayy0PLMAx8bZXbWfCt9Lo8uAxcJrhcJqWus/uxWgxsZ9ZXBJnVcva4tjPzNxXBWBGSBmAxDCyW8p+pe9mo+BkbGAZUjreKrKscelXiz6j80nC3sRhgNQz3fi0G7nml8mOcPVbF8rnHO7vuzLJR6ZBn1pumiUn5ZzzzV6vGOg3jfO8b1bap+vlq/txnt63e/kLHPrdN9QNWW6z2C0f19bUsn9Oitt9fLnQ8w4Abf96S5k3r/tb8CgIvYLUYBPhV/1/dunkTWjdvcsn7LS1znflyO/vX1TRNCkrKyCt04ix1Uepy4Swr77E4XS5Ky0xKy1w4XWf+LDMpPfO+s8xFqcuksKSMU0WlFJSUur/Afa0W/Hys2CyGu335/k1KylzufTnLyvcFYLFUfNkaZ3oR5csu03Qfz1mpnbPMRWFhmXsfhkH5F/2Zz2gAZWb55y5zlbcp/7P8M1S8rviZlx+3/EsZKA8W88yXZsWX6JlwMs+sc/8c3T9PKr1XaX2V9yu9PrP/iuCSK8vEWyN4bEjXOt+vgkAuWU1zCYZhYPezYa8heKR+mebZgHFVChrTLA8V06wcOOaZYUDODAmeXV8xNGhWaot5trdYEbbuX18vIbyqv1fzPipeXsy25wbhubl47pX+tQVn9f1duH2tx6tl/9W3gI4t7Res8VLpX6vIFapiSMdSbUBDpCqdHiIi4uU82iNITEwkPT0dwzCIj4+nV69e7nUrV65k1apVWCwWunbtSkJCAoZhXLCNiIjUPY8FQVpaGnv37iUpKYndu3cTHx9PUlISAIWFhaxdu5YVK1bg4+PDuHHj2LFjB6WlpedtIyIinuGxoaHU1FSio6MBiIiIIC8vj/z8fACaNGnCsmXL8PHxobCwkPz8fEJDQy/YRkREPMNjQZCbm0twcLB7OSQkhJycnCrbvP7668TExBAbG0u7du0uqo2IiNStepssrumhLOPHj+fDDz/kk08+Ydu2bRfVRkRE6pbH5ggcDge5ubnu5ezsbEJDQwE4ceIEu3bt4rrrrsPf35+BAweyffv2C7YBKCsrv5L1yJEjnipbROSKU/GdWfEdei6PBUFUVBSLFy9m5MiRZGRk4HA4sNvLL4YoLS0lLi6ONWvWEBAQwFdffcXtt99OSEjIedsA7mGi0aNHe6psEZErVk5ODu3bt6/2vmF6cPxl4cKFbN26FcMwSEhI4JtvviEwMJCYmBiSk5NZsWIFNpuNLl268MQTT2AYRrU2XbuevZy6qKiIr7/+mtDQUKxWq6fKFhG5opSVlZGTk0OPHj3w96/+0CuPBoGIiDR+urJYRMTLec29hi7HK5affvpptm3bRmlpKQ899BA9e/Zk+vTplJWVERoayjPPPIOvb93fkrYuFRUV8Ytf/IKHH36YAQMGXFb1r1mzhr/85S/YbDYmT55Mly5dLpv6CwoKmDFjBnl5eTidTiZOnEhoaChz5swBcA/HNjY7d+7k4Ycf5r777mPMmDEcPny4xp/5mjVrWLZsGRaLhbvuuos777yzoUsHaq5/5syZlJaWYrPZeOaZZwgNDW189ZteYMuWLeb48eNN0zTNzMxM86677mrgimqXmppq/u53vzNN0zSPHTtm3nzzzWZcXJz5/vvvm6ZpmosWLTJXrFjRkCVelD//+c/mr3/9a/Odd965rOo/duyYedttt5mnTp0ys7KyzMcff/yyqn/58uXmwoULTdM0zSNHjphDhgwxx4wZY6anp5umaZpTp041N27c2JAlVlNQUGCOGTPGfPzxx83ly5ebpmnW+DMvKCgwb7vtNvPkyZNmYWGhOXz4cPP48eMNWbppmjXXP336dHPt2rWmaZrmP/7xD3PBggWNsn6vGBq6HK9Yvu6663j++ecBCAoKorCwkC1btjB48GAAbr31VlJTUxuyxFrt3r2bzMxMbrnlFoDLqv7U1FQGDBiA3W7H4XAwd+7cy6r+4OBgTpw4AcDJkydp3rw5Bw8edPeEG2P9vr6+LFmyBIfD4X6vpp95eno6PXv2JDAwEH9/f/r27cv27dsbqmy3mupPSEhgyJAhwNn/J42xfq8IgsvximWr1UrTpk0BWLVqFQMHDqSwsNA9FNGiRYtG/xkWLFhAXFyce/lyqv/AgQMUFRUxYcIE7rnnHlJTUy+r+ocPH86hQ4eIiYlhzJgxTJ8+naCgIPf6xli/zWardkZLTT/z3NxcQkLOPra1sfx7rqn+pk2bYrVaKSsr48033+SXv/xlo6zfa+YIKjMvoxOlPvzwQ1atWsVf//pXbrvtNvf7jf0zrF69mj59+tCuXbsa1zf2+qH8wscXX3yRQ4cOMW7cuCo1N/b6//Wvf9G6dWuWLl3Kd999x8SJEwkMDHSvb+z11+R8NTf2z1JWVsb06dO5/vrrGTBgAO+9916V9Y2hfq8IgtquWG6sPvnkE1599VX+8pe/EBgYSNOmTSkqKsLf35+srKwqXdDGZuPGjezfv5+NGzdy5MgRfH19L6v6W7RowdVXX43NZiM8PJyAgACsVutlU//27du58cYbAejatSvFxcWUlpa61zf2+ivU9Hempn/Pffr0acAqL2zmzJm0b9+eSZMmATV/HzV0/V4xNBQVFUVKSgpAjVcsN0anTp3i6aef5rXXXqN58+YA3HDDDe7PsX79em666aaGLPGCnnvuOd555x1WrlzJnXfeycMPP3xZ1X/jjTfy+eef43K5OH78OKdPn76s6m/fvj3p6ekAHDx4kICAACIiIti6dSvQ+OuvUNPPvHfv3nz11VecPHmSgoICtm/fzrXXXtvAldZszZo1+Pj4MHnyZPd7jbF+r7mg7EJXLDdGSUlJLF68mI4dO7rfmz9/Po8//jjFxcW0bt2aefPm4ePj04BVXpzFixfTpk0bbrzxRmbMmHHZ1P/Pf/6TVatWAfD73/+enj17Xjb1FxQUEB8fz9GjRyktLWXKlCmEhobyP//zP7hcLnr37s3MmTMbuswqvv76axYsWMDBgwex2WyEhYWxcOFC4uLiqv3MP/jgA5YuXYphGIwZM4bbb7+9ocuvsf6jR4/i5+fn/sUzIiKCOXPmNLr6vSYIRESkZl4xNCQiIuenIBAR8XIKAhERL6cgEBHxcgoCEREvpyCQK8KBAwe4+uqrGTt2bJX/Ku6381MsXryYf/zjHxfcpkuXLnz00Ufu5S1btrB48eJLPuaWLVuqnHsu4klecWWxeIeOHTuyfPnyBjl2hw4dePHFF7n55pv19Dy57CgI5IoXFxdH06ZN+f777zl+/Djz5s2je/fuLFu2jPfffx+AwYMHM378eA4ePEhcXBxlZWW0bt2aBQsWAOX3mX/ooYfYs2cPs2bNYuDAgVWO4XA46NmzJ++++y6//e1vq6zr378/W7ZsAWDy5MmMHj2atLQ0jh8/zt69ezlw4ABTpkzhnXfe4eDBgyxZsgSAvLw8Jk6cyMGDB4mJiWHixIlkZmby5JNPYhgGAQEBzJ8/n5MnT/LYY4/RtGlTxowZw6233urpH6lcYTQ0JF6htLSUN954gylTpvDSSy+xf/9+3n33XVasWMGKFStYt24d+/bt49lnn+W+++7jzTffxOFw8PXXXwPlN6B77bXXePzxx/nnP/9Z4zEeeughli1bRlFR0UXVlJeXx9KlS4mNjWX16tXu1xs2bADg//7v/3j66adZuXIl77zzDidOnGDu3Lk8+eSTLFu2jKioKFasWAHAt99+y8KFCxUCcknUI5Arxg8//MDYsWPdyx07duTJJ58Eyu9ZA9CnTx8WLlzIt99+S+/evbHZyv8J9O3bl++++45vvvmGWbNmATB9+nQANm3aRN++fQEICwvj1KlTNR6/WbNm3HHHHfz973+nd+/etdbbs2dPgCo3QGzZsqV7XqNHjx4EBAQA5bcm2L9/P19++SWzZ88GoKSkxL2Pdu3aVbnVusiPoSCQK8aF5ghcLpf7tWEYGIZR5fa/TqcTi8WC1Wqt8bbAFYFRm7Fjx/Lb3/6WDh061Lje6XTWuM/KryuObxhGlbaGYdCkSRP+/ve/V1l34MCBRnvPI7k8aGhIvMK2bdsA2LFjBxEREXTr1o3//ve/lJaWUlpaSnp6Ot26daNHjx58/vnnADz//PN89tlnP+o4fn5+3H///bz66qvu9wzDoLCwkMLCQr799tuL3tc333xDYWEhxcXF7N69m/DwcLp27cqmTZsAWLt2baN7yphcntQjkCvGuUNDAI899hgAxcXFPPTQQxw+fJhnnnmGtm3bcvfddzNmzBhM0+TOO++kTZs2TJ48mZkzZ/Lmm2/ys5/9jEmTJrlD5GKNGDGCv/3tb+7lUaNGcddddxEREUFkZORF76d79+7Ex8ezZ88eRo4cSVBQELNmzWL27NksWbIEPz8/Fi1a1OgfuyqNn+4+Kle8uLg4hgwZoolUkfPQ0JCIiJdTj0BExMupRyAi4uUUBCIiXk5BICLi5RQEIiJeTkEgIuLlFAQiIl7u/wPg99qgU0sm5QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6686 | test accuracy: 0.512\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6681 | test accuracy: 0.495\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8682 | test accuracy: 0.485\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7143 | test accuracy: 0.498\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7106 | test accuracy: 0.488\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7384 | test accuracy: 0.508\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6720 | test accuracy: 0.505\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7905 | test accuracy: 0.525\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5805 | test accuracy: 0.495\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7175 | test accuracy: 0.498\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6711 | test accuracy: 0.512\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6461 | test accuracy: 0.478\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7204 | test accuracy: 0.532\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6515 | test accuracy: 0.488\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7987 | test accuracy: 0.488\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6487 | test accuracy: 0.481\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7605 | test accuracy: 0.522\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6843 | test accuracy: 0.505\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6183 | test accuracy: 0.515\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6886 | test accuracy: 0.515\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6903 | test accuracy: 0.498\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7498 | test accuracy: 0.495\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7332 | test accuracy: 0.488\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5959 | test accuracy: 0.515\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6408 | test accuracy: 0.502\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5698 | test accuracy: 0.505\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7097 | test accuracy: 0.498\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6926 | test accuracy: 0.498\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7048 | test accuracy: 0.492\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6783 | test accuracy: 0.498\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7605 | test accuracy: 0.519\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7072 | test accuracy: 0.502\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6772 | test accuracy: 0.495\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5945 | test accuracy: 0.512\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6434 | test accuracy: 0.508\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7153 | test accuracy: 0.505\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7107 | test accuracy: 0.495\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6770 | test accuracy: 0.495\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6076 | test accuracy: 0.515\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.8056 | test accuracy: 0.492\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6411 | test accuracy: 0.498\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5734 | test accuracy: 0.498\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7282 | test accuracy: 0.502\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6319 | test accuracy: 0.505\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6783 | test accuracy: 0.495\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7007 | test accuracy: 0.495\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6411 | test accuracy: 0.498\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7062 | test accuracy: 0.495\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6874 | test accuracy: 0.498\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6949 | test accuracy: 0.498\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6976 | test accuracy: 0.488\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6371 | test accuracy: 0.492\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7051 | test accuracy: 0.495\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6344 | test accuracy: 0.478\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6634 | test accuracy: 0.498\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6898 | test accuracy: 0.492\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6772 | test accuracy: 0.498\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6219 | test accuracy: 0.495\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.5808 | test accuracy: 0.492\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6862 | test accuracy: 0.498\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6498 | test accuracy: 0.485\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7383 | test accuracy: 0.498\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7894 | test accuracy: 0.498\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7276 | test accuracy: 0.492\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6810 | test accuracy: 0.495\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7993 | test accuracy: 0.485\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6830 | test accuracy: 0.488\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7237 | test accuracy: 0.498\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6408 | test accuracy: 0.488\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5817 | test accuracy: 0.492\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7572 | test accuracy: 0.488\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6676 | test accuracy: 0.495\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6880 | test accuracy: 0.495\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6914 | test accuracy: 0.492\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7539 | test accuracy: 0.495\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7075 | test accuracy: 0.481\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6209 | test accuracy: 0.478\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7596 | test accuracy: 0.481\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7302 | test accuracy: 0.485\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7418 | test accuracy: 0.485\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7298 | test accuracy: 0.488\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6520 | test accuracy: 0.475\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6711 | test accuracy: 0.495\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6643 | test accuracy: 0.485\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6683 | test accuracy: 0.488\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7210 | test accuracy: 0.495\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6154 | test accuracy: 0.478\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5759 | test accuracy: 0.495\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.8029 | test accuracy: 0.481\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7040 | test accuracy: 0.492\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6927 | test accuracy: 0.481\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6953 | test accuracy: 0.488\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7070 | test accuracy: 0.478\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7225 | test accuracy: 0.481\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7455 | test accuracy: 0.478\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7395 | test accuracy: 0.481\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.5806 | test accuracy: 0.485\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6620 | test accuracy: 0.471\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6668 | test accuracy: 0.478\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6838 | test accuracy: 0.478\n",
            "total time:  66.23406017600018\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6870 | test accuracy: 0.519\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6923 | test accuracy: 0.512\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8752 | test accuracy: 0.488\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6833 | test accuracy: 0.492\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7037 | test accuracy: 0.495\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7223 | test accuracy: 0.515\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6706 | test accuracy: 0.522\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7856 | test accuracy: 0.508\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5916 | test accuracy: 0.485\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7120 | test accuracy: 0.495\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6526 | test accuracy: 0.519\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6478 | test accuracy: 0.471\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6947 | test accuracy: 0.525\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6527 | test accuracy: 0.485\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7836 | test accuracy: 0.498\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6522 | test accuracy: 0.478\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7424 | test accuracy: 0.508\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6785 | test accuracy: 0.512\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6306 | test accuracy: 0.522\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6780 | test accuracy: 0.515\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7009 | test accuracy: 0.485\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7639 | test accuracy: 0.508\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7304 | test accuracy: 0.505\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6183 | test accuracy: 0.508\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6320 | test accuracy: 0.512\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5769 | test accuracy: 0.525\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7112 | test accuracy: 0.519\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6801 | test accuracy: 0.512\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7278 | test accuracy: 0.495\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6774 | test accuracy: 0.515\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7464 | test accuracy: 0.519\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7071 | test accuracy: 0.498\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6826 | test accuracy: 0.508\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5948 | test accuracy: 0.519\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6420 | test accuracy: 0.508\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6974 | test accuracy: 0.505\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7079 | test accuracy: 0.498\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6636 | test accuracy: 0.495\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6140 | test accuracy: 0.512\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7950 | test accuracy: 0.492\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6252 | test accuracy: 0.492\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5886 | test accuracy: 0.508\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7527 | test accuracy: 0.495\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6438 | test accuracy: 0.502\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6822 | test accuracy: 0.508\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7000 | test accuracy: 0.485\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6503 | test accuracy: 0.485\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7028 | test accuracy: 0.495\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6815 | test accuracy: 0.519\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6975 | test accuracy: 0.502\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6993 | test accuracy: 0.495\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6445 | test accuracy: 0.512\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7113 | test accuracy: 0.495\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6401 | test accuracy: 0.505\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6601 | test accuracy: 0.502\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6753 | test accuracy: 0.502\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6854 | test accuracy: 0.505\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6377 | test accuracy: 0.508\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.5901 | test accuracy: 0.505\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6636 | test accuracy: 0.498\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6347 | test accuracy: 0.492\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7582 | test accuracy: 0.502\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7812 | test accuracy: 0.502\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7085 | test accuracy: 0.502\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6847 | test accuracy: 0.498\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7896 | test accuracy: 0.498\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6803 | test accuracy: 0.502\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7282 | test accuracy: 0.505\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6478 | test accuracy: 0.512\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6089 | test accuracy: 0.508\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7433 | test accuracy: 0.502\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6738 | test accuracy: 0.502\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6755 | test accuracy: 0.502\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6975 | test accuracy: 0.505\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7784 | test accuracy: 0.502\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7029 | test accuracy: 0.498\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6212 | test accuracy: 0.502\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7660 | test accuracy: 0.502\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7313 | test accuracy: 0.495\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7297 | test accuracy: 0.498\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7389 | test accuracy: 0.492\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6512 | test accuracy: 0.502\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6664 | test accuracy: 0.495\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6480 | test accuracy: 0.505\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6866 | test accuracy: 0.502\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7198 | test accuracy: 0.498\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6189 | test accuracy: 0.498\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5931 | test accuracy: 0.498\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.8069 | test accuracy: 0.498\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7053 | test accuracy: 0.502\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6711 | test accuracy: 0.502\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6971 | test accuracy: 0.492\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7113 | test accuracy: 0.498\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7184 | test accuracy: 0.492\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7563 | test accuracy: 0.495\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7507 | test accuracy: 0.502\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.5930 | test accuracy: 0.492\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6491 | test accuracy: 0.502\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6699 | test accuracy: 0.502\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6907 | test accuracy: 0.502\n",
            "total time:  66.78236257700019\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999797821044922.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.3461782932281494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.48763618682112014 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20539641380310059.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.354461669921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.41620571315288546 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21513652801513672.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.3549044132232666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.382088645866939 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20790672302246094.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3488953113555908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3623183505875724 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20415854454040527.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.34787893295288086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.34889064346041 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2199547290802002.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3678724765777588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3414918678147452 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2015700340270996.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.345505952835083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.33627496915204186 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19933128356933594.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3472151756286621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3325480146067483 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21055912971496582.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35489392280578613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3296386139733451 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20114898681640625.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34203171730041504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.32760015768664225 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20893478393554688.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3502776622772217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3252024037497384 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20855474472045898.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35158538818359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3236856358391898 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20199799537658691.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3428535461425781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32226538232394625 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016611099243164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3469219207763672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32145907027380805 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2146468162536621.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3579292297363281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3201170563697815 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21291732788085938.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36046862602233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.31964701541832513 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20881295204162598.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35049986839294434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31954888658864156 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20733261108398438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.344677209854126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.31869169984545026 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20203757286071777.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35007786750793457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31812145624841964 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20495080947875977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3618428707122803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3180786009345736 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21302318572998047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35984158515930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31744203524930137 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20792484283447266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35085272789001465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3172937682696751 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21628999710083008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.37435054779052734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3167800843715668 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20239782333374023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3444075584411621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3163093277386257 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20018839836120605.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34549760818481445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31634544219289507 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19881486892700195.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34947824478149414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3159917290721621 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20204877853393555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34793829917907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3158136286905834 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21326780319213867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35950565338134766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31573504975863864 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2258141040802002.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36841917037963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31565210350922174 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20228838920593262.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3456873893737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31542190951960425 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2003781795501709.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3467135429382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3153664644275393 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22022581100463867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35706663131713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31516530428613937 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19818568229675293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.336212158203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3150091073342732 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20247459411621094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34181904792785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3152217013495309 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20505142211914062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3480358123779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.314893524987357 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21089601516723633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36069202423095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3146082077707563 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20950627326965332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537330627441406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3147653618029186 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21603989601135254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3579246997833252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.314791853087289 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19968008995056152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3395814895629883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31455822459289007 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2037487030029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34786558151245117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3145024401800973 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20977187156677246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35248637199401855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3143229825156076 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20032906532287598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35280442237854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31428141381059377 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20911002159118652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3576803207397461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3144373565912247 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2036733627319336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34427881240844727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.314313200541905 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20107626914978027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449695110321045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31431071332522803 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19851970672607422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484768867492676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31441805703299386 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010056972503662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3400235176086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31433785131999425 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19872641563415527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3434569835662842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3139905550650188 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20673322677612305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554191589355469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3142610362597874 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2025003433227539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34747958183288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31403418055602483 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20451760292053223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35094738006591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3140751110655921 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21247601509094238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3630402088165283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3140929205077035 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2025127410888672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423316478729248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31393371181828633 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024827003479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34256887435913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3139719426631927 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20377564430236816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510463237762451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31392750229154315 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19933581352233887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3424510955810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3138507821730205 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090134620666504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3555920124053955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3139919293778283 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21953868865966797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3624687194824219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31390351142202105 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20034313201904297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.342573881149292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31374739706516264 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20687413215637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527555465698242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31385630411761145 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20929455757141113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356630802154541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138188677174704 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20719075202941895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481924533843994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138301623719079 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20224571228027344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454017639160156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138464548758098 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20895910263061523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35031628608703613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138010399682181 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2022712230682373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34522509574890137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3137202458722251 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20551371574401855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457612991333008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137073112385614 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21019792556762695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35707521438598633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31369453838893346 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20127344131469727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3456602096557617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3136655441352299 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20312261581420898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442239761352539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3136752626725606 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066962718963623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34533047676086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137160586459296 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2052006721496582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521900177001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3136105639593942 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20637917518615723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.346637487411499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31368769790445056 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2113785743713379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35923242568969727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31363479707922254 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042217254638672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3612666130065918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31366022186619896 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2094876766204834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35901451110839844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31361567633492604 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20256733894348145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436131477355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31366682733808243 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20519065856933594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3547332286834717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3135702107633863 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20385289192199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506655693054199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136481089251382 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20018219947814941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3440384864807129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31357568885598863 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20238351821899414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455934524536133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3135701324258532 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20383358001708984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3541903495788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313599494099617 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20905590057373047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35503268241882324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31358020901679995 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20186328887939453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3482668399810791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3135393559932709 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21117377281188965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35909414291381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31353422658784047 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055225372314453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540971279144287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31354376333100453 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047252655029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475363254547119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31354519639696393 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21493935585021973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35804224014282227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31356980502605436 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20523452758789062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3512117862701416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31358279287815094 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21197938919067383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3638725280761719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31355338479791367 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106778621673584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3597540855407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135696606976645 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20320343971252441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432490825653076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135070477213178 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19939804077148438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501458168029785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135269318308149 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21046066284179688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34937167167663574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31357016350541794 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2050316333770752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35138845443725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31350306825978413 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982588768005371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33948206901550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135076893227441 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21486783027648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3643794059753418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31353849044867926 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19815778732299805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3365647792816162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3134862618786948 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19288301467895508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3341093063354492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3134928068944386 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21358418464660645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35649728775024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.313496071100235 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19975495338439941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34407806396484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31349798015185765 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20225119590759277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3469059467315674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.313479693872588 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21290230751037598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36069679260253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31346862699304306 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20856976509094238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35010266304016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3134843234504972 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20457220077514648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3535468578338623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134613926921572 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20839524269104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35227298736572266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31348669826984404 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.203721284866333.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455071449279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134341904095241 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20897603034973145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.365842342376709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134697254214968 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2075955867767334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350614070892334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31346190656934464 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20842480659484863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516676425933838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31343368547303335 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19979166984558105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514688014984131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134511487824576 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21114230155944824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3614687919616699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134523532220295 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20594406127929688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3466460704803467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31344926314694543 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20355510711669922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34964537620544434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134380634341921 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035527229309082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452873229980469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134571875844683 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20752263069152832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35099029541015625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134153515100479 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21564483642578125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36049723625183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31341625452041627 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2073826789855957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35057902336120605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134340733289719 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20353078842163086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34443092346191406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134352420057569 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20826005935668945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556482791900635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31344928528581345 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051408290863037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3431057929992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31342034935951235 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20061564445495605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34258604049682617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134455961840493 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21517086029052734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35727500915527344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31341327003070285 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19787120819091797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33353686332702637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134202199322837 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19881200790405273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446371555328369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134058743715286 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20501184463500977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34844303131103516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31340622092996323 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20313572883605957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3433411121368408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.313422395501818 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21248173713684082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35254478454589844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134240938084466 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148430347442627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37612366676330566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3133935958147049 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6742 | test accuracy: 0.481\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6825 | test accuracy: 0.515\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7285 | test accuracy: 0.488\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7323 | test accuracy: 0.508\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7751 | test accuracy: 0.515\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5090 | test accuracy: 0.515\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4889 | test accuracy: 0.515\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6945 | test accuracy: 0.522\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7719 | test accuracy: 0.485\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6397 | test accuracy: 0.515\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6642 | test accuracy: 0.505\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6867 | test accuracy: 0.522\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5860 | test accuracy: 0.502\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6265 | test accuracy: 0.515\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7179 | test accuracy: 0.519\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6798 | test accuracy: 0.542\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7442 | test accuracy: 0.519\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6388 | test accuracy: 0.532\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5862 | test accuracy: 0.525\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7288 | test accuracy: 0.522\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7639 | test accuracy: 0.515\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7021 | test accuracy: 0.512\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7351 | test accuracy: 0.515\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7639 | test accuracy: 0.512\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7465 | test accuracy: 0.549\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6666 | test accuracy: 0.535\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7115 | test accuracy: 0.502\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6736 | test accuracy: 0.488\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6671 | test accuracy: 0.492\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6512 | test accuracy: 0.539\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7393 | test accuracy: 0.522\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6880 | test accuracy: 0.549\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6614 | test accuracy: 0.525\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7446 | test accuracy: 0.529\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7388 | test accuracy: 0.535\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7064 | test accuracy: 0.532\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6857 | test accuracy: 0.539\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7316 | test accuracy: 0.539\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6325 | test accuracy: 0.535\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6349 | test accuracy: 0.529\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6168 | test accuracy: 0.532\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6295 | test accuracy: 0.532\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6553 | test accuracy: 0.532\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7569 | test accuracy: 0.532\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6999 | test accuracy: 0.532\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6379 | test accuracy: 0.535\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6049 | test accuracy: 0.535\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6905 | test accuracy: 0.529\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7506 | test accuracy: 0.535\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6863 | test accuracy: 0.535\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6363 | test accuracy: 0.535\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6674 | test accuracy: 0.529\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7343 | test accuracy: 0.532\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7363 | test accuracy: 0.529\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6849 | test accuracy: 0.535\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7138 | test accuracy: 0.535\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6884 | test accuracy: 0.535\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.5413 | test accuracy: 0.529\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7798 | test accuracy: 0.532\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6650 | test accuracy: 0.529\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6518 | test accuracy: 0.532\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6919 | test accuracy: 0.529\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6724 | test accuracy: 0.539\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6906 | test accuracy: 0.529\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6414 | test accuracy: 0.532\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6651 | test accuracy: 0.532\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6569 | test accuracy: 0.529\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6543 | test accuracy: 0.532\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7180 | test accuracy: 0.532\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5673 | test accuracy: 0.532\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6422 | test accuracy: 0.532\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7545 | test accuracy: 0.532\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8681 | test accuracy: 0.532\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8201 | test accuracy: 0.529\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6640 | test accuracy: 0.529\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7064 | test accuracy: 0.532\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6269 | test accuracy: 0.532\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6925 | test accuracy: 0.529\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6663 | test accuracy: 0.532\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7052 | test accuracy: 0.525\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6473 | test accuracy: 0.525\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6170 | test accuracy: 0.532\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6226 | test accuracy: 0.529\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6555 | test accuracy: 0.529\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.5591 | test accuracy: 0.525\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7745 | test accuracy: 0.525\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6762 | test accuracy: 0.525\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6709 | test accuracy: 0.525\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6862 | test accuracy: 0.525\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6891 | test accuracy: 0.532\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7835 | test accuracy: 0.525\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7217 | test accuracy: 0.532\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7343 | test accuracy: 0.525\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6540 | test accuracy: 0.529\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6191 | test accuracy: 0.529\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7067 | test accuracy: 0.522\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7307 | test accuracy: 0.525\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6783 | test accuracy: 0.529\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7560 | test accuracy: 0.522\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7017 | test accuracy: 0.522\n",
            "total time:  66.53279583400035\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7073 | test accuracy: 0.475\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6577 | test accuracy: 0.515\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7126 | test accuracy: 0.495\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7171 | test accuracy: 0.539\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7637 | test accuracy: 0.515\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4948 | test accuracy: 0.515\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4972 | test accuracy: 0.515\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7112 | test accuracy: 0.502\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7740 | test accuracy: 0.485\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6339 | test accuracy: 0.519\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6903 | test accuracy: 0.478\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6930 | test accuracy: 0.525\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6038 | test accuracy: 0.545\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6221 | test accuracy: 0.515\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7124 | test accuracy: 0.522\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6840 | test accuracy: 0.522\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7378 | test accuracy: 0.525\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6419 | test accuracy: 0.529\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5960 | test accuracy: 0.525\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7329 | test accuracy: 0.539\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7701 | test accuracy: 0.515\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7060 | test accuracy: 0.522\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7455 | test accuracy: 0.522\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7673 | test accuracy: 0.519\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7380 | test accuracy: 0.529\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6701 | test accuracy: 0.522\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7099 | test accuracy: 0.522\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6762 | test accuracy: 0.475\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6698 | test accuracy: 0.502\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6554 | test accuracy: 0.535\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7500 | test accuracy: 0.532\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6837 | test accuracy: 0.539\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6440 | test accuracy: 0.539\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7454 | test accuracy: 0.529\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7323 | test accuracy: 0.535\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7136 | test accuracy: 0.542\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6687 | test accuracy: 0.539\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7272 | test accuracy: 0.539\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6383 | test accuracy: 0.532\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6421 | test accuracy: 0.539\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6204 | test accuracy: 0.542\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6271 | test accuracy: 0.535\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6613 | test accuracy: 0.529\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7554 | test accuracy: 0.542\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7087 | test accuracy: 0.539\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6400 | test accuracy: 0.535\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6085 | test accuracy: 0.535\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6933 | test accuracy: 0.539\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7514 | test accuracy: 0.535\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6835 | test accuracy: 0.532\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6404 | test accuracy: 0.539\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6673 | test accuracy: 0.539\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7274 | test accuracy: 0.539\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7264 | test accuracy: 0.535\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6735 | test accuracy: 0.532\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7156 | test accuracy: 0.535\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6938 | test accuracy: 0.535\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.5464 | test accuracy: 0.535\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7827 | test accuracy: 0.535\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6719 | test accuracy: 0.532\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6512 | test accuracy: 0.532\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6866 | test accuracy: 0.535\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6687 | test accuracy: 0.529\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6813 | test accuracy: 0.539\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6358 | test accuracy: 0.535\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6711 | test accuracy: 0.539\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6555 | test accuracy: 0.539\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6515 | test accuracy: 0.539\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7126 | test accuracy: 0.539\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5765 | test accuracy: 0.529\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6273 | test accuracy: 0.532\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7651 | test accuracy: 0.529\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8736 | test accuracy: 0.532\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8196 | test accuracy: 0.532\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6615 | test accuracy: 0.539\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7072 | test accuracy: 0.532\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6271 | test accuracy: 0.539\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7022 | test accuracy: 0.539\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6593 | test accuracy: 0.539\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7029 | test accuracy: 0.539\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6522 | test accuracy: 0.539\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6144 | test accuracy: 0.539\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6242 | test accuracy: 0.532\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6422 | test accuracy: 0.532\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.5566 | test accuracy: 0.529\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7655 | test accuracy: 0.535\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6882 | test accuracy: 0.535\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6859 | test accuracy: 0.529\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6863 | test accuracy: 0.529\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6878 | test accuracy: 0.539\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7910 | test accuracy: 0.529\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7167 | test accuracy: 0.539\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7366 | test accuracy: 0.529\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6566 | test accuracy: 0.532\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6139 | test accuracy: 0.535\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7157 | test accuracy: 0.532\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7223 | test accuracy: 0.539\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6925 | test accuracy: 0.539\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7539 | test accuracy: 0.539\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7079 | test accuracy: 0.539\n",
            "total time:  66.60650512899974\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19576525688171387.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.3465688228607178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5694929855210441 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20524096488952637.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.34659266471862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4728758147784642 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19985342025756836.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3469350337982178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.41828068963118964 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20320820808410645.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.34661293029785156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3888738623687199 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20592188835144043.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.34776949882507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3702868504183633 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20569705963134766.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.35878920555114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.35845779478549955 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20602703094482422.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35126781463623047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.35096432736941746 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20732569694519043.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3465585708618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34348683782986233 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2094883918762207.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3564581871032715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3387709779398782 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20400357246398926.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34773969650268555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.334546531523977 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20191097259521484.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3459815979003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33347739619868144 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21535634994506836.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3634195327758789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3302646096263613 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20870757102966309.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35481715202331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32691341936588286 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21030902862548828.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3552713394165039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3261778895344053 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22044014930725098.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3693249225616455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3251476441110883 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21206307411193848.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3564748764038086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3230724219764982 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21042275428771973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36057209968566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3222147856439863 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21618938446044922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3585329055786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32143253087997437 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20740485191345215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34903526306152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32111857278006417 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047886848449707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3474869728088379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3204633959702083 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21480655670166016.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3590543270111084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3194600799254009 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21440649032592773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3650658130645752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3189345615250724 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21027636528015137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3747437000274658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3187751110110964 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20966744422912598.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3575890064239502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3183359358991895 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20609736442565918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3476548194885254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3178278386592865 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.210554838180542.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.363785982131958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3178065457514354 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20724201202392578.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34859538078308105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3172697280134473 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19955182075500488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3380718231201172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31675917889390676 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20032334327697754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.360919713973999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3167312438998904 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20478534698486328.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34617090225219727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3166105857917241 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20517468452453613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34690284729003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3164660730532238 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21371960639953613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3607184886932373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3162101085696902 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21287965774536133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3537733554840088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3157498781170164 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20310664176940918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3516106605529785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3158540380852563 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21461796760559082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3562760353088379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31561427627291 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090439796447754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3525245189666748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3155115119048527 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20153093338012695.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34410858154296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3154587698834283 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21875405311584473.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3700087070465088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3153348169156483 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027122974395752.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3453350067138672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31512321404048377 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20284104347229004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35427021980285645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3151423488344465 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21278095245361328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35605502128601074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31498868933745794 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21437764167785645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3579428195953369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3149787949664252 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21582746505737305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36896252632141113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3148697916950498 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21152901649475098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35674548149108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31469575890472956 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20501112937927246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35144925117492676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3148022502660751 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2169780731201172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37552738189697266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3146017359835761 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21667861938476562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3632216453552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31452982681138175 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20949506759643555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3578183650970459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31444706874234335 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22296428680419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3843543529510498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3145309392895017 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21053266525268555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35761308670043945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3144313871860504 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21568751335144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3610827922821045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31432782198701587 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21601200103759766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36147332191467285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31438812485763007 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20995688438415527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583567142486572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3142933530466897 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2122969627380371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3643033504486084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31429266929626465 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2252030372619629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3710775375366211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31417984110968455 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20746302604675293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349442720413208\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31406538912228177 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2099902629852295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35266828536987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3141783492905753 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21203064918518066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521277904510498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31407868734427863 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032947540283203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516228199005127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.314030208332198 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21033692359924316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36473631858825684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31402550169399807 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20374059677124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3504197597503662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.314018463236945 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035837173461914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34656596183776855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31406123851026807 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20744872093200684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36260485649108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139430101428713 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20466184616088867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34293127059936523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31396328253405437 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20148587226867676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34452366828918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31400001772812436 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20239019393920898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3525698184967041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139282418148858 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20012474060058594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33947205543518066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137995170695441 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20811223983764648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35543155670166016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139444934470313 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017972469329834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34956836700439453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138767463820321 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20215916633605957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.339144229888916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31377371975353785 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20130658149719238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33901143074035645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137484580278397 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20002222061157227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35061073303222656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137872917311532 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20221185684204102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34175586700439453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137599779026849 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20419049263000488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3460655212402344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31372977537768226 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20361828804016113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35118985176086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137478287730898 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20241141319274902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34531474113464355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137530016047614 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2118821144104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36328577995300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137310892343521 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21897673606872559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3788783550262451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31371589430740904 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19710230827331543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33945560455322266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137009641953877 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20688176155090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3530080318450928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136813440493175 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21168255805969238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35287022590637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136895601238523 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20609283447265625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35776734352111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137181788682938 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2163381576538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36646366119384766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136850642306464 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2175312042236328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35628199577331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136640765837261 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20711374282836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534247875213623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31365435208593095 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.202681303024292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34152865409851074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31359006336757117 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21564817428588867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36317944526672363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136545364345823 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2146928310394287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35740232467651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31361300476959775 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20821452140808105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3609306812286377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31361534127167295 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21172308921813965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516716957092285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31358523539134436 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20975279808044434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35602450370788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135374678032739 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21121501922607422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3638789653778076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136271932295391 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19841217994689941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35195326805114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.313567082796778 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21117568016052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554215431213379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31358609752995625 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20188045501708984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36596107482910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31356296283858165 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055511474609375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520839214324951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135394390140261 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014932632446289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446168899536133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358914588178904 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21377873420715332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35598230361938477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135622186320169 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20873785018920898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3555877208709717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31354622287409645 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20503973960876465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355424165725708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135312625340053 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21041131019592285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3474888801574707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135352807385581 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20757818222045898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487513065338135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.313550826055663 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20203042030334473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3438124656677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31351754452501024 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22417044639587402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3744802474975586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134999705212457 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21265435218811035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3587615489959717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31351437738963533 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20706725120544434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34935927391052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31349583779062545 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21411442756652832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540492057800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134891995361873 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20687532424926758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540349006652832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31347263668264663 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20120906829833984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.347853422164917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.313515248468944 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20219826698303223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487548828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31351665471281326 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20765089988708496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34910154342651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348078676632474 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2011573314666748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.354572057723999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134768864938191 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21296191215515137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3564643859863281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134662091732025 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19776320457458496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33765339851379395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31345608276980264 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055370807647705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35649871826171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31344234006745475 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2040243148803711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35491132736206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134837222950799 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21892356872558594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3639097213745117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134623352970396 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2080402374267578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35866522789001465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31347583830356596 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20617127418518066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478708267211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347532442637854 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020587921142578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3427441120147705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134569321359907 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21326661109924316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3607354164123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343381362301964 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20860815048217773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34965085983276367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344055363110135 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20496821403503418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520786762237549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31345813146659307 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20853948593139648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34796929359436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343639578138077 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20318198204040527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3536376953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134247669151851 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20282888412475586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34598851203918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134547024965286 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21370697021484375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3624458312988281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134494138615472 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2065870761871338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35039687156677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134238962616239 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6857 | test accuracy: 0.458\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7428 | test accuracy: 0.529\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8897 | test accuracy: 0.515\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6718 | test accuracy: 0.522\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7286 | test accuracy: 0.515\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6819 | test accuracy: 0.522\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7325 | test accuracy: 0.525\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6457 | test accuracy: 0.515\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6988 | test accuracy: 0.481\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7250 | test accuracy: 0.519\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6283 | test accuracy: 0.515\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8107 | test accuracy: 0.515\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7489 | test accuracy: 0.522\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6717 | test accuracy: 0.495\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7005 | test accuracy: 0.515\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7548 | test accuracy: 0.488\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7175 | test accuracy: 0.539\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7379 | test accuracy: 0.512\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6688 | test accuracy: 0.515\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6813 | test accuracy: 0.519\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6783 | test accuracy: 0.502\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6574 | test accuracy: 0.525\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6513 | test accuracy: 0.522\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7270 | test accuracy: 0.502\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7159 | test accuracy: 0.535\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7704 | test accuracy: 0.502\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6276 | test accuracy: 0.522\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6556 | test accuracy: 0.515\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7210 | test accuracy: 0.525\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7159 | test accuracy: 0.519\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7516 | test accuracy: 0.519\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6200 | test accuracy: 0.522\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5984 | test accuracy: 0.519\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7333 | test accuracy: 0.515\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7259 | test accuracy: 0.508\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6076 | test accuracy: 0.519\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6638 | test accuracy: 0.512\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5835 | test accuracy: 0.519\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6189 | test accuracy: 0.512\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6821 | test accuracy: 0.519\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7749 | test accuracy: 0.519\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7892 | test accuracy: 0.519\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6312 | test accuracy: 0.525\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6867 | test accuracy: 0.515\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5873 | test accuracy: 0.519\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7158 | test accuracy: 0.519\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6384 | test accuracy: 0.512\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6330 | test accuracy: 0.508\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7144 | test accuracy: 0.515\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6154 | test accuracy: 0.515\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7237 | test accuracy: 0.512\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7019 | test accuracy: 0.512\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6223 | test accuracy: 0.508\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7883 | test accuracy: 0.515\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6205 | test accuracy: 0.515\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6695 | test accuracy: 0.515\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7182 | test accuracy: 0.522\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7889 | test accuracy: 0.522\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7337 | test accuracy: 0.512\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6973 | test accuracy: 0.512\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7075 | test accuracy: 0.512\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6976 | test accuracy: 0.519\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6747 | test accuracy: 0.508\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6376 | test accuracy: 0.519\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5934 | test accuracy: 0.512\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6493 | test accuracy: 0.508\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6522 | test accuracy: 0.512\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6971 | test accuracy: 0.512\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6740 | test accuracy: 0.515\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6242 | test accuracy: 0.512\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6594 | test accuracy: 0.512\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6599 | test accuracy: 0.512\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6730 | test accuracy: 0.512\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6407 | test accuracy: 0.519\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6764 | test accuracy: 0.508\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7383 | test accuracy: 0.515\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7866 | test accuracy: 0.508\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6866 | test accuracy: 0.508\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6596 | test accuracy: 0.512\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6711 | test accuracy: 0.515\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6723 | test accuracy: 0.508\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6568 | test accuracy: 0.512\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7275 | test accuracy: 0.512\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6763 | test accuracy: 0.512\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6845 | test accuracy: 0.508\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6179 | test accuracy: 0.508\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7727 | test accuracy: 0.512\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7329 | test accuracy: 0.508\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6446 | test accuracy: 0.519\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7656 | test accuracy: 0.508\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6836 | test accuracy: 0.512\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.8752 | test accuracy: 0.505\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7525 | test accuracy: 0.508\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6676 | test accuracy: 0.508\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6326 | test accuracy: 0.508\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6366 | test accuracy: 0.505\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7303 | test accuracy: 0.505\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7138 | test accuracy: 0.508\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6466 | test accuracy: 0.505\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.5987 | test accuracy: 0.502\n",
            "total time:  66.34203871599993\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6728 | test accuracy: 0.529\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7239 | test accuracy: 0.502\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.9105 | test accuracy: 0.515\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6654 | test accuracy: 0.515\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6968 | test accuracy: 0.515\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6898 | test accuracy: 0.522\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7366 | test accuracy: 0.519\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6447 | test accuracy: 0.519\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7125 | test accuracy: 0.495\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7135 | test accuracy: 0.488\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6350 | test accuracy: 0.515\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8105 | test accuracy: 0.522\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7605 | test accuracy: 0.535\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6646 | test accuracy: 0.498\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6758 | test accuracy: 0.519\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7677 | test accuracy: 0.512\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7107 | test accuracy: 0.542\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7300 | test accuracy: 0.522\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6638 | test accuracy: 0.519\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6868 | test accuracy: 0.532\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6742 | test accuracy: 0.508\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6472 | test accuracy: 0.529\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6562 | test accuracy: 0.522\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7078 | test accuracy: 0.481\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7271 | test accuracy: 0.539\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7736 | test accuracy: 0.492\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6274 | test accuracy: 0.522\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6719 | test accuracy: 0.522\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7191 | test accuracy: 0.529\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7097 | test accuracy: 0.532\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7557 | test accuracy: 0.542\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6324 | test accuracy: 0.542\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6059 | test accuracy: 0.539\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7364 | test accuracy: 0.539\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7330 | test accuracy: 0.525\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5952 | test accuracy: 0.539\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6640 | test accuracy: 0.532\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5871 | test accuracy: 0.522\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6180 | test accuracy: 0.519\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6692 | test accuracy: 0.539\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7739 | test accuracy: 0.535\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7837 | test accuracy: 0.522\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6238 | test accuracy: 0.532\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6847 | test accuracy: 0.522\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5914 | test accuracy: 0.525\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7156 | test accuracy: 0.535\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6261 | test accuracy: 0.532\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6650 | test accuracy: 0.529\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7330 | test accuracy: 0.522\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6323 | test accuracy: 0.522\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7176 | test accuracy: 0.535\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7112 | test accuracy: 0.539\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6180 | test accuracy: 0.525\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.8008 | test accuracy: 0.522\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6172 | test accuracy: 0.545\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6818 | test accuracy: 0.519\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7235 | test accuracy: 0.525\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7654 | test accuracy: 0.535\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7358 | test accuracy: 0.535\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6937 | test accuracy: 0.525\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7244 | test accuracy: 0.519\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6910 | test accuracy: 0.525\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6697 | test accuracy: 0.525\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6507 | test accuracy: 0.539\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5829 | test accuracy: 0.539\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6670 | test accuracy: 0.519\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6537 | test accuracy: 0.519\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6993 | test accuracy: 0.525\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6764 | test accuracy: 0.539\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6233 | test accuracy: 0.519\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6748 | test accuracy: 0.519\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6605 | test accuracy: 0.519\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6664 | test accuracy: 0.525\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6539 | test accuracy: 0.529\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6850 | test accuracy: 0.525\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7291 | test accuracy: 0.532\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7951 | test accuracy: 0.519\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6948 | test accuracy: 0.519\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6525 | test accuracy: 0.539\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6585 | test accuracy: 0.532\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6737 | test accuracy: 0.522\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6702 | test accuracy: 0.522\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7355 | test accuracy: 0.529\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6790 | test accuracy: 0.535\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6710 | test accuracy: 0.529\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6029 | test accuracy: 0.522\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7751 | test accuracy: 0.535\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7260 | test accuracy: 0.522\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6358 | test accuracy: 0.532\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7592 | test accuracy: 0.522\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6824 | test accuracy: 0.532\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.8597 | test accuracy: 0.525\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7542 | test accuracy: 0.532\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6819 | test accuracy: 0.529\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6356 | test accuracy: 0.529\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6409 | test accuracy: 0.525\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7107 | test accuracy: 0.529\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7085 | test accuracy: 0.525\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6582 | test accuracy: 0.522\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6003 | test accuracy: 0.529\n",
            "total time:  66.43477021699982\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19449758529663086.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.34812235832214355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6845833889075688 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20117402076721191.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.34999942779541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.544056783403669 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20290040969848633.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.3447105884552002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4620571323803493 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2138073444366455.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3520469665527344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41598359261240275 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20567107200622559.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3471207618713379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39066445103713443 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2012174129486084.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.33815693855285645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37308792982782635 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064967155456543.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3448033332824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3620352357625961 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19970297813415527.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.34429287910461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3525644136326654 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20851349830627441.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3539559841156006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3468327820301056 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148118019104004.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3539891242980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3417283424309322 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20607733726501465.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34969639778137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3372294736760003 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20642685890197754.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3485562801361084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33450805587427956 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21679925918579102.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36762499809265137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3314181991985866 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21587872505187988.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3612487316131592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3297704126153673 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20653033256530762.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3470160961151123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32797649843352183 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2091684341430664.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34877872467041016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32618830033711027 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20233559608459473.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3512847423553467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3253363881792341 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20711541175842285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3500332832336426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3239842155150005 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20145130157470703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34642601013183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.323105491059167 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20439982414245605.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3461780548095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3223240677799497 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20484638214111328.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35519862174987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32148381514208657 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20918750762939453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35567760467529297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3201751687697002 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20647287368774414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34820985794067383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3199971944093704 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20124387741088867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35494399070739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3197288521698543 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20805859565734863.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3506500720977783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3191127521651132 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20318317413330078.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35228586196899414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3189222263438361 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2005016803741455.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35373663902282715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3182070178644998 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20769333839416504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35088086128234863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.318194283757891 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20540857315063477.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34656810760498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31743732477937425 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20104098320007324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3582477569580078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31726069492953163 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2083296775817871.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3539586067199707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3172142803668976 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2086644172668457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3531498908996582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31671516256672994 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21587800979614258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3610377311706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3168013457741056 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20736956596374512.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35280895233154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3164110281637737 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20261573791503906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3436086177825928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3162304197038923 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21092867851257324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3583545684814453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3160526126623154 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20853686332702637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3493826389312744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3159322874886649 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20329737663269043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34140443801879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3157422112567084 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106165885925293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3529820442199707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31567186713218687 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19971585273742676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3413376808166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3155251498733248 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2028193473815918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3445761203765869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.315333069222314 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21290254592895508.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3551921844482422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3151388019323349 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20924901962280273.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.349353551864624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3152015013354165 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20358729362487793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34882211685180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3150937101670674 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21715235710144043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3628716468811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3150514760187694 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2096726894378662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498070240020752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149646601506642 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20891046524047852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3650641441345215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31475023542131697 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20848751068115234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565938472747803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3146841619695936 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20986604690551758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3525118827819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31475001871585845 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20932865142822266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598651885986328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3146644570997783 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1990668773651123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3427600860595703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31462412646838595 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2015080451965332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34201884269714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31448843606880733 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20219087600708008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350844144821167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3145542302301952 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20392894744873047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34508538246154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144860919032778 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078688144683838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35352206230163574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31443451259817395 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21033954620361328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.364300012588501\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31434837452002934 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2163248062133789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3612680435180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31439275060381205 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19939684867858887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3474254608154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3143663074289049 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21293115615844727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35600972175598145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31424343245370046 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20659804344177246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3488614559173584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31422116841588704 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20122861862182617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3405015468597412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141311266592571 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2114865779876709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35326695442199707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3142479581492288 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20091748237609863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3419773578643799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31406458233083995 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20504212379455566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34659481048583984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140868838344302 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21688246726989746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3693408966064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31403426698275977 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20220661163330078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3427426815032959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140611342021397 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20869660377502441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35144472122192383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139941930770874 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20679044723510742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34629034996032715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31403701135090417 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20165586471557617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34242749214172363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31390880388872966 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20089983940124512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34161829948425293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31392764236245835 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078685760498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3509244918823242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31400014332362586 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20459771156311035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423194885253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3139037277017321 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039504051208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34604930877685547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138474805014474 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20919299125671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454461097717285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3139647735016687 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20129823684692383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3381223678588867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.313799518772534 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19874811172485352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3363006114959717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31375109936509815 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20823907852172852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.346768856048584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.313779735990933 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2005467414855957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33972930908203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31379807250840325 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19658994674682617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33753371238708496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137439795902797 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21143031120300293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3489809036254883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.313758196575301 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1994631290435791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3395683765411377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136989874499185 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19855904579162598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3382847309112549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31372024714946745 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20975947380065918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34916114807128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31371650653226035 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2083895206451416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36351442337036133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31371896266937255 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049884796142578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35549235343933105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137056180409023 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20678162574768066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34621739387512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136897589479174 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20749998092651367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35017991065979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31367962530681065 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20604467391967773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36360740661621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3137047495160784 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20463991165161133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34625697135925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136114576033184 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20653390884399414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537743091583252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.313638607944761 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20586442947387695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.358966588973999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31368938088417053 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049717903137207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481481075286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31363927721977236 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035408973693848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35262084007263184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31363823456423623 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20983672142028809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34856200218200684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136749518769128 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19873905181884766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3369152545928955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31360451536519185 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19480586051940918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34496498107910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135881530387061 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2168593406677246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3552570343017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.313573043687003 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069413661956787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486752510070801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31362035487379347 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20508503913879395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34909558296203613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135537815945489 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21940207481384277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598923683166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135296033961432 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20496511459350586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537018299102783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31353807960237773 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20888614654541016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35094451904296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135660094874246 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22048091888427734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.359605073928833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31353791185787744 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071390151977539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506302833557129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31353803617613657 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20067048072814941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3417031764984131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135366699525288 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21621274948120117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36710071563720703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31355073962892804 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982743740081787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34229087829589844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31351939269474577 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19902682304382324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34162044525146484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135182538202831 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20989632606506348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34911370277404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135171123913356 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2033083438873291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3424208164215088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134955108165741 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19497060775756836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33377957344055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135156967810222 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21178483963012695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3525063991546631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135183551481792 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21263504028320312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585391044616699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31350466183253695 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2065286636352539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36687564849853516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31348469853401184 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20215296745300293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452184200286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31351236573287417 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035122871398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546605110168457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31349669226578303 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2091212272644043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35688066482543945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347138030188426 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020580768585205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3405747413635254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134845823049545 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20999455451965332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520338535308838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31348058794225964 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010328769683838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34857606887817383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134723148175648 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19968199729919434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454878330230713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134958663157054 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20581459999084473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436238765716553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134707088981356 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19574451446533203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3411445617675781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31347783505916593 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016160488128662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3406858444213867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31348278735365187 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.204909086227417.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3488037586212158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31346836260386873 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20007681846618652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34861230850219727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134531663996833 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19530797004699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3411874771118164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134454254593168 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19806408882141113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34037208557128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31346287599631717 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6775 | test accuracy: 0.475\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7086 | test accuracy: 0.519\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6541 | test accuracy: 0.535\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7490 | test accuracy: 0.485\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7495 | test accuracy: 0.465\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6853 | test accuracy: 0.512\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7047 | test accuracy: 0.502\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7267 | test accuracy: 0.515\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6511 | test accuracy: 0.532\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6904 | test accuracy: 0.488\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6633 | test accuracy: 0.508\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6780 | test accuracy: 0.465\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8188 | test accuracy: 0.515\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7886 | test accuracy: 0.508\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5860 | test accuracy: 0.522\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6266 | test accuracy: 0.519\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5802 | test accuracy: 0.515\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7798 | test accuracy: 0.505\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7428 | test accuracy: 0.515\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8069 | test accuracy: 0.475\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7294 | test accuracy: 0.512\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6603 | test accuracy: 0.515\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7512 | test accuracy: 0.515\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6261 | test accuracy: 0.529\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7227 | test accuracy: 0.515\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8482 | test accuracy: 0.522\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6619 | test accuracy: 0.498\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8052 | test accuracy: 0.529\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6966 | test accuracy: 0.505\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7961 | test accuracy: 0.532\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6912 | test accuracy: 0.529\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6179 | test accuracy: 0.532\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7798 | test accuracy: 0.535\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6126 | test accuracy: 0.532\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6842 | test accuracy: 0.529\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6765 | test accuracy: 0.525\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7113 | test accuracy: 0.532\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7308 | test accuracy: 0.522\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7102 | test accuracy: 0.529\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5248 | test accuracy: 0.535\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7194 | test accuracy: 0.532\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7837 | test accuracy: 0.522\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6735 | test accuracy: 0.529\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6026 | test accuracy: 0.532\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7287 | test accuracy: 0.519\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7452 | test accuracy: 0.525\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7073 | test accuracy: 0.535\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6508 | test accuracy: 0.519\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6380 | test accuracy: 0.519\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7773 | test accuracy: 0.525\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6978 | test accuracy: 0.532\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7431 | test accuracy: 0.529\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5998 | test accuracy: 0.525\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.8850 | test accuracy: 0.525\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6856 | test accuracy: 0.525\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6886 | test accuracy: 0.525\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6457 | test accuracy: 0.525\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6840 | test accuracy: 0.515\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7377 | test accuracy: 0.515\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6977 | test accuracy: 0.522\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7400 | test accuracy: 0.519\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7391 | test accuracy: 0.515\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6789 | test accuracy: 0.529\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7146 | test accuracy: 0.532\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6821 | test accuracy: 0.532\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6833 | test accuracy: 0.522\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6459 | test accuracy: 0.519\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7662 | test accuracy: 0.522\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6138 | test accuracy: 0.525\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7182 | test accuracy: 0.515\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7171 | test accuracy: 0.525\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7590 | test accuracy: 0.512\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6091 | test accuracy: 0.525\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6986 | test accuracy: 0.529\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6872 | test accuracy: 0.525\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6919 | test accuracy: 0.512\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6344 | test accuracy: 0.519\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7212 | test accuracy: 0.522\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7840 | test accuracy: 0.519\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7392 | test accuracy: 0.522\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7349 | test accuracy: 0.508\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7375 | test accuracy: 0.515\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7102 | test accuracy: 0.515\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6021 | test accuracy: 0.522\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6837 | test accuracy: 0.508\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7205 | test accuracy: 0.512\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6308 | test accuracy: 0.515\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7343 | test accuracy: 0.508\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7262 | test accuracy: 0.508\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6932 | test accuracy: 0.508\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6811 | test accuracy: 0.508\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6420 | test accuracy: 0.525\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6779 | test accuracy: 0.508\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7185 | test accuracy: 0.505\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.8561 | test accuracy: 0.512\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6489 | test accuracy: 0.502\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.5885 | test accuracy: 0.505\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7296 | test accuracy: 0.512\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6620 | test accuracy: 0.515\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6638 | test accuracy: 0.512\n",
            "total time:  67.70564867600024\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7008 | test accuracy: 0.471\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7190 | test accuracy: 0.515\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6630 | test accuracy: 0.522\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7514 | test accuracy: 0.485\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7210 | test accuracy: 0.488\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6832 | test accuracy: 0.505\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6978 | test accuracy: 0.508\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7393 | test accuracy: 0.519\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6529 | test accuracy: 0.508\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6906 | test accuracy: 0.481\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6705 | test accuracy: 0.522\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6798 | test accuracy: 0.502\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7965 | test accuracy: 0.515\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7919 | test accuracy: 0.522\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5962 | test accuracy: 0.522\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6350 | test accuracy: 0.519\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5824 | test accuracy: 0.508\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7427 | test accuracy: 0.498\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7610 | test accuracy: 0.508\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7897 | test accuracy: 0.515\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7383 | test accuracy: 0.522\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6489 | test accuracy: 0.519\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7293 | test accuracy: 0.519\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6472 | test accuracy: 0.532\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7051 | test accuracy: 0.505\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8235 | test accuracy: 0.512\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6688 | test accuracy: 0.525\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8071 | test accuracy: 0.519\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6767 | test accuracy: 0.515\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7974 | test accuracy: 0.522\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6969 | test accuracy: 0.539\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6200 | test accuracy: 0.529\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7476 | test accuracy: 0.529\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6196 | test accuracy: 0.525\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6647 | test accuracy: 0.525\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6947 | test accuracy: 0.525\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7050 | test accuracy: 0.532\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7328 | test accuracy: 0.529\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7338 | test accuracy: 0.532\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5389 | test accuracy: 0.529\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7101 | test accuracy: 0.529\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7728 | test accuracy: 0.529\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6761 | test accuracy: 0.525\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6075 | test accuracy: 0.529\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7471 | test accuracy: 0.529\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7401 | test accuracy: 0.525\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7099 | test accuracy: 0.525\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6550 | test accuracy: 0.525\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6566 | test accuracy: 0.529\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7899 | test accuracy: 0.532\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7027 | test accuracy: 0.525\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7322 | test accuracy: 0.522\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5960 | test accuracy: 0.525\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.8626 | test accuracy: 0.525\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6751 | test accuracy: 0.529\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6863 | test accuracy: 0.522\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6577 | test accuracy: 0.522\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7009 | test accuracy: 0.525\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7631 | test accuracy: 0.525\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7009 | test accuracy: 0.529\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7289 | test accuracy: 0.529\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7454 | test accuracy: 0.525\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6738 | test accuracy: 0.525\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7221 | test accuracy: 0.525\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6862 | test accuracy: 0.525\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6924 | test accuracy: 0.522\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6770 | test accuracy: 0.539\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7581 | test accuracy: 0.529\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6218 | test accuracy: 0.532\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7063 | test accuracy: 0.535\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7334 | test accuracy: 0.529\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7566 | test accuracy: 0.529\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6180 | test accuracy: 0.529\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7089 | test accuracy: 0.522\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6845 | test accuracy: 0.519\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6902 | test accuracy: 0.532\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6540 | test accuracy: 0.522\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7178 | test accuracy: 0.519\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7752 | test accuracy: 0.522\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7558 | test accuracy: 0.522\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7283 | test accuracy: 0.529\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7428 | test accuracy: 0.522\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7101 | test accuracy: 0.522\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6149 | test accuracy: 0.529\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6917 | test accuracy: 0.535\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6892 | test accuracy: 0.529\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6346 | test accuracy: 0.522\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7306 | test accuracy: 0.522\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7271 | test accuracy: 0.522\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6780 | test accuracy: 0.522\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6731 | test accuracy: 0.532\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6526 | test accuracy: 0.532\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7008 | test accuracy: 0.522\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7291 | test accuracy: 0.532\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.8327 | test accuracy: 0.529\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6546 | test accuracy: 0.532\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.5957 | test accuracy: 0.532\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7265 | test accuracy: 0.519\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6619 | test accuracy: 0.519\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6637 | test accuracy: 0.519\n",
            "total time:  67.50122996400023\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19824838638305664.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.34838318824768066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.4866488265139716 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21260976791381836.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3656919002532959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4214908599853516 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2044374942779541.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3441007137298584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.38590886848313466 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20505690574645996.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.34876418113708496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.36467004418373106 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2161095142364502.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35678625106811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.35269396688256943 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20984196662902832.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35736775398254395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3441895310367857 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20870709419250488.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3522148132324219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.33920988738536834 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20663690567016602.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34862470626831055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3340816238096782 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021503448486328.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3453328609466553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33094484891210285 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20858287811279297.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3563191890716553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.32905688115528653 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20894646644592285.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35022759437561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32587564332144603 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20103788375854492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34341931343078613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3244761143411909 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19884443283081055.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34722089767456055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32393491395882196 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2104349136352539.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3554856777191162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32256386152335575 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21465110778808594.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3622305393218994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32109294746603284 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20680713653564453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3481025695800781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3200022305761065 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21023321151733398.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3585793972015381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.319477459362575 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057819366455078.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3479616641998291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.31903766563960484 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20031261444091797.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33646392822265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3186119471277509 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048327922821045.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34918832778930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31801277356488367 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20629549026489258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3510262966156006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31759242713451385 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20107293128967285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3486509323120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31708218497889384 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039027214050293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3481276035308838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3169394178049905 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2013537883758545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3459899425506592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3166810789278575 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039320468902588.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3559715747833252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3163865476846695 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20833230018615723.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35158443450927734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3161357994590487 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20334196090698242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.346663236618042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.315962530033929 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19921541213989258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34661006927490234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3159294609512602 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20026683807373047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3437962532043457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3158533607210432 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20170879364013672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3508307933807373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31552033722400663 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1951615810394287.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34283924102783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31548391154834204 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19221019744873047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3375990390777588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31523632747786384 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2038877010345459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34676623344421387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3152228389467512 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20609426498413086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3530721664428711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3151054586683001 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20554065704345703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34661245346069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3150765069893428 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20345616340637207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3486649990081787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31510188451835086 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21353363990783691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35946202278137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.314660878266607 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21584868431091309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36599254608154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3147319849048342 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21022963523864746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35389184951782227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31458931820733205 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2108168601989746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3502840995788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.314598748087883 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21229219436645508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36014342308044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31470773603234975 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049274444580078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459813594818115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31439238446099416 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20757246017456055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35289645195007324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31450979922498973 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199540376663208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3404221534729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31446105284350256 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19716238975524902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3377859592437744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31435676515102384 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067403793334961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.344374418258667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3142600349017552 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21156525611877441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35599493980407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3142565825155803 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20096206665039062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34586596488952637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31419436378138405 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20833277702331543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35590624809265137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3140930299248014 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20348381996154785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3453493118286133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31416184944765907 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20346450805664062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452491760253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31407697541373114 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21191668510437012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35644102096557617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31409180590084623 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20765018463134766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35187411308288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3140865457909448 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2120201587677002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36404991149902344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3139856031962803 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2044379711151123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36545610427856445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3139290464775903 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2094879150390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496358394622803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31393192197595327 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19860434532165527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3463253974914551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3139060267380306 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20333003997802734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446311950683594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3138847338301795 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20315265655517578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.340350866317749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139041768653052 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19823050498962402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3557102680206299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31384675204753876 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20002245903015137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3426055908203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31392046340874263 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20741677284240723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34816837310791016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31380739424909865 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20358514785766602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508036136627197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31382202804088594 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20643854141235352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34955596923828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31377305005277906 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20045709609985352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33884763717651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31379875242710115 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20521783828735352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3548588752746582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.313815798504012 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20150279998779297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34375834465026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31377221431050983 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20008182525634766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34761524200439453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31374432614871434 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21364116668701172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35583972930908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31370577343872613 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19986963272094727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3410465717315674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3136911519936153 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19808220863342285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33811163902282715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31369193068572454 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20783329010009766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565232753753662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31371898055076597 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2077476978302002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3524322509765625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31366565653256007 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20795822143554688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481740951538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31362308944974626 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21883392333984375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3684413433074951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136670231819153 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20700573921203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34914231300354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31364173548562185 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20730042457580566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3603942394256592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31364021003246306 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21974563598632812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3626120090484619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136323524372918 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21181011199951172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35390424728393555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31364874882357463 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20985913276672363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3485424518585205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31359116222177236 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21584320068359375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3704972267150879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31363494523933955 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20292305946350098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.344923734664917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136130286114556 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20340633392333984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35759520530700684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136059020246778 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21788382530212402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3660445213317871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31358014345169066 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039642333984375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34544825553894043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135370373725891 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20920991897583008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35934972763061523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135875016450882 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20056986808776855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.341717004776001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.313546199457986 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201981782913208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3422050476074219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31357606393950327 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19786691665649414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516874313354492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31354629354817526 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20768070220947266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3460812568664551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135213328259332 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20397305488586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34340643882751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31357202317033495 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2092282772064209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585033416748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135278948715755 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19914889335632324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3415687084197998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135429050241198 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2056291103363037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34520769119262695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135158040693828 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20680737495422363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496544361114502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31353983538491387 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20627903938293457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470330238342285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3134983663048063 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2132854461669922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583979606628418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135357056345258 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21036052703857422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540077209472656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.313488809977259 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20177698135375977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34980177879333496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3134874267237527 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21083498001098633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35938572883605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135112102542605 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2197098731994629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3631293773651123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31348654798098974 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20651531219482422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35253405570983887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134694491113935 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20636415481567383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501434326171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31346847116947174 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21696066856384277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35650110244750977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31347471135003224 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20847105979919434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3602621555328369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31348113289901186 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21129298210144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36609721183776855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31349257230758665 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2084980010986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35088539123535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31347920085702624 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20396089553833008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34624481201171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31345782918589454 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21266698837280273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3698110580444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134570202657155 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22282814979553223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3661370277404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31346263672624314 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045121192932129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498833179473877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31345359129565104 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.204728364944458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.353135347366333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134349793195724 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082676887512207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35196661949157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31344241968223024 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20937061309814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35597658157348633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31344681935650964 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21124887466430664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35863184928894043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31343284036431995 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1968545913696289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475983142852783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134406928505216 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20748496055603027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356518030166626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134248856987272 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22629261016845703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3703343868255615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31343136983258385 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2005448341369629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33771204948425293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31342649928161076 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20001792907714844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34026098251342773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134255915880203 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20911526679992676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3567314147949219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345690659114295 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20096063613891602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446054458618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31340206393173764 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21030068397521973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35338473320007324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134208445038114 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21849679946899414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3602027893066406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31340890995093756 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045273780822754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34505438804626465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134106363568987 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21058082580566406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3541452884674072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343521433217186 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20734262466430664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3499317169189453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342131197452544 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20976734161376953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35749053955078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31340418713433404 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"class\"\n",
        "rnn_arr3 = []\n",
        "nrnn_arr3 = []\n",
        "lstm_arr3 = []\n",
        "seed_arr3 = []\n",
        "method_arr3 = []\n",
        "specify_arr3 = []\n",
        "\n",
        "flip_probabilities_0 = [0.2, 0.8]\n",
        "flip_probabilities_1 = [0.2, 0.8]\n",
        "\n",
        "for flip_probability_0 in flip_probabilities_0:\n",
        "  for flip_probability_1 in flip_probabilities_1:\n",
        "\n",
        "    x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                        flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1,\n",
        "                        startprob=None, transmat=None)\n",
        "\n",
        "    class_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "    class_train_flipped_loader = data_utils.DataLoader(class_train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    class_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "    class_test_flipped_loader = data_utils.DataLoader(class_test_flipped, batch_size=25, shuffle=True)\n",
        "\n",
        "    for seed in seeds:\n",
        "      acc = driver(seed, class_train_flipped_loader, class_test_flipped_loader)\n",
        "      acc2 = driver(seed, class_train_flipped_loader, class_test_flipped_loader, nrnn = True)\n",
        "      acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
        "      rnn_arr3.append(acc)\n",
        "      nrnn_arr3.append(acc2)\n",
        "      lstm_arr3.append(acc3)\n",
        "      seed_arr3.append(seed)\n",
        "      method_arr3.append(method)\n",
        "      specify_arr3.append((flip_probability_0, flip_probability_1))"
      ],
      "metadata": {
        "id": "KIEQl0-6i6ia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "924ca605-6f04-4bcd-8de8-3743f87367e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8367 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5784 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5370 | test accuracy: 0.684\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7865 | test accuracy: 0.704\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8003 | test accuracy: 0.646\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5912 | test accuracy: 0.677\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4034 | test accuracy: 0.650\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8002 | test accuracy: 0.677\n",
            "Epoch:  8 Iteration:  630 | train loss: 1.0755 | test accuracy: 0.700\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4709 | test accuracy: 0.694\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8795 | test accuracy: 0.700\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4955 | test accuracy: 0.694\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8446 | test accuracy: 0.653\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5059 | test accuracy: 0.700\n",
            "Epoch:  14 Iteration:  1050 | train loss: 1.1636 | test accuracy: 0.690\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.9649 | test accuracy: 0.680\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6464 | test accuracy: 0.731\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8028 | test accuracy: 0.727\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5150 | test accuracy: 0.741\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9410 | test accuracy: 0.751\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7876 | test accuracy: 0.754\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7325 | test accuracy: 0.781\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5546 | test accuracy: 0.761\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2562 | test accuracy: 0.785\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2317 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7669 | test accuracy: 0.788\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3030 | test accuracy: 0.788\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.3214 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7863 | test accuracy: 0.788\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3114 | test accuracy: 0.788\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2476 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5603 | test accuracy: 0.788\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5217 | test accuracy: 0.788\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.4609 | test accuracy: 0.788\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7160 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3687 | test accuracy: 0.788\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2364 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.4119 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3932 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7914 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.8872 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6437 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.9020 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4545 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3128 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7186 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 1.0357 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2712 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.4850 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8022 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2734 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.3025 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1982 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.3190 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4262 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1749 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4298 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2422 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2650 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4071 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2710 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2544 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3671 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7986 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2228 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3361 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5444 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2224 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5288 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6518 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.3624 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.5302 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7665 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6528 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3000 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.4697 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6654 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2748 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.9477 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6420 | test accuracy: 0.788\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2979 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.3628 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2218 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7108 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.5337 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2745 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 1.0685 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2357 | test accuracy: 0.788\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2866 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.9454 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 1.0118 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7015 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2747 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.2274 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4506 | test accuracy: 0.788\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2509 | test accuracy: 0.788\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2245 | test accuracy: 0.788\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7557 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6764 | test accuracy: 0.788\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.8718 | test accuracy: 0.788\n",
            "total time:  65.58404970300035\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8453 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6175 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5867 | test accuracy: 0.694\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7907 | test accuracy: 0.690\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8295 | test accuracy: 0.646\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6026 | test accuracy: 0.653\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3781 | test accuracy: 0.646\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7765 | test accuracy: 0.663\n",
            "Epoch:  8 Iteration:  630 | train loss: 1.0479 | test accuracy: 0.684\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4859 | test accuracy: 0.670\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8473 | test accuracy: 0.697\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5216 | test accuracy: 0.694\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8639 | test accuracy: 0.657\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5195 | test accuracy: 0.690\n",
            "Epoch:  14 Iteration:  1050 | train loss: 1.2137 | test accuracy: 0.680\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8222 | test accuracy: 0.670\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6642 | test accuracy: 0.690\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8101 | test accuracy: 0.707\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5384 | test accuracy: 0.704\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8659 | test accuracy: 0.717\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7496 | test accuracy: 0.734\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7873 | test accuracy: 0.737\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5504 | test accuracy: 0.741\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2762 | test accuracy: 0.754\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3082 | test accuracy: 0.771\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6937 | test accuracy: 0.768\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2957 | test accuracy: 0.771\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.3851 | test accuracy: 0.781\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7702 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3486 | test accuracy: 0.795\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.3000 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5809 | test accuracy: 0.795\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5248 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.4611 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7122 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3889 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2738 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.3870 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4643 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7925 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.8867 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5759 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.9411 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4696 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3250 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7128 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 1.0489 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3085 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5256 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8589 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2790 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.3268 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2047 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4078 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4694 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2045 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4654 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2343 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2793 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4400 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3027 | test accuracy: 0.788\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2777 | test accuracy: 0.788\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3889 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7426 | test accuracy: 0.788\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2086 | test accuracy: 0.788\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3794 | test accuracy: 0.788\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6112 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2812 | test accuracy: 0.788\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.4870 | test accuracy: 0.788\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6394 | test accuracy: 0.788\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.3536 | test accuracy: 0.788\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.4953 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8213 | test accuracy: 0.788\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6727 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3223 | test accuracy: 0.788\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.4428 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7121 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3008 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.9619 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6647 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3174 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.3375 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2345 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7128 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.4804 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2951 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 1.0476 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2466 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.3051 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8965 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 1.0116 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6225 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2838 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.2343 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4781 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2589 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2416 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7268 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7060 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.8605 | test accuracy: 0.791\n",
            "total time:  66.08421341299982\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19789457321166992.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.34226441383361816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5685720554419926 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19976067543029785.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.34348297119140625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4691393166780472 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20038628578186035.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3549063205718994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4171785848481315 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2117171287536621.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.35214686393737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3886501823152815 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20039081573486328.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3428032398223877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36884000258786337 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20797061920166016.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.36120152473449707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.35862985977104733 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21344423294067383.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35599613189697266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3496922233275005 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20511770248413086.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35946083068847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34327378187860763 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21441054344177246.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.362170934677124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3391937851905823 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067854404449463.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3462402820587158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3351803856236594 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19800829887390137.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34015846252441406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33180698794978003 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21078181266784668.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35508227348327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.329529190489224 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19794607162475586.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3363044261932373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.327386691740581 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20041179656982422.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3481943607330322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3256808578968048 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21027612686157227.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35468482971191406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32386106891291483 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19968557357788086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3401479721069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3232240204300199 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20143485069274902.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3449592590332031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32263179421424865 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20846891403198242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34920811653137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3216468572616577 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20355868339538574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3430211544036865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32096371097224097 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19382143020629883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3415706157684326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.319810249550002 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148432731628418.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3549771308898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3193074924605233 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19472861289978027.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33238720893859863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3190872128520693 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20270037651062012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34739112854003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31845607289246153 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2072300910949707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3478565216064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3182688755648477 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19509434700012207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33710384368896484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31792101391724176 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19559121131896973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33727526664733887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3174006402492523 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048168182373047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3452112674713135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.317024575812476 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1998300552368164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33812808990478516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3167474580662591 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20152997970581055.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34412622451782227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.316601402418954 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20673894882202148.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34626007080078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3165167050702231 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1952357292175293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33559322357177734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3163542202540806 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20377421379089355.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35454750061035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31596046899046215 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2097322940826416.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35440945625305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3156338934387479 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20278358459472656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3495330810546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31565771741526466 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20244884490966797.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3491206169128418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3156548674617495 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20787429809570312.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3490254878997803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31533383301326207 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20386290550231934.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3474726676940918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3152234311614718 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20757818222045898.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3652613162994385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3153313019445964 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20126700401306152.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34223031997680664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3153765107904162 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19762229919433594.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3470621109008789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3151002824306488 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20363783836364746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35108327865600586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3150739963565554 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016303539276123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34390997886657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.314963161945343 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20530366897583008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349290132522583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31485575650419506 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039661407470703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3529822826385498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31463098738874706 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19717955589294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33541297912597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3146310142108372 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19796061515808105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3409147262573242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31456307939120703 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201615571975708.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523876667022705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3145596478666578 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027580738067627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34619927406311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.314436982359205 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20811104774475098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521902561187744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3144418908017022 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126452922821045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3633596897125244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3144679346254894 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20195388793945312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34975600242614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31426248848438265 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20308470726013184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514549732208252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3142659544944763 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2113652229309082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3550074100494385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31433854826859065 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20727252960205078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553810119628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31424603036471777 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20324087142944336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3471565246582031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3140905669757298 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111656665802002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575613498687744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31403469485895974 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2102069854736328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36591529846191406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31421263558523993 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20879626274108887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35381364822387695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3140269560473306 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20449113845825195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.348003625869751\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141106218099594 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053060531616211.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546319007873535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31404863936560495 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20188260078430176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34331727027893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140216737985611 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21122503280639648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35892486572265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3139705117259707 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039330005645752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34589695930480957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139279901981354 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2040567398071289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510434627532959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31391029953956606 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2030963897705078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34897327423095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31391828571047103 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20889878273010254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540332317352295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139077927385058 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20305705070495605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3512709140777588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31392579334122794 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016746997833252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467566967010498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3138663930552346 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20212030410766602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34920740127563477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31382612798895154 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19378376007080078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3414640426635742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138079174927303 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20156073570251465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436903953552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31378232879298074 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19640588760375977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33739566802978516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137532489640372 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19351720809936523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34622693061828613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137570266212736 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20478010177612305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35567259788513184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31371395715645384 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103564739227295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35761523246765137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31376458406448365 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126941680908203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36452198028564453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31378153605120523 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20098352432250977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479125499725342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137189873627254 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20686030387878418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34894490242004395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31370729804039 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21372580528259277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35660767555236816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31369372989450184 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20407557487487793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467528820037842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136446697371347 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20133113861083984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3419771194458008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313685724564961 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20837140083312988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3466305732727051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136513501405716 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20380210876464844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478271961212158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136937094586236 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20276832580566406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3439605236053467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136415319783347 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21048259735107422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35123181343078613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31364781771387373 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20679616928100586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515796661376953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136229693889618 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19995927810668945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3405134677886963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31361697231020247 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20708465576171875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3547649383544922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31361324872289387 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1983635425567627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3437988758087158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31361147378172194 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19895482063293457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3421461582183838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31361753727708547 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20916342735290527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3548698425292969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31364823707512446 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20833730697631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35857701301574707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.313558206813676 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20610952377319336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34593939781188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31355990852628435 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20947504043579102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537273406982422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135705815894263 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017228603363037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34218835830688477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135856407029288 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20072031021118164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34201741218566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135553232261113 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21330785751342773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3626070022583008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.313557317001479 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2073497772216797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486511707305908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31355739746774947 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024996280670166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534379005432129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135480667863573 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20511078834533691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35268139839172363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31354243201868875 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20282793045043945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455049991607666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135379603930882 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20129919052124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3513960838317871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135750561952591 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1955852508544922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34450769424438477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.313520314012255 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19915056228637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33915233612060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135142488139016 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19996333122253418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537254333496094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.313511255809239 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20637059211730957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533601760864258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134859421423503 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19724202156066895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34470653533935547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135136446782521 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2018270492553711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35274744033813477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31349491604736873 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20444369316101074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34825587272644043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31348321863583156 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021803855895996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3421974182128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31349011404173716 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20483613014221191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35489630699157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134566055876868 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2001817226409912.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3405284881591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31350257226399014 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19929862022399902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.342334508895874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.313470510499818 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21602797508239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36519432067871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134672088282449 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20334625244140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467423915863037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31346578087125504 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19972801208496094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3433418273925781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31346032874924795 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.207902193069458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35537099838256836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31344937341553825 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20549678802490234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465847969055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134724301951272 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20131802558898926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34383225440979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.313460949914796 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21025586128234863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35091519355773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134339622088841 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2085890769958496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34864211082458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345483149800984 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19565486907958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34412264823913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31343874420438494 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20740818977355957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34787607192993164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134275257587433 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049560546875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34503841400146484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134464395897729 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19620633125305176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3413515090942383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134380923850196 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2109537124633789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35367298126220703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134359125580106 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20078802108764648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3407111167907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134206703730992 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20012140274047852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432474136352539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134276096309934 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3721 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 1.0153 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4087 | test accuracy: 0.650\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7982 | test accuracy: 0.646\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5583 | test accuracy: 0.697\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6033 | test accuracy: 0.650\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3672 | test accuracy: 0.653\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7901 | test accuracy: 0.663\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4638 | test accuracy: 0.657\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5503 | test accuracy: 0.687\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4592 | test accuracy: 0.694\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5119 | test accuracy: 0.663\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8083 | test accuracy: 0.697\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3285 | test accuracy: 0.687\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.9028 | test accuracy: 0.700\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8924 | test accuracy: 0.697\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2887 | test accuracy: 0.687\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4559 | test accuracy: 0.721\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3652 | test accuracy: 0.714\n",
            "Epoch:  19 Iteration:  1400 | train loss: 1.0813 | test accuracy: 0.710\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0259 | test accuracy: 0.747\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.0545 | test accuracy: 0.764\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4199 | test accuracy: 0.771\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.9226 | test accuracy: 0.795\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5982 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.3572 | test accuracy: 0.785\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3016 | test accuracy: 0.788\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7494 | test accuracy: 0.795\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6136 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7568 | test accuracy: 0.795\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6054 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7472 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8018 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.3105 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5420 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6349 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.8823 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5372 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.0613 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6936 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2771 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1910 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7053 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7313 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2215 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 1.4261 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7100 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6660 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.9101 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6308 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5961 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.5883 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1880 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2999 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4175 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.5269 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4893 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2426 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.5508 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 1.1436 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1727 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.5625 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1669 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2793 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.3378 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.8661 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3597 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2066 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5086 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3623 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6443 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6528 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2774 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2565 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3532 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.0765 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3057 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1870 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 1.0708 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6919 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6443 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2834 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3873 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2756 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1887 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.5161 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 1.1744 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2936 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.6626 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.3634 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.1717 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.4494 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 1.0992 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6249 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.9808 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7255 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2520 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2624 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2174 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1911 | test accuracy: 0.791\n",
            "total time:  65.89304258500033\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3954 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 1.0449 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4580 | test accuracy: 0.646\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8213 | test accuracy: 0.650\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5879 | test accuracy: 0.684\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5718 | test accuracy: 0.646\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3789 | test accuracy: 0.646\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7775 | test accuracy: 0.650\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4598 | test accuracy: 0.650\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5830 | test accuracy: 0.687\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4453 | test accuracy: 0.680\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5411 | test accuracy: 0.650\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8120 | test accuracy: 0.677\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3631 | test accuracy: 0.673\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.9431 | test accuracy: 0.687\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.9758 | test accuracy: 0.694\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2978 | test accuracy: 0.660\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5079 | test accuracy: 0.697\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.4189 | test accuracy: 0.690\n",
            "Epoch:  19 Iteration:  1400 | train loss: 1.1506 | test accuracy: 0.687\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0170 | test accuracy: 0.704\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.0300 | test accuracy: 0.707\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4465 | test accuracy: 0.744\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7706 | test accuracy: 0.724\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6974 | test accuracy: 0.737\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.4688 | test accuracy: 0.758\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4251 | test accuracy: 0.768\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8332 | test accuracy: 0.751\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.5401 | test accuracy: 0.788\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6801 | test accuracy: 0.758\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6291 | test accuracy: 0.785\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6528 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7846 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.3084 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.3548 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6333 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.9641 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.4908 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.0076 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6953 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.3002 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.2423 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7857 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7572 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2232 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 1.2336 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7933 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6047 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.8869 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6295 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5544 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.5421 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2115 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.3075 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5131 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.5027 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4701 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2499 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6713 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 1.0954 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1949 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4053 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1799 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2809 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.3797 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.8851 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.4388 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2011 | test accuracy: 0.788\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5026 | test accuracy: 0.788\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3334 | test accuracy: 0.788\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6322 | test accuracy: 0.788\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6306 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3241 | test accuracy: 0.788\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.3118 | test accuracy: 0.788\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3549 | test accuracy: 0.788\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.1282 | test accuracy: 0.788\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3572 | test accuracy: 0.788\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2197 | test accuracy: 0.788\n",
            "Epoch:  78 Iteration:  5530 | train loss: 1.0743 | test accuracy: 0.788\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6734 | test accuracy: 0.788\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.5932 | test accuracy: 0.788\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2887 | test accuracy: 0.788\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3926 | test accuracy: 0.788\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.3267 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1940 | test accuracy: 0.788\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4588 | test accuracy: 0.788\n",
            "Epoch:  86 Iteration:  6090 | train loss: 1.1512 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.3005 | test accuracy: 0.788\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.6763 | test accuracy: 0.788\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.3567 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.1712 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.4838 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.9797 | test accuracy: 0.788\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6126 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.9196 | test accuracy: 0.788\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6840 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2645 | test accuracy: 0.788\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2652 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2530 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1877 | test accuracy: 0.791\n",
            "total time:  65.7003235330003\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1956334114074707.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.33711791038513184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6887833484581538 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053377628326416.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.34548020362854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5419197372027806 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2040693759918213.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.36202502250671387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4602284039769854 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2063298225402832.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.3500635623931885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41387627933706556 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051525115966797.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.34938478469848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38820311725139617 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1981353759765625.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3475148677825928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37202891749995093 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20418214797973633.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3486752510070801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36008536900792804 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20580482482910156.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.34552717208862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3528328767844609 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20354223251342773.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3495512008666992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3452743304627282 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20053839683532715.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.342775821685791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.340475600532123 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20056796073913574.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34140467643737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33569917721407755 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106952667236328.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35381102561950684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3348292636019843 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19874906539916992.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34378623962402344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3318545558622905 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19719219207763672.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.33751654624938965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32939811732087815 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2008512020111084.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3398144245147705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32814691620213643 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19839930534362793.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3412778377532959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32586898165089745 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20167303085327148.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3536086082458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3245058911187308 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20899462699890137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35724663734436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3241089727197375 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053987979888916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3490147590637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32321368839059555 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2006211280822754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3409616947174072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3224847435951233 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2110607624053955.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35285186767578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3214576891490391 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19684672355651855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34091973304748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32082136017935614 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19395732879638672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3386509418487549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3197988654885973 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067105770111084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3536090850830078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31976096885544913 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1966862678527832.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3395559787750244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31896123673234666 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1981816291809082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34366488456726074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3183850748198373 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21339154243469238.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35785508155822754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3182296531541007 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042522430419922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3455994129180908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3180524447134563 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19993853569030762.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34615206718444824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3176122963428497 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20896124839782715.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3551630973815918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.317239614043917 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19928359985351562.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33838963508605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3171561620065144 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19962739944458008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.340282678604126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3168406571660723 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21511006355285645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36157727241516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3164986823286329 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20333480834960938.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34174585342407227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31630223308290756 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19460439682006836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3364384174346924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3162424879414695 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20677661895751953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34909987449645996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3158578319208963 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20078206062316895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3411743640899658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31576188164097924 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20065784454345703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34755659103393555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3155976576464517 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19678807258605957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3402383327484131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31556212987218585 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19377350807189941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33282041549682617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31564593953745707 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20405912399291992.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3538506031036377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31544273027351927 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19663715362548828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.336226224899292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31528655120304655 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20041251182556152.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3420078754425049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.315244836466653 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19979524612426758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3546586036682129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31514970447335927 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999211311340332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34347009658813477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31494328933102744 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.193800687789917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33402228355407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31497403723852974 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19799423217773438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461625576019287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3148797886712211 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20138049125671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3412916660308838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3146505534648895 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027268409729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.348569393157959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.314909513933318 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20167016983032227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3493640422821045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3147182447569711 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20334362983703613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34485960006713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3145969808101654 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20379233360290527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349092960357666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3143979183265141 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20586180686950684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3620579242706299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31438895463943484 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21289467811584473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35228610038757324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144182762929371 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19677352905273438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34076404571533203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31454684138298034 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19628334045410156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3485255241394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3142628703798567 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19843053817749023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34043192863464355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31420542682920183 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20416712760925293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34526872634887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142188659736088 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19933772087097168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3414344787597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31414869385106226 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19635987281799316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33612489700317383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31423275300434655 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20653414726257324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3558530807495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141051484005792 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21132278442382812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35837650299072266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3140838248389108 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20281767845153809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3427872657775879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31405743743692127 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1989901065826416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34366822242736816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31404148510524205 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21351122856140137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3630518913269043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31399190042700087 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19706988334655762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428640365600586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140146894114358 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035808563232422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3468775749206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31393775939941404 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20969223976135254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35277271270751953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139519759586879 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20153164863586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34353065490722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31393185343061175 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2028956413269043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436558246612549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31394088183130536 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2179551124572754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3568110466003418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31396137050219947 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20308589935302734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34421420097351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138792736189706 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20656871795654297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520181179046631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31383894809654783 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22300982475280762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3685488700866699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31382625656468527 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20486211776733398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35394763946533203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138408669403621 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2061614990234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498265743255615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31378540354115625 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21061348915100098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3489837646484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3138097269194467 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20399069786071777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479795455932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31381629449980597 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.208726167678833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3632519245147705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137424481766565 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20564484596252441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483562469482422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31381790297372003 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20325207710266113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546769618988037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31371874979564124 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20480132102966309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575115203857422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137034015996116 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19963479042053223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34331369400024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31373543909617835 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20720481872558594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34911298751831055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137121715715953 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19835662841796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34572649002075195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31365762438092915 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014474868774414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34483861923217773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31369675057274954 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995868682861328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34800028800964355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136600353888103 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2088027000427246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35948991775512695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31363520877701895 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2022082805633545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3422696590423584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31366128495761325 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20246434211730957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34339022636413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136280677148274 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20116209983825684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35164666175842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136107806648527 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1963486671447754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33860063552856445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31366604523999353 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19924211502075195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33719658851623535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136487777744021 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19750595092773438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481752872467041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136559064899172 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024526596069336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34374332427978516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135810856308256 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20190715789794922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35032105445861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31360123327800205 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21074652671813965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520023822784424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135991645710809 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20065975189208984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34604382514953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31361937267439705 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053821086883545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.347578763961792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31355921030044553 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20842790603637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3491694927215576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31356546665940965 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21153521537780762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35898613929748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31357372488294327 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19698572158813477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3391284942626953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31352702932698384 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22076678276062012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3647627830505371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31357109759535107 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20971941947937012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35595059394836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135251517806734 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20591974258422852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34864354133605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31351952467645916 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20625543594360352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3463921546936035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.313519669004849 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2107996940612793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3560981750488281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135272447552 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060561180114746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34709644317626953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135190346411296 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2087419033050537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3562331199645996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31351999427591054 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048964500427246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34498000144958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31353655457496643 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20523381233215332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446238040924072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31349167483193535 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21029281616210938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3505847454071045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351188378674644 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1976335048675537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33770275115966797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31351215498788015 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19858312606811523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33914732933044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134793345417295 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20755720138549805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442518711090088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134809068271092 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19620585441589355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3444654941558838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31347498510565075 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19951319694519043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458285331726074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31349557978766307 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20174884796142578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35448551177978516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134958782366344 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20227527618408203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34558749198913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134838121277945 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20686578750610352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35609984397888184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134767174720764 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20366334915161133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34387874603271484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134942910500935 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19824576377868652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33943629264831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134477257728577 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19861841201782227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34697461128234863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134514080626624 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2004082202911377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446369171142578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31345987362521033 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21306848526000977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546302318572998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134727942092078 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20015859603881836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.348207950592041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343771857874736 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19932961463928223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34334897994995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31344033990587505 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19965744018554688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34336256980895996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134576857089996 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5928 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.9469 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4585 | test accuracy: 0.653\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5535 | test accuracy: 0.646\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6445 | test accuracy: 0.657\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5792 | test accuracy: 0.653\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4800 | test accuracy: 0.684\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5160 | test accuracy: 0.653\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5212 | test accuracy: 0.684\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6161 | test accuracy: 0.616\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8299 | test accuracy: 0.657\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4939 | test accuracy: 0.684\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5226 | test accuracy: 0.707\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.1539 | test accuracy: 0.673\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6208 | test accuracy: 0.673\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4750 | test accuracy: 0.710\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7944 | test accuracy: 0.677\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5705 | test accuracy: 0.724\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3290 | test accuracy: 0.731\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7711 | test accuracy: 0.737\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2921 | test accuracy: 0.747\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5095 | test accuracy: 0.758\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.1847 | test accuracy: 0.774\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.4373 | test accuracy: 0.771\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3655 | test accuracy: 0.785\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7688 | test accuracy: 0.747\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3887 | test accuracy: 0.785\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2250 | test accuracy: 0.788\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.3568 | test accuracy: 0.788\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3131 | test accuracy: 0.781\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2954 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5605 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.9565 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.9953 | test accuracy: 0.788\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2513 | test accuracy: 0.788\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2529 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6805 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.2182 | test accuracy: 0.788\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2893 | test accuracy: 0.788\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.4456 | test accuracy: 0.788\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7575 | test accuracy: 0.788\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.2385 | test accuracy: 0.788\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2823 | test accuracy: 0.788\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7066 | test accuracy: 0.788\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6125 | test accuracy: 0.788\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7859 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6199 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6846 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7257 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7787 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2702 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6301 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.9927 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1761 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7183 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.4741 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5854 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7425 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2934 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6681 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6279 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3643 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7297 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.1837 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7799 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6028 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2949 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.4128 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3848 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2464 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1760 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7971 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8765 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2923 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5381 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.3100 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6387 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3417 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7151 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3009 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.5911 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7283 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1816 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6782 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1790 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2374 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2122 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.4833 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7452 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6281 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.5854 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.2701 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.9161 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.5464 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2051 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4960 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6269 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2388 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7110 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6198 | test accuracy: 0.791\n",
            "total time:  65.63129685800004\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6044 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.9266 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4623 | test accuracy: 0.646\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5573 | test accuracy: 0.650\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6452 | test accuracy: 0.650\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5719 | test accuracy: 0.650\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5300 | test accuracy: 0.653\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5229 | test accuracy: 0.650\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4946 | test accuracy: 0.670\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6537 | test accuracy: 0.636\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8205 | test accuracy: 0.650\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5198 | test accuracy: 0.667\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5342 | test accuracy: 0.700\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.1531 | test accuracy: 0.657\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6179 | test accuracy: 0.667\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4981 | test accuracy: 0.694\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7594 | test accuracy: 0.673\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5755 | test accuracy: 0.721\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3395 | test accuracy: 0.694\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7910 | test accuracy: 0.737\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2957 | test accuracy: 0.747\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5608 | test accuracy: 0.747\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.1691 | test accuracy: 0.747\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.4369 | test accuracy: 0.761\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.4247 | test accuracy: 0.754\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7999 | test accuracy: 0.737\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4296 | test accuracy: 0.788\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2524 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.4053 | test accuracy: 0.785\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.4072 | test accuracy: 0.771\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.3133 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5986 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.9720 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 1.0445 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2743 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2845 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7379 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.1272 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3548 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5010 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7228 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3092 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2677 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7893 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5686 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7358 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.5991 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6705 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7424 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7128 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2893 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6546 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.8364 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1966 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7599 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.5305 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6214 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7672 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.3197 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6574 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.5915 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4044 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7298 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.1827 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7115 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5597 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3264 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.4430 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3656 | test accuracy: 0.795\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2710 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1765 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.8087 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8476 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.3369 | test accuracy: 0.788\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4728 | test accuracy: 0.788\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.2844 | test accuracy: 0.795\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6503 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3801 | test accuracy: 0.788\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7039 | test accuracy: 0.795\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3048 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6145 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6673 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1888 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6009 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1548 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2368 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2399 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.4725 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7108 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.5953 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.5200 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.4023 | test accuracy: 0.795\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.8761 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.5316 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2222 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4867 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6077 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2610 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6773 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.5838 | test accuracy: 0.791\n",
            "total time:  66.04213430100026\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20761847496032715.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.35981225967407227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.4908716908523015 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20427632331848145.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3601095676422119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.42194120245320454 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21415257453918457.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.37267565727233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.3867108711174556 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19785261154174805.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3429300785064697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3643570005893707 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20675969123840332.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35227513313293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3512700889791761 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20808076858520508.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3565695285797119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3441011961017336 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035531997680664.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35251665115356445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.33744618466922216 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20311903953552246.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34215354919433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3343145911182676 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047281265258789.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35118865966796875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33034986853599546 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20464229583740234.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34809136390686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.32902165140424455 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20180964469909668.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35051417350769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3256826324122293 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20596909523010254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35308265686035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3248664213078363 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972646713256836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.340986967086792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32267568153994425 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1979668140411377.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34128665924072266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3220125466585159 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20363330841064453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34720754623413086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3217273346015385 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20400619506835938.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34828710556030273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3200039220707757 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19478535652160645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3356747627258301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31968900859355925 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19927310943603516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3384439945220947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3185405250106539 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19994735717773438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3440086841583252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3183984918253762 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20072102546691895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3438832759857178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3181291856936046 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2154064178466797.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3585963249206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3177770742348262 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066659927368164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3514244556427002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31729877420834135 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20512151718139648.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34967923164367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3168963836772101 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20969033241271973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34853506088256836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3165777019092015 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20218586921691895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3421440124511719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3166441202163696 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1971292495727539.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33960890769958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3160503464085715 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21004605293273926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35424232482910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3158777858529772 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21890735626220703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3635678291320801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3158796293394906 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20904302597045898.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3509666919708252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31568030544689724 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21336984634399414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35526013374328613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3155397491795676 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21102595329284668.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3554096221923828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3155830579144614 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010788917541504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34116125106811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3150211751461029 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20623278617858887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3569979667663574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3151015749999455 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2006685733795166.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34484219551086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31500977490629467 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19838285446166992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3381657600402832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3147908134119851 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20823121070861816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34659862518310547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3148007724966322 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20265722274780273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34815073013305664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31477621623447966 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19881677627563477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34566497802734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31462079244000574 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19702768325805664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34139227867126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3146450127874102 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20197367668151855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3441581726074219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3145169373069491 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19664502143859863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34182047843933105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3143999866076878 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20534873008728027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34612059593200684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31435664211000713 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19701337814331055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33521509170532227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3144121697970799 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.198228120803833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34644365310668945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31434509285858697 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19778990745544434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3413231372833252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31430101777826036 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2056570053100586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447399139404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3142687124865396 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19881439208984375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479645252227783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3142634915454047 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20440459251403809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34450840950012207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3141089426619666 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19889020919799805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3368382453918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3140117994376591 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19444012641906738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33841753005981445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31412257850170133 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20373749732971191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34647274017333984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31393496905054363 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2001190185546875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34034204483032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.313972482085228 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995680332183838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35016775131225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141046868903296 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20867657661437988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352245569229126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3140148154326848 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19811177253723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3405289649963379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31393838269369945 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19609308242797852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3435227870941162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3139172502926418 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20073890686035156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3440883159637451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31395552839551655 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19896245002746582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34099388122558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3139008117573602 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19748806953430176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35322117805480957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3138154225690024 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19559764862060547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33779478073120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3138324252196721 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055497169494629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465895652770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31382773688861304 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20278406143188477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3535890579223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3137905840362821 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20307111740112305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470332622528076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.313800967165402 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19937562942504883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3424389362335205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31375170648097994 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20873761177062988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36289119720458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31375597204480854 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20919013023376465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3528287410736084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137148891176496 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048325538635254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350513219833374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137895366975239 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21657848358154297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36120080947875977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31370882093906405 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20086193084716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3453946113586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137180660452161 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19800877571105957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475961685180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3136910715273448 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2149951457977295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3599209785461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31367945671081543 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20673727989196777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35053443908691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3136487079518182 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20406746864318848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34658265113830566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3136477891887937 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21493148803710938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36061596870422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31368278222424645 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20663142204284668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34796929359436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136897461754935 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20223069190979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35193347930908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31365812633718765 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20955514907836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355313777923584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.313598153420857 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21019506454467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35504603385925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136249269757952 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21193742752075195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37093567848205566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136576201234545 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2177135944366455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3651919364929199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31356057907853807 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20247173309326172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3462388515472412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3135518274136952 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328783988952637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3647630214691162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3135829065527235 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20656490325927734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3488044738769531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31356037259101865 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2063770294189453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508892059326172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31357713980334145 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21187591552734375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3611257076263428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31355370794023785 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2079181671142578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35090088844299316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135569138186319 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19876742362976074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.344113826751709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135524264403752 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2102067470550537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34795212745666504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31356223693915775 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19762444496154785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33719754219055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135495935167585 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19347262382507324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33379125595092773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31353277393749784 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.208237886428833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35012006759643555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135473153420857 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20047426223754883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3438138961791992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135171426194055 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049882411956787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3531947135925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135140359401703 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21477723121643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3609504699707031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31348516515323094 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20298051834106445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34578680992126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135333542312895 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111806869506836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35709166526794434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31349225640296935 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2150096893310547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35741496086120605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135087924344199 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20270633697509766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34099745750427246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3134683745247977 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19986343383789062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34328293800354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31348007917404175 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21216249465942383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35706186294555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31347716620990207 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19654583930969238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3396143913269043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3134987047740391 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20159530639648438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3437306880950928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134591711418969 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103593349456787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34942626953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31346742297921865 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1994485855102539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465697765350342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134710052183696 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20033955574035645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3422987461090088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31346006734030585 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21249914169311523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3555901050567627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134653810943876 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20166516304016113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3411366939544678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31344756441456934 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2033400535583496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35646605491638184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134646207094193 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027733325958252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34474658966064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134649119206837 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20139765739440918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3406984806060791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31345044927937643 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064065933227539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36054205894470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31342966045652115 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19925975799560547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3399829864501953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134462173495974 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2013847827911377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523890972137451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134440000568117 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21352434158325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3665778636932373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31342489038194926 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20137667655944824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3411121368408203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134248814412526 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20268511772155762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34349560737609863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134272430624281 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20630550384521484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.362048864364624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31341992488929205 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20280814170837402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3450348377227783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31342031146798816 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21706604957580566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3632199764251709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134239209549768 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21480154991149902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35857367515563965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134266563824245 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2054462432861328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35010480880737305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343648987157 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20357203483581543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35114049911499023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31341162409101214 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2140827178955078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35666441917419434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31340568789414 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20290875434875488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34658336639404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31341046137469153 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1993541717529297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3441765308380127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134121652160372 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2092304229736328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34949350357055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134084667478289 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20003890991210938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3393077850341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134273959057672 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20166587829589844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470165729522705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31340903001172205 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6706 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6861 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1964 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6664 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1579 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6935 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.1383 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1876 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1654 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1372 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2103 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7435 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1540 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1809 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7067 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 1.0350 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7584 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1635 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7714 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1910 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1908 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7023 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7145 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2136 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1347 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6456 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 1.3088 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1829 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1481 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7292 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6549 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2087 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.1467 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1769 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 1.1279 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2473 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7374 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2346 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2092 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1727 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6409 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1493 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1984 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1644 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8307 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1895 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1777 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2391 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6049 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.1873 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2132 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2114 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.2575 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.3208 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1680 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1589 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6200 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6778 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1430 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1921 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2022 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1529 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1871 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2063 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7613 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1547 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2200 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1853 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1897 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2053 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2287 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2008 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2276 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7561 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1800 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1550 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1662 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.1737 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2577 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1471 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1566 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2124 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 1.2574 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1616 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7455 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2234 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6760 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6103 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1366 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7019 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.1684 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2545 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1761 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1725 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7261 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6426 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1579 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6547 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1753 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1470 | test accuracy: 0.805\n",
            "total time:  66.26771522399986\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6676 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6544 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2150 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6681 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1613 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6868 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.1353 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1881 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1551 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1312 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2237 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7307 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1469 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1771 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7330 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 1.0293 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7609 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1617 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7786 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1929 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1946 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7052 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7064 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2151 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1363 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6419 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 1.3368 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1832 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1545 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7193 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6541 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2044 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.1468 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1804 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 1.1185 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2441 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7471 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2378 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2088 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1733 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6403 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1484 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2003 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1611 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8340 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1904 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1756 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2425 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6140 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.1898 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2203 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2079 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.2356 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.3093 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1674 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1621 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6318 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6912 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1417 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1917 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2031 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1531 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1809 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2087 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7523 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1488 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2251 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1902 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1903 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2055 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2340 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2012 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2238 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7464 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1823 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1485 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1626 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.1730 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2595 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1495 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1537 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2185 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 1.2458 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1649 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7367 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2269 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6713 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6210 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1369 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7011 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.1742 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2619 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1764 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1761 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7221 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6442 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1591 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6628 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1770 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1522 | test accuracy: 0.805\n",
            "total time:  66.21629913700008\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21208786964416504.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.3575427532196045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5661198948110853 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19702720642089844.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.34230947494506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4681536934205464 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20680475234985352.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.35103869438171387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.41482294159276145 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21100497245788574.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3525857925415039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.38504656255245207 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20393800735473633.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.35036134719848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36745288584913527 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20346379280090332.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.34856581687927246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3571075520345143 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21988940238952637.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.36046719551086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34795152928148 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1967780590057373.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.33684778213500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34184256706918986 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20430397987365723.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35530900955200195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3367020406893321 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1952965259552002.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3315391540527344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3339403254645211 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19718122482299805.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3423142433166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33094165878636495 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20068573951721191.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35256004333496094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32815373241901397 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21345090866088867.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3574666976928711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32701137363910676 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20871329307556152.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.357419490814209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32543760538101196 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20613765716552734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35569310188293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32382922896317073 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20012259483337402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.339707612991333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32255335577896665 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20081615447998047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34528517723083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32192078615937914 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20177316665649414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3484039306640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3206091667924609 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.197920560836792.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3390662670135498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3199262546641486 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20241093635559082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3445117473602295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.319329885499818 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20261645317077637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35152268409729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3192856852497373 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20095443725585938.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34455084800720215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3184044859239033 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20880913734436035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35001707077026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.318047525201525 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20734071731567383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34665727615356445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3178980312177113 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19707584381103516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.342512845993042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31733916316713606 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1970529556274414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33611440658569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3173296149287905 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082676887512207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3479597568511963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31703485250473024 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19434547424316406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33412766456604004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31676548634256635 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20207428932189941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3435094356536865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3161320613963263 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20809197425842285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3458523750305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31634020805358887 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19777250289916992.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3435661792755127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3161313257047108 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20400500297546387.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3484358787536621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3158269856657301 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21001148223876953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3487248420715332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3156545515571322 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20229744911193848.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34438395500183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3155390007155282 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19997310638427734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3407418727874756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31527114893708913 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21532034873962402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.354433536529541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31535702816077643 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1954352855682373.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.335615873336792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31516638994216917 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20012354850769043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34585118293762207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3150782210486276 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20841479301452637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3518509864807129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31492988509791237 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039330005645752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3477621078491211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3149376954351153 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20311856269836426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34252452850341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3147302772317614 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2118520736694336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3525662422180176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3148063834224428 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20426225662231445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34986376762390137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3146424761840275 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20399808883666992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445591926574707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3146676595721926 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20534586906433105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34558534622192383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3145550800221307 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2033522129058838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3460822105407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3145750769547054 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20232439041137695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420524597167969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31441851854324343 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20739245414733887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3469531536102295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3143746078014374 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20759940147399902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506660461425781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31439279104982104 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19922423362731934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34193992614746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3142744324037007 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20690178871154785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3548893928527832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31423532792500086 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20279359817504883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34389352798461914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3142622858285904 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19871854782104492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3412160873413086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141994718994413 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19649267196655273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3357381820678711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31414959132671355 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20325684547424316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34875059127807617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3142414467675345 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19656682014465332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486645221710205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31405854650906156 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1933302879333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33640551567077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31403459225382124 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2006676197052002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420379161834717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3141043931245804 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20510244369506836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3520669937133789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140095915113177 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19927740097045898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3413991928100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31395880792822156 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20226168632507324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34376072883605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.313993011202131 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20043468475341797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35170841217041016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138949913637979 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021315097808838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487823009490967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31390145548752374 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21036052703857422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508484363555908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31396187884466986 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199171781539917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34357190132141113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31383547953196933 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19851207733154297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446822166442871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31384735192571367 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20155000686645508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34038591384887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31389086161340984 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20023798942565918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34452033042907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137960114649364 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20094919204711914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445003032684326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31374341121741706 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016899585723877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34273433685302734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31374686445508687 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19697260856628418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34819507598876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137452406542642 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20338106155395508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483285903930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137309683220727 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20099687576293945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33937716484069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137090551001685 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20384430885314941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.347705602645874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31374062555176874 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2029857635498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3438549041748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137157414640699 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995100975036621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34001612663269043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136774378163474 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20548582077026367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34841346740722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137319505214691 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19871783256530762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442561626434326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31369714736938475 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19986557960510254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436434268951416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136748901435307 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2120983600616455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35550618171691895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31364273514066426 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19868969917297363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33793115615844727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313657260792596 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19736862182617188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33787059783935547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136655122041702 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20900368690490723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35396766662597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31364643062864034 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20253539085388184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34422850608825684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136153927871159 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19962358474731445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442418575286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.313619835461889 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2164151668548584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3604569435119629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31362338832446507 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064368724822998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3518669605255127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136216184922627 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20576071739196777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35265350341796875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136335364409855 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21443533897399902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36011385917663574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136284611054829 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20766425132751465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35024571418762207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135733676808221 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20024824142456055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34166431427001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.313570499420166 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21834588050842285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.366041898727417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31354559787682124 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068338394165039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3504455089569092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135630969490324 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20190763473510742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36586928367614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31355971566268376 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035981178283691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35619258880615234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135679189647947 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19477319717407227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33615946769714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31352664104529787 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20805048942565918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3593895435333252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135777435132435 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20173907279968262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343904972076416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31352176581110275 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19829392433166504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33756446838378906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135134484086718 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20034575462341309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3526268005371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135536117213113 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.197554349899292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34114575386047363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135514838354928 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19755816459655762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3362541198730469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.313569678579058 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20029449462890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498971462249756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31351043837411063 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20043611526489258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3392472267150879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135337535824094 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20040082931518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3391144275665283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31349538096359797 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20088553428649902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527822494506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135180801153183 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19893550872802734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3388853073120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31350380948611667 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20351862907409668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3456003665924072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.313495688353266 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20689749717712402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3629941940307617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31348521028246196 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20487666130065918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447701930999756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134811367307391 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19902563095092773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3360140323638916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348596385547095 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21453142166137695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3592216968536377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.313470653125218 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20520925521850586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481576442718506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134678504296711 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19808602333068848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34699010848999023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134737036057881 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21778559684753418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36312007904052734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31347580083778925 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20371437072753906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447692394256592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134605829204832 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2030651569366455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34352660179138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134535551071167 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2180342674255371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3679533004760742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134560721261161 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010056972503662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428616523742676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134677244084222 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20218610763549805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486618995666504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134462113891329 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21610260009765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35979652404785156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31348292785031456 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21237421035766602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35217881202697754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.313456557903971 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20509815216064453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34708690643310547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31345134122031076 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21472692489624023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35488247871398926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134666668517249 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20293736457824707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3469703197479248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31342360079288484 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062969207763672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3643920421600342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134322136640549 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20595264434814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34968066215515137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31343516111373904 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20723724365234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35063910484313965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134538335459573 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVyVZf7/8dd9zmERDqAoYLkPk/uWlS2W5UKiTurMtGhK2fIrS8eWcVxQv1hOlmmrreM4ZY3NWEqOfc1o0sxsSLP6YlKNYmniBqggIMuBc//+QI4goKgcDni/n4+HD899n3v5HKzz5rqu+7pvwzRNExERsSybrwsQERHfUhCIiFicgkBExOIUBCIiFqcgEBGxOAWBiIjFOXxdgFy4OnXqxGeffUbLli2rvPfWW2/x3nvv4XK5cLlcXH755cyePZsDBw7whz/8AYC8vDzy8vI8+//2t79l+PDhDBo0iLvvvptp06ZVOub48eP55ZdfWL9+fY01bdq0iT//+c8AHD16lNLSUlq0aAHAhAkTGDVqVK0+26FDh7jnnnv43//939NuN3XqVGJjYxk4cGCtjnsmxcXFvPzyyyQlJVF+5XdsbCwTJ07E39+/Ts4h1mNoHoF4S01BsHHjRp588kmWLVtGeHg4xcXF/OlPfyIsLIzHH3/cs11iYiKrV6/mzTff9KxLT0/n1ltvJTg4mKSkJGy2skZtVlYWt956K8Bpg6CiRYsWcfDgQZ544onz/KT15+GHH6agoIAFCxYQGhpKdnY206ZNw+l08swzz/i6PGmk1DUk9W7Hjh20a9eO8PBwAPz9/XniiSeYOnVqrfYPDAykbdu2bN261bNu7dq1XH311edd28CBA3nppZcYMmQI+/fv56effmLMmDEMHTqUmJgYTwsgPT2drl27AmWBNXnyZOLj4xkyZAjDhg1j586dAMTFxfGvf/0LKAvGVatWMWrUKK699lpPwLndbubOnUu/fv0YM2YMf/nLX4iLi6tS286dO/nss8+YP38+oaGhADRt2pR58+Zx8803Vzlfded//fXXGTJkCPPnz2fu3Lme7Y4cOULv3r3Jzc0lLS2NcePGMWTIEG666Sa+++47APLz85k4cSJDhw5l0KBBzJo1C5fLdd4/c/E9BYHUu2uuuYZNmzYxbdo0PvvsM/Ly8nA6nTidzlofIzY2tlK3zJo1a4iNja2T+g4dOkRSUhIXX3wxTz/9NAMGDGDt2rXMmzePmTNnVvvlt3HjRm6//XaSkpK48sorWbp0abXHTktLY9WqVbzyyis8++yzlJaW8tlnn7Fx40Y+/vhjXn31Vd5///1q992yZQu9e/emadOmldY3b9681iFomiZJSUkMHTqUTz/91LP+008/5aqrriI4OJiJEycycuRIkpKSmDNnDg8++CAlJSWsWrWK0NBQ1q5dS1JSEna7nbS0tFqdVxo2BYHUu65du/KPf/wDt9vN9OnTueqqq5g4cSL79++v9TFuvPFG1q9fj8vlYt++fRQWFtKhQ4c6qe+GG27wvH7llVe45557ALjssssoKioiMzOzyj7R0dF0794dKPt8Bw4cqPbYI0eOBKBbt24UFRVx+PBhtm7dyg033EBwcDBNmzZl+PDh1e6bk5ND8+bNz+ejeT5bz549MU2TH3/8EYB///vfDB06lJ9++onDhw97WhiXXXYZ4eHhfPvtt56/N23ahNvt5rHHHqNLly7nVY80DBosFp/o0aMHCxYswDRNUlNTeeGFF3jkkUdYvnx5rfYPCwuje/fubNq0ibS0NIYOHVpntYWFhXlef/7557z66qscPXoUwzAwTRO3211ln5CQEM9ru91OaWlptccu385utwNl3ULHjh0jKirKs03F1xU1a9aMQ4cOnf0HqqBia+LGG29k3bp1tG3blm+++YaFCxeyY8cOCgsLK/088/LyyM7OZujQoeTk5PDCCy/w008/MWLECGbMmKFB6guAWgRS77Zu3er5QjMMg+7duzNlyhR27NhxVscZPnw4SUlJfPTRRwwbNqzO63S5XDz88MM88MADJCUlsXr1agzDqPPzOJ1Ojh8/7lmursUB0LdvX1JSUqqEwbFjx3jhhRcwTRObzVYpqHJycmo875AhQ1i/fj2bNm3iiiuuwOl0EhkZSXBwMB999JHnz6ZNm4iJiQFg9OjRvPfee3z44YekpqayatWq8/no0kAoCKTeffDBByQkJJCXlwdASUkJa9as4Yorrjir4wwaNIgtW7Zgt9tp06ZNnddZUFDA8ePHPV0+S5cuxc/Pr9KXdl3o0aMHGzZsoLCwkGPHjrF27dpqt4uOjmbYsGE8+uijZGVlAZCdnc2jjz7qabFERER4unu+/fZbdu/eXeN5L730Ug4fPkxiYqKnBdCqVStatmzJRx99BJQNIj/66KMcP36cl19+mRUrVgBlrZbWrVt7JRil/qlrSLwqLi7O0w0C8Oc//5mZM2fy3HPP8fvf/x4oC4Irr7ySJ5988qyOHRQURK9evejRo0ed1lwuNDSUe++9l1GjRtG8eXMeeOABBg8ezIQJE3j99dfr7DwxMTFs2LCB2NhY2rVrx9ChQ0lOTq5227lz5/Lqq68yduxYDMPAz8+PESNGeMYx7rrrLh599FE2btxI37596devX43nNQyDwYMH895773kuPTUMg2effZY5c+bw/PPPY7PZuOuuuwgKCmLkyJHMmDGDxYsXYxgGvXr18ox5SOOmeQQiDYBpmp7frpctW8Z//vMfXn75ZR9XJVahriERH/vhhx8YNGgQOTk5lJSU8PHHH9O7d29flyUWoq4hER/r0qULo0aN4ne/+x12u53evXszbtw4X5clFqKuIRERi1PXkIiIxTWqrqHCwkK2b99OREREpStRRESkZqWlpWRmZtK9e3cCAwOrvN+ogmD79u2MHTvW12WIiDRKy5Yt4/LLL6+yvlEFQUREBFD2Yaq7x72IiFR18OBBxo4d6/kOPVWjCoLy7qCWLVvSunVrH1cjItK41NSlrsFiERGLUxCIiFicgkBExOIUBCIiFqcgEBGxOAWBiIjFWSYIMnOL6PfUetIy8nxdiohY1FNPPUVcXByxsbFcf/31xMXFMWnSpDPu98gjj1BYWOi1uhrVPILzkZFbyL7sAtIy8vh1pNPX5YiIBU2fPh2AxMREdu7cybRp02q133PPPefNsqwTBAGOsokURSXVP1RcRMQXpk+fjp+fH9nZ2Tz55JP88Y9/5Pjx4xQWFjJ79mx69uzJwIED+eCDD5g7dy6RkZGkpqayf/9+Fi5cSLdu3c67BgsFQVkvWFGJ+wxbiogVrPw6nXe37q3TY956eRt+f9nZ3/UgLCyMuXPn8vPPP3PLLbcwePBgkpOTWbx4MYsWLaq0bXFxMUuWLOEf//gHq1atUhCcjQA/BYGINEw9e/YEoEWLFrzyyissWbKE4uJigoKCqmxbftO4li1bsm3btjo5v3WCoLxryKWuIRGB31/W+px+e/cGPz8/AJYuXUpUVBQLFizgu+++4+mnn66ybcX7BdXVc8Usc9WQuoZEpKE7evQobdu2BeCTTz7B5XLVy3kVBCIiDcTIkSN54403uPvuu+nZsyeZmZmsXLnS6+dtVM8sTk9PZ9CgQaxbt+6cbkPdcdZa7urXnhlDu3ihOhGRhulM352WaRFAWaugyKUWgYhIRV4dLJ43bx4pKSkYhkF8fLxnZBxg4MCBtGzZ0jPwsXDhQnbv3s1DDz3EJZdcAkDHjh2ZPXt2ndUT4LCra0hE5BReC4ItW7awZ88eli9fzq5du4iPj2f58uWVtlm8eDHBwcGe5d27d9O3b19efPFFr9QU4LBpQpmIyCm81jWUnJzM4MGDAYiOjiYnJ4e8PN/e5yfAz6YWgYjIKbwWBFlZWTRr1syzHB4eTmZmZqVtEhISGDNmDAsXLvRcD5uWlsaECRMYM2YMX3zxRZ3WFOiwa4xAROQU9Tah7NSLkyZPnsx1111HWFgYEydOJCkpiUsvvZRJkyYxdOhQ9u7dyx133MHHH3+Mv79/ndRQ1iJQ15CISEVeaxFERkaSlZXlWc7IyCAiIsKzPGrUKJo3b47D4aB///7s2LGDqKgohg0bhmEYtG3blhYtWnDo0KE6q6lsjEAtAhGRirwWBP369SMpKQmA1NRUIiMjcTrLbv+cm5vLPffcQ3FxMQBfffUVl1xyCatXr2bJkiUAZGZmcvjwYaKiouqsJl01JCJSlde6hvr06UO3bt0YPXo0hmGQkJBAYmIiISEhxMTE0L9/f2677TYCAgLo2rUrsbGx5OfnM2XKFNatW4fL5WLOnDl11i0E5fMI1DUkIlKRV8cIpkyZUmm5c+fOntd33nknd955Z6X3nU4nr732mtfqCfCzU6wWgYhIJdabWawgEBGpxIJBoK4hEZGKLBYEmkcgInIqawWBZhaLiFRhrSBw2CgudeN2N5o7b4uIeJ3FgqDsTqfFpWoViIiUs1gQnHhKmcYJREQ8rBUEfuWPq9SVQyIi5awVBCe6hjRgLCJyksWCQC0CEZFTWTIICjVGICLiYa0g8FPXkIjIqawVBOoaEhGpwqJBoBaBiEg5iwXBia4hjRGIiHhYKwg0j0BEpAprBYG6hkREqrBYEOiqIRGRU1krCMq7hvTcYhERD2sFgbqGRESqsFQQ+NsVBCIip7JUEBiGoecWi4icwlJBACceYK95BCIiHtYLAj+7uoZERCqwXhCoa0hEpBKLBoFaBCIi5SwYBHaNEYiIVGC9IPBT15CISEXWCwJ1DYmIVGLBINBVQyIiFTm8efB58+aRkpKCYRjEx8fTs2dPz3sDBw6kZcuW2O1lN4JbuHAhUVFRp92nLpTNI1DXkIhIOa8FwZYtW9izZw/Lly9n165dxMfHs3z58krbLF68mODg4LPa53wF+NkpVotARMTDa11DycnJDB48GIDo6GhycnLIy8ur833OlsYIREQq81oQZGVl0axZM89yeHg4mZmZlbZJSEhgzJgxLFy4ENM0a7XP+dKEMhGRyrw6RlCRaZqVlidPnsx1111HWFgYEydOJCkp6Yz71AXNIxARqcxrQRAZGUlWVpZnOSMjg4iICM/yqFGjPK/79+/Pjh07zrhPXSibR6AgEBEp57WuoX79+nl+y09NTSUyMhKn0wlAbm4u99xzD8XFxQB89dVXXHLJJafdp64EOGwUl7pxu+u+tSEi0hh5rUXQp08funXrxujRozEMg4SEBBITEwkJCSEmJob+/ftz2223ERAQQNeuXYmNjcUwjCr71LXy5xYXl7oJtNnr/PgiIo2NV8cIpkyZUmm5c+fOntd33nknd9555xn3qWuex1W63AT6KQhERKw3s7j8Afa6ckhEBLBiEJzoGtKAsYhIGQsGgVoEIiIVWTYICjWXQEQEsGIQ+KlrSESkIusFgbqGREQqsXAQqEUgIgKWDIITXUMaIxARAawYBJpHICJSifWCQF1DIiKVWDAIdNWQiEhF1guC8q4hPbdYRASwYhCoa0hEpBLLBYG/XUEgIlKR5YLAMAw9t1hEpALLBQGceIC95hGIiABWDQI/u7qGREROsGYQqGtIRMTDwkGgFoGICFg0CAL97BQWq0UgIgIWDQJngIPcohJflyEi0iBYMghCAv3ILVQQiIiARYMgNNBBXpHL12WIiDQIlgwCZ6BDLQIRkRPOGAR5eXn8/PPPAGzZsoU333yTI0eOeL0wbwo5EQSmafq6FBERnztjEDz88MNkZGSwc+dO5s+fT3h4ODNmzKiP2rwmJNCPUrdJge5AKiJy5iAoLi7myiuvZO3atYwfP54RI0ZQVFRUH7V5jTPAAUCeuodERGoXBKtXr2bNmjUMGDCA9PR0cnNz66M2rwkJLAuCYwoCEZEzB0FCQgLbtm1jzpw5OJ1OPvvsMx5++OH6qM1rQgP9AMgt1JVDIiKOM23Qpk0bbr/9dn71q1+xZcsWXC4X3bp1q4/avKa8RZCnSWUiIrUbLM7MzDynweJ58+Zx2223MXr0aLZt21btNs888wxxcXEAbN68mauuuoq4uDji4uKYO3fuWXyU2nOeCAJdQioiUosWQflg8Ysvvsj48eO56aabSExMPOOBt2zZwp49e1i+fDm7du0iPj6e5cuXV9omLS2Nr776Cj8/P8+6vn378uKLL57DR6m9EHUNiYh4eG2wODk5mcGDBwMQHR1NTk4OeXl5lbZ56qmneOSRR86x9HMXohaBiIhHrQeLH3vssbMaLM7KyqJZs2ae5fDwcDIzMz3LiYmJ9O3bl1atWlXaLy0tjQkTJjBmzBi++OKLs/kstRbsryAQESl3xq6hLl26EBMTww8//MCOHTvo3r07ffr0OesTVZzFm52dTWJiIm+88QaHDh3yrG/fvj2TJk1i6NCh7N27lzvuuIOPP/4Yf3//sz7f6dhtRtkdSBUEIiJnbhHMmzePN998E9M0KSws5JVXXuG5554744EjIyPJysryLGdkZBAREQHAl19+yZEjRxg7diyTJk0iNTWVefPmERUVxbBhwzAMg7Zt29KiRYtKQVGXym4zoTECEZEztghSU1NZtmyZZ/m+++5j3LhxZzxwv379WLRoEaNHjyY1NZXIyEicTicAsbGxxMbGApCens6MGTOIj49n9erVZGZmcs8995CZmcnhw4eJioo61892WiG68ZyICFCLICgpKaGwsJDAwEAAjh8/Tmnpme/R06dPH7p168bo0aMxDIOEhAQSExMJCQkhJiam2n0GDhzIlClTWLduHS6Xizlz5tR5t1A5Z4BD8whERKhFENx5552MGDGC9u3b43a7+eWXX5g6dWqtDj5lypRKy507d66yTevWrXn77bcBcDqdvPbaa7U69vkKCfQj+3hxvZxLRKQhO2MQDBs2jBtuuIHdu3djGAbt27evdN1/YxUS6GDvkeO+LkNExOdq9WCaoKAgunbtSpcuXWjSpAl33323t+vyupBAPbdYRATO8QllF8IDXcqeW6yrhkREzikIDMOo6zrqXUiAg0KXG1ep29eliIj4VI1jBPPnz6/2C980Tfbu3evVoupDxRvPhQd758okEZHGoMYg6NixY407ne69xqL8xnN5CgIRsbgag+C3v/1tfdZR704+pUzjBCJibec0RnAh0B1IRUTKWDcIAvRMAhEROE0QbN68udJycfHJWbjvvfee9yqqJ3pcpYhImRqD4OWXX660fO+993pef/DBB96rqJ6oa0hEpEyNQXDqpLGKyxfChLKTl4+qa0hErK3GIDh1DkHF5QthQlmAw46/w6bbTIiI5dV4+ajb7aawsNDz23/5stvtxu2+MGbjhuqZBCIiNQfB/v37GT58eKVuoGHDhgEXRosAyu83pCAQEWurMQjWr19fn3X4RNlzizVGICLWVuMYgcvl4vnnn8flOvlFuXPnTl588cV6Kaw+hAQ6yFOLQEQsrsYgmD9/Pnl5eZW6htq1a0deXh4vvfRSvRTnbXpusYjIaYLg22+/ZdasWZWeGezv78/06dP54osv6qU4b3MG6JkEIiI1BoHdbq9+B5utUndRY6YWgYjIaYKgWbNmbN26tcr6DRs20KJFC68WVV9CAx3kFZfgdjf+CXIiIueqxquG4uPj+cMf/kB0dDRdunShtLSUlJQUDhw4wJIlS+qzRq9xBjowTcgvLvE8n0BExGpqDIJ27dqxatUqvvjiC3766ScMw2DcuHH069fvgplH0CyobPzjSH6xgkBELKvGIICy8YDrrruO6667rr7qqVdRoYEAZOQW0a55sI+rERHxDcs+jwBOBsHBnEIfVyIi4jsWD4IAAA4dUxCIiHVZOgjCmvgR4LCRkVvk61JERHzG0kFgGAZRoYHqGhIRS7N0EEBZ95C6hkTEyhQEoYHqGhIRS/NqEMybN4/bbruN0aNHs23btmq3eeaZZ4iLizurfepSVGggh44VXhCP3xQRORdeC4ItW7awZ88eli9fzhNPPMETTzxRZZu0tDS++uqrs9qnrkWFBnC8uFSPrBQRy/JaECQnJzN48GAAoqOjycnJIS8vr9I2Tz31FI888shZ7VPXPJPKNE4gIhbltSDIysqiWbNmnuXw8HAyMzM9y4mJifTt25dWrVrVeh9vKA+CQ8c0TiAi1lRvg8UV++Czs7NJTEzkrrvuqvU+3qLZxSJidae919D5iIyMJCsry7OckZFBREQEAF9++SVHjhxh7NixFBcX88svvzBv3rzT7uMtntnFuQoCEbEmr7UI+vXrR1JSEgCpqalERkbidDoBiI2N5cMPP+Tdd9/lpZdeolu3bsTHx592H28J8ncQEuggQ11DImJRXmsR9OnTh27dujF69GgMwyAhIYHExERCQkKIiYmp9T71QbOLRcTKvBYEAFOmTKm03Llz5yrbtG7dmrfffrvGfepDVGiAuoZExLIsP7MYTswuVteQiFiUgoCTs4v17GIRsSIFARAVEkCJ2+TI8WJflyIiUu8UBEDLsPJJZRonEBHrURAAkaEKAhGxLgUBus2EiFibggCIDAnAbjPYd7TA16WIiNQ7BQHgZ7fRvnkQ/z2U6+tSRETqnYLghM4tQ9mhIBARC1IQnNAxKoRfjhzneLEeUCMi1qIgOKFTyxBME3Yc8u6DcEREGhoFwQmdW4YAsOOguodExFoUBCe0CQ8i0M/GjwoCEbEYBcEJdptBx6gQDRiLiOUoCCroGBWiFoGIWI6CoILOLUPIyivicJ5mGIuIdSgIKuh0YsBYE8tExEoUBBV0itKVQyJiPQqCCiJCAmgW5KcWgYhYioKgAsMwNGAsIpajIDhFz9ZhpO47Rn6RbjUhItagIDjFgM6RFJe6+SIty9eliIjUCwXBKa5oH05IgIP1P2b4uhQRkXqhIDiFn91G/44RrP8xA7fb9HU5IiJepyCoxsDOkWTkFpG6/5ivSxER8ToFQTVu6BSBYaDuIRGxBAVBNZo7A7i0TVPW/3jI16WIiHidgqAGg7pEkZKeQ0Zuoa9LERHxKgVBDQZ1iQTg39+rVSAiFzYFQQ06RYXwq4hgPvzugK9LERHxKoc3Dz5v3jxSUlIwDIP4+Hh69uzpee/dd99lxYoV2Gw2OnfuTEJCAlu2bOGhhx7ikksuAaBjx47Mnj3bmyXWyDAMhve4iJc/TSMrr4gWzgCf1CEi4m1eC4ItW7awZ88eli9fzq5du4iPj2f58uUAFBQUsGbNGpYtW4afnx933HEH3377LQB9+/blxRdf9FZZZ2V4z4tYtD6Nj7YfZNxV7XxdjoiIV3itayg5OZnBgwcDEB0dTU5ODnl5eQA0adKEpUuX4ufnR0FBAXl5eURERHirlHOm7iERsQKvBUFWVhbNmjXzLIeHh5OZmVlpm7/85S/ExMQQGxtLmzZtAEhLS2PChAmMGTOGL774wlvl1YphGPymx0V8+dNhMnP11DIRuTDV22CxaVa9XcN9993HJ598wueff87XX39N+/btmTRpEq+++irz589n5syZFBcX11eJ1RrW8yLcJnyUetCndYiIeIvXgiAyMpKsrJN38MzIyPB0/2RnZ/PVV18BEBgYSP/+/fnmm2+Iiopi2LBhGIZB27ZtadGiBYcO+fbyzU5RIVwS6eQfm3+pNsxERBo7rwVBv379SEpKAiA1NZXIyEicTicAJSUlTJ8+nfz8fAC+++47OnTowOrVq1myZAkAmZmZHD58mKioKG+VWCuGYTDh+mi+P3BMcwpE5ILktauG+vTpQ7du3Rg9ejSGYZCQkEBiYiIhISHExMQwceJE7rjjDhwOB506dWLQoEHk5+czZcoU1q1bh8vlYs6cOfj7+3urxFob2ftiFq3fyfOf7CSmaxSGYfi6JBGROmOYjai/Iz09nUGDBrFu3Tpat25dr+de+XU6f3wvhdfjLmNIt5b1em4RkfNxpu9OzSyupZG9L6Z98yCe/2SnnlMgIhcUBUEtOew2Hh7ckR8OHGPFN+m+LkdEpM4oCM7CiF4Xc1m7Zsxf+yM5BS5flyMiUicUBGfBZjN4bEQ3jh4v5rl/7/B1OSIidUJBcJa6twpj7JXteCt5N9vSs31djojIeVMQnIM/3tiRyJBA4pZsYevuI74uR0TkvCgIzkHTIH/em3A1zYP9GfvXzZpoJiKNmoLgHLUJD+K9CVfT+aJQJvz9az5I2e/rkkREzomC4Dw0dwaw7N4ruaxtMx7657e8/60uKxWRxkdBcJ6cAQ7evPsKruzQnEffTWHZ5j2+LklE5KwoCOpAkL+Dv42/ggGdIpn5/nZe+GSn7lQqIo2GgqCONPG383rcZfyuTyue+2QHd735FUmpBykucfu6NBGR0/Lqw+utxs9u45lbehEd4eTN/+zm/re/poXTnwdu+DVjr2xLoJ/d1yWKiFShFkEdMwyDiQN+TfL0gbwx/go6tQxh7v9+z4CFG3hn8y+4StVCEJGGRUHgJQ67jQGdI1l271W88/+u5KKwQOLf/47Bz37Gu1v3Uugq9XWJIiKAgqBeXBPdgpUPXMPfxl9OsL+DqSu2cfWT63hizfd8+t8MjuT79rnMImJtGiOoJ4ZhMLBzFAM6RZK86zBvJe/hb1/sZvHnPwPQq01Txl/TjmE9LiLAobEEEak/CoJ6ZhgG1/y6Bdf8ugV5RSVs35fDN78cZcXX6TyyPIWEf6XSu20zerUOo1frpvRsE0ZkSKCvyxaRC5iCwIecAQ6u+lVzrvpVcyb0j+bztCw+3HaAlPRsXv40k/IHoQX523HYDJr427m8fTgDOkXSq3UYFzVtgjNA/4Qicn70LdJA2GwG13eM4PqOEQAcLy4hdf8xUvZmcyCnkFK3SU6Bi01pWazZdsCznzPAQXCAHWeAgw4tgulyUSgXhTXBMMBuGIQH+xMREkCLkABaOP3V7SQiVSgIGqggfwdXtA/nivbhlda73SY/HDxGWkYe+7MLycgt5HhRKccKXaRl5PHpfzMpPc0zlVs4/eneKozuF4cBkFPgwsQkrIkfTZv4ExbkR9MmfjQN8qdpkB9hTcr+aA6EyIVLQdDI2GwG3S4Oo9uJL/JTFbpKy77cTXCVujmSX0xmbhGZeUVk5Rax+/BxtqIHeBcAAAzmSURBVO/LYeOOTADCmvgBZYFwmvwg0M9G0yZl4eBnt1Fc4qbE7aaJv50gPwdBAXaC/R0E+dsJDij72zDK9vW322ka5IczwEGp26S41I2/w4YzwEGTE91edsPAbjNw2Mt2Kr9DR4DDToCfjQCHjUA/O/52G3a7UbaPzcBhs2G3GXXzwxWxKAXBBSbQz17pt/c24UHVbldc4sZhM7DZyr94TfKKSsg+7iKnwEX2cRfZBcWe5bJ1xRw97sJV6ibAUfYFXOhyk19UwpH8YvYeOc7x4lLyi0o4XlyKCRhAyekSpo7YDLDbDAzjZKgYJ9bZDAObUfaZi0vdOGw2QgLLwqr8/bJjGNhsYFC2PSf2sxkGxon3MTi57sTfnn2NsosByn6kRpXtjArvlx/P8Lx/4ry2sm1Ovl9hX8rPcbIWTDBP/PtVVH6MsoWyfcvW41lvVLf+xOcxTr7EwKi6n2HUeIzydYangMp1VVqu8j5neN+o9banbnC6cxmnvHvmOk59v+YdTvcZanXsCv8O113SgubOAOqagsCi/B2Vp5AYhkFIoB8hgX60qeNzuUrdHCtwkVdUgsNuw89mUFzqJq+ohILiUkrdpueP60Ro2IyyVkFxiZuiEjeFrlKKStwUlZzcvsRtUlJqUmqauN0mbrPiayh1m5jl60zwt9vwd9goKTXJLXSRX1yC2w0mZe+bZtkXqts0MeHEOhPTpGzdib/Lju32vO+u8GV8crtT9uXkMU7uY5ad/5T3PftS+bxV9jWr/+I1T7x/8vUJNawvP5fuk9jw/WHgr/njjZ3q/LgKAvE6P7uN5s4Ar/wmI95R3sIwzcotDpOTQVMxOCquO3X7ittUPsmpi5VXnLp95WOZNb5X/b41H+xs9z1dXafWdqZwPdtzta2hhX++FAQiUkXFrp8Ta3xWi3ifbjEhImJxCgIREYtTEIiIWJyCQETE4hQEIiIWpyAQEbG4RnX5aGlp2VO9Dh486ONKREQaj/LvzPLv0FM1qiDIzCy7P87YsWN9XImISOOTmZlJu3btqqw3zFOn6DVghYWFbN++nYiICOx23Q1TRKQ2SktLyczMpHv37gQGVn3QVaMKAhERqXsaLBYRsbhGNUZwPubNm0dKSgqGYRAfH0/Pnj19XdIZPf3003z99deUlJRw//3306NHD6ZOnUppaSkREREsWLAAf39/X5d5WoWFhfzmN7/hwQcf5Oqrr25U9a9evZq//vWvOBwOJk+eTKdOnRpN/fn5+UybNo2cnBxcLhcTJ04kIiKCOXPmANCpUycee+wx3xZZjR07dvDggw8yfvx4xo0bx4EDB6r9ma9evZqlS5dis9m49dZbueWWW3xdOlB9/TNmzKCkpASHw8GCBQuIiIhoePWbFrB582bzvvvuM03TNNPS0sxbb73VxxWdWXJysnnvvfeapmmaR44cMa+//npz+vTp5ocffmiapmk+88wz5rJly3xZYq08++yz5u9+9ztz5cqVjar+I0eOmDfeeKOZm5trHjp0yJw1a1ajqv/tt982Fy5caJqmaR48eNAcMmSIOW7cODMlJcU0TdN89NFHzQ0bNviyxCry8/PNcePGmbNmzTLffvtt0zTNan/m+fn55o033mgeO3bMLCgoMIcPH24ePXrUl6Wbpll9/VOnTjXXrFljmqZp/v3vfzfnz5/fIOu3RNdQcnIygwcPBiA6OpqcnBzy8vJ8XNXpXXHFFbzwwgsAhIaGUlBQwObNmxk0aBAAAwYMIDk52ZclntGuXbtIS0vjhhtuAGhU9ScnJ3P11VfjdDqJjIxk7ty5jar+Zs2akZ2dDcCxY8do2rQp+/bt87SEG2L9/v7+LF68mMjISM+66n7mKSkp9OjRg5CQEAIDA+nTpw/ffPONr8r2qK7+hIQEhgwZApz8N2mI9VsiCLKysmjWrJlnOTw83HMpakNlt9sJCiq79/iKFSvo378/BQUFnq6I5s2bN/jPMH/+fKZPn+5Zbkz1p6enU1hYyIQJE7j99ttJTk5uVPUPHz6c/fv3ExMTw7hx45g6dSqhoaGe9xti/Q6Ho8oVLdX9zLOysggPP/ks74by/3N19QcFBWG32yktLeWdd97hpptuapD1W2aMoCKzEV0o9cknn7BixQr+9re/ceONN3rWN/TPsGrVKnr37k2bNtU/76yh1w+QnZ3NSy+9xP79+7njjjtOeeBIw67/X//6FxdffDFLlizhxx9/ZOLEiYSEhHjeb+j1V6emmhv6ZyktLWXq1KlcddVVXH311XzwwQeV3m8I9VsiCCIjI8nKyvIsZ2RkEBER4cOKaufzzz/ntdde469//SshISEEBQVRWFhIYGAghw4dqtQEbWg2bNjA3r172bBhAwcPHsTf379R1d+8eXMuvfRSHA4Hbdu2JTg4GLvd3mjq/+abb7j22msB6Ny5M0VFRZSUlHjeb+j1l6vuv5nq/n/u3bu3D6s8vRkzZtCuXTsmTZoEVP995Ov6LdE11K9fP5KSkgBITU0lMjISp9Pp46pOLzc3l6effprXX3+dpk2bAnDNNdd4PsfHH3/Mdddd58sST+v5559n5cqVvPvuu9xyyy08+OCDjar+a6+9li+//BK3283Ro0c5fvx4o6q/Xbt2pKSkALBv3z6Cg4OJjo5m69atQMOvv1x1P/NevXrx3XffcezYMfLz8/nmm2+4/PLLfVxp9VavXo2fnx+TJ0/2rGuI9VtmQtnChQvZunUrhmGQkJBA586dfV3SaS1fvpxFixbRoUMHz7qnnnqKWbNmUVRUxMUXX8yTTz6Jn5+fD6usnUWLFtGqVSuuvfZapk2b1mjq/+c//8mKFSsAeOCBB+jRo0ejqT8/P5/4+HgOHz5MSUkJDz30EBEREfzP//wPbrebXr16MWPGDF+XWcn27duZP38++/btw+FwEBUVxcKFC5k+fXqVn/lHH33EkiVLMAyDcePGMWLECF+XX239hw8fJiAgwPOLZ3R0NHPmzGlw9VsmCEREpHqW6BoSEZGaKQhERCxOQSAiYnEKAhERi1MQiIhYnIJALgjp6elceumlxMXFVfpTfr+d87Fo0SL+/ve/n3abTp06sX79es/y5s2bWbRo0Tmfc/PmzZWuPRfxJkvMLBZr6NChA2+//bZPzt2+fXteeuklrr/+ej09TxodBYFc8KZPn05QUBA//fQTR48e5cknn6Rr164sXbqUDz/8EIBBgwZx3333sW/fPqZPn05paSkXX3wx8+fPB8ruM3///feze/duZs6cSf/+/SudIzIykh49evD+++9z8803V3rvyiuvZPPmzQBMnjyZsWPHsmXLFo4ePcqePXtIT0/noYceYuXKlezbt4/FixcDkJOTw8SJE9m3bx8xMTFMnDiRtLQ0Hn/8cQzDIDg4mKeeeopjx47xpz/9iaCgIMaNG8eAAQO8/SOVC4y6hsQSSkpKePPNN3nooYd4+eWX2bt3L++//z7Lli1j2bJlrF27ll9++YXnnnuO8ePH88477xAZGcn27duBshvQvf7668yaNYt//vOf1Z7j/vvvZ+nSpRQWFtaqppycHJYsWUJsbCyrVq3yvF63bh0A//3vf3n66ad59913WblyJdnZ2cydO5fHH3+cpUuX0q9fP5YtWwbADz/8wMKFCxUCck7UIpALxs8//0xcXJxnuUOHDjz++ONA2T1rAHr37s3ChQv54Ycf6NWrFw5H2f8Cffr04ccff+T7779n5syZAEydOhWAjRs30qdPHwCioqLIzc2t9vxhYWGMHDmSt956i169ep2x3h49egBUugFiixYtPOMa3bt3Jzg4GCi7NcHevXvZtm0bs2fPBqC4uNhzjDZt2lS61brI2VAQyAXjdGMEbrfb89owDAzDqHT7X5fLhc1mw263V3tb4PLAOJO4uDhuvvlm2rdvX+37Lper2mNWfF1+fsMwKu1rGAZNmjThrbfeqvReenp6g73nkTQO6hoSS/j6668B+Pbbb4mOjqZLly783//9HyUlJZSUlJCSkkKXLl3o3r07X375JQAvvPAC//nPf87qPAEBAdx111289tprnnWGYVBQUEBBQQE//PBDrY/1/fffU1BQQFFREbt27aJt27Z07tyZjRs3ArBmzZoG95QxaZzUIpALxqldQwB/+tOfACgqKuL+++/nwIEDLFiwgNatW3Pbbbcxbtw4TNPklltuoVWrVkyePJkZM2bwzjvvcNFFFzFp0iRPiNTWqFGjeOONNzzLY8aM4dZbbyU6Oppu3brV+jhdu3YlPj6e3bt3M3r0aEJDQ5k5cyazZ89m8eLFBAQE8MwzzzT4x65Kw6e7j8oFb/r06QwZMkQDqSI1UNeQiIjFqUUgImJxahGIiFicgkBExOIUBCIiFqcgEBGxOAWBiIjFKQhERCzu/wOMYTYGiimh1AAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.2360 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2112 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1587 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2278 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7803 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1862 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.1546 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1453 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2544 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6602 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1511 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2361 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2102 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1004 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6541 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2717 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6934 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1658 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6721 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2282 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1799 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7001 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6349 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7055 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6887 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1916 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6752 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2504 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.4872 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2187 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1685 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2640 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.3190 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7509 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6810 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1511 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1593 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1750 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2179 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2050 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.1702 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7018 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.8249 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1959 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7147 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7244 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1754 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2139 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2027 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8122 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7150 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1897 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1683 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7719 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1772 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1911 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5890 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.1631 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.8201 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5648 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.8998 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1783 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1956 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7920 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1775 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6944 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2323 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2232 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1623 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1636 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 1.2729 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.5687 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7385 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1733 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.2041 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6976 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1855 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.2198 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2040 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1816 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2221 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6711 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1575 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1762 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6967 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1429 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6952 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2108 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1811 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1730 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2541 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.1217 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7464 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1980 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1798 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2099 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7538 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6463 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7773 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6558 | test accuracy: 0.805\n",
            "total time:  65.725007172\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.2134 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2228 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1327 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2349 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7689 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1851 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.1582 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1287 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2462 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6691 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1611 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2444 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2151 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0968 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6387 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2668 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7000 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1565 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6837 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2313 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1768 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7040 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6469 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6990 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6903 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2019 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6962 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2471 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.4972 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2136 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1714 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2657 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.3238 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7660 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6803 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1552 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1583 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1742 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2193 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2001 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.1661 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6933 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.8212 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1954 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6970 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7188 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1721 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2210 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2037 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8102 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7147 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1929 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1699 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7806 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1815 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1854 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5742 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.1753 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.8147 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5505 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.9040 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1787 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1973 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7947 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1856 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6987 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2359 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2258 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1635 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1686 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 1.2784 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.5690 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7310 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1749 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.2074 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7086 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1846 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.2454 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2036 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1807 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2256 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6744 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1566 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1760 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7045 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1432 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7079 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2136 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1883 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1766 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2531 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.0999 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7423 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1941 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1776 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2141 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7769 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6446 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7821 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6510 | test accuracy: 0.805\n",
            "total time:  66.68951298000002\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19842004776000977.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.3482229709625244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6839309266635349 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2131178379058838.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.35407233238220215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5395361742803029 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20791220664978027.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.349412202835083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.45856252270085474 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045459747314453.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.34500956535339355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41219628282955717 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21284842491149902.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3601398468017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3895618826150894 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20380902290344238.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.34906959533691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.370361938221114 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19949007034301758.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.34430551528930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.35956117595945086 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21580028533935547.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3587639331817627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3503775643450873 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20248889923095703.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34391045570373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3434117432151522 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20907354354858398.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3572971820831299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34055415519646237 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21530365943908691.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.36017298698425293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33585369970117296 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20460128784179688.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3480038642883301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33278467782906124 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2038569450378418.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35474562644958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3299597190959113 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2182481288909912.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3622560501098633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32765948815005164 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.210845947265625.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3534579277038574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.328268278496606 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20652174949645996.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3613760471343994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3261485993862152 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20785236358642578.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3610994815826416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32408099004200525 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2056431770324707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35230565071105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32357489083494456 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068486213684082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3540074825286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3227331493582044 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042217254638672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34416770935058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32209474572113583 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20009231567382812.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3424992561340332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32090939836842675 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2056279182434082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3614463806152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3207057590995516 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20815396308898926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3551943302154541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31956987508705686 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20440196990966797.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35511279106140137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3193264322621482 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20678257942199707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35483646392822266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31847320369311743 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20240354537963867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.342972993850708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3180384308099747 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20932650566101074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35162997245788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31783271176474437 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21422362327575684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3596677780151367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31761250070163183 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20223784446716309.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3477349281311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31774244223322184 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2075333595275879.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3566620349884033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31699853624616353 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21433043479919434.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36508655548095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3169525504112244 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20039844512939453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3462202548980713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3165483031954084 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090921401977539.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35600948333740234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31670989436762675 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21480083465576172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3572397232055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3161773396389825 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20067262649536133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34938693046569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31614314360277995 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20653390884399414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35158443450927734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3158685994999749 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20978713035583496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3513350486755371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3159079134464264 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2073347568511963.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34946393966674805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3155429422855377 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20773077011108398.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35411763191223145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3153800917523248 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21230196952819824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35498857498168945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31560459307261873 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20303130149841309.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35056352615356445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3153800189495087 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20259547233581543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35536742210388184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3150934743029731 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21465730667114258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.360933780670166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31520904515470777 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20383310317993164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34937119483947754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31501190960407255 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048335075378418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35501885414123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31490287397589 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048802375793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34644269943237305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31486626863479616 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20177054405212402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34749555587768555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3147121305976595 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21321535110473633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36324596405029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147163680621556 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20419836044311523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3431053161621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3145722440310887 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20757722854614258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527500629425049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31453759585108076 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2185685634613037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3618309497833252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3145274920122964 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199601411819458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483846187591553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31441732943058015 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057187557220459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549380302429199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144593732697623 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2206108570098877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36536216735839844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143521547317505 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20850825309753418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3530125617980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31429716902119775 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21010351181030273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36065053939819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143316371100289 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23074054718017578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3753232955932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142357873065131 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069094181060791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3619425296783447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142501511744091 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20738720893859863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34920501708984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142679329429354 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21325206756591797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546426296234131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3140192142554692 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2036581039428711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34549689292907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31404446576322825 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20402979850769043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351123571395874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31410019993782046 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20142674446105957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34259843826293945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139712712594441 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2011854648590088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35034775733947754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140378330435072 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047426700592041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3547539710998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3139607982976096 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972510814666748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3351776599884033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139655441045761 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2008988857269287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38430333137512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31393243031842366 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20175766944885254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3495464324951172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139537057706288 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995682716369629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445718288421631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31388530475752696 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066655158996582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34893226623535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31393908219678063 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20375800132751465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35305333137512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139086080448968 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2075519561767578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35375070571899414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138200696025576 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20477700233459473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35170912742614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31379477509430476 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21370887756347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3528249263763428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3137799710035324 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20563602447509766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355273962020874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138119093009404 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20705056190490723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510251045227051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137616668428693 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143099308013916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35334229469299316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137624749115535 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20380425453186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3462491035461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137723939759391 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2023329734802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3431122303009033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136658753667559 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20905709266662598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35558152198791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31375162644045695 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2050633430480957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467433452606201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31370136865547726 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20066165924072266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3402397632598877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136806058032172 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20568180084228516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34363865852355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31366863335881917 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062673568725586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34621644020080566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136852979660034 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19964241981506348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3406515121459961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136624438422067 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2119140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3594841957092285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136625558137894 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20314574241638184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516554832458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136521999325071 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20756173133850098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349780797958374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136508277484349 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2093653678894043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523106575012207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136164984532765 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1984424591064453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33942151069641113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136487432888576 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20173239707946777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33980393409729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31359335482120515 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21117877960205078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35601258277893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31362483884607045 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20259904861450195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461489677429199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136333308049611 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20311927795410156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483889102935791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135732765708651 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20366621017456055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34571242332458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135825195482799 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20272135734558105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34851646423339844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31356556458132606 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20212078094482422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3495192527770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358395431722913 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19981002807617188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430302143096924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135494691984994 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066798210144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3471372127532959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31356382838317326 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2015552520751953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3482487201690674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135308048554829 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20022940635681152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3388948440551758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135693622486932 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19882798194885254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34529852867126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31352720047746385 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010035514831543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35074782371520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354317494801115 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19927406311035156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481886386871338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31354599254471915 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20351529121398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34464359283447266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.313558834365436 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20392560958862305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36043310165405273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135018506220409 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19870924949645996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3416004180908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135327092238835 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20190739631652832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432786464691162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135093603815351 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21009469032287598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35298681259155273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31351566059248787 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20040035247802734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34902262687683105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135333167655127 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19654273986816406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3329918384552002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134861903531211 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20435237884521484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34981560707092285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134952030011586 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2036752700805664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34529829025268555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31350943275860377 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20012211799621582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.342085599899292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135076024702617 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21988797187805176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36675310134887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135062443358558 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20972180366516113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35448408126831055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3135069830077035 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20265889167785645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34389781951904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134885213204793 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21158790588378906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35963010787963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31347913912364417 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20479345321655273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34696030616760254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31348451290811813 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20636320114135742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34880971908569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134679283414568 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21304798126220703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36423707008361816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31347876404012953 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21026897430419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35743188858032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31347814202308655 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21595501899719238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598480224609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3135195813008717 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048189640045166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3477206230163574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31345365132604325 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20324134826660156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34624409675598145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345926012311665 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027294635772705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534712791442871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31345382758549284 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20719242095947266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3572275638580322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31343332486493247 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20516061782836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34747934341430664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134414779288428 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7110 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7163 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2080 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.1994 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6514 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7197 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7311 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2995 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1562 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2364 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6278 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 1.0341 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2135 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6105 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5826 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1987 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1706 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1704 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5995 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1756 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1950 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1678 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1990 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7802 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6976 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6637 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2403 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2205 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1505 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6073 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2634 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.8905 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8141 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2102 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6826 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2057 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1757 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1958 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6961 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7626 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6249 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8144 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2203 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.5792 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2550 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1935 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2032 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1715 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5472 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7969 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.1829 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7450 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6232 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.8102 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1738 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.1874 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2782 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2172 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6353 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2184 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1623 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1683 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7259 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.8164 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2395 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1703 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1412 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1883 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7531 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6195 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1895 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6714 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1667 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2284 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1757 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1902 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1838 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2144 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7051 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.9253 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1997 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6391 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1569 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.1081 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1891 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.5880 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7556 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1695 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1644 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2007 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9518 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1725 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6650 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.8142 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1648 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6921 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.2087 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2340 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.1654 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2289 | test accuracy: 0.805\n",
            "total time:  66.32162857000003\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6654 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7247 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2202 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.1984 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6815 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7149 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7365 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.3077 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1531 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2300 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6321 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 1.0149 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2054 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6160 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5891 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1932 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1850 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1720 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6054 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1612 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1999 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1689 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1859 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7786 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6967 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6685 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2379 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2226 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1435 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6124 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2827 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.8773 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7976 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2170 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6797 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1924 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1727 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2015 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6884 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7522 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6427 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7934 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2255 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.5774 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2543 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2015 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2044 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1662 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5402 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7870 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.1768 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7318 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6376 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7993 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1737 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.1704 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2869 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2195 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6277 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2101 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1690 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1760 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7252 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.8154 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2509 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1699 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1402 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1898 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7413 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6140 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1902 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6795 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1683 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2212 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1764 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1930 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1780 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2095 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7020 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.9271 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2041 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5986 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1493 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.1167 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1934 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6012 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7632 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1642 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1620 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1974 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9466 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1759 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6659 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7994 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1723 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7003 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.2226 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2492 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.1667 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2267 | test accuracy: 0.805\n",
            "total time:  66.91212959899985\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1957533359527588.\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epoch took: 0.3394794464111328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.47959653351988113 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1955885887145996.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.3424980640411377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.41368315134729655 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20628023147583008.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.35414695739746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.38150632083415986 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20808005332946777.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3494689464569092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3608319537980216 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.196943998336792.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3490276336669922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3502060430390494 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2038555145263672.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3478410243988037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3421269476413727 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19956254959106445.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34102582931518555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.33681248639311107 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21318364143371582.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3657865524291992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33315920404025484 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20033693313598633.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3405938148498535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.32958585577351707 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19710993766784668.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3339378833770752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3260199725627899 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19876599311828613.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3527963161468506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3253453808171409 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19676423072814941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3417243957519531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3236959159374237 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19924664497375488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3436732292175293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32222060220582144 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21194767951965332.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36400413513183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32128044750009266 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.193817138671875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33283329010009766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3206708469561168 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20321416854858398.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3466506004333496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3198397385222571 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22341465950012207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36844420433044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31957498277936663 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20841574668884277.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3533799648284912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.31858235895633696 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051229476928711.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3488123416900635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3180776574781963 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21222996711730957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35465192794799805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3178163243191583 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20166349411010742.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34029555320739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.317360212121691 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20240426063537598.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3466067314147949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3166693159512111 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2186877727508545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3653287887573242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3168927426849093 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20978879928588867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3528707027435303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3166715656008039 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20821547508239746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3605778217315674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.316742114509855 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095177173614502.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35433030128479004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3161423312766211 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20259428024291992.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35373878479003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31581889178071704 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064218521118164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35950541496276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3155455023050308 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042691707611084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3544337749481201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31548813240868706 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21453332901000977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3649439811706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.315508171916008 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21432852745056152.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3726215362548828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31552661657333375 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2123105525970459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35572052001953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3150138237646648 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20160913467407227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34633541107177734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3149919407708304 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20215368270874023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35213565826416016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31500751205853056 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2075197696685791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3489837646484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31489752062729426 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20313167572021484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34731054306030273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31480588487216404 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2212235927581787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36926984786987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3147593868630273 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20928120613098145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35364222526550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31449926580701554 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2093827724456787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35697054862976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3146165132522583 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2191455364227295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3639230728149414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31443617045879363 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20579242706298828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34882688522338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3143268214804786 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21227574348449707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556375503540039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3143749062504087 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20430827140808105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458409309387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31432615986892154 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2008802890777588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446817398071289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3142068317958287 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2061915397644043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35413336753845215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31422678359917233 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21384572982788086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35779571533203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31414992000375475 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2101895809173584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.353604793548584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31414737105369567 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21157479286193848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36184167861938477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3141490706375667 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21628761291503906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35940980911254883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31402297403131213 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20688700675964355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35801148414611816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3140786771263395 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20532870292663574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35715174674987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31418010890483855 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21077466011047363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3557438850402832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.314003791979381 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21038532257080078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554108142852783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3139536095517022 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20859932899475098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3634331226348877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3139010459184647 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20299577713012695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345306396484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3139982142618724 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2089705467224121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356273889541626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3139200917312077 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21038269996643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36539769172668457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31389979507241933 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21614933013916016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3581702709197998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3138583736760276 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20691704750061035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3532097339630127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31387333273887635 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20843291282653809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34842538833618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3138685200895582 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2040388584136963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3482511043548584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138057930128915 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.207871675491333.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3563079833984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31386395267077855 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21737957000732422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575596809387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3137335909264428 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047724723815918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467998504638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31370937526226045 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20245575904846191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3469357490539551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3137532889842987 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2185990810394287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35923075675964355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137017560856683 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20240068435668945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3507685661315918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31369741601603374 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20110678672790527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34390807151794434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31372860882963455 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20784854888916016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523547649383545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3136999594313758 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20140457153320312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3407936096191406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3136968374252319 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20366716384887695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35146570205688477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31368530648095266 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2094259262084961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3489668369293213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3136536002159119 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19762039184570312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423142433166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31366994721548896 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20281624794006348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34458041191101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3136610129049846 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20966887474060059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35078978538513184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136173469679696 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111527919769287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533458709716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31364214633192333 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20070600509643555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34932827949523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31362940796783995 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20255684852600098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31360431483813694 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21095824241638184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3578953742980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3135606999908175 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20989537239074707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36240482330322266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31360892866339 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20408868789672852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455333709716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3135771759918758 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20326590538024902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454020023345947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31359679656369344 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046976089477539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3522987365722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3135669661419732 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19991755485534668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3497440814971924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31355972971235 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20581531524658203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465142250061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31354835544313703 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20131659507751465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35272884368896484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31357079233442037 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2268967628479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3734009265899658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.313594725728035 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082076072692871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35506129264831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135641655751637 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21239328384399414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3568253517150879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31354497841426304 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20733976364135742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35492587089538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31351348204272134 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2109363079071045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3586885929107666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31359064536435266 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21705865859985352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36179685592651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135063069207328 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20909953117370605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35334157943725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135051497391292 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20791244506835938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3499312400817871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135269548211779 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106320858001709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3550560474395752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31351789619241444 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20715832710266113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352031946182251\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31350898827825274 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20721125602722168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35175538063049316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31349712652819495 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21252965927124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35430359840393066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31348834974425177 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2092452049255371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35446715354919434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31347569567816597 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057178020477295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35540294647216797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31347561010292596 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.206498384475708.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35234498977661133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3134963069643293 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20734620094299316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3490912914276123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31348165784563337 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20208334922790527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3511836528778076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31346481314727237 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20937156677246094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3560340404510498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134561594043459 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21302318572998047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3568267822265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134767430169242 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20820069313049316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35545945167541504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31343823288168227 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20706534385681152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34978628158569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134434780904225 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20954513549804688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349153995513916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31345768911497934 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21718096733093262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36339640617370605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31347341239452364 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21103286743164062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598470687866211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134596973657608 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20841574668884277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35074758529663086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31344201394489835 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20588207244873047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35183286666870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134389366422381 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20350193977355957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34317803382873535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31345533302852085 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2086787223815918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3548722267150879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31344092999185835 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22365880012512207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38681912422180176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134268764938627 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20673561096191406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559145927429199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31342116381440843 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2036747932434082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3476252555847168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134241908788681 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21651482582092285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3699650764465332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134523102215358 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199873685836792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33962035179138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134519010782242 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20353198051452637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35602593421936035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134097789015089 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21112704277038574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35819530487060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134415132658822 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21010947227478027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537404537200928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134047197444098 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21309971809387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36891961097717285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.313400291119303 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21193456649780273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3649017810821533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134201518126896 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21024036407470703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515751361846924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31340342164039614 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20572614669799805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349719762802124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343374592917306 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20130348205566406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3507668972015381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31341687100274224 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19949626922607422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3413355350494385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134188392332622 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.1522 | test accuracy: 0.822\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2203 | test accuracy: 0.822\n",
            "Epoch:  2 Iteration:  210 | train loss: 1.3377 | test accuracy: 0.822\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6634 | test accuracy: 0.822\n",
            "Epoch:  4 Iteration:  350 | train loss: 1.3559 | test accuracy: 0.822\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7354 | test accuracy: 0.822\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6344 | test accuracy: 0.822\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6804 | test accuracy: 0.822\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7270 | test accuracy: 0.822\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7194 | test accuracy: 0.822\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5529 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7166 | test accuracy: 0.822\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6658 | test accuracy: 0.822\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.4216 | test accuracy: 0.822\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.1984 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8059 | test accuracy: 0.822\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2119 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6142 | test accuracy: 0.822\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3264 | test accuracy: 0.822\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2808 | test accuracy: 0.822\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2875 | test accuracy: 0.822\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6210 | test accuracy: 0.822\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2306 | test accuracy: 0.822\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2125 | test accuracy: 0.822\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6735 | test accuracy: 0.822\n",
            "Epoch:  25 Iteration:  1820 | train loss: 1.0028 | test accuracy: 0.822\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2063 | test accuracy: 0.822\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1908 | test accuracy: 0.822\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7467 | test accuracy: 0.822\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6404 | test accuracy: 0.822\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2721 | test accuracy: 0.822\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6944 | test accuracy: 0.822\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5931 | test accuracy: 0.822\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6220 | test accuracy: 0.822\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7750 | test accuracy: 0.822\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2164 | test accuracy: 0.822\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6003 | test accuracy: 0.822\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2463 | test accuracy: 0.822\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6903 | test accuracy: 0.822\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6335 | test accuracy: 0.822\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6905 | test accuracy: 0.822\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1785 | test accuracy: 0.822\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.9077 | test accuracy: 0.822\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1752 | test accuracy: 0.822\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.0146 | test accuracy: 0.822\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7772 | test accuracy: 0.822\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1718 | test accuracy: 0.822\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7403 | test accuracy: 0.822\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2251 | test accuracy: 0.822\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6288 | test accuracy: 0.822\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5595 | test accuracy: 0.822\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2278 | test accuracy: 0.822\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7824 | test accuracy: 0.822\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7996 | test accuracy: 0.822\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6679 | test accuracy: 0.822\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1969 | test accuracy: 0.822\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7151 | test accuracy: 0.822\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7604 | test accuracy: 0.822\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2702 | test accuracy: 0.822\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5344 | test accuracy: 0.822\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.1931 | test accuracy: 0.822\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2022 | test accuracy: 0.822\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1590 | test accuracy: 0.822\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5655 | test accuracy: 0.822\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1754 | test accuracy: 0.822\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.6140 | test accuracy: 0.822\n",
            "Epoch:  66 Iteration:  4690 | train loss: 1.0409 | test accuracy: 0.822\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7941 | test accuracy: 0.822\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2126 | test accuracy: 0.822\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1827 | test accuracy: 0.822\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6804 | test accuracy: 0.822\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1947 | test accuracy: 0.822\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8140 | test accuracy: 0.822\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.3043 | test accuracy: 0.822\n",
            "Epoch:  74 Iteration:  5250 | train loss: 1.1529 | test accuracy: 0.822\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2017 | test accuracy: 0.822\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1877 | test accuracy: 0.822\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1837 | test accuracy: 0.822\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.9161 | test accuracy: 0.822\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2455 | test accuracy: 0.822\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6527 | test accuracy: 0.822\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.2398 | test accuracy: 0.822\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2522 | test accuracy: 0.822\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6978 | test accuracy: 0.822\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1573 | test accuracy: 0.822\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2417 | test accuracy: 0.822\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1935 | test accuracy: 0.822\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1994 | test accuracy: 0.822\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6998 | test accuracy: 0.822\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8082 | test accuracy: 0.822\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2022 | test accuracy: 0.822\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2555 | test accuracy: 0.822\n",
            "Epoch:  92 Iteration:  6510 | train loss: 1.1025 | test accuracy: 0.822\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6714 | test accuracy: 0.822\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7293 | test accuracy: 0.822\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7421 | test accuracy: 0.822\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.0031 | test accuracy: 0.822\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2293 | test accuracy: 0.822\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.9950 | test accuracy: 0.822\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1909 | test accuracy: 0.822\n",
            "total time:  66.55529339500026\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.2324 | test accuracy: 0.822\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2036 | test accuracy: 0.822\n",
            "Epoch:  2 Iteration:  210 | train loss: 1.3449 | test accuracy: 0.822\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6716 | test accuracy: 0.822\n",
            "Epoch:  4 Iteration:  350 | train loss: 1.3376 | test accuracy: 0.822\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7343 | test accuracy: 0.822\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6244 | test accuracy: 0.822\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7140 | test accuracy: 0.822\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7215 | test accuracy: 0.822\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7390 | test accuracy: 0.822\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5615 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6965 | test accuracy: 0.822\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6794 | test accuracy: 0.822\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.4326 | test accuracy: 0.822\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.1999 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8181 | test accuracy: 0.822\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2091 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6039 | test accuracy: 0.822\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3290 | test accuracy: 0.822\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2876 | test accuracy: 0.822\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2873 | test accuracy: 0.822\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6332 | test accuracy: 0.822\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2276 | test accuracy: 0.822\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2103 | test accuracy: 0.822\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6710 | test accuracy: 0.822\n",
            "Epoch:  25 Iteration:  1820 | train loss: 1.0189 | test accuracy: 0.822\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2095 | test accuracy: 0.822\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1902 | test accuracy: 0.822\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7497 | test accuracy: 0.822\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6463 | test accuracy: 0.822\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2724 | test accuracy: 0.822\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6832 | test accuracy: 0.822\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5937 | test accuracy: 0.822\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6065 | test accuracy: 0.822\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7721 | test accuracy: 0.822\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2249 | test accuracy: 0.822\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5869 | test accuracy: 0.822\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2494 | test accuracy: 0.822\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6988 | test accuracy: 0.822\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6261 | test accuracy: 0.822\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6971 | test accuracy: 0.822\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1710 | test accuracy: 0.822\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.9172 | test accuracy: 0.822\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1737 | test accuracy: 0.822\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.0231 | test accuracy: 0.822\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7796 | test accuracy: 0.822\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1676 | test accuracy: 0.822\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7503 | test accuracy: 0.822\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2351 | test accuracy: 0.822\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6092 | test accuracy: 0.822\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5593 | test accuracy: 0.822\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2217 | test accuracy: 0.822\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7885 | test accuracy: 0.822\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7917 | test accuracy: 0.822\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6521 | test accuracy: 0.822\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2094 | test accuracy: 0.822\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7176 | test accuracy: 0.822\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7635 | test accuracy: 0.822\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2773 | test accuracy: 0.822\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5169 | test accuracy: 0.822\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.1718 | test accuracy: 0.822\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1985 | test accuracy: 0.822\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1634 | test accuracy: 0.822\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5558 | test accuracy: 0.822\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1803 | test accuracy: 0.822\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.6298 | test accuracy: 0.822\n",
            "Epoch:  66 Iteration:  4690 | train loss: 1.0066 | test accuracy: 0.822\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7885 | test accuracy: 0.822\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2080 | test accuracy: 0.822\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1834 | test accuracy: 0.822\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6883 | test accuracy: 0.822\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1945 | test accuracy: 0.822\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.8132 | test accuracy: 0.822\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.3117 | test accuracy: 0.822\n",
            "Epoch:  74 Iteration:  5250 | train loss: 1.1657 | test accuracy: 0.822\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2008 | test accuracy: 0.822\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1945 | test accuracy: 0.822\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1849 | test accuracy: 0.822\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.9175 | test accuracy: 0.822\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2414 | test accuracy: 0.822\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6474 | test accuracy: 0.822\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.2558 | test accuracy: 0.822\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2497 | test accuracy: 0.822\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6707 | test accuracy: 0.822\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1629 | test accuracy: 0.822\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2459 | test accuracy: 0.822\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1918 | test accuracy: 0.822\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1982 | test accuracy: 0.822\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7004 | test accuracy: 0.822\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8076 | test accuracy: 0.822\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2006 | test accuracy: 0.822\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2609 | test accuracy: 0.822\n",
            "Epoch:  92 Iteration:  6510 | train loss: 1.1008 | test accuracy: 0.822\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6699 | test accuracy: 0.822\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7248 | test accuracy: 0.822\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7319 | test accuracy: 0.822\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.0164 | test accuracy: 0.822\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2317 | test accuracy: 0.822\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.0026 | test accuracy: 0.822\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1879 | test accuracy: 0.822\n",
            "total time:  66.69563480899978\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19888830184936523.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.3465743064880371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5728576276983534 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20771217346191406.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.35283637046813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4706203392573765 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2081286907196045.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.35277271270751953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4182753767286028 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20299339294433594.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3518373966217041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.390077161363193 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095344066619873.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3553309440612793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.372053342631885 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21274638175964355.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3559224605560303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.36074969215052466 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20178580284118652.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35447168350219727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.35009799386773793 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20392847061157227.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35756897926330566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34403235742024013 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20941162109375.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35329341888427734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33941997587680817 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20349502563476562.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35378575325012207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3357085534504482 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20937585830688477.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35526394844055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3329982097659792 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2062525749206543.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34718894958496094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32944585595812115 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20915985107421875.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35837411880493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3283283101660865 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20751690864562988.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34859776496887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3265216380357742 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017192840576172.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3447105884552002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3252192552600588 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21420836448669434.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3561420440673828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3236388657774244 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20857501029968262.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3568911552429199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.322586208156177 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20070743560791016.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34015798568725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32172145630632126 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20813775062561035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3521721363067627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32111950261252264 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20184659957885742.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3445272445678711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3201477438211441 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19955873489379883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3433389663696289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3195954173803329 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20929384231567383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34833621978759766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31874758047716956 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20196533203125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33996129035949707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31866027414798737 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20088982582092285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34352636337280273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31794365090983256 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20962214469909668.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3520629405975342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3179164784295218 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2033677101135254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3422818183898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3176012511764254 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017514705657959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3454718589782715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31746269336768557 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21451091766357422.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36254024505615234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3168914828981672 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20334887504577637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34142374992370605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31666169847760883 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20268559455871582.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3460979461669922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3162777168410165 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066950798034668.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.348560094833374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31599992641380853 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20678043365478516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3473803997039795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31598431425435203 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1980893611907959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33640432357788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3160425028630665 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21354413032531738.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35324931144714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31588870925562723 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21352410316467285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3526477813720703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31564354853970666 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19995880126953125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34848499298095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31544826669352394 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20228242874145508.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34656643867492676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31535132697650364 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19655537605285645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33918094635009766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3152254083326885 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20713400840759277.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3636195659637451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31505778644766125 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20335936546325684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34724950790405273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3152840746300561 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016146183013916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34591078758239746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31506398235048566 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21947884559631348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.374847412109375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3149986552340644 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21202659606933594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35446882247924805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3148188889026642 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20697402954101562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479022979736328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3147822686604091 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2123723030090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35903382301330566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3145597504717963 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20180869102478027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3409278392791748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31458952512059896 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2023155689239502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34395503997802734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3145919442176819 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21474933624267578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3582644462585449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31448081093175073 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21294569969177246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35551977157592773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31451714975493295 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20419883728027344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3567981719970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3144240609237126 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21662259101867676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3668670654296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31429905593395235 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19840288162231445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423490524291992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3143629227365766 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20546793937683105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35483217239379883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3143476856606347 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22367048263549805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36893439292907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3142797120979854 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20630931854248047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34836506843566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31410488145692006 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20693588256835938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3544459342956543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31420959532260895 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22075414657592773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3634154796600342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142510005405971 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20171189308166504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3415186405181885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31412269175052643 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21088624000549316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36090683937072754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31418504757540566 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19970917701721191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3433237075805664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31400933180536544 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20166850090026855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34153151512145996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31403302209717887 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20842790603637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36328887939453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3140491051333291 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20675325393676758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478665351867676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139908803360803 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20831513404846191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3529069423675537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139986263854163 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2115175724029541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36957573890686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31398949325084685 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2061610221862793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34796714782714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31390209879193987 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20403790473937988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34852170944213867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3138717502355576 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2107396125793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36008644104003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31389253565243314 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2050316333770752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34700942039489746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.313850582071713 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20833873748779297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35434961318969727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138090934072222 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21349668502807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35607242584228516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138457362140928 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20201730728149414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34351205825805664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.313731250166893 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111196517944336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559377193450928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138014405965805 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21561932563781738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3577535152435303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3137729010411671 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014601230621338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.341447114944458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31372016114848 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20464563369750977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35233545303344727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137243918010167 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21577930450439453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565680980682373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137635431119374 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19982290267944336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33937978744506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31367265965257374 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20392394065856934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487083911895752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137111668075834 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21106481552124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35070252418518066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137025207281113 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20796513557434082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35213351249694824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31372013773236956 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20728588104248047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35744524002075195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137010710580008 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21407079696655273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35541200637817383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.313677225794111 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20106172561645508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3505864143371582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136581293174199 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1944441795349121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33954286575317383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136833748647145 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20900964736938477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35123300552368164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31365684611456734 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2059323787689209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34734177589416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136179519551141 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20562100410461426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36487483978271484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31362823247909544 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20584845542907715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455660343170166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136430923427854 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19957900047302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34820008277893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31361869616167887 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20324301719665527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508474826812744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31358241992337366 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20289111137390137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3435344696044922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135501086711884 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20621967315673828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498854637145996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135962562901633 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20053339004516602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3509235382080078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31356835280145917 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2050037384033203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35187768936157227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31360384268420083 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20441269874572754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35661911964416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135440289974213 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22662758827209473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36978602409362793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135800493615014 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20330214500427246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35109972953796387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135274627379009 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20847201347351074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3555610179901123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357794318880355 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21396708488464355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35550689697265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135568256889071 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20272254943847656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553788661956787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135064721107483 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20991873741149902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583521842956543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31353605602468765 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21872735023498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3589043617248535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31350724399089813 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060532569885254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475918769836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31350307379450115 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20353269577026367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35250234603881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31354597551482066 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21451330184936523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3547806739807129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135139673948288 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069995403289795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484978675842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135213396378926 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20666170120239258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478565216064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31350396701267785 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21664929389953613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3629481792449951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135123997926712 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21456122398376465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3643167018890381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31349803549902777 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20906662940979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3613123893737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348208614758083 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055656909942627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35563135147094727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31349901471819197 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20548725128173828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349351167678833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31350822491305214 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2081766128540039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.359468936920166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134819903544017 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20476150512695312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540027141571045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134677652801786 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20764684677124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35521507263183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134747049638203 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20904183387756348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35800743103027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134808416877474 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20426511764526367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3517599105834961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31348334337983813 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20558547973632812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34565043449401855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134306107248579 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20876502990722656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35224199295043945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31344776025840215 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103583812713623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35657739639282227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31344955350671494 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20633769035339355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3495354652404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134406626224518 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2207338809967041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3633866310119629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31343978132520406 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21304535865783691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36017441749572754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344862367425647 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024838924407959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34854674339294434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134202152490616 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21551799774169922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36417078971862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134294331073761 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20676827430725098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35544466972351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134297843490328 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20215296745300293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3413527011871338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31343295787061964 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6689 | test accuracy: 0.822\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6677 | test accuracy: 0.822\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2177 | test accuracy: 0.822\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8316 | test accuracy: 0.822\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1921 | test accuracy: 0.822\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2045 | test accuracy: 0.822\n",
            "Epoch:  6 Iteration:  490 | train loss: 1.1234 | test accuracy: 0.822\n",
            "Epoch:  7 Iteration:  560 | train loss: 1.3180 | test accuracy: 0.822\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2324 | test accuracy: 0.822\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2239 | test accuracy: 0.822\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.9950 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1687 | test accuracy: 0.822\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2922 | test accuracy: 0.822\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2676 | test accuracy: 0.822\n",
            "Epoch:  14 Iteration:  1050 | train loss: 1.1917 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2156 | test accuracy: 0.822\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6143 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8006 | test accuracy: 0.822\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6353 | test accuracy: 0.822\n",
            "Epoch:  19 Iteration:  1400 | train loss: 1.2559 | test accuracy: 0.822\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1933 | test accuracy: 0.822\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6777 | test accuracy: 0.822\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2171 | test accuracy: 0.822\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1976 | test accuracy: 0.822\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6561 | test accuracy: 0.822\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5830 | test accuracy: 0.822\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1653 | test accuracy: 0.822\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2305 | test accuracy: 0.822\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6792 | test accuracy: 0.822\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1613 | test accuracy: 0.822\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1637 | test accuracy: 0.822\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6350 | test accuracy: 0.822\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6293 | test accuracy: 0.822\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6352 | test accuracy: 0.822\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2016 | test accuracy: 0.822\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1899 | test accuracy: 0.822\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7425 | test accuracy: 0.822\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.3488 | test accuracy: 0.822\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6950 | test accuracy: 0.822\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2280 | test accuracy: 0.822\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2225 | test accuracy: 0.822\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1792 | test accuracy: 0.822\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1980 | test accuracy: 0.822\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2860 | test accuracy: 0.822\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6021 | test accuracy: 0.822\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2015 | test accuracy: 0.822\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2049 | test accuracy: 0.822\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2055 | test accuracy: 0.822\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7187 | test accuracy: 0.822\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6135 | test accuracy: 0.822\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5962 | test accuracy: 0.822\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.8751 | test accuracy: 0.822\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.1234 | test accuracy: 0.822\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2155 | test accuracy: 0.822\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.3800 | test accuracy: 0.822\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0513 | test accuracy: 0.822\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2597 | test accuracy: 0.822\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.1368 | test accuracy: 0.822\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2772 | test accuracy: 0.822\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2728 | test accuracy: 0.822\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6727 | test accuracy: 0.822\n",
            "Epoch:  61 Iteration:  4340 | train loss: 1.2384 | test accuracy: 0.822\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2636 | test accuracy: 0.822\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5858 | test accuracy: 0.822\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2033 | test accuracy: 0.822\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1905 | test accuracy: 0.822\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1991 | test accuracy: 0.822\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6813 | test accuracy: 0.822\n",
            "Epoch:  68 Iteration:  4830 | train loss: 1.2693 | test accuracy: 0.822\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6457 | test accuracy: 0.822\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1884 | test accuracy: 0.822\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2116 | test accuracy: 0.822\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.5273 | test accuracy: 0.822\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1961 | test accuracy: 0.822\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7258 | test accuracy: 0.822\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2265 | test accuracy: 0.822\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1717 | test accuracy: 0.822\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6587 | test accuracy: 0.822\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8135 | test accuracy: 0.822\n",
            "Epoch:  79 Iteration:  5600 | train loss: 1.3617 | test accuracy: 0.822\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.8943 | test accuracy: 0.822\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2457 | test accuracy: 0.822\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1733 | test accuracy: 0.822\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.4895 | test accuracy: 0.822\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2646 | test accuracy: 0.822\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2244 | test accuracy: 0.822\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2676 | test accuracy: 0.822\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2340 | test accuracy: 0.822\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2264 | test accuracy: 0.822\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2096 | test accuracy: 0.822\n",
            "Epoch:  90 Iteration:  6370 | train loss: 1.1041 | test accuracy: 0.822\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.5766 | test accuracy: 0.822\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2576 | test accuracy: 0.822\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6684 | test accuracy: 0.822\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1663 | test accuracy: 0.822\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7631 | test accuracy: 0.822\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2475 | test accuracy: 0.822\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1908 | test accuracy: 0.822\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7916 | test accuracy: 0.822\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7736 | test accuracy: 0.822\n",
            "total time:  66.06251224900006\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6435 | test accuracy: 0.822\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6878 | test accuracy: 0.822\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2059 | test accuracy: 0.822\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7663 | test accuracy: 0.822\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2004 | test accuracy: 0.822\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2152 | test accuracy: 0.822\n",
            "Epoch:  6 Iteration:  490 | train loss: 1.1219 | test accuracy: 0.822\n",
            "Epoch:  7 Iteration:  560 | train loss: 1.3327 | test accuracy: 0.822\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2468 | test accuracy: 0.822\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2196 | test accuracy: 0.822\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.9802 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1586 | test accuracy: 0.822\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.3140 | test accuracy: 0.822\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2626 | test accuracy: 0.822\n",
            "Epoch:  14 Iteration:  1050 | train loss: 1.2549 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2112 | test accuracy: 0.822\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6176 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8358 | test accuracy: 0.822\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6321 | test accuracy: 0.822\n",
            "Epoch:  19 Iteration:  1400 | train loss: 1.2619 | test accuracy: 0.822\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2044 | test accuracy: 0.822\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6894 | test accuracy: 0.822\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2105 | test accuracy: 0.822\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1876 | test accuracy: 0.822\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6536 | test accuracy: 0.822\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5963 | test accuracy: 0.822\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1724 | test accuracy: 0.822\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2296 | test accuracy: 0.822\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6788 | test accuracy: 0.822\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1584 | test accuracy: 0.822\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1650 | test accuracy: 0.822\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6500 | test accuracy: 0.822\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6293 | test accuracy: 0.822\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6204 | test accuracy: 0.822\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1987 | test accuracy: 0.822\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1877 | test accuracy: 0.822\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7424 | test accuracy: 0.822\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.2984 | test accuracy: 0.822\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7032 | test accuracy: 0.822\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2348 | test accuracy: 0.822\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2151 | test accuracy: 0.822\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1835 | test accuracy: 0.822\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1949 | test accuracy: 0.822\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2914 | test accuracy: 0.822\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6111 | test accuracy: 0.822\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2035 | test accuracy: 0.822\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2051 | test accuracy: 0.822\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2035 | test accuracy: 0.822\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7127 | test accuracy: 0.822\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6164 | test accuracy: 0.822\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6103 | test accuracy: 0.822\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.8877 | test accuracy: 0.822\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.1441 | test accuracy: 0.822\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2201 | test accuracy: 0.822\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.3938 | test accuracy: 0.822\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0541 | test accuracy: 0.822\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2638 | test accuracy: 0.822\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.1461 | test accuracy: 0.822\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2797 | test accuracy: 0.822\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2738 | test accuracy: 0.822\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6564 | test accuracy: 0.822\n",
            "Epoch:  61 Iteration:  4340 | train loss: 1.2217 | test accuracy: 0.822\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2664 | test accuracy: 0.822\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6114 | test accuracy: 0.822\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1994 | test accuracy: 0.822\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1912 | test accuracy: 0.822\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1939 | test accuracy: 0.822\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6827 | test accuracy: 0.822\n",
            "Epoch:  68 Iteration:  4830 | train loss: 1.2728 | test accuracy: 0.822\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6494 | test accuracy: 0.822\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1842 | test accuracy: 0.822\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2118 | test accuracy: 0.822\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.5406 | test accuracy: 0.822\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2043 | test accuracy: 0.822\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7255 | test accuracy: 0.822\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2258 | test accuracy: 0.822\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1730 | test accuracy: 0.822\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6683 | test accuracy: 0.822\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8026 | test accuracy: 0.822\n",
            "Epoch:  79 Iteration:  5600 | train loss: 1.3822 | test accuracy: 0.822\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.8315 | test accuracy: 0.822\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2444 | test accuracy: 0.822\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1764 | test accuracy: 0.822\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.4825 | test accuracy: 0.822\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2706 | test accuracy: 0.822\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2324 | test accuracy: 0.822\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2651 | test accuracy: 0.822\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2448 | test accuracy: 0.822\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2245 | test accuracy: 0.822\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2070 | test accuracy: 0.822\n",
            "Epoch:  90 Iteration:  6370 | train loss: 1.0873 | test accuracy: 0.822\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.5737 | test accuracy: 0.822\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2552 | test accuracy: 0.822\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6567 | test accuracy: 0.822\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1628 | test accuracy: 0.822\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7685 | test accuracy: 0.822\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2385 | test accuracy: 0.822\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1876 | test accuracy: 0.822\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7862 | test accuracy: 0.822\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7580 | test accuracy: 0.822\n",
            "total time:  67.17736583799979\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20227551460266113.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.34931230545043945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6860731252602168 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21507549285888672.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.3552985191345215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5426125326326915 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071552276611328.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.3506896495819092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4614436545542308 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1994161605834961.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.33974385261535645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4147946132080896 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20734477043151855.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3461318016052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38979504108428953 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2041316032409668.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3442859649658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37217636448996405 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039041519165039.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.34588098526000977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3612048523766654 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21340203285217285.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35431909561157227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35207845526082177 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20527958869934082.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.34612369537353516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3465330894504275 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20358753204345703.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34566473960876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3411098603691374 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143104076385498.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.37184906005859375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33672657012939455 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071375846862793.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35303688049316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33474925117833276 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20488786697387695.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3450202941894531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33143529040472847 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20393085479736328.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3449404239654541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32900097497871944 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21717119216918945.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.367398738861084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32761144169739315 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2038285732269287.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3502838611602783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.326301337991442 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19898128509521484.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34726548194885254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3254515494619097 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20474576950073242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3482546806335449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3245347235883985 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20380473136901855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3515632152557373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32295745100293843 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20265555381774902.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34698486328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32195705132825037 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2023606300354004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34446287155151367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3205891809293202 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1993873119354248.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3518979549407959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3210044264793396 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20775508880615234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3575105667114258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31958782289709364 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055213451385498.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35016345977783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31950790286064146 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20746827125549316.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3495810031890869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3190747141838074 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20342421531677246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34450435638427734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3189088148730142 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20370745658874512.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34331846237182617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3182834540094648 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21236085891723633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35955810546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3180103276457105 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2080707550048828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3533906936645508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31789232109274185 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2221682071685791.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36313390731811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3174632272550038 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2137129306793213.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35379600524902344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31707715690135957 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2114095687866211.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3590695858001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3168299483401435 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20731735229492188.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3488171100616455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3169084736279079 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21062064170837402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3596608638763428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3163434731108802 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20728778839111328.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3469505310058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31609514568533215 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20248627662658691.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3446183204650879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31597483711583274 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22068238258361816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36626577377319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3158331113202231 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20927691459655762.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3577902317047119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31589676652635845 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20254158973693848.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35341787338256836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3155539887292044 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20690560340881348.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3503696918487549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31547226905822756 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1993565559387207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34704041481018066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3154117413929531 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2043747901916504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3591501712799072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3152269772120884 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21119999885559082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36504220962524414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3151660318885531 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20814132690429688.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3531806468963623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3150494720254626 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21099209785461426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3751814365386963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31504353966031756 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20581436157226562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475029468536377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31499441478933604 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20183181762695312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3410611152648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31478634987558635 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2182636260986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3634483814239502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3149132681744439 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19921040534973145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3375527858734131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31474919532026563 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20140552520751953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34855127334594727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31460088534014563 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21210098266601562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35901975631713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146051266363689 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20811676979064941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352433443069458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31441831290721894 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051846981048584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34609532356262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31453747749328614 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21691465377807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36043643951416016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.314430399451937 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2022416591644287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3450167179107666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31432883015700747 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20785760879516602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3558642864227295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31430795533316475 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2123856544494629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506922721862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31429125368595123 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19996118545532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36717867851257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31423755884170534 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19860315322875977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33710694313049316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141678354569844 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20777606964111328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546109199523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141986459493637 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20155930519104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33896756172180176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31421914781842913 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1965618133544922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3415794372558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.314205887062209 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20620512962341309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478555679321289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31412299573421476 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19851279258728027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33617663383483887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140899777412415 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20516538619995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3528919219970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31404377860682353 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20849275588989258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510103225708008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31401495209762026 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2028965950012207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34207701683044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139529892376491 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19704914093017578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35532546043395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31403237623827795 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20764636993408203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470494747161865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139186484473092 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20433473587036133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34403181076049805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31398217635495324 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2115020751953125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36570167541503906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31395621725491113 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078084945678711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35091638565063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138744047709874 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20112848281860352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3511347770690918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.313855128628867 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20812463760375977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3596351146697998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138594597578049 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21041011810302734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3522305488586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138756717954363 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2003474235534668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3422584533691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31376775545733315 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21529102325439453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.363356351852417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31385924305234636 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20737004280090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508892059326172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31380769738129205 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20711946487426758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516216278076172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.313739355972835 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21971988677978516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36653971672058105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137363348688398 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20298314094543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.342573881149292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31375891821725027 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19953656196594238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3434779644012451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137390307017735 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21233415603637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540372848510742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137476448501859 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20563936233520508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452882766723633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31375246814319063 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20382213592529297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34888267517089844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31372560007231576 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2177896499633789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356231689453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31366052074091777 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021467685699463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34441423416137695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31369741899626596 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20975232124328613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35428905487060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3137164247887475 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21342945098876953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35644006729125977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136807918548584 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20267653465270996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514900207519531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136692409004484 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20644474029541016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34946274757385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136590080601828 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2180795669555664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36130309104919434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31361075341701505 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20342254638671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3448977470397949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.313651567697525 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21599698066711426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36997127532958984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135929818664278 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20412206649780273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470265865325928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31361545068877084 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20036625862121582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34828686714172363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31364853382110597 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126612663269043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36788177490234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136052919285638 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20924115180969238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506021499633789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31356343797275 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20566248893737793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3494718074798584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31356688737869265 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057340145111084.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3567492961883545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135464532034738 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20608878135681152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34574055671691895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135771508727755 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20042109489440918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34345102310180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31360162028244565 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21900415420532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35817551612854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135818464415414 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20583176612854004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458223342895508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31356492936611174 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20900702476501465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35272812843322754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31354602490152633 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2102069854736328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3500082492828369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31351763520921977 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2115306854248047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585796356201172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135518878698349 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082359790802002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3530459403991699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135261786835534 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22583341598510742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3652799129486084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135160033191953 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20472335815429688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455696105957031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135111574615751 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20735597610473633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546946048736572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134823058332716 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21904969215393066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36646318435668945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135081035750253 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20686769485473633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35579466819763184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31350830367633276 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2240440845489502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37395739555358887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135105622666223 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21181654930114746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549675941467285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135071324450629 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2112290859222412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35906553268432617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134983909981591 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21173501014709473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3623504638671875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347191078322273 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2101271152496338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34995031356811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134695563997541 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2025744915008545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34854722023010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134892123086112 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20517182350158691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35351037979125977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31351072915962763 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19797992706298828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33685946464538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.313478798525674 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20676827430725098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3473660945892334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.313458856514522 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20206117630004883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35004472732543945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134518657411848 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1992957592010498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34468817710876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134632178715297 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20148396492004395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486790657043457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134508852447782 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027580738067627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501310348510742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134463701929365 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20674586296081543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34935927391052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134444394281932 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19974589347839355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343259334564209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134430080652237 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.1455 | test accuracy: 0.822\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2174 | test accuracy: 0.822\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2476 | test accuracy: 0.822\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.1944 | test accuracy: 0.822\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2262 | test accuracy: 0.822\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6432 | test accuracy: 0.822\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5874 | test accuracy: 0.822\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6947 | test accuracy: 0.822\n",
            "Epoch:  8 Iteration:  630 | train loss: 1.2931 | test accuracy: 0.822\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1982 | test accuracy: 0.822\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6556 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1931 | test accuracy: 0.822\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6698 | test accuracy: 0.822\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2623 | test accuracy: 0.822\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5928 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2698 | test accuracy: 0.822\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7552 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1870 | test accuracy: 0.822\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7757 | test accuracy: 0.822\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6127 | test accuracy: 0.822\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.2297 | test accuracy: 0.822\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2692 | test accuracy: 0.822\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2305 | test accuracy: 0.822\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1235 | test accuracy: 0.822\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6243 | test accuracy: 0.822\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1907 | test accuracy: 0.822\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6112 | test accuracy: 0.822\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6768 | test accuracy: 0.822\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2214 | test accuracy: 0.822\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1240 | test accuracy: 0.822\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1739 | test accuracy: 0.822\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7252 | test accuracy: 0.822\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2256 | test accuracy: 0.822\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1941 | test accuracy: 0.822\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1971 | test accuracy: 0.822\n",
            "Epoch:  35 Iteration:  2520 | train loss: 1.2067 | test accuracy: 0.822\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6785 | test accuracy: 0.822\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2121 | test accuracy: 0.822\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2150 | test accuracy: 0.822\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6509 | test accuracy: 0.822\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6559 | test accuracy: 0.822\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1965 | test accuracy: 0.822\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7793 | test accuracy: 0.822\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2684 | test accuracy: 0.822\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1763 | test accuracy: 0.822\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1960 | test accuracy: 0.822\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2057 | test accuracy: 0.822\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5643 | test accuracy: 0.822\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6352 | test accuracy: 0.822\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.0550 | test accuracy: 0.822\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6105 | test accuracy: 0.822\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7318 | test accuracy: 0.822\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7531 | test accuracy: 0.822\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2340 | test accuracy: 0.822\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.0230 | test accuracy: 0.822\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7420 | test accuracy: 0.822\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5362 | test accuracy: 0.822\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.9546 | test accuracy: 0.822\n",
            "Epoch:  58 Iteration:  4130 | train loss: 1.0725 | test accuracy: 0.822\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6656 | test accuracy: 0.822\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1994 | test accuracy: 0.822\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2392 | test accuracy: 0.822\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2078 | test accuracy: 0.822\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2645 | test accuracy: 0.822\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2143 | test accuracy: 0.822\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7308 | test accuracy: 0.822\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3069 | test accuracy: 0.822\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5968 | test accuracy: 0.822\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2071 | test accuracy: 0.822\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7098 | test accuracy: 0.822\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2548 | test accuracy: 0.822\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2380 | test accuracy: 0.822\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6520 | test accuracy: 0.822\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7560 | test accuracy: 0.822\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5642 | test accuracy: 0.822\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7658 | test accuracy: 0.822\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1833 | test accuracy: 0.822\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2273 | test accuracy: 0.822\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6274 | test accuracy: 0.822\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6658 | test accuracy: 0.822\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2034 | test accuracy: 0.822\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2403 | test accuracy: 0.822\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2868 | test accuracy: 0.822\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.2013 | test accuracy: 0.822\n",
            "Epoch:  84 Iteration:  5950 | train loss: 1.2319 | test accuracy: 0.822\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2727 | test accuracy: 0.822\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1984 | test accuracy: 0.822\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.8141 | test accuracy: 0.822\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2247 | test accuracy: 0.822\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2043 | test accuracy: 0.822\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7089 | test accuracy: 0.822\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.8471 | test accuracy: 0.822\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2238 | test accuracy: 0.822\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7067 | test accuracy: 0.822\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2858 | test accuracy: 0.822\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6689 | test accuracy: 0.822\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1422 | test accuracy: 0.822\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6278 | test accuracy: 0.822\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2520 | test accuracy: 0.822\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1934 | test accuracy: 0.822\n",
            "total time:  66.17778597899996\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.1674 | test accuracy: 0.822\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2252 | test accuracy: 0.822\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2437 | test accuracy: 0.822\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.1955 | test accuracy: 0.822\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2268 | test accuracy: 0.822\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6411 | test accuracy: 0.822\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5686 | test accuracy: 0.822\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6873 | test accuracy: 0.822\n",
            "Epoch:  8 Iteration:  630 | train loss: 1.2913 | test accuracy: 0.822\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1917 | test accuracy: 0.822\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6653 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1971 | test accuracy: 0.822\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6543 | test accuracy: 0.822\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2594 | test accuracy: 0.822\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5803 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2693 | test accuracy: 0.822\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7732 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1872 | test accuracy: 0.822\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7580 | test accuracy: 0.822\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5991 | test accuracy: 0.822\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.2218 | test accuracy: 0.822\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2701 | test accuracy: 0.822\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2282 | test accuracy: 0.822\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1221 | test accuracy: 0.822\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6187 | test accuracy: 0.822\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1893 | test accuracy: 0.822\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6158 | test accuracy: 0.822\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6897 | test accuracy: 0.822\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2215 | test accuracy: 0.822\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1298 | test accuracy: 0.822\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1691 | test accuracy: 0.822\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7241 | test accuracy: 0.822\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2285 | test accuracy: 0.822\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1951 | test accuracy: 0.822\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1892 | test accuracy: 0.822\n",
            "Epoch:  35 Iteration:  2520 | train loss: 1.2072 | test accuracy: 0.822\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6923 | test accuracy: 0.822\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2124 | test accuracy: 0.822\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2217 | test accuracy: 0.822\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6589 | test accuracy: 0.822\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6436 | test accuracy: 0.822\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1972 | test accuracy: 0.822\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7966 | test accuracy: 0.822\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2760 | test accuracy: 0.822\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1710 | test accuracy: 0.822\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1903 | test accuracy: 0.822\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2009 | test accuracy: 0.822\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5475 | test accuracy: 0.822\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6482 | test accuracy: 0.822\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.0736 | test accuracy: 0.822\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6058 | test accuracy: 0.822\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7290 | test accuracy: 0.822\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7603 | test accuracy: 0.822\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2355 | test accuracy: 0.822\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.0248 | test accuracy: 0.822\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7375 | test accuracy: 0.822\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5318 | test accuracy: 0.822\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.9639 | test accuracy: 0.822\n",
            "Epoch:  58 Iteration:  4130 | train loss: 1.0701 | test accuracy: 0.822\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6548 | test accuracy: 0.822\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1890 | test accuracy: 0.822\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2454 | test accuracy: 0.822\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2068 | test accuracy: 0.822\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2713 | test accuracy: 0.822\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2148 | test accuracy: 0.822\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7255 | test accuracy: 0.822\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3173 | test accuracy: 0.822\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6043 | test accuracy: 0.822\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2148 | test accuracy: 0.822\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7098 | test accuracy: 0.822\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2655 | test accuracy: 0.822\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2376 | test accuracy: 0.822\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6695 | test accuracy: 0.822\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7687 | test accuracy: 0.822\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5647 | test accuracy: 0.822\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7514 | test accuracy: 0.822\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1817 | test accuracy: 0.822\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2227 | test accuracy: 0.822\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6121 | test accuracy: 0.822\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6798 | test accuracy: 0.822\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2040 | test accuracy: 0.822\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2367 | test accuracy: 0.822\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2883 | test accuracy: 0.822\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.2077 | test accuracy: 0.822\n",
            "Epoch:  84 Iteration:  5950 | train loss: 1.2347 | test accuracy: 0.822\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2897 | test accuracy: 0.822\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2038 | test accuracy: 0.822\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.8132 | test accuracy: 0.822\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2273 | test accuracy: 0.822\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1987 | test accuracy: 0.822\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6932 | test accuracy: 0.822\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.8543 | test accuracy: 0.822\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2264 | test accuracy: 0.822\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6903 | test accuracy: 0.822\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.3025 | test accuracy: 0.822\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6877 | test accuracy: 0.822\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1396 | test accuracy: 0.822\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6149 | test accuracy: 0.822\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2478 | test accuracy: 0.822\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1958 | test accuracy: 0.822\n",
            "total time:  66.59655155200016\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20166826248168945.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.34799647331237793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.4919131330081395 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2089700698852539.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.35081028938293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.42023555082934244 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19942069053649902.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.34042811393737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.38602672815322875 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21332406997680664.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3556978702545166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3653399326971599 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20860576629638672.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3564567565917969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.35259722088064466 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069227695465088.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35562705993652344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3447731188365391 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201948881149292.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.342879056930542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3390632139784949 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082827091217041.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3534226417541504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3338623251233782 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20873212814331055.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35750508308410645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3317176354782922 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20305085182189941.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34704017639160156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.32765046060085296 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19833707809448242.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.33588504791259766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32671010196208955 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20461726188659668.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35161590576171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32522813422339303 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010211944580078.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3462491035461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32292527215821404 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21642255783081055.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3636176586151123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3221261475767408 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21247100830078125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3570253849029541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32128538829939707 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20740675926208496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35601329803466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3202830561569759 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20564985275268555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34982991218566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3193835684231349 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21131014823913574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35089778900146484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3191945173910686 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21106338500976562.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35410475730895996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3187694660254887 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20363736152648926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3458542823791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3182367380176272 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2145860195159912.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35466599464416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3178105639559882 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20272564888000488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34745311737060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3171668031385967 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2076737880706787.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3466475009918213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3170460969209671 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2097480297088623.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34882140159606934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3169074961117336 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060396671295166.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3472025394439697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3168953967945916 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19995927810668945.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3408336639404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31606904183115275 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21014022827148438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35724449157714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3161113547427314 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2001638412475586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34163904190063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31584271490573884 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021787166595459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3448140621185303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31576225800173624 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20964384078979492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34998345375061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31558343172073366 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20532512664794922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34485292434692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31534020858151574 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20453310012817383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3457610607147217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3151998792375837 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21654033660888672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36818671226501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31526841861861093 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20561552047729492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3556480407714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3151461269174303 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2077033519744873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598973751068115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31496352425643376 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21107244491577148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3591024875640869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3149729630776814 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1977078914642334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3402895927429199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3148309507540294 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19779419898986816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3476536273956299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3146501702921731 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21025633811950684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35181188583374023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3147212986435209 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20338916778564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34556031227111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31466886741774425 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20210933685302734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498656749725342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3145717169557299 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20599794387817383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34758734703063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31440058520862035 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2058858871459961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34630393981933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3144305557012558 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20221567153930664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3491480350494385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31439133882522585 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2091224193572998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35175299644470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3143282839230129 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20362353324890137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423130512237549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3143223179238183 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2155914306640625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35967516899108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3142359218427113 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20675039291381836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35624194145202637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3142364842551095 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21588826179504395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35814642906188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31410981501851765 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2140054702758789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36258864402770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3141362513814654 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2113792896270752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3567471504211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31402318477630614 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057201862335205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34743380546569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.314064074414117 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21895360946655273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3638172149658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3140458000557763 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20685744285583496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515632152557373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31399387546948027 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2082681655883789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467419147491455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3138521202972957 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095026969909668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3538851737976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31392540676253183 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2065141201019287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455638885498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31404280960559844 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19669699668884277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3429563045501709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3138734238488334 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21472930908203125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3602712154388428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31389046055930003 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2076115608215332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34778714179992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3138520419597626 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2022390365600586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35069894790649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138622420174735 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21450114250183105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35942769050598145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31386270352772305 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069401741027832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34903955459594727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31379161817686896 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20703911781311035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355116605758667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3137857220002583 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20653963088989258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527545928955078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.313835757119315 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20297646522521973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3488304615020752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31374724677630833 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20084834098815918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35241174697875977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137953911508833 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20340871810913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34913110733032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137733425412859 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017345428466797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3463938236236572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137426733970642 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21328043937683105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36055493354797363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31375166603497096 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010178565979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.346219539642334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31363528668880464 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2092597484588623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484528064727783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31374705348696025 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143692970275879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35356593132019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3136450959103448 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19875144958496094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33605027198791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31366031212466106 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20406556129455566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34369325637817383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31363373398780825 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21765995025634766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36220574378967285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31369764932564326 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20787978172302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3489654064178467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31365011973040446 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19502830505371094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.334000825881958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136114401476724 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21455121040344238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35658788681030273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31360087266990117 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20307040214538574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34361791610717773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136234870978764 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2061154842376709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3500044345855713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31363606963838847 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21793055534362793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3597989082336426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31359812574727197 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2105882167816162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35216593742370605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136076956987381 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20179510116577148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3421196937561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31360128692218237 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21160602569580078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36075496673583984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136072120496205 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2084944248199463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34801745414733887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135902898652213 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20293736457824707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.347719669342041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135641932487488 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.210341215133667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3524050712585449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31357101968356543 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20441031455993652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34766125679016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31355579580579485 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21329069137573242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36738157272338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31352270373276303 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20827531814575195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510143756866455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31355168776852743 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060544490814209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446316719055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31355714627674647 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060108184814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36156749725341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31349726702485764 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20557141304016113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479440212249756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31351361998489924 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20490074157714844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34636592864990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31349526728902544 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21371006965637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3699774742126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31351314399923597 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20685982704162598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487398624420166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31352594877992357 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20203351974487305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34598588943481445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31350947873932977 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055366039276123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3562905788421631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31348317095211575 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20348763465881348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442544937133789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31349422207900457 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20548319816589355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34619832038879395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31351525953837805 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21217775344848633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35590434074401855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135006410734994 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20269155502319336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34581565856933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135106103760856 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20755624771118164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35629963874816895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134954375880105 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22287940979003906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3693349361419678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31348521837166377 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053067684173584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349245548248291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31345933037144796 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20914053916931152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35628628730773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134590655565262 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21821093559265137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3604710102081299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31347021886280607 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2038280963897705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34305596351623535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134307507957731 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.203413724899292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498799800872803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31346454960959297 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21813392639160156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35553455352783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31344820984772276 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035067081451416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34311819076538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31349056575979506 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20134592056274414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533437252044678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134417521102088 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20839524269104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35225534439086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31343868630273003 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2072756290435791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3593556880950928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31342726945877075 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2093031406402588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3649463653564453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134153817381178 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21029067039489746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3560614585876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134359402315957 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.208449125289917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35129404067993164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31343372336455755 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20537996292114258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35954856872558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31344230983938487 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20003962516784668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34035491943359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31344019387449534 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20251035690307617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516855239868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343506319182257 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21125078201293945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3681602478027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134207768099649 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20272254943847656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34386348724365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134175079209464 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20441937446594238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34729766845703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134070383650916 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20758819580078125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3505284786224365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313388797215053 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19764971733093262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3349263668060303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134093799761363 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1994309425354004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470146656036377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31341374942234584 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21234130859375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3528625965118408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.313420455796378 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAESCAYAAAAWtRmOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVyVZf7/8dd9FvZFUQ6muA1fV9wrS0lHRRT1O6kzaebC15ZHNpm5jINIFpYT7i2mbWalRhOG5NhPDUcbUxsEt1CZGsVKxQVBWQQ5cJb798eBowgCFYeD8nk+Hj48933u5XMoz5vruu77uhVVVVWEEEKIm2icXYAQQoiGR8JBCCFEJRIOQgghKpFwEEIIUYmEgxBCiEokHIQQQlSic3YBovHp1KkT33zzDS1atKj03oYNG/j8888xmUyYTCbuu+8+XnzxRS5evMiMGTMAKCwspLCw0L7/2LFjGTVqFKGhoTzxxBPMmzevwjGnTp3K2bNn+frrr29b0/79+/nb3/4GQG5uLhaLhebNmwPwzDPPMGbMmFp9tqysLJ588kn+3//7f9VuFxkZSXh4OEOGDKnVcWtSWlrKmjVrSEpKovzq9PDwcKZPn46Li0udnEM0Lorc5yDq2+3CYe/evSxevJi4uDj8/PwoLS3lr3/9K76+vrzyyiv27RITE9m6dSsff/yxfV1mZibjx4/H09OTpKQkNBpbozgnJ4fx48cDVBsON3vrrbe4dOkSr7766m/8pPVn1qxZFBcXs3z5cnx8fMjLy2PevHl4eXmxcuVKZ5cn7kDSrSQajJMnT9K2bVv8/PwAcHFx4dVXXyUyMrJW+7u5udGmTRsOHTpkX7djxw769ev3m2sbMmQIq1evZvjw4Vy4cIEff/yRxx57jBEjRhAWFmZvKWRmZtK1a1fAFmLPP/880dHRDB8+nJEjR3Lq1CkApkyZwj/+8Q/AFpZbtmxhzJgxPPTQQ/bQs1qtLFq0iJCQEB577DHef/99pkyZUqm2U6dO8c0337B06VJ8fHwAaNKkCbGxsTzyyCOVzlfV+d977z2GDx/O0qVLWbRokX27q1ev0qtXL65du0ZGRgaTJ09m+PDh/OEPf+D48eMAFBUVMX36dEaMGEFoaCgLFizAZDL95p+5cC4JB9Fg9O/fn/379zNv3jy++eYbCgsL8fLywsvLq9bHCA8Pr9Cls23bNsLDw+ukvqysLJKSkmjZsiXLli1j8ODB7Nixg9jYWF544YUqvxD37t3LxIkTSUpK4oEHHmD9+vVVHjsjI4MtW7bw9ttv89prr2GxWPjmm2/Yu3cvO3fu5J133uGLL76oct/U1FR69epFkyZNKqxv1qxZrYNRVVWSkpIYMWIE//rXv+zr//Wvf/Hggw/i6enJ9OnTGT16NElJSSxcuJBnn30Ws9nMli1b8PHxYceOHSQlJaHVasnIyKjVeUXDJeEgGoyuXbvy97//HavVSlRUFA8++CDTp0/nwoULtT7GsGHD+PrrrzGZTJw/fx6j0Uj79u3rpL5BgwbZX7/99ts8+eSTANx7772UlJSQnZ1daZ+goCC6desG2D7fxYsXqzz26NGjAQgODqakpIQrV65w6NAhBg0ahKenJ02aNGHUqFFV7pufn0+zZs1+y0ezf7YePXqgqio//PADAP/85z8ZMWIEP/74I1euXLG3RO699178/Pw4evSo/e/9+/djtVp5+eWX6dKly2+qRzifDEiLBqV79+4sX74cVVVJT0/nzTffZPbs2cTHx9dqf19fX7p168b+/fvJyMhgxIgRdVabr6+v/fW+fft45513yM3NRVEUVFXFarVW2sfb29v+WqvVYrFYqjx2+XZarRawdSkVFBQQEBBg3+bm1zdr2rQpWVlZv/wD3eTmVsewYcPYvXs3bdq04ciRI6xYsYKTJ09iNBor/DwLCwvJy8tjxIgR5Ofn8+abb/Ljjz/y8MMPM3/+fBkIv8NJy0E0GIcOHbJ/ySmKQrdu3Zg7dy4nT578RccZNWoUSUlJfPXVV4wcObLO6zSZTMyaNYs///nPJCUlsXXrVhRFqfPzeHl5cf36dftyVS0TgL59+5KWllYpIAoKCnjzzTdRVRWNRlMhvPLz82973uHDh/P111+zf/9+7r//fry8vDAYDHh6evLVV1/Z/+zfv5+wsDAAJkyYwOeff8727dtJT09ny5Ytv+WjiwZAwkE0GF9++SUxMTEUFhYCYDab2bZtG/fff/8vOk5oaCipqalotVpat25d53UWFxdz/fp1e3fR+vXr0ev1Fb7I60L37t3Zs2cPRqORgoICduzYUeV2QUFBjBw5kjlz5pCTkwNAXl4ec+bMsbds/P397V1FR48e5eeff77teXv37s2VK1dITEy0txRatWpFixYt+OqrrwDbQPWcOXO4fv06a9asISEhAbC1bgIDAx0SlqJ+SbeScIopU6bYu1AA/va3v/HCCy/w+uuv86c//QmwhcMDDzzA4sWLf9GxPTw86NmzJ927d6/Tmsv5+Pjw1FNPMWbMGJo1a8af//xnhg4dyjPPPMN7771XZ+cJCwtjz549hIeH07ZtW0aMGEFycnKV2y5atIh33nmHSZMmoSgKer2ehx9+2D4u8vjjjzNnzhz27t1L3759CQkJue15FUVh6NChfP755/bLYBVF4bXXXmPhwoW88cYbaDQaHn/8cTw8PBg9ejTz589n7dq1KIpCz5497WMo4s4l9zkI0YCpqmr/LTwuLo5///vfrFmzxslVicZAupWEaKC+//57QkNDyc/Px2w2s3PnTnr16uXsskQjId1KQjRQXbp0YcyYMfzxj39Eq9XSq1cvJk+e7OyyRCPh0G6l2NhY0tLSUBSF6OhoevToYX9vyJAhtGjRwt7vvGLFCgICAqrc5+LFi0RGRmKxWPD392f58uVymZwQQjiQw1oOqampnDlzhvj4eE6fPk10dHSla9XXrl2Lp6dnjfusWrWKiRMnMmLECF577TUSEhKYOHGio0oXQohGz2HhkJyczNChQwHbpXb5+fn26RB+6T4pKSm8/PLLAAwePJgPP/ywQjgYjUZOnDiBv79/hStghBBC3J7FYiE7O5tu3brh5uZW4T2HhUNOTg7BwcH2ZT8/P7KzsyuEQ0xMDOfPn+fee+/lL3/5y233KS4utncjNWvWrNLNQCdOnGDSpEmO+ihCCHFXi4uL47777quwrt4GpG8d2nj++ecZMGAAvr6+TJ8+naSkpBr3ud06f39/wPYBq3pGgBBCiMouXbrEpEmT7N+hN3NYOBgMBvvdmgCXL1+uUMDND08ZOHAgJ0+evO0+Hh4eGI1G3NzcyMrKwmAwVDhXeVdSixYtCAwMdNRHEkKIu1JV3fEOu88hJCTE3hpIT0/HYDDYu5SuXbvGk08+SWlpKQAHDx6kQ4cOt92nf//+9vU7d+5kwIABjipbCCEEDmw59OnTh+DgYCZMmICiKMTExJCYmIi3tzdhYWEMHDiQRx99FFdXV7p27Up4eDiKolTaB2DGjBnMmzeP+Ph4WrZsWetHNgohhPh17orpMzIzMwkNDWX37t3SrSSEELVU3XenTJ8hhBCiEgkHIYQQlUg4CCGEqKTRT7z3yYEz/PM/Wax/oq+zSxFCNFJLliwhPT3dftNvmzZt8PX1ZfXq1dXuN3v2bBYvXlzp7ua60OjD4XR2IUfO5Dq7DCFEIxYVFQVAYmIip06dYt68ebXa7/XXX3dYTY0+HFx1WkoslR8ML4QQzhQVFYVerycvL4/Fixfzl7/8hevXr2M0GnnxxRfp0aMHQ4YM4csvv2TRokUYDAbS09O5cOECK1asqDAV0a8h4aDTUGq2VnjilhCi8dp8OJNNh87V6THH39eaP937yy+z9/X1ZdGiRfz000+MGzeOoUOHkpyczNq1a3nrrbcqbFtaWsq6dev4+9//zpYtWyQcfisXnW1MvsRsxU0vM7oKIRqO8mfgNG/enLfffpt169ZRWlqKh4dHpW3LJ85r0aIFx44d+83nbvTh4CrhIIS4yZ/uDfxVv+U7gl6vB2D9+vUEBASwfPlyjh8/zrJlyypte/P8SHVxb3Ojv5TVtSwQSs0y7iCEaJhyc3Np06YNALt27cJkMjn8nBIO2vKWg8XJlQghRNVGjx7NRx99xBNPPEGPHj3Izs5m8+bNDj1no59b6R/fnWfmZ9+x+y+/J8j/9k+pE0KIu43MrVSN8jEH6VYSQogbJBx0tjGHEgkHIYSwa/ThYL+U1SRjDkIIUa7Rh8PNl7IKIYSwkXDQyaWsQghxKwkHvbQchBDiVo0+HFzkPgchhKjEodNnxMbGkpaWhqIoREdH2+cJudnKlSv57rvv2LhxI59//jlbt261v3fixAmOHj3KlClTuH79un0+kXnz5tGtW7c6qVFaDkIIUZnDwiE1NZUzZ84QHx/P6dOniY6OJj4+vsI2GRkZHDx40D5/yLhx4xg3bpx9/x07dti3Xbx4MR07dqzzOmXMQQghKnNYt1JycjJDhw4FICgoiPz8fAoLCytss2TJEmbPnl3l/mvWrOHZZ591VHl2N65Wkm4lIYQo57CWQ05OToX5xP38/MjOzsbLyzZFRWJiIn379qVVq1aV9j127Bj33HMP/v7+9nWrVq0iNzeXoKAgoqOj6+yxeDfuc5CWgxBClKu3Aembp3DKy8sjMTGRxx9/vMptExISGDt2rH05IiKCyMhI4uLiUBSFuLi4OqtLp1HQKFAqT4MTQgg7h4WDwWAgJyfHvnz58mV7S+DAgQNcvXqVSZMm8dxzz5Genk5sbKx925SUFHr37m1fDgsLs09XO2TIEE6ePFlndSqKYntUqIw5CCGEncPCISQkhKSkJADS09MxGAz2LqXw8HC2b9/Opk2bWL16NcHBwURHRwOQlZWFp6cnLi4ugK3FMXXqVAoKCgBbcHTo0KFOa3XRaWT6DCGEuInDxhz69OlDcHAwEyZMQFEUYmJiSExMxNvbm7CwsNvul52djZ+fn31ZURTGjx/P1KlTcXd3JyAggBkzZtRpra46jbQchBDiJg69z2Hu3LkVljt37lxpm8DAQDZu3Ghf7tatGx988EGFbUaOHMnIkSMdUyS2ex3kUlYhhLih0d8hDciYgxBC3ELCAdsUGnKfgxBC3CDhgK1bSVoOQghxg4QDMiAthBC3knBAxhyEEOJWEg7IfQ5CCHErCQds3UoyfYYQQtwg4UBZt5JMvCeEEHYSDsjVSkIIcSsJB+Q+ByGEuJWEAzJ9hhBC3ErCgRuXst78zAkhhGjMJBy48ahQuWJJCCFsJBy4KRyka0kIIQAJB+BGOMgVS0IIYSPhgG3MASQchBCinIQDtukzAJlCQwghykg4IAPSQghxK4c+JjQ2Npa0tDQURSE6OpoePXpU2mblypV89913bNy4kZSUFGbOnEmHDh0A6NixIy+++CIXL14kMjISi8WCv78/y5cvx8XFpc7qdNWXtxwkHIQQAhwYDqmpqZw5c4b4+HhOnz5NdHQ08fHxFbbJyMjg4MGD6PV6+7q+ffuyatWqCtutWrWKiRMnMmLECF577TUSEhKYOHFindUqYw5CCFGRw7qVkpOTGTp0KABBQUHk5+dTWFhYYZslS5Ywe/bsGo+VkpJCaGgoAIMHDyY5OblOa7WPOcgUGkIIATgwHHJycmjatKl92c/Pj+zsbPtyYmIiffv2pVWrVhX2y8jI4JlnnuGxxx7j22+/BaC4uNjejdSsWbMKx6kLcp+DEEJU5NAxh5vdPDVFXl4eiYmJfPTRR2RlZdnXt2vXjueee44RI0Zw7tw5IiIi2Llz522PU1ekW0kIISpyWDgYDAZycnLsy5cvX8bf3x+AAwcOcPXqVSZNmkRpaSlnz54lNjaW6OhoRo4cCUCbNm1o3rw5WVlZeHh4YDQacXNzIysrC4PBUKe1ukq3khBCVOCwbqWQkBCSkpIASE9Px2Aw4OXlBUB4eDjbt29n06ZNrF69muDgYKKjo9m6dSvr1q0DIDs7mytXrhAQEED//v3tx9q5cycDBgyo01pdpFtJCCEqcFjLoU+fPgQHBzNhwgQURSEmJobExES8vb0JCwurcp8hQ4Ywd+5cdu/ejclkYuHChbi4uDBjxgzmzZtHfHw8LVu2ZMyYMXVaq0yfIYQQFTl0zGHu3LkVljt37lxpm8DAQDZu3AiAl5cX7777bqVtDAYDH330kWOKBFz1ZWMOcp+DEEIAcoc0IGMOQghxKwkHQKdRUBQZcxBCiHISDoCiKLjqNDLmIIQQZSQcypQ/KlQIIYSEg52LTiNjDkIIUUbCoYx0KwkhxA0SDmUkHIQQ4gYJhzKuOq3c5yCEEGUkHMq46DTyJDghhCgj4VDGVaeRZ0gLIUQZCYcyrnq5lFUIIcpJOJSRAWkhhLhBwqGMi05DqdznIIQQgISDnbQchBDiBgmHMjJ9hhBC3CDhUEauVhJCiBskHMq4yn0OQghhJ+FQpnzMQVVVZ5cihBBOJ+FQxlWvRVXBZJFwEEIIhz5DOjY2lrS0NBRFITo6mh49elTaZuXKlXz33Xf250gvW7aMw4cPYzabmTZtGsOGDSMqKor09HSaNGkCwJNPPsmgQYPqtNbyR4WWWqy46CQzhRCNm8PCITU1lTNnzhAfH8/p06eJjo4mPj6+wjYZGRkcPHgQvV4PwIEDBzh16hTx8fHk5uYyduxYhg0bBsCcOXMYPHiwo8q1B0KJyYKXq0MzUwghGjyH/YqcnJzM0KFDAQgKCiI/P5/CwsIK2yxZsoTZs2fbl++//37efPNNAHx8fCguLsZiqZ8riMpbDnI5qxBCODAccnJyaNq0qX3Zz8+P7Oxs+3JiYiJ9+/alVatW9nVarRYPDw8AEhISGDhwIFqtFoBPPvmEiIgIZs+ezdWrV+u8Xled7TwSDkIIUY8D0jdfBZSXl0diYiKPP/54ldvu2rWLhIQEXnrpJQBGjx7N3Llz2bBhA126dGH16tV1Xl95t1KphIMQQjguHAwGAzk5Ofbly5cv4+/vD9jGFq5evcqkSZN47rnnSE9PJzY2FoB9+/bx7rvvsnbtWry9vQHo168fXbp0AWDIkCGcPHmyzuu90a0kN8IJIYTDwiEkJISkpCQA0tPTMRgMeHl5ARAeHs727dvZtGkTq1evJjg4mOjoaK5du8ayZct477337FcmAcyYMYNz584BkJKSQocOHeq8XulWEkKIGxx2WU6fPn0IDg5mwoQJKIpCTEwMiYmJeHt7ExYWVuU+27dvJzc3l1mzZtnXLV26lEmTJjFr1izc3d3x8PBg8eLFdV6vq166lYQQopxDr9mcO3duheXOnTtX2iYwMNB+j8Ojjz7Ko48+Wmmbli1bsnnzZscUWcZFK91KQghRTu72KuPhYutWKiqRcBBCCAmHMj7uthvxCowmJ1cihBDOV2M4FBYW8tNPPwG2u54//vhjh9xn4Gy+5eFQbHZyJUII4Xw1hsOsWbO4fPkyp06dYunSpfj5+TF//vz6qK1eueo0uGg10nIQQghqEQ6lpaU88MAD7Nixg6lTp/Lwww9TUlJSH7XVK0VR8HHXUVAs4SCEELUKh61bt7Jt2zYGDx5MZmYm165dq4/a6p2Pm54Co3QrCSFEjeEQExPDsWPHWLhwIV5eXnzzzTcV7kO4m3i768mXloMQQtR8n0Pr1q2ZOHEiv/vd70hNTcVkMhEcHFwftdU7X3e9dCsJIQS1HJDOzs6+6wekAXzcdDIgLYQQyIB0BT7uermUVQghkAHpCmwD0tJyEEKIWg9Iv/zyy3f9gLSPu45SsxWjSabQEEI0bjUOSHfp0oWwsDC+//57Tp48Sbdu3ejTp0991FbvbtwlbcJNr3VyNUII4Tw1thxiY2P5+OOPUVUVo9HI22+/zeuvv14ftdU7HzeZX0kIIaAWLYf09HTi4uLsy08//TSTJ092aFHOUj75Xr4MSgshGrkaWw5msxmj0Whfvn79OhbL3dkn7+Nmy0ppOQghGrsaWw7/93//x8MPP0y7du2wWq2cPXuWyMjI+qit3vncNOYghBCNWY3hMHLkSAYNGsTPP/+Moii0a9cOvV5fH7XVO18JByGEAGr5sB8PDw+6du1Kly5dcHd354knnnB0XU7hbe9WkjEHIUTj9queBKeqaq22i42N5dFHH2XChAkcO3asym1WrlzJlClTqt3n4sWLTJkyhYkTJzJz5kxKS0t/Tdk1ctVpcdNrpOUghGj0flU4KIpS4zapqamcOXOG+Ph4Xn31VV599dVK22RkZHDw4MEa91m1ahUTJ07k008/pW3btiQkJPyasmtF7pIWQohqxhyWLl1aZQioqsq5c+dqPHBycjJDhw4FICgoiPz8fAoLC/Hy8rJvs2TJEmbPns3q1aur3SclJYWXX34ZgMGDB/Phhx8yceLEX/Axa89Hpu0WQojbh0PHjh1vu1N175XLycmpMLW3n58f2dnZ9nBITEykb9++tGrVqsZ9iouLcXFxAaBZs2ZkZ2fXeP5fy1cm3xNCiNuHw9ixY+v0RDePU+Tl5ZGYmMhHH31EVlZWrfapbl1d8nHTcaXIMWMaQghxp6jxUtZfy2AwkJOTY1++fPky/v7+ABw4cICrV68yadIkSktLOXv2LLGxsbfdx8PDA6PRiJubG1lZWRgMBkeVjY+7np9yihx2fCGEuBP8qgHp2ggJCSEpKQmwTcFhMBjsXUrh4eFs376dTZs2sXr1aoKDg4mOjr7tPv3797ev37lzJwMGDHBU2fIcaSGEoJqWQ0pKCg888IB9ubS01N7v//nnnzNu3LhqD9ynTx+Cg4OZMGECiqIQExNDYmIi3t7ehIWF1XofgBkzZjBv3jzi4+Np2bIlY8aM+cUftLZ83HXkF5tQVbVWV2UJIcTd6LbhsGbNmgrh8NRTT7FhwwYAvvzyyxrDAWDu3LkVljt37lxpm8DAQDZu3HjbfcDWRfXRRx/VeL664Ouux2JVuV5qwdPVYb1uQgjRoN22W+nWgd+blx09KOxMMm23EEJUEw63dqncvHw3d7fcmHxPxh2EEI3XbftNrFYrRqPR3kooX7ZarVit1norsL5Jy0EIIaoJhwsXLjBq1KgKXUgjR44E7vaWg+1Hkn9dwkEI0XjdNhy+/vrr+qyjwZCWgxBCVDPmYDKZeOONNzCZbnxJnjp1ilWrVtVLYc4iz3QQQohqwmHp0qUUFhZW6FZq27YthYWF9ony7kbyTAchhKgmHI4ePcqCBQvsN74BuLi4EBUVxbffflsvxTmDTqvB00UrLQchRKN223DQarVV76DRVOhquhvJtN1CiMbutuHQtGlTDh06VGn9nj17aN68uUOLcjYfNwkHIUTjdturlaKjo5kxYwZBQUF06dIFi8VCWloaFy9eZN26dfVZY73z83ThqkzbLYRoxG4bDm3btmXLli18++23/PjjjyiKwuTJkwkJCbmr73MAaOHrxsGfrzq7DCGEcJpqZ5bTaDQMGDDAoVNkN0QBPm5cLiiRmVmFEI2Ww57ncCcL8HGl1GKVriUhRKMl4VCFFj5uAFwqMDq5EiGEcA4JhyoE+NrC4XJBiZMrEUII55BwqIK0HIQQjZ2EQxX8vV1RFLiUL+EghGicJByqoNdqaObpSpa0HIQQjZRDH5IcGxtLWloaiqIQHR1Njx497O9t2rSJhIQENBoNnTt3JiYmhoSEBLZu3Wrf5sSJExw9epQpU6Zw/fp1PDw8AJg3bx7dunVzZOm08HWVbiUhRKPlsHBITU3lzJkzxMfHc/r0aaKjo4mPjweguLiYbdu2ERcXh16vJyIigqNHjzJu3DjGjRtn33/Hjh324y1evJiOHTs6qtxKWvi4cT5PwkEI0Tg5rFspOTmZoUOHAhAUFER+fj6FhYUAuLu7s379evR6PcXFxRQWFuLv719h/zVr1vDss886qrwaGXzcpFtJCNFoOSwccnJyaNq0qX3Zz8+P7OzsCtu8//77hIWFER4eTuvWre3rjx07xj333FMhMFatWsWkSZN46aWXMBod/6XdwseNq0WllJgtDj+XEEI0NPU2IH3zQ4PKPf300+zatYt9+/Zx+PBh+/qEhATGjh1rX46IiCAyMpK4uDgURSEuLs7h9ZZfzir3OgghGiOHhYPBYCAnJ8e+fPnyZXtLIC8vj4MHDwLg5ubGwIEDOXLkiH3blJQUevfubV8OCwujTZs2AAwZMoSTJ086qmy78hvhZFBaCNEYOSwcQkJCSEpKAiA9PR2DwYCXlxcAZrOZqKgoioqKADh+/Djt27cHICsrC09PT/sT6FRVZerUqRQUFAC24OjQoYOjyrYrbznIuIMQojFy2NVKffr0ITg4mAkTJqAoCjExMSQmJuLt7U1YWBjTp08nIiICnU5Hp06dCA0NBSA7Oxs/Pz/7cRRFYfz48UydOhV3d3cCAgKYMWOGo8q2C/BxBeRGOCFE46SoVQ0G3GEyMzMJDQ1l9+7dBAYG1skxVVWl84tfEdGvLS+M6lonxxRCiIakuu9OuUP6NhRFoYWvG5dkQFoI0QhJOFQjwMeNLOlWEkI0QhIO1QjwcSPrmoSDEKLxkXCoRgsfVy7lG6u8R0MIIe5mEg7VCPBxo8RsJe+6ydmlCCFEvZJwqEbbZp4A/JhT5ORKhBCifkk4VKNTgDcAp7KuObkSIYSoXxIO1Qhs6o67XsvJrEJnlyKEEPVKwqEaGo1ChwAvTkrLQQjRyEg41KCDwVvCQQjR6Eg41KBTCy8uXysh73qps0sRQoh6I+FQgw5lg9Iy7iCEaEwkHGpQfsXSf6VrSQjRiEg41OAeXze8XXVyOasQolGRcKiBotiuWPrvJQkHIUTjIeFQCx0DbFcsyRxLQojGQsKhFjoGeJN73UROoVyxJIRoHCQcaqGjTKMhhGhkHPYMaYDY2FjS0tJQFIXo6Gh69Ohhf2/Tpk0kJCSg0Wjo3LkzMTExpKamMnPmTDp06ABAx44defHFF7l48SKRkZFYLBb8/f1Zvnw5Li4ujiy9go4tvAD44dI1+v9P83o7rxBCOIvDwiE1NZUzZ84QHx/P6dOniY6OJj4+HoDi4mK2bdtGXFwcer2eiIgIjo+U5XEAABYNSURBVB49CkDfvn1ZtWpVhWOtWrWKiRMnMmLECF577TUSEhKYOHGio0qvxN/LlXt83Tj481WeeKh9vZ1XCCGcxWHdSsnJyQwdOhSAoKAg8vPzKSy03Ujm7u7O+vXr0ev1FBcXU1hYiL+//22PlZKSQmhoKACDBw8mOTnZUWVXSVEUBnRozrcZOZgt1no9txBCOIPDwiEnJ4emTZval/38/MjOzq6wzfvvv09YWBjh4eG0bt0agIyMDJ555hkee+wxvv32W8DW0ijvRmrWrFml49SHgR39KTCaScvMr/dzCyFEfau3AemqLgN9+umn2bVrF/v27ePw4cO0a9eO5557jnfeeYelS5fywgsvUFpaWuNx6sND/9McjQJ7T9Z/MAkhRH1zWDgYDAZycnLsy5cvX7Z3HeXl5XHw4EEA3NzcGDhwIEeOHCEgIICRI0eiKApt2rShefPmZGVl4eHhgdFoBCArKwuDweCosm+riYcLPQKbsPeUhIMQ4u7nsHAICQkhKSkJgPT0dAwGA15etqt+zGYzUVFRFBXZHr95/Phx2rdvz9atW1m3bh0A2dnZXLlyhYCAAPr3728/1s6dOxkwYICjyq7WwA7NSTuXR748U1oIcZdz2NVKffr0ITg4mAkTJqAoCjExMSQmJuLt7U1YWBjTp08nIiICnU5Hp06dCA0NpaioiLlz57J7925MJhMLFy7ExcWFGTNmMG/ePOLj42nZsiVjxoxxVNnVGtjRn1VfZ/Dt6RxGdr/HKTUIIUR9UNS7YE6IzMxMQkND2b17N4GBgQ47j9lipfeifzKq+z0s+VOPmncQQogGrLrvTrlD+hfQaTWEBDVnz3+zsVjv+EwVQojbknD4hUb1uIdLBUb+fTqn5o2FEOIOJeHwC4V1DcDHTcfnhzKdXYoQQjiMhMMv5KbXMqZ3K5LSL5FfLFctCSHuThIOv8K4e1tTYrbyZdoFZ5cihBAOIeHwK3Rr5UPnFt58fli6loQQdycJh19BURQeuTeQtHN5nJRnPAgh7kISDr/S2N6tcNFqiDtwxtmlCCFEnZNw+JWaebkyqsc9bD5ynqISs7PLEUKIOiXh8BtMfrAthSVmvjh63tmlCCFEnZJw+A36tGlC13t8+OTAGadNJS6EEI4g4fAbKIpCRL+2/HDpGofO5Dq7HCGEqDMSDr/Rw71a4u2mY/lX/8Vosji7HCGEqBMSDr+Rh4uOV0YHc/DMVZ5af4jiUgkIIcSdT8KhDoztHciKR3ry7ekcntpwELPF6uyShBDiN5FwqCN/ujeQxWO7823GFRLkzmkhxB1OwqEOPXp/a3q3acIbu07J+IMQ4o4m4VCHFEVhXnhnLhUY2ZD8s7PLEUKIX03CoY49+Ltm/L6jP2v+dVqm9BZC3LF0jjx4bGwsaWlpKIpCdHQ0PXrceO7ypk2bSEhIQKPR0LlzZ2JiYlAUhWXLlnH48GHMZjPTpk1j2LBhREVFkZ6eTpMmTQB48sknGTRokCNL/03+OrwT//vWfh5evZ+p/dvxyL2BeLvpnV2WEELUmsPCITU1lTNnzhAfH8/p06eJjo4mPj4egOLiYrZt20ZcXBx6vZ6IiAiOHj1KaWkpp06dIj4+ntzcXMaOHcuwYcMAmDNnDoMHD3ZUuXWqWytfPoi4j7f3ZPDyl/9h9dcZxP6xO8ODWzi7NCGEqBWHhUNycjJDhw4FICgoiPz8fAoLC/Hy8sLd3Z3169cDtqAoLCzE39+fli1b2lsXPj4+FBcXY7HcmQO7Q7sGMLRrAEfP5rJgywmmbTzMn/oE8rcx3XB30Tq7PCGEqJbDxhxycnJo2rSpfdnPz4/s7OwK27z//vuEhYURHh5O69at0Wq1eHh4AJCQkMDAgQPRam1fpJ988gkRERHMnj2bq1evOqrsOte7TVO+eDaEGUP+hy+OZjLtk8OUmO/MwBNCNB71NiBd1cR0Tz/9NLt27WLfvn0cPnzYvn7Xrl0kJCTw0ksvATB69Gjmzp3Lhg0b6NKlC6tXr66vsuuEi07DX4Z1Yskfe7D3ZDazPvtObpQTQjRoDgsHg8FATk6Offny5cv4+/sDkJeXx8GDBwFwc3Nj4MCBHDlyBIB9+/bx7rvvsnbtWry9vQHo168fXbp0AWDIkCGcPHnSUWU71Pj7W7NgVBd2nLjEY2sP8I/vzsv9EEKIBslh4RASEkJSUhIA6enpGAwGvLy8ADCbzURFRVFUVATA8ePHad++PdeuXWPZsmW899579iuTAGbMmMG5c+cASElJoUOHDo4q2+GeGvA7Fo3pxsV8IzM/+477X91F9BfHOXI2V6b9FkI0GA4bkO7Tpw/BwcFMmDABRVGIiYkhMTERb29vwsLCmD59OhEREeh0Ojp16kRoaCibNm0iNzeXWbNm2Y+zdOlSJk2axKxZs3B3d8fDw4PFixc7qux6MeXBtkzq24YDP10h4VAmiUcy+TTlLMEtfXgipD3/2/MeXHUyaC2EcB5FvQt+Xc3MzCQ0NJTdu3cTGBjo7HJ+sWtGE1vTLvDxtz9z6nIh/t6utgB5oA3NvFydXZ4Q4i5V3XenQ2+CE7Xj7aZn0gNtmdi3DftO5fDhtz/x2j9P8ubuU/h7udLc24W2zTzp1tKXHoG+9G7TBA8X+U8nhHAc+YZpQBRFYWBHfwZ29Cfj8jX+8d0FLuUbySks4VhmHtuOXQRAp1HoHuhLr9ZN6N7Klz5tmtKuuaeTqxdC3E0kHBqo/zF485dhnSqsy79u4ui5XFJ/ukrKT1f5e+pZPjLZLon9XXNPBnb0x9/bFTe9Fg8X25/y156uOrre44ObXsYyhBA1k3C4g/h66BnUycCgTgYALFaV09mFJJ++wu4fLvNp6llKzbe/f8LHTceY3q3oH9QMixVUVFr4uBHY1INmXi7otTIPoxDCRsLhDqbVKHQM8KZjgDf/178dqqpSarFiLLVy3WTmeqmF4lILxSYLV4tK2X78Ip8dPMeG5DNVHs9dr8XbTYe3mw4fdz0Gb1fu8XXH39sVX3c9Xq46TBYrJWYrLloN3m46vNx0eLvpbfu52l676TUoilLPPw0hRF2ScLiLKIqCq06Lq06LL5VngR0e3IJXik2cu3odvVaDisqlfCOZucXkFpVSYDRxzWimwGgiv9jE6ewi9p/KoegXPhdbp1HKQkOHl6stOABKTBYsqoqrToubXoObztbt5eWqo6mnC77uesobLwpK2WcCV50GvVaDi872p/y1q1aDXqfBRXtjvU6jYFFVVBXcXbR4uehw0Wmwqioq2P5WQaNg316rUSqFmclixWSx4q7XStCJRknCoZHxddfj28rXvty5hU+126uqSonZSn6xicISMy5aDa46DaUWK9eMZq4ZzRSWlIeKmUKjmWtlIVNYYntdYDSjUaCJhwtajUKJ2YLRZCXvugmjyUJhiZmrRaWYLM67qlqnUdBpFfQaDSarFWPZWI6LVkMTDz0uuhtdbrdmxc1BptXYjqHTKug0CigKZosVs0XFVa/BXa9Fq1FQVVu3nlUFVECxBZZWo6BRbGGlVbjxWmN7rVEUNBoFjXLTsu00mCyqfd4ujaKUBZ8tADUa27LmF+bcrcFYvnjzZ7Yt3/K+otjXUdt9bvN++Yqbt6/N+X9V3SiV//tWcf6Ky3VYNxXfqHSOW7ZXVfB20xHaJQDtL/2PWwMJB1EtRVFw09t+ww9w4HlUVaXYZCn70rzBYlUxWayUmm1/yru1Si1WTGV/37zeYlXt/0iMJgvXjGbMVhWF8i/a8vOByWr70jZbrJisZX9bVFx0GjzLWhz5xSZyi0oxWcvGcm7Jr5sXraqKuew4trptrRUXrVIWilaul1ooNVvR2L4pUAClLHesVjBbrFhVFYtq+5lYVdU2PmR/bWv52LZRsdrfw96yUsp+bhZVxWyx7WO23ti/tg2h8jugym+Fsn9WtcJfld4vD74Kx6i0b9XvV3UsUT2tRuGfswfyO3+vOj2uhINoEBRFkXs3RI3Usm5BqDlQbg0ganhfreZYvzbU1JuStKb6Kh2rFnUrCni46PD3rvubZeVfoxDijqEolbt9burIEXVIrl0UQghRiYSDEEKISiQchBBCVCLhIIQQohIJByGEEJVIOAghhKjkrriU1WKx3RV66dIlJ1cihBB3jvLvzPLv0JvdFeGQnZ0NwKRJk5xciRBC3Hmys7Np27ZthXV3xWNCjUYjJ06cwN/fH61WnlcghBC1YbFYyM7Oplu3bri5uVV4764IByGEEHVLBqSFEEJUcleMOfwWsbGxpKWloSgK0dHR9OjRw9kl1WjZsmUcPnwYs9nMtGnT6N69O5GRkVgsFvz9/Vm+fDkuLi7OLrNaRqOR//3f/+XZZ5+lX79+d1T9W7du5YMPPkCn0/H888/TqVOnO6b+oqIi5s2bR35+PiaTienTp+Pv78/ChQsB6NSpEy+//LJzi6zCyZMnefbZZ5k6dSqTJ0/m4sWLVf7Mt27dyvr169FoNIwfP55x48Y5u3Sg6vrnz5+P2WxGp9OxfPly/P39G1b9aiOWkpKiPv3006qqqmpGRoY6fvx4J1dUs+TkZPWpp55SVVVVr169qv7+979Xo6Ki1O3bt6uqqqorV65U4+LinFlirbz22mvqH//4R3Xz5s13VP1Xr15Vhw0bpl67dk3NyspSFyxYcEfVv3HjRnXFihWqqqrqpUuX1OHDh6uTJ09W09LSVFVV1Tlz5qh79uxxZomVFBUVqZMnT1YXLFigbty4UVVVtcqfeVFRkTps2DC1oKBALS4uVkeNGqXm5uY6s3RVVauuPzIyUt22bZuqqqr6ySefqEuXLm1w9TfqbqXk5GSGDh0KQFBQEPn5+RQWFjq5qurdf//9vPnmmwD4+PhQXFxMSkoKoaGhAAwePJjk5GRnllij06dPk5GRwaBBgwDuqPqTk5Pp168fXl5eGAwGFi1adEfV37RpU/Ly8gAoKCigSZMmnD9/3t5iboj1u7i4sHbtWgwGg31dVT/ztLQ0unfvjre3N25ubvTp04cjR444q2y7quqPiYlh+PDhwI3/Jg2t/kYdDjk5OTRt2tS+7OfnZ78stqHSarV4eHgAkJCQwMCBAykuLrZ3YzRr1qzBf4alS5cSFRVlX76T6s/MzMRoNPLMM88wceJEkpOT76j6R40axYULFwgLC2Py5MlERkbi43PjaYANsX6dTlfpSpqqfuY5OTn4+fnZt2ko/56rqt/DwwOtVovFYuHTTz/lD3/4Q4Orv9GPOdxMvYMu3Nq1axcJCQl8+OGHDBs2zL6+oX+GLVu20KtXL1q3bl3l+w29foC8vDxWr17NhQsXiIiIqFBzQ6//H//4By1btmTdunX88MMPTJ8+HW9vb/v7Db3+qtyu5ob+WSwWC5GRkTz44IP069ePL7/8ssL7zq6/UYeDwWAgJyfHvnz58mX8/f2dWFHt7Nu3j3fffZcPPvgAb29vPDw8MBqNuLm5kZWVVaH52tDs2bOHc+fOsWfPHi5duoSLi8sdVX+zZs3o3bs3Op2ONm3a4OnpiVarvWPqP3LkCA899BAAnTt3pqSkBLPZbH+/oddfrqr/Z6r699yrVy8nVlm9+fPn07ZtW5577jmg6u8jZ9bfqLuVQkJCSEpKAiA9PR2DwYCXV90+h7WuXbt2jWXLlvHee+/RpEkTAPr372//HDt37mTAgAHOLLFab7zxBps3b2bTpk2MGzeOZ5999o6q/6GHHuLAgQNYrVZyc3O5fv36HVV/27ZtSUtLA+D8+fN4enoSFBTEoUOHgIZff7mqfuY9e/bk+PHjFBQUUFRUxJEjR7jvvvucXGnVtm7dil6v5/nnn7eva2j1N/qb4FasWMGhQ4dQFIWYmBg6d+7s7JKqFR8fz1tvvUX79u3t65YsWcKCBQsoKSmhZcuWLF68GL1e78Qqa+ett96iVatWPPTQQ8ybN++Oqf+zzz4jISEBgD//+c907979jqm/qKiI6Ohorly5gtlsZubMmfj7+/PSSy9htVrp2bMn8+fPd3aZFZw4cYKlS5dy/vx5dDodAQEBrFixgqioqEo/86+++op169ahKAqTJ0/m4Ycfdnb5VdZ/5coVXF1d7b+MBgUFsXDhwgZVf6MPByGEEJU16m4lIYQQVZNwEEIIUYmEgxBCiEokHIQQQlQi4SCEEKISCQdxV8vMzKR3795MmTKlwp/y+YV+i7feeotPPvmk2m06derE119/bV9OSUnhrbfe+tXnTElJqXBtvBCO0qjvkBaNQ/v27dm4caNTzt2uXTtWr17N73//e3lKobijSDiIRisqKgoPDw9+/PFHcnNzWbx4MV27dmX9+vVs374dgNDQUJ5++mnOnz9PVFQUFouFli1bsnTpUsA2T/+0adP4+eefeeGFFxg4cGCFcxgMBrp3784XX3zBI488UuG9Bx54gJSUFACef/55Jk2aRGpqKrm5uZw5c4bMzExmzpzJ5s2bOX/+PGvXrgUgPz+f6dOnc/78ecLCwpg+fToZGRm88sorKIqCp6cnS5YsoaCggL/+9a94eHgwefJkBg8e7OgfqbiLSLeSaNTMZjMff/wxM2fOZM2aNZw7d44vvviCuLg44uLi2LFjB2fPnuX1119n6tSpfPrppxgMBk6cOAHYJuF77733WLBgAZ999lmV55g2bRrr16/HaDTWqqb8/HzWrVtHeHg4W7Zssb/evXs3AP/9739ZtmwZmzZtYvPmzeTl5bFo0SJeeeUV1q9fT0hICHFxcQB8//33rFixQoJB/GLSchB3vZ9++okpU6bYl9u3b88rr7wC2OboAejVqxcrVqzg+++/p2fPnuh0tn8affr04YcffuA///kPL7zwAgCRkZEA7N27lz59+gAQEBDAtWvXqjy/r68vo0ePZsOGDfTs2bPGert37w5QYRLI5s2b28dJunXrhqenJ2CbduHcuXMcO3aMF198EYDS0lL7MVq3bl1hWnohakvCQdz1qhtzsFqt9teKoqAoSoWpkk0mExqNBq1WW+UUyuUhUpMpU6bwyCOP0K5duyrfN5lMVR7z5tfl51cUpcK+iqLg7u7Ohg0bKryXmZnZYOd4Eg2fdCuJRu3w4cMAHD16lKCgILp06cJ3332H2WzGbDaTlpZGly5d6NatGwcOHADgzTff5N///vcvOo+rqyuPP/447777rn2doigUFxdTXFzM999/X+tj/ec//6G4uJiSkhJOnz5NmzZt6Ny5M3v37gVg27ZtDe5pbuLOIy0Hcde7tVsJ4K9//SsAJSUlTJs2jYsXL7J8+XICAwN59NFHmTx5MqqqMm7cOFq1asXzzz/P/Pnz+fTTT7nnnnt47rnn7MFSW2PGjOGjjz6yLz/22GOMHz+eoKAggoODa32crl27Eh0dzc8//8yECRPw8fHhhRde4MUXX2Tt2rW4urqycuXKBv/IW9GwyaysotGKiopi+PDhMlgrRBWkW0kIIUQl0nIQQghRibQchBBCVCLhIIQQohIJByGEEJVIOAghhKhEwkEIIUQlEg5CCCEq+f9Aqf6UpC61qQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4766 | test accuracy: 0.515\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7600 | test accuracy: 0.556\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8995 | test accuracy: 0.593\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6002 | test accuracy: 0.593\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9311 | test accuracy: 0.579\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4732 | test accuracy: 0.569\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7514 | test accuracy: 0.606\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.3391 | test accuracy: 0.609\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4988 | test accuracy: 0.626\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4205 | test accuracy: 0.633\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8864 | test accuracy: 0.626\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6891 | test accuracy: 0.616\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.4916 | test accuracy: 0.623\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.9731 | test accuracy: 0.616\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7338 | test accuracy: 0.626\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4780 | test accuracy: 0.626\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4090 | test accuracy: 0.630\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8450 | test accuracy: 0.636\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6790 | test accuracy: 0.640\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9389 | test accuracy: 0.626\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0823 | test accuracy: 0.650\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.3591 | test accuracy: 0.657\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.3054 | test accuracy: 0.657\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.9489 | test accuracy: 0.677\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7404 | test accuracy: 0.673\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8239 | test accuracy: 0.670\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6576 | test accuracy: 0.670\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2869 | test accuracy: 0.670\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6562 | test accuracy: 0.673\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8102 | test accuracy: 0.680\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1456 | test accuracy: 0.680\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6955 | test accuracy: 0.694\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2641 | test accuracy: 0.690\n",
            "Epoch:  33 Iteration:  2380 | train loss: 1.1378 | test accuracy: 0.690\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2932 | test accuracy: 0.690\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3277 | test accuracy: 0.690\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5532 | test accuracy: 0.690\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5546 | test accuracy: 0.694\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.9822 | test accuracy: 0.690\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6153 | test accuracy: 0.690\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.9051 | test accuracy: 0.690\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8923 | test accuracy: 0.690\n",
            "Epoch:  42 Iteration:  3010 | train loss: 1.0773 | test accuracy: 0.690\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6661 | test accuracy: 0.690\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5840 | test accuracy: 0.687\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1797 | test accuracy: 0.690\n",
            "Epoch:  46 Iteration:  3290 | train loss: 1.0126 | test accuracy: 0.690\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7511 | test accuracy: 0.690\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7165 | test accuracy: 0.690\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6021 | test accuracy: 0.697\n",
            "Epoch:  50 Iteration:  3570 | train loss: 1.0548 | test accuracy: 0.690\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1558 | test accuracy: 0.690\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5796 | test accuracy: 0.690\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.5862 | test accuracy: 0.694\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6228 | test accuracy: 0.690\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.3445 | test accuracy: 0.697\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.3319 | test accuracy: 0.697\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.9118 | test accuracy: 0.694\n",
            "Epoch:  58 Iteration:  4130 | train loss: 1.1618 | test accuracy: 0.697\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2401 | test accuracy: 0.697\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3483 | test accuracy: 0.694\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.8517 | test accuracy: 0.697\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7028 | test accuracy: 0.694\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2190 | test accuracy: 0.694\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5461 | test accuracy: 0.694\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6581 | test accuracy: 0.697\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.8978 | test accuracy: 0.697\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3253 | test accuracy: 0.694\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1890 | test accuracy: 0.697\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6243 | test accuracy: 0.697\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2621 | test accuracy: 0.694\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6269 | test accuracy: 0.697\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3004 | test accuracy: 0.697\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2038 | test accuracy: 0.697\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4577 | test accuracy: 0.697\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6627 | test accuracy: 0.697\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3364 | test accuracy: 0.697\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6455 | test accuracy: 0.697\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8530 | test accuracy: 0.697\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3127 | test accuracy: 0.697\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1135 | test accuracy: 0.697\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6684 | test accuracy: 0.697\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2683 | test accuracy: 0.697\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7819 | test accuracy: 0.697\n",
            "Epoch:  84 Iteration:  5950 | train loss: 1.0452 | test accuracy: 0.697\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2773 | test accuracy: 0.697\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.8798 | test accuracy: 0.697\n",
            "Epoch:  87 Iteration:  6160 | train loss: 1.0227 | test accuracy: 0.697\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1478 | test accuracy: 0.697\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.5518 | test accuracy: 0.697\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2176 | test accuracy: 0.697\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6063 | test accuracy: 0.697\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2193 | test accuracy: 0.697\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.4419 | test accuracy: 0.697\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7771 | test accuracy: 0.697\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2639 | test accuracy: 0.697\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6318 | test accuracy: 0.697\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6075 | test accuracy: 0.697\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6243 | test accuracy: 0.697\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6036 | test accuracy: 0.697\n",
            "total time:  66.09108720600034\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4056 | test accuracy: 0.525\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7564 | test accuracy: 0.582\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.9231 | test accuracy: 0.606\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6423 | test accuracy: 0.616\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9654 | test accuracy: 0.596\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4512 | test accuracy: 0.569\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7934 | test accuracy: 0.613\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.3220 | test accuracy: 0.599\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4857 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4036 | test accuracy: 0.616\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8869 | test accuracy: 0.599\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7180 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5071 | test accuracy: 0.630\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.9738 | test accuracy: 0.633\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7471 | test accuracy: 0.620\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4865 | test accuracy: 0.616\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4186 | test accuracy: 0.630\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8762 | test accuracy: 0.626\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6547 | test accuracy: 0.616\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9312 | test accuracy: 0.620\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0741 | test accuracy: 0.646\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.3467 | test accuracy: 0.626\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.2262 | test accuracy: 0.653\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.9247 | test accuracy: 0.657\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6816 | test accuracy: 0.650\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8265 | test accuracy: 0.660\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6975 | test accuracy: 0.663\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.3024 | test accuracy: 0.677\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7533 | test accuracy: 0.670\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7446 | test accuracy: 0.663\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1684 | test accuracy: 0.684\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7146 | test accuracy: 0.687\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.3107 | test accuracy: 0.680\n",
            "Epoch:  33 Iteration:  2380 | train loss: 1.1076 | test accuracy: 0.680\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.3265 | test accuracy: 0.680\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3171 | test accuracy: 0.677\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5693 | test accuracy: 0.680\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5480 | test accuracy: 0.684\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.9810 | test accuracy: 0.680\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5889 | test accuracy: 0.680\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.9018 | test accuracy: 0.684\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8341 | test accuracy: 0.680\n",
            "Epoch:  42 Iteration:  3010 | train loss: 1.0872 | test accuracy: 0.684\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6135 | test accuracy: 0.680\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5772 | test accuracy: 0.684\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2048 | test accuracy: 0.677\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.9799 | test accuracy: 0.680\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6794 | test accuracy: 0.677\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7050 | test accuracy: 0.680\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6013 | test accuracy: 0.680\n",
            "Epoch:  50 Iteration:  3570 | train loss: 1.0698 | test accuracy: 0.680\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1533 | test accuracy: 0.680\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5580 | test accuracy: 0.680\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4732 | test accuracy: 0.687\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6551 | test accuracy: 0.680\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.3963 | test accuracy: 0.690\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4339 | test accuracy: 0.690\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7682 | test accuracy: 0.680\n",
            "Epoch:  58 Iteration:  4130 | train loss: 1.1560 | test accuracy: 0.680\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2916 | test accuracy: 0.680\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3786 | test accuracy: 0.690\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7900 | test accuracy: 0.690\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6838 | test accuracy: 0.684\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2461 | test accuracy: 0.690\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5643 | test accuracy: 0.690\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6540 | test accuracy: 0.690\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.8804 | test accuracy: 0.690\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3528 | test accuracy: 0.690\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1900 | test accuracy: 0.690\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5681 | test accuracy: 0.690\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2740 | test accuracy: 0.684\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7120 | test accuracy: 0.690\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3305 | test accuracy: 0.690\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2109 | test accuracy: 0.690\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4281 | test accuracy: 0.694\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6662 | test accuracy: 0.694\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2951 | test accuracy: 0.690\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6034 | test accuracy: 0.694\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8662 | test accuracy: 0.694\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3042 | test accuracy: 0.690\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1035 | test accuracy: 0.694\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6352 | test accuracy: 0.690\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3178 | test accuracy: 0.694\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7826 | test accuracy: 0.694\n",
            "Epoch:  84 Iteration:  5950 | train loss: 1.0345 | test accuracy: 0.694\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.3619 | test accuracy: 0.697\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7627 | test accuracy: 0.697\n",
            "Epoch:  87 Iteration:  6160 | train loss: 1.0518 | test accuracy: 0.697\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1517 | test accuracy: 0.694\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.5597 | test accuracy: 0.694\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2256 | test accuracy: 0.694\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6176 | test accuracy: 0.697\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2359 | test accuracy: 0.694\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.4490 | test accuracy: 0.697\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7759 | test accuracy: 0.697\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2932 | test accuracy: 0.697\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.5526 | test accuracy: 0.697\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6820 | test accuracy: 0.697\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6538 | test accuracy: 0.697\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6496 | test accuracy: 0.697\n",
            "total time:  66.06305315899954\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21311450004577637.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.35724353790283203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5647511448178972 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19626140594482422.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.33867645263671875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.47224784706320083 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20054936408996582.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.34578418731689453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.41546376858438766 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22381067276000977.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.36533260345458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.38629133956772943 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995232105255127.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3415358066558838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36920073202678133 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2104964256286621.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.35498976707458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.35857862276690344 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21708321571350098.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3577535152435303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34838102247033803 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032642364501953.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34687304496765137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34285395869186946 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19884109497070312.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3392601013183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33830329307488033 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21178841590881348.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3527250289916992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33367674904210226 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21089410781860352.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3564643859863281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33199556384767803 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21216177940368652.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3744528293609619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.329468891450337 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20999431610107422.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35123610496520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32627994801316945 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039651870727539.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35310935974121094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32551128693989345 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21068239212036133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36016011238098145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3238418400287628 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20654559135437012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3457813262939453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3236959917204721 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2059323787689209.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3505682945251465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3220136455127171 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045137882232666.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3576631546020508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3213985736880984 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2115185260772705.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35873866081237793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3210628105061395 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2108914852142334.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3564872741699219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31986876257828306 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2122948169708252.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3537147045135498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3194262926067625 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19910001754760742.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3450651168823242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3187607152121408 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2117626667022705.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3586888313293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3181637167930603 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21300506591796875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35669493675231934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31784980722836087 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078993320465088.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3528773784637451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3179911860397884 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20542573928833008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34883618354797363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31745915498052324 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069261074066162.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35195326805114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31702879752431595 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21532011032104492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3659067153930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31683647632598877 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20411467552185059.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.351651668548584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.316604260461671 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066965103149414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3489527702331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31633852635111126 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20308899879455566.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35430359840393066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3163165935448238 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20231318473815918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3459937572479248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31578377272401537 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21979188919067383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3581278324127197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31593239860875266 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20802736282348633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.352278470993042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31571959555149076 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.212479829788208.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36480164527893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31531668433121274 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045269012451172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35263490676879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31540749626500264 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21384167671203613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35617756843566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31529672316142493 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20954346656799316.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36150503158569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31512707897595 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20737934112548828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3497605323791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31509434708527156 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21222448348999023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35732197761535645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3150487929582596 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20396161079406738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35913610458374023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31494709508759633 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20374608039855957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3522331714630127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3148666960852487 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20688247680664062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35064125061035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3148280356611524 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21217656135559082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35474371910095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31479903970445905 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21036958694458008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35774970054626465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31476452222892215 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20885705947875977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35171055793762207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3144901918513434 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21436405181884766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3552427291870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31446267919881005 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2076413631439209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553299903869629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3144567430019379 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090587615966797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35143017768859863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3144753634929657 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21915054321289062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35786962509155273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3143432698079518 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20713043212890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3512899875640869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3142364455120904 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20525336265563965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3451857566833496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3143046149185726 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20845627784729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3502495288848877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31415999702044894 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20164060592651367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445572853088379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31426863287176404 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21498990058898926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3548259735107422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31418591269424984 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21838021278381348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36394429206848145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3141670371804919 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2129225730895996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3634762763977051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3141059236867087 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032606601715088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3580319881439209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31406737353120534 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20227384567260742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35138368606567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140284410544804 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20273423194885254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34718871116638184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3140944119010653 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20401334762573242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508436679840088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141379220145089 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2054729461669922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34674906730651855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3139613764626639 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20493221282958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470757007598877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139007644993918 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20447301864624023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35326600074768066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138769362654005 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199995756149292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465902805328369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3139243905033384 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20563936233520508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458871841430664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138857994760786 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2019190788269043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.348175048828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31393495968409946 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20699167251586914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498096466064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3138304531574249 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2119152545928955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3613882064819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138783037662506 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20831656455993652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496248722076416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31379526002066477 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19844889640808105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3378489017486572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137713347162519 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19839191436767578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3395407199859619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31381010242870877 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20947599411010742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3477334976196289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137757646186011 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20434904098510742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34813380241394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138149550982884 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.197662353515625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3347461223602295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31376789595399585 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21259617805480957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35805273056030273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31369646191596984 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20218324661254883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3442397117614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137297736746924 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1989753246307373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.339524507522583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136932262352535 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21026873588562012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35046839714050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136729163782937 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20869040489196777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35201406478881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136860353606088 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20925235748291016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35056400299072266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136698637689863 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20778894424438477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35205960273742676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31372254874025074 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2007591724395752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34306764602661133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31365724205970763 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20714116096496582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496091365814209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136541268655232 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2130112648010254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3542325496673584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.313624473129 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21149230003356934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3625307083129883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136765782322202 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035527229309082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34406399726867676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.313625316960471 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21867895126342773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36341166496276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136083611420223 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051548957824707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449256420135498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136137421642031 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20316243171691895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3490633964538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136087315423148 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20545029640197754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3499326705932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136588739497321 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1980602741241455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3348388671875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31353786715439386 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21167302131652832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3682281970977783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31354845932551795 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19811224937438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420569896697998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135598348719733 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19855833053588867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3398160934448242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135660124676568 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20292091369628906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3589019775390625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135278080190931 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017662525177002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34343862533569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31356090477534704 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20945477485656738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35396528244018555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31355733701160976 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21040058135986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36385536193847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135073989629745 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20455121994018555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3451838493347168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31353134896074025 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20182180404663086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34073472023010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135045749800546 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22247529029846191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36827802658081055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135251517806734 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20498991012573242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34386658668518066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135086808885847 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201218843460083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34102344512939453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135071860892432 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22004914283752441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36365652084350586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31350883969238824 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010509967803955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3464314937591553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134939900466374 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20117902755737305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3463609218597412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134959063359669 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21461033821105957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35350847244262695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31349742327417646 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20538568496704102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3474414348602295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135186127253941 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20612740516662598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479323387145996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31347893817084177 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21730589866638184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3662698268890381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134902558156422 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.210435152053833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514275550842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135051041841507 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1979081630706787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34382033348083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31347058841160363 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21246886253356934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3548452854156494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134438361440386 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2054903507232666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470027446746826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134554765054158 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2040550708770752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449974060058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134668018136706 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21085715293884277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3531949520111084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134520496640887 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20058083534240723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3416566848754883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31344701264585767 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2099168300628662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3624551296234131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134359406573432 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21291399002075195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35534071922302246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134500120367323 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20619583129882812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3502161502838135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134516737290791 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20779132843017578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3637378215789795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31342186459473204 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2117595672607422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3542044162750244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31345811358519965 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20298147201538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3582441806793213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344239711761473 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20819473266601562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35949182510375977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134489302124296 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20138883590698242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432447910308838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31344712504318784 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106163501739502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35471558570861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342157678944726 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23113656044006348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3778238296508789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134387446301324 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU5b3H8c+ZmSwkkwQCmVBlbYqEHVFRiyICkQC3SltFEHB/KQrFpcgS4AaloCyu4FbKVaTYohIRL2K8gojYCKIIiiAEZJeQAFnJMsmc+0fImJAAATKZhPN9v17InDnbb6LON8/znPMcwzRNExERsSybvwsQERH/UhCIiFicgkBExOIUBCIiFqcgEBGxOAWBiIjFOfxdgFy82rZty+eff07Tpk0rrXvrrbd49913cbvduN1urrzySqZMmcIvv/zCX/7yFwByc3PJzc317v/HP/6RgQMH0qdPH+69917Gjx9f4Zh33303+/btY/Xq1aetad26dfztb38D4Pjx45SUlNCkSRMARo4cyaBBg6r12dLS0rjvvvv43//93zNuN27cOOLj4+ndu3e1jns2RUVFvPzyyyQnJ1N25Xd8fDyjRo0iMDCwRs4h1mPoPgLxldMFwdq1a3n66adZvHgxkZGRFBUV8cQTTxAREcFTTz3l3S4pKYnly5fz5ptvet87cOAAgwcPJjQ0lOTkZGy20kZtRkYGgwcPBjhjEJQ3d+5cDh8+zPTp0y/wk9aeRx99lPz8fGbPnk14eDiZmZmMHz8ep9PJs88+6+/ypJ5S15DUuh07dtCyZUsiIyMBCAwMZPr06YwbN65a+wcHB9OiRQs2btzofW/lypVce+21F1xb7969mTdvHv369ePQoUPs3r2boUOH0r9/f+Li4rwtgAMHDtC+fXugNLDGjBlDQkIC/fr1Y8CAAezcuROAESNG8MEHHwClwbhs2TIGDRrEdddd5w04j8fDtGnT6NGjB0OHDuXvf/87I0aMqFTbzp07+fzzz5k5cybh4eEANGzYkBkzZnDrrbdWOl9V53/99dfp168fM2fOZNq0ad7tjh07RteuXcnJySE1NZXhw4fTr18//vCHP/D9998DkJeXx6hRo+jfvz99+vRh8uTJuN3uC/6Zi/8pCKTW/f73v2fdunWMHz+ezz//nNzcXJxOJ06ns9rHiI+Pr9Ats2LFCuLj42ukvrS0NJKTk7nkkkuYNWsWN954IytXrmTGjBlMmjSpyi+/tWvXcscdd5CcnMzVV1/NwoULqzx2amoqy5Yt45VXXuG5556jpKSEzz//nLVr1/LJJ5/w6quv8v7771e574YNG+jatSsNGzas8H7jxo2rHYKmaZKcnEz//v357LPPvO9/9tlnXHPNNYSGhjJq1ChuueUWkpOTmTp1Kg8//DDFxcUsW7aM8PBwVq5cSXJyMna7ndTU1GqdV+o2BYHUuvbt2/Ovf/0Lj8fDhAkTuOaaaxg1ahSHDh2q9jFuuukmVq9ejdvt5uDBgxQUFNC6desaqa9Xr17e16+88gr33XcfAFdccQWFhYWkp6dX2icmJoaOHTsCpZ/vl19+qfLYt9xyCwAdOnSgsLCQo0ePsnHjRnr16kVoaCgNGzZk4MCBVe6blZVF48aNL+SjeT9b586dMU2T7du3A/B///d/9O/fn927d3P06FFvC+OKK64gMjKSTZs2ef9et24dHo+HJ598knbt2l1QPVI3aLBY/KJTp07Mnj0b0zTZunUrL774Io899hhLliyp1v4RERF07NiRdevWkZqaSv/+/WustoiICO/rL774gldffZXjx49jGAamaeLxeCrtExYW5n1tt9spKSmp8thl29ntdqC0Wyg7O5vo6GjvNuVfl9eoUSPS0tLO/QOVU741cdNNN7Fq1SpatGjBt99+y5w5c9ixYwcFBQUVfp65ublkZmbSv39/srKyePHFF9m9ezc333wzEydO1CD1RUAtAql1Gzdu9H6hGYZBx44dGTt2LDt27Din4wwcOJDk5GQ+/vhjBgwYUON1ut1uHn30UR566CGSk5NZvnw5hmHU+HmcTicnTpzwLlfV4gDo3r07mzdvrhQG2dnZvPjii5imic1mqxBUWVlZpz1vv379WL16NevWreOqq67C6XTicrkIDQ3l448/9v5Zt24dcXFxAAwZMoR3332Xjz76iK1bt7Js2bIL+ehSRygIpNZ9+OGHJCYmkpubC0BxcTErVqzgqquuOqfj9OnThw0bNmC322nevHmN15mfn8+JEye8XT4LFy4kICCgwpd2TejUqRNr1qyhoKCA7OxsVq5cWeV2MTExDBgwgMcff5yMjAwAMjMzefzxx70tlqioKG93z6ZNm9izZ89pz3v55Zdz9OhRkpKSvC2ASy+9lKZNm/Lxxx8DpYPIjz/+OCdOnODll1/mvffeA0pbLc2aNfNJMErtU9eQ+NSIESO83SAAf/vb35g0aRLPP/88f/7zn4HSILj66qt5+umnz+nYISEhdOnShU6dOtVozWXCw8O5//77GTRoEI0bN+ahhx6ib9++jBw5ktdff73GzhMXF8eaNWuIj4+nZcuW9O/fn5SUlCq3nTZtGq+++irDhg3DMAwCAgK4+eabveMY99xzD48//jhr166le/fu9OjR47TnNQyDvn378u6773ovPTUMg+eee46pU6fywgsvYLPZuOeeewgJCeGWW25h4sSJzJ8/H8Mw6NKli3fMQ+o33UcgUgeYpun97Xrx4sX85z//4eWXX/ZzVWIV6hoS8bNt27bRp08fsrKyKC4u5pNPPqFr167+LkssRF1DIn7Wrl07Bg0axJ/+9Cfsdjtdu3Zl+PDh/i5LLERdQyIiFqeuIRERi6tXXUMFBQX88MMPREVFVbgSRURETq+kpIT09HQ6duxIcHBwpfX1Kgh++OEHhg0b5u8yRETqpcWLF3PllVdWer9eBUFUVBRQ+mGqmuNeREQqO3z4MMOGDfN+h56qXgVBWXdQ06ZNadasmZ+rERGpX07Xpa7BYhERi1MQiIhYnIJARMTiFAQiIhanIBARsTgFgYiIxVkmCNJzCunxzGpSj+T6uxQRsahnnnmGESNGEB8fzw033MCIESMYPXr0Wfd77LHHKCgo8Fld9eo+ggtxJKeAg5n5pB7J5Xcup7/LERELmjBhAgBJSUns3LmT8ePHV2u/559/3pdlWScIggNKb6QoLK76oeIiIv4wYcIEAgICyMzM5Omnn+avf/0rJ06coKCggClTptC5c2d69+7Nhx9+yLRp03C5XGzdupVDhw4xZ84cOnTocME1WCYIghylvWCFbs9ZthQRK1j6zQHe2bi/Ro85+Mrm/PmKc5/1ICIigmnTpvHzzz9z22230bdvX1JSUpg/fz5z586tsG1RURELFizgX//6F8uWLVMQnIuyFkGBWgQiUsd07twZgCZNmvDKK6+wYMECioqKCAkJqbRt2aRxTZs2ZcuWLTVyfssEgVoEIlLen69odl6/vftCQEAAAAsXLiQ6OprZs2fz/fffM2vWrErblp8vqKaeK2aZq4Y0RiAidd3x48dp0aIFAJ9++ilut7tWzmuZIHDYDGwGFKhFICJ11C233MIbb7zBvffeS+fOnUlPT2fp0qU+P2+9embxgQMH6NOnD6tWrTqvaajb//fHDLu6BZMGtvdBdSIiddPZvjt9OkYwY8YMNm/ejGEYJCQkeAdEAHr37k3Tpk29/V1z5sxhz549PPLII7Rp0waAyy67jClTptRYPUEOm1oEIiKn8FkQbNiwgb1797JkyRJ27dpFQkICS5YsqbDN/PnzCQ0N9S7v2bOH7t2789JLL/mkpiCHXWMEIiKn8NkYQUpKCn379gUgJiaGrKwscnP9O71DcIBaBCIip/JZEGRkZNCoUSPvcmRkJOnp6RW2SUxMZOjQocyZM8d7GVRqaiojR45k6NChfPnllzVak1oEIiKV1dp9BKeOSY8ZM4brr7+eiIgIRo0aRXJyMpdffjmjR4+mf//+7N+/nzvvvJNPPvmEwMDAGqkhOMBGYbFaBCIi5fmsReByucjIyPAuHzlyhKioKO/yoEGDaNy4MQ6Hg549e7Jjxw6io6MZMGAAhmHQokULmjRpQlpaWo3VFOSwU+BWi0BEpDyfBUGPHj1ITk4GYOvWrbhcLpzO0lk/c3JyuO+++ygqKgLg66+/pk2bNixfvpwFCxYAkJ6eztGjR4mOjq6xmoLUIhARqcRnXUPdunWjQ4cODBkyBMMwSExMJCkpibCwMOLi4ujZsye33347QUFBtG/fnvj4ePLy8hg7diyrVq3C7XYzderUGusWgtIWQUZuUY0dT0TkYuDTMYKxY8dWWI6NjfW+vuuuu7jrrrsqrHc6nbz22ms+q6e0RaCuIRGR8iwzxQRAsMOuSedERE5hqSBQi0BEpDJLBYFaBCIilVkqCIICbHowjYjIKSwVBMEOO+4SkxJPvZlwVUTE5ywVBEEBJ59SplaBiIiXpYIgWI+rFBGpxFJBEKQH2IuIVGKtIFCLQESkEksFwa8PsFcQiIiUsVQQlLUINAOpiMivLBUEahGIiFRmqSBQi0BEpDJLBYFaBCIilVkqCNQiEBGpzGJBoBaBiMipLBUEwZpiQkSkEksFQVmLoEA3lImIeFkrCNQiEBGpxFpB4B0sVotARKSMpYLAMAyCHHpcpYhIeZYKAihtFWjSORGRX1kuCIID7GoRiIiUY7kgCApQi0BEpDzrBYHDrgfTiIiUY7kgCFaLQESkAssFgVoEIiIVWS4I1CIQEanIckEQ5LBr0jkRkXIsFwTBATZNQy0iUo7DlwefMWMGmzdvxjAMEhIS6Ny5s3dd7969adq0KXZ76URwc+bMITo6+oz71AS1CEREKvJZEGzYsIG9e/eyZMkSdu3aRUJCAkuWLKmwzfz58wkNDT2nfS5UkEMtAhGR8nzWNZSSkkLfvn0BiImJISsri9zc3Brf51yV3lmsFoGISBmfBUFGRgaNGjXyLkdGRpKenl5hm8TERIYOHcqcOXMwTbNa+1wotQhERCry6RhBeaZpVlgeM2YM119/PREREYwaNYrk5OSz7lMTgk62CEzTxDCMGj++iEh947MgcLlcZGRkeJePHDlCVFSUd3nQoEHe1z179mTHjh1n3acmlD2ToKjE431imYiIlfmsa6hHjx7e3/K3bt2Ky+XC6XQCkJOTw3333UdRUREAX3/9NW3atDnjPjUlOECPqxQRKc9nLYJu3brRoUMHhgwZgmEYJCYmkpSURFhYGHFxcfTs2ZPbb7+doKAg2rdvT3x8PIZhVNqnppW1CEqnog6o8eOLiNQ3Ph0jGDt2bIXl2NhY7+u77rqLu+6666z71LSyFoGmmRARKWW5O4srtghERMSyQaAxAhGRUpYLAm/XkFoEIiKABYPA2zWkFoGICGDBIPBePqoWgYgIYMEgCApQi0BEpDzLBUGwQy0CEZHyLBcEahGIiFRkuSDwtgg0A6mICGDBIPC2CPRMAhERwIpB4Ci7j0BBICICFgwCu80gwG6oa0hE5CTLBQHoAfYiIuVZMgiCA/S4ShGRMpYMArUIRER+Zc0gUItARMTLkkEQ7LBrGmoRkZMsGQTOIAd5hcX+LkNEpE6wZBCEBtnJVRCIiAAWDQJncIBaBCIiJ1kzCIIc5CgIREQAiwZBWLCD3AIFgYgIWDQIQgMd5LtLKC7RlUMiIpYMAmewA4C8It1LICJiySAICyoNAl05JCJSjSDIzc3l559/BmDDhg28+eabHDt2zOeF+VJZi0DjBCIi1QiCRx99lCNHjrBz505mzpxJZGQkEydOrI3afCbU2yJw+7kSERH/O2sQFBUVcfXVV7Ny5Uruvvtubr75ZgoLC2ujNp9xeoNAYwQiItUKguXLl7NixQpuvPFGDhw4QE5OTm3U5jNh6hoSEfE6axAkJiayZcsWpk6ditPp5PPPP+fRRx+tjdp8xqmuIRERL8fZNmjevDl33HEHv/3tb9mwYQNut5sOHTpU6+AzZsxg8+bNGIZBQkICnTt3rrTNs88+y3fffceiRYtYv349jzzyCG3atAHgsssuY8qUKef4kc6ubIwgRy0CEZHqDRanp6ef82Dxhg0b2Lt3L0uWLGH69OlMnz690japqal8/fXXFd7r3r07ixYtYtGiRT4JAfi1RZCnMQIREd8NFqekpNC3b18AYmJiyMrKIjc3t8I2zzzzDI899th5ln7+7DaDkEC7uoZERPDhYHFGRgaNGjXyLkdGRpKenu5dTkpKonv37lx66aUV9ktNTWXkyJEMHTqUL7/88lw+yzlxBjl0Q5mICNUYI0hMTGTp0qU8+eSTOJ1OPvjgg/MaLDZN0/s6MzOTpKQk3njjDdLS0rzvt2rVitGjR9O/f3/279/PnXfeySeffEJgYOA5n+9snEEOjRGIiFCNIGjXrh1xcXFs27aNHTt20LFjR7p163bWA7tcLjIyMrzLR44cISoqCoCvvvqKY8eOMWzYMIqKiti3bx8zZswgISGBAQMGANCiRQuaNGlCWloazZs3P9/Pd1rOYD2lTEQEqtE1NGPGDN58801M06SgoIBXXnmF559//qwH7tGjB8nJyQBs3boVl8uF0+kEID4+no8++oh33nmHefPm0aFDBxISEli+fDkLFiwAID09naNHjxIdHX0hn++01DUkIlLqrC2CrVu3snjxYu/yAw88wPDhw8964G7dutGhQweGDBmCYRgkJiaSlJREWFgYcXFxVe7Tu3dvxo4dy6pVq3C73UydOtUn3UJQegnp/mMnfHJsEZH65KxBUFxcTEFBAcHBwQCcOHGCkpLqXXY5duzYCsuxsbGVtmnWrBmLFi0CwOl08tprr1Xr2BcqTC0CERGgGkFw1113cfPNN9OqVSs8Hg/79u1j3LhxtVGbT2mMQESk1FmDYMCAAfTq1Ys9e/ZgGAatWrUiICCgNmrzqbIxAtM0MQzD3+WIiPhNtR5MExISQvv27WnXrh0NGjTg3nvv9XVdPhca5MBdYlJYrMdVioi1ndcTysrfE1BfeWcgVfeQiFjceQXBxdCV4p2BVDeViYjFnXaMYObMmVV+4Zumyf79+31aVG1w6rnFIiLAGYLgsssuO+1OZ1pXXygIRERKnTYI/vjHP9ZmHbVOD7AXESl1XmMEFwO1CERESlk3CHTVkIgIcIYgWL9+fYXloqIi7+t3333XdxXVErUIRERKnTYIXn755QrL999/v/f1hx9+6LuKakmDADs2Q2MEIiKnDYJTbxorv3wx3FBmGIamohYR4QxBcOo9BOWXL4YbygDCggMUBCJieae9fNTj8VBQUOD97b9s2ePx4PFcHPPzhAbZ1TUkIpZ32iA4dOgQAwcOrNANVPYYyYulRaCuIRGRMwTB6tWra7MOv3AGB5CV7/Z3GSIifnXaMQK3280LL7yA2/3rF+XOnTt56aWXaqWw2hAWpIfTiIicNghmzpxJbm5uha6hli1bkpuby7x582qlOF/TGIGIyBmCYNOmTUyePLnCw+MDAwOZMGECX375Za0U52vOIF01JCJy2iCw2+1V72CzVeguqs+cwaWDxR5P/b8vQkTkfJ02CBo1asTGjRsrvb9mzRqaNGni06JqizOoNOxOuEv8XImIiP+c9qqhhIQE/vKXvxATE0O7du0oKSlh8+bN/PLLLyxYsKA2a/SZhg1Ku72O5xV55x4SEbGa0377tWzZkmXLlvHll1+ye/duDMNg+PDh9OjR46K5j8AVHgRAWnYBzSND/FyNiIh/nPHXYJvNxvXXX8/1119fW/XUqqYRwQAczi7wcyUiIv5j2ecRADQNPxkEWQoCEbEuSwdBRIMAAh02juQU+rsUERG/sXQQGIZB0/BgtQhExNIsHQRQ2j2kMQIRsTLLB0F0RDBpCgIRsTAFQVgQadkFF8VT10REzodPg2DGjBncfvvtDBkyhC1btlS5zbPPPsuIESPOaZ+a1DQimAK3h+x8zTkkItbksyDYsGEDe/fuZcmSJUyfPp3p06dX2iY1NZWvv/76nPapadHhupdARKzNZ0GQkpJC3759AYiJiSErK4vc3NwK2zzzzDM89thj57RPTVMQiIjV+SwIMjIyaNSokXc5MjKS9PR073JSUhLdu3fn0ksvrfY+vlB2U5kGjEXEqmptsLj8YGxmZiZJSUncc8891d7HV7zzDeleAhGxKJ9NuelyucjIyPAuHzlyhKioKAC++uorjh07xrBhwygqKmLfvn3MmDHjjPv4SnCAnUYhAeoaEhHL8lmLoEePHiQnJwOwdetWXC4XTqcTgPj4eD766CPeeecd5s2bR4cOHUhISDjjPr4UHR5MWrammRARa/JZi6Bbt2506NCBIUOGYBgGiYmJJCUlERYWRlxcXLX3qQ2lQaAWgYhYk0+fxjJ27NgKy7GxsZW2adasGYsWLTrtPrWhaXgwP/6SXevnFRGpCyx/ZzFAdHgQGbmFuEs8/i5FRKTWKQgonW/INCEjV+MEImI9CgL0gBoRsTYFAb/eXawBYxGxIgUB5aaZUItARCxIQQA0Dg0kJNDOnqMn/F2KiEitUxAANptBm+gwfjqc4+9SRERqnYLgpHZNw9h+OFsPqBERy1EQnBTbNIzjJ9yk5+gSUhGxFgXBSW2bhgOwTd1DImIxCoKTYpuGAbBdU02IiMUoCE5qFBpI0/BgDRiLiOUoCMpp2zRMXUMiYjkKgnJifxPGriO5mnxORCxFQVBOu6bhFJV4+Dkjz9+liIjUGgVBOW1PDhhv04CxiFiIgqCcmCgnDpuhAWMRsRQFQTmBDhu/cznZriAQEQtREJyi3W/C+f5glqaaEBHLUBCc4vcxjUnPKdQzjEXEMhQEp+jV1gXAZ9uP+LkSEZHaoSA4RVRYEF2aRbBaQSAiFqEgqEKvti427c/kWF6Rv0sREfE5BUEVese6ME34fIdaBSJy8VMQVKHTpRE0cQaxenu6v0sREfE5BUEVbDaDXm2j+PynIxRr3iERucgpCE6jd6yL7IJivt2X6e9SRER8SkFwGte1aYLDZujqIRG56CkITiM8OICrWkXqfgIRuegpCM6gd6yLn9JyOJiZ7+9SRER8xqdBMGPGDG6//XaGDBnCli1bKqx75513GDx4MEOGDGHq1KmYpsn69eu55pprGDFiBCNGjGDatGm+LO+sbowtvctY3UMicjFz+OrAGzZsYO/evSxZsoRdu3aRkJDAkiVLAMjPz2fFihUsXryYgIAA7rzzTjZt2gRA9+7deemll3xV1jmJiQqlRWQIn20/wohrWvq7HBERn/BZiyAlJYW+ffsCEBMTQ1ZWFrm5uQA0aNCAhQsXEhAQQH5+Prm5uURFRfmqlPNmGAa9Y138Z1cGBe4Sf5cjIuITPguCjIwMGjVq5F2OjIwkPb3iDVp///vfiYuLIz4+nubNmwOQmprKyJEjGTp0KF9++aWvyqu2G2NdFLg9pOw66u9SRER8otYGi6ua3/+BBx7g008/5YsvvuCbb76hVatWjB49mldffZWZM2cyadIkior8O9/P1a0jaRBg55MfD/u1DhERX/FZELhcLjIyMrzLR44c8Xb/ZGZm8vXXXwMQHBxMz549+fbbb4mOjmbAgAEYhkGLFi1o0qQJaWlpviqxWoID7Nzc5RKWfnOQ/cdO+LUWERFf8FkQ9OjRg+TkZAC2bt2Ky+XC6XQCUFxczIQJE8jLywPg+++/p3Xr1ixfvpwFCxYAkJ6eztGjR4mOjvZVidX2WNxl2GwwK/knf5ciIlLjfHbVULdu3ejQoQNDhgzBMAwSExNJSkoiLCyMuLg4Ro0axZ133onD4aBt27b06dOHvLw8xo4dy6pVq3C73UydOpXAwEBflVhtTSOCuf+63zLvs1Tuv641XZo39HdJIiI1xjDr0cN5Dxw4QJ8+fVi1ahXNmjWr1XPnFLjpNXsNMS4nSx64BsMwavX8IiLn62zfnbqzuJrCggN4LO4yNvx8jKXfHvR3OSIiNUZBcA7u6N6C7q0iefLDrRzOKvB3OSIiNUJBcA5sNoOZt3bGXeIh4f3vq7wkVkSkvlEQnKPWTUJ5ol8sq7cf4f1N6iISkfpPQXAe7v59K65s2Yipy7dyJFtdRCJSvykIzoPdZjDr1s4UFquLSETqPwXBefptlJMn+rXl023qIhKR+k1BcAHu6dGaK1o2Yuy7mxn/3hbS1E0kIvWQguAC2G0G/3PXVdzTozVJmw7Qa/YaPvhOrQMRqV8UBBcoIiSAKf/VnlWP96JTswge+fd3zEn+CY9H4wYiUj8oCGpIi8Yh/PO+qxlyVXPmfZbKY+98R4nCQETqAZ9NOmdFgQ4bT/+pE80jQ5id/BNBDhvP/KkzNpvmJRKRuktBUMMMw2DUjb+jsNjDS6t2YmBw73WtuSzaqYnqRKROUhD4yGN921DoLuH1tbtZsnE/jUMDuePqFjx4QwzOIP3YRaTu0DeSjxiGwcQB7Rh+TUtSdh/l0x/TmLs6lbfX7+PWK5vRsEEgkaEBDOj0G8KCA/xdrohYmILAx5pHhtA8MoTBVzbnu/2ZzFy5nflrd1M2jjzr4594tG8bbr+qBYEOjd2LSO1TENSirs0b8q8HrsE0TQrcHrYdzmbmyu1M+WAr01Zso/1vwmn3m3AubRjMbyIacH2bJrjCg/1dtohc5BQEfmAYBg0C7XRr0Yh/P3ANa3dmsG5nOlsOZJG89TDH8oqA0hvWese66N+xKb+NctK6SSgRDdSNJCI1S0HgZ4ZhcMNlUdxwWZT3vQJ3CXuO5rFs0yHe++YA//djmnddi8gQOjeLoEuzhnRuFkH7S8I1xiAiF0RBUAcFB9iJbRrOhP7h/PWmy9iTkcfPGXmkpufy/YEsNu3L5H+3/OLdPizIQVRYEB7TJK+ohOAAGzFRTu+f37mcuMKCCAt2EBYcoLEIEalAQVDHBdhttIkOo010GDeVez89p5DvD2by0+Fc0rILSM8pxGE3CAl0kFtYzK4juXy1+ygFbk+lYwY5bIQFBxAe7PCGQ1iwg4YhgfzO5aRtdBjBATbyikrweEzCGwTQMCSASyIa0CDQXnsfXkRqhYKgnooKC8Ys7R4AAAyUSURBVKJ3bDS9Y6NPu43HY3IwM59d6bkczS0ip8BNTkExOYXF5BS4yS4oLl0ucHM4u4CjuYUcP+E+63nDgx0UFnsoLjEJcBgEOewEOWwEOmwn/664XLa+/Ht2mw27DWyGgc0wsNsMbDYDu2EQGmSniTOIRiGBOOwGNqO0C61029J9DOPXfX9d/+u68ssVt6+4jYgoCC5qNpvhvXy1ujJyC9lxOAe3x8QZZMdmGGQXFHMsr5CDx/PZd+wEeYUlBDlsOOwG7hKTomIPhcUlFBZ7KCz2kJXv/vU9t4eiEg+F7pLSv4s91JXn+JQPh7MHR/n1J7e3neP2VR3fdo7bV3V825m3Lws8w/sPMCjdpuwt4zTvU27f023jMaHYU/rv1W4zCLAb2G02AuyldZ4at6fmb+X1lQP6jPuUW2lU/TanVlFxXeX3DYwKKw3veuOU5arrq0rln0QV25xlk+vbRBEZGnj2k50jBYFU0MQZRJPfBfns+KZp4i4xKfGYlJgmHtPE4ym37IHcQjcZuUVkniiixEPpNqaJaZa95uTyr689ZumxPZ7y68+8PVXtb1a1f/njn+P2VR3/5DFKPKU/i7NuX9XxPdXbvmziQxPTG8Cm9x+V3y972t7JH0+1GcavoSC+85fev+OvN7Wt8eMqCKRWGYZBoONsvxkF8ztXrZQj1eQNCNObIZimiQnYDcM7saLnZKAXl5gUezx4ThmiMqmYFKeGTVU5cuqjYM0K605z7KpfnnGfCkFJxc9c1f5l+1YnMKuTj9V55G3LxqHVONK5UxCIyFl5u0Qq9slU2s5mM7BhEGAH0IUF9YWuIxQRsTgFgYiIxSkIREQsTkEgImJxCgIREYtTEIiIWFy9uny0pKQEgMOHD/u5EhGR+qPsO7PsO/RU9SoI0tPTARg2bJifKxERqX/S09Np2bJlpfcNszq3s9URBQUF/PDDD0RFRWG362YVEZHqKCkpIT09nY4dOxIcXPmph/UqCEREpOZpsFhExOLq1RjBhZgxYwabN2/GMAwSEhLo3Lmzv0s6q1mzZvHNN99QXFzMgw8+SKdOnRg3bhwlJSVERUUxe/ZsAgNrfkramlRQUMB//dd/8fDDD3PttdfWq/qXL1/OP/7xDxwOB2PGjKFt27b1pv68vDzGjx9PVlYWbrebUaNGERUVxdSpUwFo27YtTz75pH+LrMKOHTt4+OGHufvuuxk+fDi//PJLlT/z5cuXs3DhQmw2G4MHD+a2227zd+lA1fVPnDiR4uJiHA4Hs2fPJioqqu7Vb1rA+vXrzQceeMA0TdNMTU01Bw8e7OeKzi4lJcW8//77TdM0zWPHjpk33HCDOWHCBPOjjz4yTdM0n332WXPx4sX+LLFannvuOfNPf/qTuXTp0npV/7Fjx8ybbrrJzMnJMdPS0szJkyfXq/oXLVpkzpkzxzRN0zx8+LDZr18/c/jw4ebmzZtN0zTNxx9/3FyzZo0/S6wkLy/PHD58uDl58mRz0aJFpmmaVf7M8/LyzJtuusnMzs428/PzzYEDB5rHjx/3Z+mmaVZd/7hx48wVK1aYpmma//znP82ZM2fWyfot0TWUkpJC3759AYiJiSErK4vc3Fw/V3VmV111FS+++CIA4eHh5Ofns379evr06QPAjTfeSEpKij9LPKtdu3aRmppKr169AOpV/SkpKVx77bU4nU5cLhfTpk2rV/U3atSIzMxMALKzs2nYsCEHDx70toTrYv2BgYHMnz8fl+vXOcir+plv3ryZTp06ERYWRnBwMN26dePbb7/1V9leVdWfmJhIv379gF//ndTF+i0RBBkZGTRq1Mi7HBkZ6b0Uta6y2+2EhJQ+Wey9996jZ8+e5Ofne7siGjduXOc/w8yZM5kwYYJ3uT7Vf+DAAQoKChg5ciR33HEHKSkp9ar+gQMHcujQIeLi4hg+fDjjxo0jPDzcu74u1u9wOCpd0VLVzzwjI4PIyEjvNnXl/+eq6g8JCcFut1NSUsLbb7/NH/7whzpZv2XGCMoz69GFUp9++invvfce//M//8NNN/36+Pq6/hmWLVtG165dad68eZXr63r9AJmZmcybN49Dhw5x5513Vqi5rtf/wQcfcMkll7BgwQK2b9/OqFGjCAsL866v6/VX5XQ11/XPUlJSwrhx47jmmmu49tpr+fDDDyusrwv1WyIIXC4XGRkZ3uUjR44QFRXlx4qq54svvuC1117jH//4B2FhYYSEhFBQUEBwcDBpaWkVmqB1zZo1a9i/fz9r1qzh8OHDBAYG1qv6GzduzOWXX47D4aBFixaEhoZit9vrTf3ffvst1113HQCxsbEUFhZSXFzsXV/X6y9T1X8zVf3/3LVrVz9WeWYTJ06kZcuWjB49Gqj6+8jf9Vuia6hHjx4kJycDsHXrVlwuF06n089VnVlOTg6zZs3i9ddfp2HDhgD8/ve/936OTz75hOuvv96fJZ7RCy+8wNKlS3nnnXe47bbbePjhh+tV/ddddx1fffUVHo+H48ePc+LEiXpVf8uWLdm8eTMABw8eJDQ0lJiYGDZu3AjU/frLVPUz79KlC99//z3Z2dnk5eXx7bffcuWVV/q50qotX76cgIAAxowZ432vLtZvmRvK5syZw8aNGzEMg8TERGJjY/1d0hktWbKEuXPn0rp1a+97zzzzDJMnT6awsJBLLrmEp59+moCAAD9WWT1z587l0ksv5brrrmP8+PH1pv5///vfvPfeewA89NBDdOrUqd7Un5eXR0JCAkePHqW4uJhHHnmEqKgo/vu//xuPx0OXLl2YOHGiv8us4IcffmDmzJkcPHgQh8NBdHQ0c+bMYcKECZV+5h9//DELFizAMAyGDx/OzTff7O/yq6z/6NGjBAUFeX/xjImJYerUqXWufssEgYiIVM0SXUMiInJ6CgIREYtTEIiIWJyCQETE4hQEIiIWpyCQi8KBAwe4/PLLGTFiRIU/ZfPtXIi5c+fyz3/+84zbtG3bltWrV3uX169fz9y5c8/7nOvXr69w7bmIL1nizmKxhtatW7No0SK/nLtVq1bMmzePG264QU/Pk3pHQSAXvQkTJhASEsLu3bs5fvw4Tz/9NO3bt2fhwoV89NFHAPTp04cHHniAgwcPMmHCBEpKSrjkkkuYOXMmUDrP/IMPPsiePXuYNGkSPXv2rHAOl8tFp06deP/997n11lsrrLv66qtZv349AGPGjGHYsGFs2LCB48ePs3fvXg4cOMAjjzzC0qVLOXjwIPPnzwcgKyuLUaNGcfDgQeLi4hg1ahSpqak89dRTGIZBaGgozzzzDNnZ2TzxxBOEhIQwfPhwbrzxRl//SOUio64hsYTi4mLefPNNHnnkEV5++WX279/P+++/z+LFi1m8eDErV65k3759PP/889x99928/fbbuFwufvjhB6B0ArrXX3+dyZMn8+9//7vKczz44IMsXLiQgoKCatWUlZXFggULiI+PZ9myZd7Xq1atAuCnn35i1qxZvPPOOyxdupTMzEymTZvGU089xcKFC+nRoweLFy8GYNu2bcyZM0chIOdFLQK5aPz888+MGDHCu9y6dWueeuopoHTOGoCuXbsyZ84ctm3bRpcuXXA4Sv8X6NatG9u3b+fHH39k0qRJAIwbNw6AtWvX0q1bNwCio6PJycmp8vwRERHccsstvPXWW3Tp0uWs9Xbq1AmgwgSITZo08Y5rdOzYkdDQUKB0aoL9+/ezZcsWpkyZAkBRUZH3GM2bN68w1brIuVAQyEXjTGMEHo/H+9owDAzDqDD9r9vtxmazYbfbq5wWuCwwzmbEiBHceuuttGrVqsr1bre7ymOWf112fsMwKuxrGAYNGjTgrbfeqrDuwIEDdXbOI6kf1DUklvDNN98AsGnTJmJiYmjXrh3fffcdxcXFFBcXs3nzZtq1a0fHjh356quvAHjxxRf5z3/+c07nCQoK4p577uG1117zvmcYBvn5+eTn57Nt27ZqH+vHH38kPz+fwsJCdu3aRYsWLYiNjWXt2rUArFixos49ZUzqJ7UI5KJxatcQwBNPPAFAYWEhDz74IL/88guzZ8+mWbNm3H777QwfPhzTNLntttu49NJLGTNmDBMnTuTtt9/mN7/5DaNHj/aGSHUNGjSIN954w7s8dOhQBg8eTExMDB06dKj2cdq3b09CQgJ79uxhyJAhhIeHM2nSJKZMmcL8+fMJCgri2WefrfOPXZW6T7OPykVvwoQJ9OvXTwOpIqehriEREYtTi0BExOLUIhARsTgFgYiIxSkIREQsTkEgImJxCgIREYtTEIiIWNz/A0PrZjAj3Mp0AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7185 | test accuracy: 0.525\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6300 | test accuracy: 0.532\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7216 | test accuracy: 0.609\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6243 | test accuracy: 0.599\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7673 | test accuracy: 0.620\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7173 | test accuracy: 0.606\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6705 | test accuracy: 0.623\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7448 | test accuracy: 0.623\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4743 | test accuracy: 0.599\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7377 | test accuracy: 0.613\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5926 | test accuracy: 0.633\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5017 | test accuracy: 0.599\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8818 | test accuracy: 0.626\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3846 | test accuracy: 0.630\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6256 | test accuracy: 0.633\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3797 | test accuracy: 0.633\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6460 | test accuracy: 0.636\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8265 | test accuracy: 0.636\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6446 | test accuracy: 0.643\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2358 | test accuracy: 0.650\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3422 | test accuracy: 0.657\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.3995 | test accuracy: 0.660\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5079 | test accuracy: 0.670\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2746 | test accuracy: 0.670\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6623 | test accuracy: 0.663\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6097 | test accuracy: 0.670\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3228 | test accuracy: 0.684\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.9329 | test accuracy: 0.684\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.8318 | test accuracy: 0.687\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8103 | test accuracy: 0.687\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7805 | test accuracy: 0.687\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5814 | test accuracy: 0.687\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7226 | test accuracy: 0.687\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2011 | test accuracy: 0.690\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.4307 | test accuracy: 0.690\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1551 | test accuracy: 0.694\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.3794 | test accuracy: 0.694\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.4256 | test accuracy: 0.694\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5137 | test accuracy: 0.697\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5808 | test accuracy: 0.697\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6158 | test accuracy: 0.694\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5141 | test accuracy: 0.697\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6356 | test accuracy: 0.690\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3762 | test accuracy: 0.690\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3106 | test accuracy: 0.694\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1876 | test accuracy: 0.694\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4481 | test accuracy: 0.694\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3589 | test accuracy: 0.694\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6061 | test accuracy: 0.697\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.2921 | test accuracy: 0.694\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.3830 | test accuracy: 0.704\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.4656 | test accuracy: 0.700\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.8233 | test accuracy: 0.700\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.5389 | test accuracy: 0.694\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2346 | test accuracy: 0.697\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0961 | test accuracy: 0.697\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.3599 | test accuracy: 0.697\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3175 | test accuracy: 0.697\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.4625 | test accuracy: 0.697\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.3107 | test accuracy: 0.697\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.2203 | test accuracy: 0.697\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2386 | test accuracy: 0.697\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.5971 | test accuracy: 0.697\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.4283 | test accuracy: 0.700\n",
            "Epoch:  64 Iteration:  4550 | train loss: 1.4699 | test accuracy: 0.700\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3053 | test accuracy: 0.697\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.9167 | test accuracy: 0.700\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3327 | test accuracy: 0.700\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6001 | test accuracy: 0.700\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2418 | test accuracy: 0.700\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7168 | test accuracy: 0.700\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3697 | test accuracy: 0.704\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3778 | test accuracy: 0.700\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2408 | test accuracy: 0.707\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4498 | test accuracy: 0.700\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.4844 | test accuracy: 0.700\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3500 | test accuracy: 0.700\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3621 | test accuracy: 0.707\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3029 | test accuracy: 0.707\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2104 | test accuracy: 0.707\n",
            "Epoch:  80 Iteration:  5670 | train loss: 1.5155 | test accuracy: 0.707\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2779 | test accuracy: 0.707\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4931 | test accuracy: 0.704\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2235 | test accuracy: 0.704\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2716 | test accuracy: 0.700\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.5925 | test accuracy: 0.707\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4104 | test accuracy: 0.700\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2863 | test accuracy: 0.707\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2881 | test accuracy: 0.707\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4682 | test accuracy: 0.704\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6007 | test accuracy: 0.707\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.5986 | test accuracy: 0.707\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6280 | test accuracy: 0.707\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.8031 | test accuracy: 0.707\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.5142 | test accuracy: 0.707\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6071 | test accuracy: 0.707\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.0222 | test accuracy: 0.704\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7934 | test accuracy: 0.707\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6322 | test accuracy: 0.707\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6668 | test accuracy: 0.704\n",
            "total time:  66.03294598000048\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7188 | test accuracy: 0.522\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6392 | test accuracy: 0.545\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7221 | test accuracy: 0.576\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5996 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7689 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6879 | test accuracy: 0.606\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6240 | test accuracy: 0.609\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7028 | test accuracy: 0.623\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4768 | test accuracy: 0.620\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7972 | test accuracy: 0.606\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5740 | test accuracy: 0.623\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4479 | test accuracy: 0.616\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8985 | test accuracy: 0.620\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3696 | test accuracy: 0.613\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6202 | test accuracy: 0.633\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3617 | test accuracy: 0.636\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6295 | test accuracy: 0.643\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8456 | test accuracy: 0.640\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6627 | test accuracy: 0.646\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2476 | test accuracy: 0.657\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3413 | test accuracy: 0.667\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.4130 | test accuracy: 0.663\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5544 | test accuracy: 0.663\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2830 | test accuracy: 0.667\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6708 | test accuracy: 0.680\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6082 | test accuracy: 0.673\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3200 | test accuracy: 0.677\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.9208 | test accuracy: 0.684\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.8401 | test accuracy: 0.680\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8326 | test accuracy: 0.680\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7452 | test accuracy: 0.690\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5913 | test accuracy: 0.690\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8086 | test accuracy: 0.687\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2268 | test accuracy: 0.690\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.4427 | test accuracy: 0.690\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1602 | test accuracy: 0.690\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.4002 | test accuracy: 0.690\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.4098 | test accuracy: 0.690\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4841 | test accuracy: 0.694\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5826 | test accuracy: 0.690\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6101 | test accuracy: 0.694\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5064 | test accuracy: 0.694\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6137 | test accuracy: 0.690\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3700 | test accuracy: 0.690\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3093 | test accuracy: 0.690\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1785 | test accuracy: 0.694\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4728 | test accuracy: 0.694\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3723 | test accuracy: 0.694\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5874 | test accuracy: 0.694\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.3791 | test accuracy: 0.694\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.3720 | test accuracy: 0.694\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.4570 | test accuracy: 0.694\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.8359 | test accuracy: 0.694\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.5307 | test accuracy: 0.694\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2270 | test accuracy: 0.694\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0351 | test accuracy: 0.694\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.3486 | test accuracy: 0.694\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2917 | test accuracy: 0.694\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.4798 | test accuracy: 0.694\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2919 | test accuracy: 0.694\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.1774 | test accuracy: 0.694\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2499 | test accuracy: 0.694\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.5829 | test accuracy: 0.694\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.4323 | test accuracy: 0.694\n",
            "Epoch:  64 Iteration:  4550 | train loss: 1.4499 | test accuracy: 0.694\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2927 | test accuracy: 0.694\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.8882 | test accuracy: 0.694\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3187 | test accuracy: 0.694\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5955 | test accuracy: 0.694\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2564 | test accuracy: 0.694\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.8089 | test accuracy: 0.694\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3625 | test accuracy: 0.694\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3709 | test accuracy: 0.694\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2503 | test accuracy: 0.694\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4505 | test accuracy: 0.694\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.5263 | test accuracy: 0.694\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3404 | test accuracy: 0.694\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.4185 | test accuracy: 0.694\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2786 | test accuracy: 0.694\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2022 | test accuracy: 0.694\n",
            "Epoch:  80 Iteration:  5670 | train loss: 1.4847 | test accuracy: 0.694\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2789 | test accuracy: 0.694\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4419 | test accuracy: 0.694\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1968 | test accuracy: 0.694\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2630 | test accuracy: 0.694\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.5823 | test accuracy: 0.694\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4038 | test accuracy: 0.694\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2648 | test accuracy: 0.694\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2981 | test accuracy: 0.697\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4712 | test accuracy: 0.694\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6105 | test accuracy: 0.697\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6080 | test accuracy: 0.697\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6176 | test accuracy: 0.697\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6871 | test accuracy: 0.694\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4735 | test accuracy: 0.697\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6797 | test accuracy: 0.694\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.0129 | test accuracy: 0.694\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.8131 | test accuracy: 0.697\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6278 | test accuracy: 0.697\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7067 | test accuracy: 0.694\n",
            "total time:  66.6647029780006\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060849666595459.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.35022878646850586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6819497312818255 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047872543334961.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.3565967082977295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5387231515986579 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20482659339904785.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.3467397689819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4601032401834215 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20711898803710938.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3516044616699219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41507923688207354 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2102036476135254.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.36377835273742676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38836886073861804 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20827436447143555.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.35195398330688477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3726391455956868 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21131205558776855.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3582322597503662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.359940465433257 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20627331733703613.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35895395278930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35254901817866735 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2091817855834961.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35097813606262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3460008983101164 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20056772232055664.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34134864807128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3417816570826939 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21116089820861816.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3546152114868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33747485492910656 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2029705047607422.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34723544120788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33432982308523995 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20697903633117676.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3527548313140869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3317468562296459 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21084213256835938.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3555881977081299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3286638477018901 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20366454124450684.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3459045886993408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3273220104830606 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148756980895996.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3623378276824951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32622517049312594 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21334075927734375.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3589181900024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3251183309725353 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20207524299621582.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3476746082305908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3242592466729028 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035057544708252.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34595632553100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3228655125413622 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2158493995666504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3609614372253418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.322504700081689 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2041611671447754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34438037872314453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3212477058172226 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20731544494628906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34978222846984863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32062069049903325 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20642566680908203.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3460681438446045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3195223923240389 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20790362358093262.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3513221740722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3198879118476595 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21405291557312012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36737895011901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31889639965125494 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20665502548217773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35227489471435547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3187835957322802 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20495033264160156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34586286544799805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31857754843575614 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20366930961608887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35790491104125977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3181138698543821 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20165467262268066.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3433191776275635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3175272775547845 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2034919261932373.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3465111255645752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31735605682645524 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20931243896484375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36467790603637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3170822960989816 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20429539680480957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34528160095214844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31660928045000347 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20656561851501465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3541104793548584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31665516793727877 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20018339157104492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3506176471710205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31629563527447835 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20331406593322754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3454279899597168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3161060733454568 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20423197746276855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3500535488128662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31599449004445757 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21296262741088867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3509244918823242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31595496365002224 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20317292213439941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3450050354003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3158944926091603 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21424317359924316.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3606297969818115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3154813310929707 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21742963790893555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3591160774230957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31556998917034695 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21065545082092285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35164308547973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31539701989718844 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21640872955322266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3637828826904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3153372666665486 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21851444244384766.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36292147636413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3152590585606439 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20433425903320312.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3476862907409668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31502866872719354 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999828815460205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34038257598876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31497361063957213 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064497470855713.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34958720207214355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31485512937818255 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21168041229248047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36002397537231445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3149783270699637 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068922519683838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3570749759674072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147029685122626 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21181678771972656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3611321449279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.314691110593932 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20903253555297852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.354508638381958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3145860454865864 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20841002464294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36009669303894043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146297872066498 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21276211738586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36173367500305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3144740296261651 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21188020706176758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556947708129883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31450304559298925 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21100950241088867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36792778968811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31438573939459663 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20726418495178223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470652103424072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31447100681918005 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20258855819702148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34485960006713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3144205723490034 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20506072044372559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35468626022338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31429432034492494 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071819305419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3543884754180908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142903098038265 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103264331817627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35617589950561523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142128701720919 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2147212028503418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35783815383911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142304258687156 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078869342803955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352630615234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140890577009746 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21277618408203125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35654616355895996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31413138168198723 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21708917617797852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35620999336242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31403196283749174 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2034318447113037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34905362129211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31408013445990424 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2074282169342041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34865617752075195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31400085857936316 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2120046615600586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549354076385498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31401444290365493 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20385122299194336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430166244506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139735975435802 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20365118980407715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508610725402832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31400577766554694 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21443843841552734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3594181537628174\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138758433716638 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20277690887451172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34712934494018555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139414399862289 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20274901390075684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447291851043701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.313948232786996 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103862762451172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35840678215026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138611308165959 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21238923072814941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.361253023147583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31388999394008094 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20626473426818848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.367612361907959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138630300760269 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19739866256713867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3422057628631592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31378557128565654 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20897889137268066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35317564010620117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31381341133798873 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20357251167297363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35326194763183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31374476296561105 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20875215530395508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35131359100341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3138154834508896 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2025001049041748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3431816101074219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137719294854573 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20062708854675293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34874820709228516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136828941958291 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20215559005737305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3550293445587158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31373128294944763 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21113228797912598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36344265937805176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31367390964712416 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21152234077453613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498091697692871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31383532200540815 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20446491241455078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34592485427856445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137085961444037 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039928436279297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34337806701660156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137205170733588 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21973323822021484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36834287643432617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.313668663586889 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20767927169799805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351560115814209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31368975767067503 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20567631721496582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35065627098083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136644631624222 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20930790901184082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3555898666381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31363758955683024 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2156691551208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3621680736541748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31362858414649963 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20309948921203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34319472312927246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136540928057262 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21174240112304688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35698866844177246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31362234950065615 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20514535903930664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533935546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136011796338218 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20158767700195312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3443338871002197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136360032217843 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21808075904846191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3631021976470947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31361454001494815 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20526647567749023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452599048614502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136043506009238 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20227551460266113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34500932693481445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135642856359482 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2123880386352539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3595743179321289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.313630462544305 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21255254745483398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36563897132873535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135839483567647 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20693087577819824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3582162857055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31356113297598703 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21082544326782227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565378189086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31354442877428873 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2133336067199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35734105110168457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135320727314268 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20726919174194336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.364581823348999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354443175452096 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20607471466064453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3492920398712158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31354204756872994 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20362138748168945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3468599319458008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31349937192031313 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22315025329589844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3701934814453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31350466225828444 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19673752784729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33770060539245605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135186140026365 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19966983795166016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33957791328430176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31351897503648485 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042522430419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35073280334472656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31349579180989945 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067267894744873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446052074432373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135154511247362 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020554542541504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33879852294921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31347795384270805 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21578097343444824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3563389778137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.313510377066476 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20626020431518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3464243412017822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31352569801466806 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20817327499389648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35038280487060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347615803991047 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21754932403564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598446846008301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31351303373064315 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20285344123840332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34285855293273926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31347348051411766 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20834136009216309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34977269172668457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31348455590861185 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21498918533325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3558175563812256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31347799173423224 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21074819564819336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510627746582031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31346523208277566 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1986386775970459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34357738494873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31346791216305325 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21014189720153809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514242172241211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31347996847970144 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20272254943847656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430521488189697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31345634417874474 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20341849327087402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34612083435058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31348559473242077 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20571184158325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345853328704834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134629377296993 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20136809349060059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3413271903991699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313463733451707 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20260834693908691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35524797439575195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134547744478498 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2001194953918457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34445667266845703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134469053574971 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20264101028442383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3460962772369385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134672556604658 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6392 | test accuracy: 0.525\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7746 | test accuracy: 0.582\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6337 | test accuracy: 0.613\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4991 | test accuracy: 0.626\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9303 | test accuracy: 0.589\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7216 | test accuracy: 0.616\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4966 | test accuracy: 0.586\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4277 | test accuracy: 0.616\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6147 | test accuracy: 0.613\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5344 | test accuracy: 0.623\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.9910 | test accuracy: 0.613\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8841 | test accuracy: 0.620\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7955 | test accuracy: 0.626\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6648 | test accuracy: 0.616\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.3138 | test accuracy: 0.626\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5026 | test accuracy: 0.633\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3894 | test accuracy: 0.616\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7071 | test accuracy: 0.633\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5401 | test accuracy: 0.630\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4598 | test accuracy: 0.626\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5396 | test accuracy: 0.636\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6937 | test accuracy: 0.673\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7909 | test accuracy: 0.673\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2733 | test accuracy: 0.650\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5999 | test accuracy: 0.626\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.3368 | test accuracy: 0.667\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3840 | test accuracy: 0.667\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6396 | test accuracy: 0.660\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2321 | test accuracy: 0.677\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3223 | test accuracy: 0.677\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6937 | test accuracy: 0.673\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6195 | test accuracy: 0.687\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8211 | test accuracy: 0.690\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6721 | test accuracy: 0.684\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2811 | test accuracy: 0.680\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2477 | test accuracy: 0.684\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2602 | test accuracy: 0.684\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.3334 | test accuracy: 0.684\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5967 | test accuracy: 0.684\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6757 | test accuracy: 0.684\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.3569 | test accuracy: 0.684\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5068 | test accuracy: 0.684\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2164 | test accuracy: 0.684\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6184 | test accuracy: 0.684\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5321 | test accuracy: 0.684\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4303 | test accuracy: 0.690\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2507 | test accuracy: 0.687\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5584 | test accuracy: 0.687\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.3295 | test accuracy: 0.687\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.4509 | test accuracy: 0.687\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7744 | test accuracy: 0.690\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2446 | test accuracy: 0.690\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5897 | test accuracy: 0.687\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.3124 | test accuracy: 0.690\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.3417 | test accuracy: 0.687\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1868 | test accuracy: 0.687\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2664 | test accuracy: 0.694\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3484 | test accuracy: 0.690\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7263 | test accuracy: 0.690\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6710 | test accuracy: 0.694\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.8982 | test accuracy: 0.694\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1437 | test accuracy: 0.694\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6956 | test accuracy: 0.694\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3210 | test accuracy: 0.694\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6459 | test accuracy: 0.694\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.2334 | test accuracy: 0.694\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5257 | test accuracy: 0.694\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3652 | test accuracy: 0.694\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1890 | test accuracy: 0.694\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6288 | test accuracy: 0.694\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.3104 | test accuracy: 0.694\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3772 | test accuracy: 0.694\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.9705 | test accuracy: 0.694\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.4174 | test accuracy: 0.694\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3122 | test accuracy: 0.697\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6049 | test accuracy: 0.694\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8615 | test accuracy: 0.697\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.2911 | test accuracy: 0.694\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2722 | test accuracy: 0.697\n",
            "Epoch:  79 Iteration:  5600 | train loss: 1.0782 | test accuracy: 0.697\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4794 | test accuracy: 0.694\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.3751 | test accuracy: 0.694\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4588 | test accuracy: 0.694\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2382 | test accuracy: 0.697\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2231 | test accuracy: 0.697\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4866 | test accuracy: 0.697\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4215 | test accuracy: 0.694\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.4874 | test accuracy: 0.697\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2803 | test accuracy: 0.694\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4833 | test accuracy: 0.697\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.4385 | test accuracy: 0.697\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.3276 | test accuracy: 0.697\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7722 | test accuracy: 0.697\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7714 | test accuracy: 0.697\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4669 | test accuracy: 0.697\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6100 | test accuracy: 0.694\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.2897 | test accuracy: 0.697\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3622 | test accuracy: 0.694\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.4151 | test accuracy: 0.694\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6986 | test accuracy: 0.694\n",
            "total time:  66.38232820899975\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6347 | test accuracy: 0.519\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7860 | test accuracy: 0.539\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5787 | test accuracy: 0.593\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5261 | test accuracy: 0.609\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9407 | test accuracy: 0.589\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6813 | test accuracy: 0.599\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4944 | test accuracy: 0.593\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4145 | test accuracy: 0.609\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6097 | test accuracy: 0.613\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6136 | test accuracy: 0.613\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.9695 | test accuracy: 0.596\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.9117 | test accuracy: 0.616\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7809 | test accuracy: 0.623\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6284 | test accuracy: 0.613\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.3013 | test accuracy: 0.633\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4974 | test accuracy: 0.633\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4023 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6908 | test accuracy: 0.643\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5276 | test accuracy: 0.657\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4388 | test accuracy: 0.633\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5592 | test accuracy: 0.643\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7093 | test accuracy: 0.663\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7882 | test accuracy: 0.663\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2814 | test accuracy: 0.663\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5689 | test accuracy: 0.653\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.3366 | test accuracy: 0.667\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3432 | test accuracy: 0.687\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6443 | test accuracy: 0.680\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2411 | test accuracy: 0.687\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3563 | test accuracy: 0.690\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7239 | test accuracy: 0.670\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6097 | test accuracy: 0.690\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8277 | test accuracy: 0.687\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6553 | test accuracy: 0.690\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2679 | test accuracy: 0.687\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2558 | test accuracy: 0.687\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2486 | test accuracy: 0.694\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.3468 | test accuracy: 0.694\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5678 | test accuracy: 0.690\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6942 | test accuracy: 0.694\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.3575 | test accuracy: 0.694\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5357 | test accuracy: 0.694\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1971 | test accuracy: 0.694\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6108 | test accuracy: 0.690\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5291 | test accuracy: 0.687\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4717 | test accuracy: 0.690\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2521 | test accuracy: 0.697\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5780 | test accuracy: 0.690\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.3141 | test accuracy: 0.687\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.4516 | test accuracy: 0.687\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7615 | test accuracy: 0.690\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2630 | test accuracy: 0.687\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6058 | test accuracy: 0.687\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.3392 | test accuracy: 0.690\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.3568 | test accuracy: 0.687\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1927 | test accuracy: 0.687\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2979 | test accuracy: 0.687\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3442 | test accuracy: 0.690\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6254 | test accuracy: 0.690\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6509 | test accuracy: 0.687\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.9378 | test accuracy: 0.687\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1346 | test accuracy: 0.690\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7384 | test accuracy: 0.687\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3042 | test accuracy: 0.687\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6891 | test accuracy: 0.687\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.3256 | test accuracy: 0.690\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5422 | test accuracy: 0.690\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3455 | test accuracy: 0.687\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1843 | test accuracy: 0.687\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6469 | test accuracy: 0.690\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2848 | test accuracy: 0.690\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3372 | test accuracy: 0.690\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.9466 | test accuracy: 0.687\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.4642 | test accuracy: 0.687\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3173 | test accuracy: 0.690\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.5847 | test accuracy: 0.694\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8274 | test accuracy: 0.694\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.2724 | test accuracy: 0.687\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2536 | test accuracy: 0.690\n",
            "Epoch:  79 Iteration:  5600 | train loss: 1.0742 | test accuracy: 0.690\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4553 | test accuracy: 0.690\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.3740 | test accuracy: 0.690\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4413 | test accuracy: 0.694\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2322 | test accuracy: 0.694\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2404 | test accuracy: 0.687\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4833 | test accuracy: 0.687\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4202 | test accuracy: 0.694\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5287 | test accuracy: 0.694\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.3016 | test accuracy: 0.694\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4763 | test accuracy: 0.694\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.4264 | test accuracy: 0.694\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.3278 | test accuracy: 0.694\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7953 | test accuracy: 0.694\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.8250 | test accuracy: 0.694\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4350 | test accuracy: 0.694\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6222 | test accuracy: 0.694\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.3242 | test accuracy: 0.694\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3860 | test accuracy: 0.694\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.3811 | test accuracy: 0.697\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6534 | test accuracy: 0.697\n",
            "total time:  67.40134224099984\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20990800857543945.\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epoch took: 0.358048677444458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.48316701991217476 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21590590476989746.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.3653683662414551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.417292879308973 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2140209674835205.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.3586907386779785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.3823488972016743 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22046208381652832.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.36699724197387695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.36300240286758967 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20397329330444336.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3521850109100342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3513029051678521 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20968055725097656.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35425496101379395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34279860598700385 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21838736534118652.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.36856937408447266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3377906109605517 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20944738388061523.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3558924198150635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33228534417493005 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20778799057006836.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35412120819091797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3307916066476277 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21587777137756348.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35921335220336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.32739121956484657 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20769119262695312.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34824562072753906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32628275070871626 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21715879440307617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36823058128356934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32412861713341307 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20806670188903809.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3558313846588135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32314349029745376 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071084976196289.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.351193904876709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.321687165754182 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20429706573486328.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35926365852355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32078696957656316 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2161424160003662.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3578824996948242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3198443378720965 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.205155611038208.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35393476486206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31942903782640186 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23171567916870117.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3768653869628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.31899036041327883 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22155261039733887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36183619499206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31853217695440567 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20914483070373535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3624260425567627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31789830795356205 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21518158912658691.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3586852550506592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3176935736622129 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20751118659973145.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34786272048950195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31719427321638377 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20685720443725586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34984803199768066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31644795494420186 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21672677993774414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3689765930175781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3167077017681939 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20998144149780273.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35187339782714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31664480652127946 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064352035522461.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3545067310333252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3159955573933465 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2167038917541504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.359022855758667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3161128465618406 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20785808563232422.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34688496589660645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31585088244506293 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20279431343078613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3420281410217285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3157036755766187 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21465754508972168.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35687875747680664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31563357029642375 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1985013484954834.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.342648983001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.315522877233369 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20084881782531738.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35771727561950684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3150980489594596 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2026677131652832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475356101989746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31499612373965125 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19588112831115723.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3422391414642334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3150725956474032 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20919060707092285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36184239387512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31507510074547357 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20377230644226074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34699535369873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3148254535027913 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20329713821411133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34999537467956543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3148194989987782 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20171618461608887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3543245792388916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31478615743773325 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20480012893676758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452138900756836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3144649807895933 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21432852745056152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3630204200744629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3144853034189769 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064681053161621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3600189685821533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3144364425114223 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20945215225219727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3528616428375244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3145348336015429 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20518851280212402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351825475692749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31444698103836605 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21918559074401855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598332405090332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3144443754638944 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2059478759765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34571409225463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31433227785996026 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2114567756652832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3509092330932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3141879967280797 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21480941772460938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35643506050109863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31417961716651915 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20712900161743164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559091091156006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31414198875427246 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20971345901489258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349668025970459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3140666672161647 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2153329849243164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35772109031677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3140953195946557 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20067191123962402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455190658569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3141470560005733 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2093188762664795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3563079833984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3140926420688629 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2172870635986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35742855072021484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31407451459339686 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027132511138916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35099267959594727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31394628371511185 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20650124549865723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559694290161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31392216980457305 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064213752746582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3462238311767578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31393919501985823 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21153831481933594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35396838188171387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3139368580920356 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20804262161254883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3570261001586914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31395849457808905 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20371103286743164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432183265686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3138553866318294 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2083909511566162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3530237674713135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31379045588629584 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21119165420532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35546374320983887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138302551848548 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19701385498046875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33371615409851074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138208474431719 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21027088165283203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352813720703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31377513578959876 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2037525177001953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35269832611083984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31379298865795135 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21652841567993164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36406469345092773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31377416678837367 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20610642433166504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34445714950561523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.313785372035844 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20365166664123535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34804725646972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31373218681131093 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19739317893981934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33942699432373047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137401925665992 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20260119438171387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34786295890808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137248226574489 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.213331937789917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3568115234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3136877864599228 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.207014799118042.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35506510734558105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31363840103149415 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20418524742126465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34990644454956055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31365624708788736 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21315598487854004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3526287078857422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3136681816407612 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21342849731445312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356475830078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31362741802419936 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20954370498657227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35657262802124023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136034454618182 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2113795280456543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501110076904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31364640508379255 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20864439010620117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35605311393737793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136429939951215 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20726776123046875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446204662322998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136372148990631 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20690274238586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34558892250061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136684702975409 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2022109031677246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34247779846191406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3135400061096464 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20321393013000488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34145355224609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3135709175041744 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20579051971435547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35065388679504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3135656131165368 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2054595947265625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3511638641357422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3135554279599871 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20421838760375977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34572935104370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3135832782302584 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2122642993927002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35320091247558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135408571788243 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20886731147766113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35309839248657227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135561836617334 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2041606903076172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447434902191162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31355324941022056 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21110963821411133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3584787845611572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135608455964497 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2059326171875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459634780883789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135180809668132 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1964564323425293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436279296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31351524335997444 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2034473419189453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3473062515258789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135858207941055 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20262694358825684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447251319885254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135550741638456 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20447754859924316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3512911796569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135045234646116 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20110726356506348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34390711784362793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31350517230374475 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027738094329834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34655117988586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135084786585399 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20509123802185059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35652709007263184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3134992518595287 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21215081214904785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534085750579834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31351236147539957 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048625946044922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3444852828979492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3134701669216156 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20549249649047852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35432004928588867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3134678087064198 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20461678504943848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449838161468506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135306473289217 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20528078079223633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3517136573791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31347651311329433 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22012615203857422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36084508895874023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134577402046749 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2179253101348877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36359071731567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135007270744869 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060699462890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34597110748291016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31347213770662036 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2134866714477539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35571932792663574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134826694216047 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21340131759643555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36089324951171875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31344193305288043 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21154165267944336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501002788543701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134638956614903 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21391654014587402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585209846496582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31346629304545265 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20925092697143555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3504638671875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.313455206155777 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2086474895477295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34848618507385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134594189269202 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22086000442504883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3663005828857422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31344409797872813 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20693445205688477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349564790725708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134175343172891 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035386562347412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34293603897094727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134576576096671 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21140646934509277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35175251960754395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.313441812140601 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20482540130615234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34409546852111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134440451860428 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20248174667358398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3516075611114502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.313437505705016 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2110309600830078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35309720039367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134388919387545 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20384812355041504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3453505039215088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134344565016883 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048802375793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36559081077575684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134423741272518 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21194767951965332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35620880126953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31342567077704836 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328831672668457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34595489501953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134117386170796 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20265793800354004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540520668029785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134064848933901 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21282649040222168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3598308563232422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134270178420203 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090287208557129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34761810302734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31340464523860384 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20139670372009277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559255599975586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134104034730366 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20077753067016602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3402884006500244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.313406691806657 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20264577865600586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449077606201172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134166747331619 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2162332534790039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585209846496582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.313404210124697 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"time\"\n",
        "rnn_arr4 = []\n",
        "nrnn_arr4 = []\n",
        "lstm_arr4 = []\n",
        "seed_arr4 = []\n",
        "method_arr4 = []\n",
        "specify_arr4 = []\n",
        "\n",
        "startprob = random_startprob(2)\n",
        "transmat1 = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])\n",
        "\n",
        "transmat2 = np.array([[0.05, 0.95],\n",
        "                     [0.05, 0.95]])\n",
        "\n",
        "transmat3 = np.array([[0.95, 0.05],\n",
        "                     [0.05, 0.95]])\n",
        "\n",
        "transmats = [transmat1, transmat2, transmat3]\n",
        "names = [\"transmat1\", \"transmat2\", \"transmat3\"]\n",
        "\n",
        "for idx, transmat in enumerate(transmats):\n",
        "  x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features,length, window_length, train_ratio, method, \n",
        "                      flip_probability=None, flip_probability_0=None, flip_probability_1=None,\n",
        "                      startprob=startprob, transmat=transmat1)\n",
        "\n",
        "  time_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "  time_train_flipped_loader = data_utils.DataLoader(time_train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "  time_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "  time_test_flipped_loader = data_utils.DataLoader(time_test_flipped, batch_size=25, shuffle=True)\n",
        "\n",
        "  for seed in seeds:\n",
        "      acc = driver(seed, time_train_flipped_loader, time_test_flipped_loader)\n",
        "      acc2 = driver(seed, time_train_flipped_loader, time_test_flipped_loader, nrnn = True)\n",
        "      acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
        "      rnn_arr4.append(acc)\n",
        "      nrnn_arr4.append(acc2)\n",
        "      lstm_arr4.append(acc3)\n",
        "      seed_arr4.append(seed)\n",
        "      method_arr4.append(method)\n",
        "      specify_arr4.append(names[idx])"
      ],
      "metadata": {
        "id": "IFriPAp0lm7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "331738be-ad11-4050-a769-b792413d7c37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6284 | test accuracy: 0.630\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6199 | test accuracy: 0.673\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7834 | test accuracy: 0.737\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7623 | test accuracy: 0.724\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6691 | test accuracy: 0.771\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2774 | test accuracy: 0.815\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7189 | test accuracy: 0.828\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5047 | test accuracy: 0.953\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3402 | test accuracy: 0.990\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1091 | test accuracy: 0.993\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1288 | test accuracy: 1.000\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0616 | test accuracy: 0.997\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0714 | test accuracy: 0.997\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0287 | test accuracy: 0.997\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0175 | test accuracy: 1.000\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0191 | test accuracy: 1.000\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0134 | test accuracy: 1.000\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0212 | test accuracy: 1.000\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0050 | test accuracy: 1.000\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0059 | test accuracy: 1.000\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0090 | test accuracy: 1.000\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0051 | test accuracy: 1.000\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0052 | test accuracy: 1.000\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0035 | test accuracy: 1.000\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0509 | test accuracy: 1.000\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0253 | test accuracy: 1.000\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0043 | test accuracy: 1.000\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0025 | test accuracy: 1.000\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0331 | test accuracy: 1.000\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0025 | test accuracy: 1.000\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0157 | test accuracy: 1.000\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0020 | test accuracy: 1.000\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0012 | test accuracy: 1.000\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0017 | test accuracy: 1.000\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0018 | test accuracy: 1.000\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0022 | test accuracy: 1.000\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0012 | test accuracy: 1.000\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0018 | test accuracy: 1.000\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0014 | test accuracy: 1.000\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0012 | test accuracy: 1.000\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0018 | test accuracy: 1.000\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0020 | test accuracy: 1.000\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0016 | test accuracy: 1.000\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0015 | test accuracy: 1.000\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0017 | test accuracy: 1.000\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0036 | test accuracy: 1.000\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0009 | test accuracy: 1.000\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0017 | test accuracy: 1.000\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0030 | test accuracy: 1.000\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0012 | test accuracy: 1.000\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0033 | test accuracy: 1.000\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0115 | test accuracy: 1.000\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0025 | test accuracy: 1.000\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0014 | test accuracy: 1.000\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0039 | test accuracy: 1.000\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0017 | test accuracy: 1.000\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0018 | test accuracy: 1.000\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0014 | test accuracy: 1.000\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0039 | test accuracy: 1.000\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0018 | test accuracy: 1.000\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0016 | test accuracy: 1.000\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0014 | test accuracy: 1.000\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0009 | test accuracy: 1.000\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0009 | test accuracy: 1.000\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0014 | test accuracy: 1.000\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0013 | test accuracy: 1.000\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0012 | test accuracy: 1.000\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0008 | test accuracy: 1.000\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0016 | test accuracy: 1.000\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0013 | test accuracy: 1.000\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0095 | test accuracy: 1.000\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0008 | test accuracy: 1.000\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0034 | test accuracy: 1.000\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0008 | test accuracy: 1.000\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0010 | test accuracy: 1.000\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0007 | test accuracy: 1.000\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0008 | test accuracy: 1.000\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0007 | test accuracy: 1.000\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0011 | test accuracy: 1.000\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0004 | test accuracy: 1.000\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0007 | test accuracy: 1.000\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0014 | test accuracy: 1.000\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0004 | test accuracy: 1.000\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0004 | test accuracy: 1.000\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0010 | test accuracy: 1.000\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0009 | test accuracy: 1.000\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0009 | test accuracy: 1.000\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0010 | test accuracy: 1.000\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0019 | test accuracy: 1.000\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0043 | test accuracy: 1.000\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0019 | test accuracy: 1.000\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0004 | test accuracy: 1.000\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0009 | test accuracy: 1.000\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0011 | test accuracy: 1.000\n",
            "total time:  65.1961260979997\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6262 | test accuracy: 0.593\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6554 | test accuracy: 0.657\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7738 | test accuracy: 0.734\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7560 | test accuracy: 0.700\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7135 | test accuracy: 0.751\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3241 | test accuracy: 0.795\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7294 | test accuracy: 0.781\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5220 | test accuracy: 0.892\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4071 | test accuracy: 0.939\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1295 | test accuracy: 0.980\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2344 | test accuracy: 0.993\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1249 | test accuracy: 0.997\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0556 | test accuracy: 0.997\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0345 | test accuracy: 0.997\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0302 | test accuracy: 1.000\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0277 | test accuracy: 0.997\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0218 | test accuracy: 1.000\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0224 | test accuracy: 0.997\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0074 | test accuracy: 1.000\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0084 | test accuracy: 1.000\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0107 | test accuracy: 1.000\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0075 | test accuracy: 1.000\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0063 | test accuracy: 1.000\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0053 | test accuracy: 1.000\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0675 | test accuracy: 1.000\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0233 | test accuracy: 1.000\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0054 | test accuracy: 1.000\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0030 | test accuracy: 1.000\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0391 | test accuracy: 0.997\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0040 | test accuracy: 0.997\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0236 | test accuracy: 0.997\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0027 | test accuracy: 0.997\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0038 | test accuracy: 0.997\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0022 | test accuracy: 0.997\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0042 | test accuracy: 0.997\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0039 | test accuracy: 0.997\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0034 | test accuracy: 0.997\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0126 | test accuracy: 0.997\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0039 | test accuracy: 0.997\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0052 | test accuracy: 0.997\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0020 | test accuracy: 0.997\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0083 | test accuracy: 0.997\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0036 | test accuracy: 0.997\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0044 | test accuracy: 0.997\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0013 | test accuracy: 0.997\n",
            "total time:  66.07839239999976\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19733929634094238.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.3584427833557129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5688539002622877 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1969587802886963.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.34099555015563965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.46993644833564757 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2029104232788086.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.35211181640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4146625646523067 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19574332237243652.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.34210968017578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.389030887399401 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19947052001953125.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.34073424339294434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3670480966567993 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20799565315246582.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3529636859893799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.35624650801931107 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1962578296661377.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.33619165420532227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3463760546275548 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1945488452911377.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3339042663574219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.34071628493922096 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328068733215332.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34767723083496094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.336795511841774 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045907974243164.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.345339298248291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33259286284446715 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19653058052062988.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35237574577331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33062359137194497 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20785808563232422.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3618764877319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32915298640727997 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999971866607666.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34832119941711426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3258489689656666 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20382356643676758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3543984889984131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32475041917392183 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972510814666748.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3348371982574463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3241937496832439 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19606518745422363.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34340572357177734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3229784544025149 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.202437162399292.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.352217435836792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.321412051149777 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20235514640808105.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34478187561035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3209176506314959 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19853425025939941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33891820907592773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3205447294882366 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19947504997253418.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3518095016479492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3194113825048719 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20750784873962402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3495368957519531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31854712324483053 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19669389724731445.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33997273445129395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31885892152786255 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19757604598999023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35344696044921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3181493831532342 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20353317260742188.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34729933738708496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3176762180668967 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2034149169921875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3429427146911621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3171848650489535 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19407105445861816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34191060066223145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31682063255991255 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19937801361083984.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3452115058898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3165837040969304 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19650816917419434.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3342406749725342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31642337696892875 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19333815574645996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3468022346496582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31657135316303797 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1986691951751709.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3386969566345215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3162763331617628 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19406580924987793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33347249031066895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31589403450489045 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20771050453186035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3518381118774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3158378439290183 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19166064262390137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3304729461669922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3155665048531124 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19321799278259277.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34012293815612793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3155291421072824 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2074897289276123.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35367298126220703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31536440082958767 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19979023933410645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3414919376373291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31526081306593756 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19634127616882324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33882641792297363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31526973843574524 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2110280990600586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35042786598205566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31517968688692366 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071690559387207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35013842582702637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31505460952009473 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19550728797912598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33786678314208984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3149839286293302 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20853328704833984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3563063144683838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31486147556986127 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20381712913513184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34988951683044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3147797120468957 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982264518737793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3365790843963623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3146508387156895 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20360183715820312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470323085784912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3147001130240304 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20318317413330078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34379100799560547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31452344017369405 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19169378280639648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.32804083824157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3144906691142491 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20566225051879883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3463609218597412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3144574161086764 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19462347030639648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33655738830566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3144632088286536 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19871115684509277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3464205265045166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31427502674715857 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20776128768920898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352161169052124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3143582625048501 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049264907836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34490013122558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3142904728651047 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032780647277832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34401893615722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31427120055471147 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21182680130004883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3610975742340088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31422115777220044 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20299506187438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436405658721924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141102675880705 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1970503330230713.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34083127975463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31414567700454166 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126445770263672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.360745906829834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31401592578206744 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2011723518371582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34168529510498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3141726004225867 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20203709602355957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34557247161865234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31401364292417255 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20276474952697754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3503425121307373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139991181237357 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20012593269348145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3391704559326172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31397182515689304 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2018415927886963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3473479747772217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.314006501861981 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19944238662719727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34358811378479004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31396189204284125 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19503045082092285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3326113224029541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139300273997443 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19520092010498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34271812438964844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31386485142367226 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1945333480834961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.333176851272583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.313895805818694 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995677947998047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3363063335418701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138498646872384 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19478535652160645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3367340564727783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137963397162301 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19518303871154785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33666443824768066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3138356242861067 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19672679901123047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33898091316223145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31375238597393035 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19790887832641602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3477945327758789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137467512062618 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20737981796264648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34882307052612305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137477159500122 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19602584838867188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3354640007019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137222715786525 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20138120651245117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455967903137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137417869908469 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19826221466064453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33389997482299805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31370926243918285 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20561790466308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459935188293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137267061642238 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2000870704650879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34602928161621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137247179235731 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19865202903747559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3412010669708252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31365632159369333 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2000136375427246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3424041271209717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.313717074905123 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19517087936401367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.341341495513916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31369107791355677 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20197439193725586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34339356422424316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31367150587694986 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19405174255371094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33240461349487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31366841878209795 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20345687866210938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3566248416900635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31366221393857685 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19272613525390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.332777738571167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136410449232374 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19513988494873047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33858561515808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31364891401359013 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.18919038772583008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34359169006347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31361855821950096 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19561982154846191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3336005210876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136452849422182 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19492292404174805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3369638919830322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31359280049800875 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20200562477111816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36128830909729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31363605218274254 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19240689277648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3334827423095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136283853224346 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20140933990478516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34755992889404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136106627328055 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032308578491211.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35892724990844727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136044421366283 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19175219535827637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3337440490722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135055295058659 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20713305473327637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537325859069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135526431458337 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19608497619628906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34386682510375977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31354212803500037 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19796299934387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3374011516571045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31353738137653897 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19754481315612793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3337266445159912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31355981060436794 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19663381576538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3412492275238037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31354662265096395 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19333839416503906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33078551292419434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31354047911507743 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1916804313659668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389286994934082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135236390999385 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1986691951751709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3450310230255127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31353246314185007 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20221686363220215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454570770263672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31350538219724383 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20406293869018555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35531187057495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31351823721613203 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2071986198425293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34961700439453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135155384029661 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19565773010253906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420369625091553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135232938187463 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.206101655960083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465719223022461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135234326124191 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20812082290649414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449418544769287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134900024959019 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014322280883789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34188175201416016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134775936603546 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20273447036743164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523998260498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134829389197486 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2139267921447754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3547549247741699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.313525613290923 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1960158348083496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3422551155090332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135030473981585 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20115351676940918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457834720611572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31346829576151714 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20508766174316406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449361324310303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31348079655851635 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19707012176513672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34420132637023926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31348207337515693 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1960909366607666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34057092666625977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134561313050134 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20906305313110352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3503444194793701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134863521371569 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20076608657836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34418654441833496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134635703904288 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972332000732422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34578371047973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134877915893282 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20655012130737305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34603452682495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31345593077795847 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.198563814163208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33815574645996094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.313477634532111 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20055198669433594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423473834991455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134420778070177 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2065873146057129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35236334800720215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31344726255961824 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20306611061096191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3497347831726074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344538884503503 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19762349128723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33689379692077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134487049920218 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051558494567871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465418815612793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31345484810216084 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1942281723022461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3435075283050537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345759076731544 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20038247108459473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34110450744628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134409815073013 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21632027626037598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3580350875854492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342293832983287 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20721077919006348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3661520481109619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134220084973744 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7059 | test accuracy: 0.566\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6246 | test accuracy: 0.707\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7665 | test accuracy: 0.734\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3959 | test accuracy: 0.700\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5860 | test accuracy: 0.764\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4213 | test accuracy: 0.771\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4140 | test accuracy: 0.828\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5255 | test accuracy: 0.906\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3613 | test accuracy: 0.963\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1504 | test accuracy: 0.966\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1718 | test accuracy: 0.997\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0902 | test accuracy: 0.997\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0363 | test accuracy: 0.997\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0364 | test accuracy: 0.997\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0333 | test accuracy: 0.997\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0187 | test accuracy: 0.997\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0180 | test accuracy: 1.000\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0137 | test accuracy: 0.997\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0075 | test accuracy: 0.997\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0098 | test accuracy: 1.000\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0070 | test accuracy: 1.000\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0294 | test accuracy: 1.000\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0082 | test accuracy: 1.000\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0058 | test accuracy: 1.000\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0040 | test accuracy: 0.997\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0042 | test accuracy: 0.997\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0335 | test accuracy: 0.997\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0029 | test accuracy: 1.000\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0256 | test accuracy: 1.000\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0025 | test accuracy: 0.997\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0039 | test accuracy: 0.997\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0042 | test accuracy: 0.997\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0126 | test accuracy: 0.997\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0072 | test accuracy: 0.997\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0063 | test accuracy: 0.997\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0025 | test accuracy: 0.997\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0054 | test accuracy: 0.997\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0020 | test accuracy: 0.997\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0040 | test accuracy: 0.997\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0539 | test accuracy: 0.997\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0043 | test accuracy: 0.997\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0025 | test accuracy: 0.997\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0045 | test accuracy: 0.997\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0049 | test accuracy: 0.997\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0027 | test accuracy: 0.997\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0030 | test accuracy: 0.997\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0072 | test accuracy: 0.997\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0181 | test accuracy: 0.997\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0112 | test accuracy: 0.997\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0022 | test accuracy: 0.997\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0225 | test accuracy: 0.997\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0264 | test accuracy: 0.997\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0022 | test accuracy: 0.997\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0004 | test accuracy: 0.997\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0006 | test accuracy: 0.997\n",
            "total time:  67.38779003999935\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6103 | test accuracy: 0.667\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6755 | test accuracy: 0.727\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8666 | test accuracy: 0.747\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3308 | test accuracy: 0.737\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5367 | test accuracy: 0.754\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3400 | test accuracy: 0.788\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3652 | test accuracy: 0.855\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4592 | test accuracy: 0.923\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2897 | test accuracy: 0.983\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1068 | test accuracy: 0.993\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1521 | test accuracy: 0.997\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0609 | test accuracy: 1.000\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0271 | test accuracy: 0.997\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0271 | test accuracy: 0.997\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0253 | test accuracy: 1.000\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0133 | test accuracy: 1.000\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0130 | test accuracy: 1.000\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0159 | test accuracy: 1.000\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0058 | test accuracy: 0.997\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0080 | test accuracy: 1.000\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0078 | test accuracy: 1.000\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0312 | test accuracy: 1.000\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0073 | test accuracy: 1.000\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0049 | test accuracy: 1.000\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0031 | test accuracy: 0.997\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0038 | test accuracy: 0.997\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0228 | test accuracy: 0.997\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0024 | test accuracy: 1.000\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0367 | test accuracy: 1.000\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0027 | test accuracy: 0.997\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0037 | test accuracy: 0.997\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0153 | test accuracy: 0.997\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0093 | test accuracy: 0.997\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0020 | test accuracy: 0.997\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0049 | test accuracy: 0.997\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0040 | test accuracy: 0.997\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0034 | test accuracy: 0.997\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0576 | test accuracy: 0.997\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0025 | test accuracy: 0.997\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0044 | test accuracy: 0.997\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0039 | test accuracy: 0.997\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0052 | test accuracy: 0.997\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0094 | test accuracy: 0.997\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0077 | test accuracy: 0.997\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0167 | test accuracy: 0.997\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0317 | test accuracy: 0.997\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0004 | test accuracy: 0.997\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0004 | test accuracy: 0.997\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0006 | test accuracy: 0.997\n",
            "total time:  67.60598323399972\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20168137550354004.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.34967708587646484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6892129821436745 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2036123275756836.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.346790075302124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5419475772551128 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2100050449371338.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.36159181594848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4559070587158203 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19685006141662598.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.34624314308166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4116865886109216 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2029867172241211.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.35010385513305664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3866940106664385 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20955657958984375.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.35192131996154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3687705363546099 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20813941955566406.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.35260796546936035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3577148228883743 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039659023284912.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.344907283782959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3500372456652778 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21092581748962402.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35710763931274414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34273372420242854 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20232319831848145.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3468911647796631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3388713674885886 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021470069885254.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3436267375946045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3363663503101894 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20627236366271973.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34853172302246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3329070602144514 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.205366849899292.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3430914878845215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.329896998831204 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19628572463989258.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3476104736328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.328152870280402 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19728994369506836.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3362743854522705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3265408217906952 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20089077949523926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3487849235534668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3249132820538112 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069554328918457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3584895133972168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32426828954901016 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20762085914611816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3528883457183838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3230503273861749 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21941709518432617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.364241361618042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32188076845237185 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1988506317138672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3471395969390869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3212510543210166 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21399426460266113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3552570343017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32054333218506403 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19490337371826172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33664917945861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31986627493585856 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1990950107574463.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35056567192077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31915343531540463 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20518779754638672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35302734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3191593076501574 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20548677444458008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3505089282989502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3185950896569661 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21305155754089355.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3540632724761963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3184032448700496 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20788860321044922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3548910617828369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3175051484789167 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20305562019348145.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35140204429626465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31760119199752807 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20881199836730957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3495948314666748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31759781454290664 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20546865463256836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3515770435333252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3168350900922503 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20346546173095703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34705066680908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3167195405278887 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21320748329162598.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35955190658569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31645723411015103 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2119302749633789.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35739803314208984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31651551510606496 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20163750648498535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3440732955932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31600366873400554 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20304036140441895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.350480318069458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31583291121891566 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20043349266052246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3447866439819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31573048106261664 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.205322265625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3491837978363037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31580559270722525 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.213395357131958.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35848331451416016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3155772988285337 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20534205436706543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3548445701599121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31537938075406213 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20546245574951172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3527030944824219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3152866393327713 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21966814994812012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.368114709854126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31530854659421104 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2073049545288086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35077738761901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31520894510405406 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20240020751953125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3545560836791992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3149008014372417 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20163822174072266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345261812210083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31486860045364923 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017827033996582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34239649772644043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31496890698160446 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201521635055542.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534274101257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31477070578507016 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20178985595703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35931944847106934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.314664535011564 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19848847389221191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3497958183288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3146888962813786 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021796703338623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575105667114258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31457349487713404 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20841169357299805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3567821979522705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31444744978632244 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2077040672302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35463786125183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3144425055810383 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22004389762878418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36859560012817383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31435698100498743 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20224642753601074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3470437526702881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31443569447313036 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19875550270080566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34572482109069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144606760569981 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2171165943145752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36614084243774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31431400861058917 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20972371101379395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3643453121185303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.314343581029347 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2015235424041748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447751998901367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142811932734081 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21723651885986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35830140113830566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3141596542937415 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20819878578186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35111284255981445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31401976346969607 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1984546184539795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3468925952911377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141714513301849 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21001672744750977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3569469451904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140519414629255 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21007585525512695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.352888822555542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31404671924454824 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045741081237793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556957244873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31398888868944985 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2070479393005371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3578531742095947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139572650194168 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20161032676696777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35131144523620605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3139099985361099 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20159626007080078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35788655281066895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.313993610228811 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2039341926574707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35077905654907227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31392411930220465 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20034146308898926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34616827964782715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.313870074067797 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21136236190795898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3637375831604004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31387105413845606 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20039749145507812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389904499053955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31385331579617093 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19628405570983887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.337815523147583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139276398079736 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2084665298461914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3582286834716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138020817722593 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20170092582702637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430807590484619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138636984995433 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1958904266357422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.338275671005249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138939606291907 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2148292064666748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575732707977295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31379427356379375 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22275972366333008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.40703296661376953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.313758083326476 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23551511764526367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3817148208618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137854516506195 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2034168243408203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459646701812744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137748748064041 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20137357711791992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34519529342651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137247779539653 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21116065979003906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36121630668640137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.313726230604308 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2106630802154541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3542513847351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137033739260265 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995992660522461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34417724609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31370377157415663 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19897985458374023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35034632682800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136590812887464 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20988106727600098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35192155838012695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137000586305346 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20649290084838867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3512732982635498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136992007493973 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20253467559814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35454320907592773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136814296245575 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2012016773223877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34301161766052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.313609796336719 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20536494255065918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35430312156677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31368492543697357 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21151947975158691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3641684055328369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136176475456783 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20133662223815918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3440890312194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136833071708679 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20351648330688477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34652209281921387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136159837245941 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21052336692810059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3613128662109375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136097835642951 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2001485824584961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389570713043213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135777303150722 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2012786865234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3489212989807129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136229655572346 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20853710174560547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.355288028717041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31366420345646995 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19800639152526855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33829569816589355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136350699833461 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20604586601257324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498201370239258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135736205748149 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2044522762298584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.346451997756958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31356881431170874 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024242877960205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34430480003356934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357333617550986 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19468998908996582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3364536762237549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31353732517787386 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20668625831604004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486762046813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31358179109437123 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972370147705078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3432149887084961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135467009884971 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2099897861480713.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3543071746826172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31356095799377987 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21261358261108398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35625338554382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135536513158253 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20089435577392578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35582876205444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31348712061132705 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20384740829467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34851741790771484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135690680571965 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20752501487731934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35012292861938477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31355963051319125 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20234918594360352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34538793563842773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31353625697749 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20329594612121582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35549092292785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31352888090269904 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21442198753356934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3609755039215088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31352271905967166 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20527052879333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34839582443237305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135493359395436 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20818734169006348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36063075065612793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134928396769932 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2080843448638916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35591602325439453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31347424771104543 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20717287063598633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549833297729492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134878098964691 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20694923400878906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36264729499816895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134881509201867 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.203216552734375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34237194061279297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31349341358457294 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20059585571289062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35195255279541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31350444981030057 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066495418548584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3582911491394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31350093824522834 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20662283897399902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35188961029052734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134593039751053 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328211784362793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.358379602432251\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.313472923210689 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090752124786377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35997557640075684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345258355140687 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21469569206237793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36829137802124023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134573268038886 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20388388633728027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34769177436828613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134937507765634 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21118950843811035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35976147651672363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343752656664164 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19926881790161133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3386399745941162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313467401266098 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20056557655334473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34357762336730957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134771010705403 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21328449249267578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3573429584503174\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31345970673220497 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20714068412780762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350297212600708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31346147102969035 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6229 | test accuracy: 0.663\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7941 | test accuracy: 0.680\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5564 | test accuracy: 0.650\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3675 | test accuracy: 0.761\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3785 | test accuracy: 0.744\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5334 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2765 | test accuracy: 0.761\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5237 | test accuracy: 0.892\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2138 | test accuracy: 0.896\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4954 | test accuracy: 0.929\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2272 | test accuracy: 0.976\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0998 | test accuracy: 0.980\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0786 | test accuracy: 0.997\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0878 | test accuracy: 0.997\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0629 | test accuracy: 0.997\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0359 | test accuracy: 0.997\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0263 | test accuracy: 0.997\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0090 | test accuracy: 0.997\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0228 | test accuracy: 0.997\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0147 | test accuracy: 1.000\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0087 | test accuracy: 0.997\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0056 | test accuracy: 0.997\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1835 | test accuracy: 1.000\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0042 | test accuracy: 1.000\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0082 | test accuracy: 0.997\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2708 | test accuracy: 0.997\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0027 | test accuracy: 0.997\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0025 | test accuracy: 0.997\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0037 | test accuracy: 0.997\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0028 | test accuracy: 0.997\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0318 | test accuracy: 0.997\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0030 | test accuracy: 0.997\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0030 | test accuracy: 0.997\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0027 | test accuracy: 0.997\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0024 | test accuracy: 0.997\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0039 | test accuracy: 0.997\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0050 | test accuracy: 0.997\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0044 | test accuracy: 0.997\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0036 | test accuracy: 0.997\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0032 | test accuracy: 0.997\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0049 | test accuracy: 0.997\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0020 | test accuracy: 0.997\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0035 | test accuracy: 0.997\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0087 | test accuracy: 0.997\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0053 | test accuracy: 0.997\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0020 | test accuracy: 0.997\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0011 | test accuracy: 0.997\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0028 | test accuracy: 0.997\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0263 | test accuracy: 0.997\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0034 | test accuracy: 0.997\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0030 | test accuracy: 0.997\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0263 | test accuracy: 0.997\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0034 | test accuracy: 0.997\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0148 | test accuracy: 0.997\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0025 | test accuracy: 0.997\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0129 | test accuracy: 0.997\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0004 | test accuracy: 0.997\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0112 | test accuracy: 0.997\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0002 | test accuracy: 0.997\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0004 | test accuracy: 0.997\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0008 | test accuracy: 0.997\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0014 | test accuracy: 0.997\n",
            "total time:  65.83236438699987\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6312 | test accuracy: 0.684\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7967 | test accuracy: 0.694\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5183 | test accuracy: 0.670\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3351 | test accuracy: 0.758\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3315 | test accuracy: 0.751\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5277 | test accuracy: 0.798\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2994 | test accuracy: 0.832\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4452 | test accuracy: 0.933\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1708 | test accuracy: 0.946\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3311 | test accuracy: 0.983\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1499 | test accuracy: 1.000\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0392 | test accuracy: 1.000\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0443 | test accuracy: 1.000\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0439 | test accuracy: 1.000\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0555 | test accuracy: 1.000\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0201 | test accuracy: 1.000\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0163 | test accuracy: 0.997\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0057 | test accuracy: 0.997\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0134 | test accuracy: 1.000\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0089 | test accuracy: 1.000\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0018 | test accuracy: 1.000\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0058 | test accuracy: 1.000\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0049 | test accuracy: 0.997\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1202 | test accuracy: 1.000\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0031 | test accuracy: 1.000\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0047 | test accuracy: 0.997\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2600 | test accuracy: 0.997\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0026 | test accuracy: 0.997\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0026 | test accuracy: 0.997\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0029 | test accuracy: 0.997\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0020 | test accuracy: 0.997\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0357 | test accuracy: 0.997\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0019 | test accuracy: 0.997\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0022 | test accuracy: 0.997\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0013 | test accuracy: 0.997\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0036 | test accuracy: 0.997\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0027 | test accuracy: 0.997\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0033 | test accuracy: 0.997\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0056 | test accuracy: 0.997\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0033 | test accuracy: 0.997\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0283 | test accuracy: 0.997\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0023 | test accuracy: 0.997\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0012 | test accuracy: 0.997\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0014 | test accuracy: 0.997\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0007 | test accuracy: 0.997\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0021 | test accuracy: 0.997\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0222 | test accuracy: 0.997\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0032 | test accuracy: 0.997\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0050 | test accuracy: 0.997\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0010 | test accuracy: 0.997\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0016 | test accuracy: 0.997\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0017 | test accuracy: 0.997\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0018 | test accuracy: 0.997\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0091 | test accuracy: 0.997\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0006 | test accuracy: 0.997\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0009 | test accuracy: 0.997\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0015 | test accuracy: 0.997\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0079 | test accuracy: 0.997\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0003 | test accuracy: 0.997\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0003 | test accuracy: 0.997\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0005 | test accuracy: 0.997\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0004 | test accuracy: 0.997\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0011 | test accuracy: 0.997\n",
            "total time:  65.76048590500068\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19650030136108398.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.3366422653198242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.48692302363259454 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20300841331481934.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.33970165252685547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.41552981904574804 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20690608024597168.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.34915804862976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.38202905910355706 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19753336906433105.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3424704074859619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3604639926127025 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21582722663879395.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35692477226257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3493460727589471 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19820189476013184.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35321736335754395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3403723124946867 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20072031021118164.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3415083885192871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3367812054497855 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20441794395446777.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34307384490966797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33221099759851186 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19474267959594727.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3369414806365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3298524192401341 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2041914463043213.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34874629974365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3266228535345623 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19706034660339355.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3417513370513916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3258592592818396 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1972959041595459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34685420989990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32350004059927806 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20440173149108887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3634073734283447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3224466528211321 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20146536827087402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.340193510055542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32139588296413424 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20094990730285645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3447117805480957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32006004239831654 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19848251342773438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.355165958404541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.31962469603334154 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19487977027893066.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3361625671386719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31952879684312 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20714473724365234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3519718647003174\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3184420645236969 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20168209075927734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35405874252319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.318291763322694 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1967787742614746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3337874412536621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31773178577423095 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19688916206359863.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3396642208099365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3169672123023442 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999952793121338.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34754204750061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3169165526117597 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19876551628112793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33765316009521484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3164007816995893 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20833039283752441.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35208654403686523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31614682504108976 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20864343643188477.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.359560489654541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3161320081778935 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1969304084777832.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3371920585632324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3160103678703308 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19797158241271973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34246110916137695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31582127596650805 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20366430282592773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3526420593261719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3155865682022912 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1936337947845459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33728671073913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3155271815402167 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20593762397766113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3435971736907959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31509984263351987 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20793581008911133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3475048542022705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31531148041997636 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19759511947631836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3368492126464844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3152670932667596 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2052900791168213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34212350845336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3148474322898047 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20915627479553223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34893202781677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31494782865047455 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1948835849761963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3519151210784912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31478135968957627 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19607114791870117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34021949768066406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31466897811208455 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20959711074829102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34876489639282227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3148152002266475 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20561814308166504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35056591033935547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31459683903626035 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024085521697998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3453099727630615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31448104892458234 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21035146713256836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496522903442383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31449198467390876 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1971299648284912.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389565944671631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3144608659403665 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20000433921813965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34134697914123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3143695971795491 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20674896240234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34589099884033203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31427617839404515 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20206499099731445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3474600315093994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31431984305381777 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20240116119384766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34554076194763184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3141938490527017 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21050286293029785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34858036041259766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31423197303499495 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20986247062683105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35520029067993164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3141778439283371 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20313644409179688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3493077754974365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3141150857721056 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21352005004882812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35608720779418945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31406004301139284 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20899009704589844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3557243347167969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3140191376209259 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20334625244140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.344318151473999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3139645427465439 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2054288387298584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3448057174682617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31397425404616763 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2041149139404297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465423583984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31397433706692285 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1980729103088379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3469061851501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31393500311034067 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068941593170166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3550570011138916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3138528679098402 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20417332649230957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34348559379577637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3138519074235644 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.195542573928833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3566248416900635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3138923645019531 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20468759536743164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3490560054779053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3137648331267493 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20508933067321777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35153961181640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3138905555009842 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20020508766174316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506276607513428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3137759723833629 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19642901420593262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34249043464660645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31384922862052916 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19993305206298828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430144786834717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138476661273411 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19891786575317383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515329360961914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31375162516321453 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20981216430664062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37197327613830566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31369960137775965 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19646167755126953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34014248847961426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31374669245311193 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20726537704467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35489845275878906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31375099761145453 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20782113075256348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37317776679992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.313772428887231 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2117769718170166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36037731170654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31373941004276273 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21491765975952148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533899784088135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31371204044137685 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20237112045288086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3438754081726074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3136535584926605 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2044987678527832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345287561416626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137014644486564 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2092297077178955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510251045227051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31367745314325607 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20225024223327637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34426069259643555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31367609713758743 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19687581062316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3390533924102783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31360547925744736 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20639681816101074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34678149223327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3135819937501635 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20617938041687012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35633158683776855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31361796600478037 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2043619155883789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34649062156677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31363304087093896 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21419000625610352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3615422248840332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3135662087372371 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20131731033325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34482407569885254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136219267334257 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20039629936218262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33832740783691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136144408157894 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20446395874023438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3485400676727295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31359428763389585 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20342683792114258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3467268943786621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31360128564493994 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19633269309997559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3353085517883301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3135488216366087 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20452070236206055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479804992675781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31360255096639905 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19562959671020508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33632326126098633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135264017752239 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1965925693511963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3369300365447998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136003847633089 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21981191635131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35909247398376465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135159530809947 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2009601593017578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.342008113861084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135701877730233 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20526385307312012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35190629959106445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31353778668812343 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20066380500793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3395419120788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135173499584198 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2003490924835205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3402688503265381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31349688853536334 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20039892196655273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3497178554534912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135340648038047 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2047266960144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585653305053711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31352466898305076 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19690203666687012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3355703353881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31348710783890316 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1998758316040039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534379005432129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3134954516376768 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20377540588378906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3486649990081787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31351455322333743 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20405101776123047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459300994873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135348400899342 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19699525833129883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35532212257385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31346132840429036 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19991707801818848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3419783115386963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3134897564138685 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19380450248718262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3325834274291992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.313504695892334 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19945025444030762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35378098487854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.313479711328234 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19979000091552734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33630943298339844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31349820111479076 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19855880737304688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3379957675933838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31351707364831655 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19361066818237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498992919921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.313455753667014 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1947793960571289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3353612422943115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134629952056067 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19619321823120117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33875346183776855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134324869939259 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2110757827758789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3564331531524658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134604807410921 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2000432014465332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33618736267089844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134860851934978 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1979665756225586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33864927291870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134302978004728 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21407628059387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35680222511291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134320033448083 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20375394821166992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445403575897217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31344669376100814 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020263671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428654670715332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134348916155951 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21058869361877441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35338473320007324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134472574506487 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19882512092590332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33580780029296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31342576571873254 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20505309104919434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3472018241882324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31343880210603986 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21476268768310547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3603496551513672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31343731241566797 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19963932037353516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3396639823913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31342571548053194 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19500255584716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389565944671631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134245740515845 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20927047729492188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34951066970825195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31346921920776366 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20196986198425293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3445417881011963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134240061044693 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20096254348754883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3385889530181885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343454335417065 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21019411087036133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533174991607666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31342233291694094 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20928287506103516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34728312492370605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134170000042234 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22963953018188477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.38908839225769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134124866553715 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25005102157592773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3894617557525635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31341533022267476 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20243406295776367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34160470962524414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134057015180588 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1978893280029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34214353561401367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134468346834183 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21318292617797852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3525111675262451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134055231298719 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4171 | test accuracy: 0.690\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4253 | test accuracy: 0.721\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6421 | test accuracy: 0.758\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5363 | test accuracy: 0.778\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9399 | test accuracy: 0.778\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4638 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8380 | test accuracy: 0.818\n",
            "Epoch:  7 Iteration:  560 | train loss: 1.2180 | test accuracy: 0.832\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7112 | test accuracy: 0.845\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4699 | test accuracy: 0.845\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2071 | test accuracy: 0.872\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0658 | test accuracy: 0.909\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.4131 | test accuracy: 0.919\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5922 | test accuracy: 0.963\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0850 | test accuracy: 0.976\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0755 | test accuracy: 0.980\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0551 | test accuracy: 0.976\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0436 | test accuracy: 0.983\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2183 | test accuracy: 0.983\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0149 | test accuracy: 0.983\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0200 | test accuracy: 0.983\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0689 | test accuracy: 0.983\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0050 | test accuracy: 0.983\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0017 | test accuracy: 0.980\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1144 | test accuracy: 0.980\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0948 | test accuracy: 0.980\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0016 | test accuracy: 0.980\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0078 | test accuracy: 0.980\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0329 | test accuracy: 0.976\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0068 | test accuracy: 0.987\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0652 | test accuracy: 0.980\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0194 | test accuracy: 0.983\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0007 | test accuracy: 0.983\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0371 | test accuracy: 0.983\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0007 | test accuracy: 0.983\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0062 | test accuracy: 0.983\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0057 | test accuracy: 0.983\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0106 | test accuracy: 0.983\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.1387 | test accuracy: 0.983\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0037 | test accuracy: 0.983\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0293 | test accuracy: 0.983\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0055 | test accuracy: 0.983\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0736 | test accuracy: 0.983\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0169 | test accuracy: 0.983\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0398 | test accuracy: 0.983\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0304 | test accuracy: 0.983\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1512 | test accuracy: 0.983\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0047 | test accuracy: 0.983\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0901 | test accuracy: 0.983\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0038 | test accuracy: 0.983\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0067 | test accuracy: 0.983\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1113 | test accuracy: 0.983\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0410 | test accuracy: 0.983\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0058 | test accuracy: 0.983\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0565 | test accuracy: 0.983\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0016 | test accuracy: 0.983\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0066 | test accuracy: 0.983\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0051 | test accuracy: 0.983\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0062 | test accuracy: 0.983\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0049 | test accuracy: 0.983\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0023 | test accuracy: 0.983\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0061 | test accuracy: 0.983\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0022 | test accuracy: 0.983\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5017 | test accuracy: 0.983\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0180 | test accuracy: 0.983\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0019 | test accuracy: 0.983\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0032 | test accuracy: 0.983\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1618 | test accuracy: 0.983\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0322 | test accuracy: 0.983\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0325 | test accuracy: 0.983\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2539 | test accuracy: 0.983\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0057 | test accuracy: 0.983\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0019 | test accuracy: 0.983\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0058 | test accuracy: 0.983\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0022 | test accuracy: 0.983\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0028 | test accuracy: 0.983\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0647 | test accuracy: 0.983\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0078 | test accuracy: 0.983\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0212 | test accuracy: 0.983\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3995 | test accuracy: 0.983\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0273 | test accuracy: 0.983\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0024 | test accuracy: 0.983\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0071 | test accuracy: 0.983\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1032 | test accuracy: 0.983\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0025 | test accuracy: 0.983\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0011 | test accuracy: 0.983\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0095 | test accuracy: 0.983\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0107 | test accuracy: 0.983\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0017 | test accuracy: 0.983\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0845 | test accuracy: 0.983\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0042 | test accuracy: 0.983\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0022 | test accuracy: 0.983\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0011 | test accuracy: 0.983\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0184 | test accuracy: 0.983\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.8481 | test accuracy: 0.983\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0044 | test accuracy: 0.983\n",
            "total time:  65.71642630800034\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3689 | test accuracy: 0.704\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3983 | test accuracy: 0.727\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6336 | test accuracy: 0.771\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5400 | test accuracy: 0.771\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8887 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4608 | test accuracy: 0.815\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7501 | test accuracy: 0.825\n",
            "Epoch:  7 Iteration:  560 | train loss: 1.2657 | test accuracy: 0.859\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7804 | test accuracy: 0.852\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3961 | test accuracy: 0.865\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1748 | test accuracy: 0.906\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0602 | test accuracy: 0.936\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.3691 | test accuracy: 0.946\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5188 | test accuracy: 0.976\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0652 | test accuracy: 0.976\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0513 | test accuracy: 0.983\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0443 | test accuracy: 0.980\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0328 | test accuracy: 0.983\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.1916 | test accuracy: 0.987\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0144 | test accuracy: 0.983\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0158 | test accuracy: 0.983\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0776 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0047 | test accuracy: 0.987\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0010 | test accuracy: 0.980\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1035 | test accuracy: 0.980\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1066 | test accuracy: 0.983\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0009 | test accuracy: 0.980\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0064 | test accuracy: 0.980\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0271 | test accuracy: 0.976\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0051 | test accuracy: 0.987\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0810 | test accuracy: 0.983\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0179 | test accuracy: 0.983\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0537 | test accuracy: 0.983\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0005 | test accuracy: 0.983\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0055 | test accuracy: 0.983\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0045 | test accuracy: 0.983\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0119 | test accuracy: 0.983\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.1563 | test accuracy: 0.983\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0029 | test accuracy: 0.983\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0300 | test accuracy: 0.983\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0039 | test accuracy: 0.983\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0817 | test accuracy: 0.983\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0147 | test accuracy: 0.983\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0534 | test accuracy: 0.983\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0511 | test accuracy: 0.983\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1155 | test accuracy: 0.983\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0039 | test accuracy: 0.983\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0769 | test accuracy: 0.983\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0060 | test accuracy: 0.983\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0981 | test accuracy: 0.983\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0413 | test accuracy: 0.983\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0042 | test accuracy: 0.983\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0599 | test accuracy: 0.983\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0011 | test accuracy: 0.983\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0038 | test accuracy: 0.983\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0062 | test accuracy: 0.983\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0029 | test accuracy: 0.983\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0018 | test accuracy: 0.983\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0020 | test accuracy: 0.983\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.4766 | test accuracy: 0.983\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0169 | test accuracy: 0.983\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0012 | test accuracy: 0.983\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0030 | test accuracy: 0.983\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1974 | test accuracy: 0.983\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0377 | test accuracy: 0.983\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0373 | test accuracy: 0.983\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1939 | test accuracy: 0.983\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0052 | test accuracy: 0.983\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0004 | test accuracy: 0.983\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0010 | test accuracy: 0.983\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0052 | test accuracy: 0.983\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0018 | test accuracy: 0.983\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0023 | test accuracy: 0.983\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0004 | test accuracy: 0.983\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0839 | test accuracy: 0.983\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0042 | test accuracy: 0.983\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0359 | test accuracy: 0.983\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4625 | test accuracy: 0.983\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0244 | test accuracy: 0.983\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0021 | test accuracy: 0.983\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0050 | test accuracy: 0.983\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1001 | test accuracy: 0.983\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0033 | test accuracy: 0.983\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0008 | test accuracy: 0.983\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0114 | test accuracy: 0.983\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0066 | test accuracy: 0.983\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0012 | test accuracy: 0.983\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1122 | test accuracy: 0.983\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0027 | test accuracy: 0.983\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0017 | test accuracy: 0.983\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0008 | test accuracy: 0.983\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0176 | test accuracy: 0.983\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.9999 | test accuracy: 0.983\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0027 | test accuracy: 0.983\n",
            "total time:  65.64497791600024\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1925513744354248.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.3415253162384033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5741151009287153 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1970539093017578.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.3490438461303711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.469669040611812 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19751930236816406.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.34502506256103516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4172542827469962 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1989879608154297.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.33767199516296387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.38722922163350243 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20601296424865723.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.36295533180236816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36925304063728875 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20806503295898438.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.3493509292602539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3588777107851846 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19867253303527832.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.34056639671325684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3483732385294778 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032938003540039.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3576931953430176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3429603785276413 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20571231842041016.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34520888328552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3383637628384999 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035231590270996.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.343477725982666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33428892195224763 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20880651473999023.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3513355255126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3316452567066465 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20285725593566895.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3439908027648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3290633993489402 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19811558723449707.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3492450714111328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3266136667558125 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2151956558227539.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35854339599609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32577212921210696 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20929408073425293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3532724380493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32466851047107154 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045881748199463.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3456847667694092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3231318554707936 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2138659954071045.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36214733123779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3222909165280206 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20852279663085938.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34947848320007324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3210805382047381 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19797778129577637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34186649322509766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3210612573793956 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21163153648376465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3576798439025879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31975965414728436 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19987893104553223.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34409117698669434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3191821204764502 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20543551445007324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3565223217010498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3186697500092643 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21930980682373047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36717844009399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31820138309683116 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.202880859375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3435933589935303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3179392180272511 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1992042064666748.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3506133556365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3179779359272548 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.214371919631958.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36281323432922363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.317101759995733 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2009572982788086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3449246883392334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3170361408165523 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20851993560791016.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3585360050201416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31669398503644125 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20648717880249023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34816837310791016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31639041304588317 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19833779335021973.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34209561347961426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31630469177450454 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20275497436523438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35390400886535645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31595691059316905 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20423102378845215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3443911075592041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3159389913082123 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1961827278137207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33561229705810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31559674995286124 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20563769340515137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3603684902191162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3159396456820624 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20335030555725098.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.342700719833374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31542276867798397 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19895172119140625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3393523693084717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3155303265367235 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19779467582702637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35270214080810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3152794978448323 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20048880577087402.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.340045690536499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31529427000454496 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.199843168258667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483424186706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31497329005173275 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1963191032409668.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35030579566955566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3150652604443686 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20210909843444824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3441121578216553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31501508142266954 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2073371410369873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514583110809326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31482132460389817 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20165467262268066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3494901657104492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3147792616060802 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1981513500213623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3382298946380615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3146808624267578 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19239282608032227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3371589183807373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3146766211305346 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20650267601013184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465437889099121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3146387823990413 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.200974702835083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420546054840088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31454608653272903 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20729327201843262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3499016761779785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3144158823149545 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20859456062316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34883546829223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31437865665980746 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2003035545349121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34400153160095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3143551456076758 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20293736457824707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34671950340270996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3143385120800563 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090914249420166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527524471282959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31433678184236796 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19734907150268555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33919787406921387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.314294416989599 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2076728343963623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3485286235809326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31411785781383517 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.215256929397583.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35868287086486816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31409462434904917 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19353222846984863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.340470552444458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140254054750715 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20046639442443848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33667778968811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31406887131077904 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20620465278625488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34462833404541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3140276708773204 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20811796188354492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35265398025512695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31403948792389463 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20084404945373535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3431582450866699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3140284529754094 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21212124824523926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35562801361083984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140339366027287 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19898152351379395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34180736541748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3140020442860467 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20163226127624512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34520697593688965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31393324307032994 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2019822597503662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33875370025634766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31387188690049306 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1909809112548828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33957552909851074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31388050700937 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19540858268737793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33698105812072754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138955759150641 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19997143745422363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34108662605285645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3138666617018836 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068178653717041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3510406017303467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.313824297274862 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20342063903808594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3439617156982422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31384425248418535 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20797085762023926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3476591110229492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138330404247556 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19770121574401855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3490164279937744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138088511569159 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20281314849853516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35360121726989746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31373404349599565 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201751708984375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3390657901763916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137574391705649 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20322036743164062.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3479008674621582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31378215891974315 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20052194595336914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3539857864379883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137310028076172 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.18988823890686035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3349597454071045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136985042265483 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20069003105163574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3377695083618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137412467173168 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19885706901550293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3475658893585205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136832450117384 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20072484016418457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3388814926147461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136822359902518 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19780278205871582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34050583839416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31368790864944457 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19563722610473633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34143519401550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136529232774462 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19673943519592285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3356921672821045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31363234860556466 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2070937156677246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34945154190063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31366398802825385 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20325207710266113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3504948616027832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.313669410773686 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19914531707763672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3508169651031494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136074560029166 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014145851135254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3425116539001465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31358782436166494 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19836950302124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35112428665161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136557464088712 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20174598693847656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3434925079345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135700366326741 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20525765419006348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35344958305358887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135852711541312 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20003533363342285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459901809692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31360101529530116 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20347380638122559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553144931793213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31357349710805077 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20329785346984863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34557509422302246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135658089603697 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21406888961791992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3563723564147949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.313592056291444 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20545053482055664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34729838371276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31358294614723753 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19939279556274414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3431539535522461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31353900645460403 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2063295841217041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483562469482422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135615327528545 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20200729370117188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3501715660095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31355668689523425 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20893645286560059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349456787109375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31353575204099926 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21400690078735352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35371851921081543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31354064004761834 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20569133758544922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3474454879760742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31353806342397417 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20973539352416992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34909701347351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31352202211107527 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20747089385986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35341930389404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31355754818235126 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20793771743774414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35233092308044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135153510740825 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20080852508544922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.346297025680542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134994915553502 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20951151847839355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3532721996307373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31351318614823476 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20563983917236328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3529853820800781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31348909224782673 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2115318775177002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3534965515136719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134875906365258 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21063590049743652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3630986213684082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31346567571163175 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2142798900604248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559255599975586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31348217214856827 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20167207717895508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350299596786499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134864215339933 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20492863655090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34621310234069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348686260836467 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20154261589050293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3389892578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31346194318362647 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19760441780090332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34947896003723145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134968953473227 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20881366729736328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35407137870788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347777119704656 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060258388519287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3491189479827881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134698097194944 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20697331428527832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575479984283447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134614190884999 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090463638305664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3543527126312256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134616902896336 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20259404182434082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3439195156097412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134503287928445 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20325994491577148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36266517639160156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347566843032837 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027266025543213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35224127769470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134357222488948 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.207566499710083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34925174713134766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31342905078615463 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21346330642700195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.359935998916626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344111348901477 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2010488510131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34697961807250977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134369415896279 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20508050918579102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34868884086608887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344338059425353 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21144366264343262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36026859283447266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134494147130421 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21388912200927734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3590419292449951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343798339366913 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20293116569519043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34476304054260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134519636631012 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21645474433898926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3601717948913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134333712714059 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXgV5d3/8fecJQnZgJCtspta9kVUxEZQgUiAp0JbhSAEtPpTFIpLkSXgA0pFEFxxLeVRpNjiEikWMVQoIBpZ1IIgCAFBwhISICvZz/z+CBwSSCBATk7CfF7XxZUzc2bu+Z60nk/u+57FME3TRERELMvm7QJERMS7FAQiIhanIBARsTgFgYiIxSkIREQsTkEgImJxDm8XIFeuNm3asHbtWiIjI89579133+WDDz6guLiY4uJirr/+ep588kkOHz7MH//4RwByc3PJzc117//b3/6WgQMH0qdPH/7whz8wceLECm3ec889/Pzzz6xevbrKmtavX8+f//xnAE6cOEFpaSmhoaEAjB49msGDB1frs6WlpXHffffxr3/967zbTZgwgdjYWHr37l2tdi+kqKiI1157jaSkJE6f+R0bG8uYMWPw8fGpkWOI9Ri6jkA8paogWLduHc8++yyLFy8mJCSEoqIinnjiCRo2bMjTTz/t3i4xMZFly5bxzjvvuNelpqYyZMgQAgICSEpKwmYr69RmZGQwZMgQgPMGQXnz5s3jyJEjPPPMM5f5SWvPo48+Sn5+PnPmzCE4OJjMzEwmTpxIYGAgzz//vLfLk3pKQ0NS63bt2kXLli0JCQkBwMfHh2eeeYYJEyZUa38/Pz9atGjB5s2b3etWrFjBTTfddNm19e7dm1dffZV+/fpx6NAh9u7dy7Bhw+jfvz8xMTHuHkBqairt27cHygJr3LhxJCQk0K9fPwYMGMDu3bsBiI+P55///CdQFoxLly5l8ODB3Hzzze6Ac7lczJgxg+joaIYNG8Zf/vIX4uPjz6lt9+7drF27ltmzZxMcHAxAo0aNmDlzJnfeeec5x6vs+G+99Rb9+vVj9uzZzJgxw73d8ePH6dq1Kzk5OaSkpDBixAj69evHb37zG77//nsA8vLyGDNmDP3796dPnz5MnTqV4uLiy/6di/cpCKTW/frXv2b9+vVMnDiRtWvXkpubS2BgIIGBgdVuIzY2tsKwzPLly4mNja2R+tLS0khKSuKqq67iueee47bbbmPFihXMnDmTKVOmVPrlt27dOu6++26SkpK48cYbWbhwYaVtp6SksHTpUl5//XVeeOEFSktLWbt2LevWrWPlypW88cYbfPzxx5Xuu3HjRrp27UqjRo0qrG/SpEm1Q9A0TZKSkujfvz//+c9/3Ov/85//0KNHDwICAhgzZgyDBg0iKSmJ6dOn8/DDD1NSUsLSpUsJDg5mxYoVJCUlYbfbSUlJqdZxpW5TEEita9++PX//+99xuVxMmjSJHj16MGbMGA4dOlTtNm6//XZWr15NcXExBw8epKCggNatW9dIfbfeeqv79euvv859990HwHXXXUdhYSHp6enn7BMVFUXHjh2Bss93+PDhStseNGgQAB06dKCwsJBjx46xefNmbr31VgICAmjUqBEDBw6sdN+srCyaNGlyOR/N/dk6d+6MaZrs3LkTgH//+9/079+fvXv3cuzYMXcP47rrriMkJITvvvvO/XP9+vW4XC6eeuop2rVrd1n1SN2gyWLxik6dOjFnzhxM02T79u28/PLLPPbYYyxZsqRa+zds2JCOHTuyfv16UlJS6N+/f43V1rBhQ/frL774gjfeeIMTJ05gGAamaeJyuc7ZJygoyP3abrdTWlpaadunt7Pb7UDZsFB2djYRERHubcq/Lq9x48akpaVd/Acqp3xv4vbbb2fVqlW0aNGCb7/9lrlz57Jr1y4KCgoq/D5zc3PJzMykf//+ZGVl8fLLL7N3717uuOMOJk+erEnqK4B6BFLrNm/e7P5CMwyDjh07Mn78eHbt2nVR7QwcOJCkpCQ+++wzBgwYUON1FhcX8+ijj/LQQw+RlJTEsmXLMAyjxo8TGBjIyZMn3cuV9TgAunfvzpYtW84Jg+zsbF5++WVM08Rms1UIqqysrCqP269fP1avXs369eu54YYbCAwMJDw8nICAAD777DP3v/Xr1xMTEwNAXFwcH3zwAZ9++inbt29n6dKll/PRpY5QEEit++STT5g2bRq5ubkAlJSUsHz5cm644YaLaqdPnz5s3LgRu91O8+bNa7zO/Px8Tp486R7yWbhwIU6ns8KXdk3o1KkTa9asoaCggOzsbFasWFHpdlFRUQwYMIDHH3+cjIwMADIzM3n88cfdPZawsDD3cM93333Hvn37qjzutddey7Fjx0hMTHT3AJo2bUpkZCSfffYZUDaJ/Pjjj3Py5Elee+01PvzwQ6Cs19KsWTOPBKPUPg0NiUfFx8e7h0EA/vznPzNlyhRefPFFfv/73wNlQXDjjTfy7LPPXlTb/v7+dOnShU6dOtVozacFBwdz//33M3jwYJo0acJDDz1E3759GT16NG+99VaNHScmJoY1a9YQGxtLy5Yt6d+/P8nJyZVuO2PGDN544w2GDx+OYRg4nU7uuOMO9zzGvffey+OPP866devo3r070dHRVR7XMAz69u3LBx984D711DAMXnjhBaZPn85LL72EzWbj3nvvxd/fn0GDBjF58mTmz5+PYRh06dLFPech9ZuuIxCpA0zTdP91vXjxYr766itee+01L1clVqGhIREv27FjB3369CErK4uSkhJWrlxJ165dvV2WWIiGhkS8rF27dgwePJjf/e532O12unbtyogRI7xdlliIhoZERCxOQ0MiIhZXr4aGCgoK2LZtG2FhYRXORBERkaqVlpaSnp5Ox44d8fPzO+f9ehUE27ZtY/jw4d4uQ0SkXlq8eDHXX3/9OevrVRCEhYUBZR+msnvci4jIuY4cOcLw4cPd36Fnq1dBcHo4KDIykmbNmnm5GhGR+qWqIXVNFouIWJyCQETE4hQEIiIWpyAQEbE4BYGIiMUpCERELM4yQZCeU0j0rNXsSc/1dikiYlGzZs0iPj6e2NhYbrnlFuLj4xk7duwF93vssccoKCjwWF316jqCy3E0p4CDmfnsTsslKizQ2+WIiAVNmjQJgMTERHbv3s3EiROrtd+LL77oybKsEwS+jrILKYpKz33wuIiIt0yaNAmn00lmZibPPvssf/rTnzh58iQFBQU8+eSTdO7cmd69e/PJJ58wY8YMwsPD2b59O4cOHWLu3Ll06NDhsmuwUBCUjYIVFpd6uRIRqQs++iaV9zcfqNE2h1zfnN9fd/F3PWjYsCEzZszgp59+4q677qJv374kJyczf/585s2bV2HboqIiFixYwN///neWLl2qILgY7iAoUY9AROqWzp07AxAaGsrrr7/OggULKCoqwt/f/5xtT980LjIykq1bt9bI8S0UBGVDQwoCEQH4/XXNLumvd09wOp0ALFy4kIiICObMmcP333/Pc889d8625e8XVFPPFbPMWUO+zrKPWqQgEJE66sSJE7Ro0QKAzz//nOLi4lo5rmWCwMd+emhIcwQiUjcNGjSIt99+mz/84Q907tyZ9PR0PvroI48ft149szg1NZU+ffqwatWqS7oN9TVTPuX+nlczMbatB6oTEambLvTdaZkeAZTNExQWa2hIRKQ8iwWBjaJSDQ2JiJRnuSBQj0BEpCJLBYGPw6bTR0VEzmKpIPB12HXWkIjIWawVBE6briMQETmLR68snjlzJlu2bMEwDBISEtyXUQP07t2byMhI91Vyc+fOZd++fTzyyCNcc801APzqV7/iySefrLF6fDU0JCJyDo8FwcaNG9m/fz9Llixhz549JCQksGTJkgrbzJ8/n4CAAPfyvn376N69O6+88opHavJx2CjQZLGISAUeGxpKTk6mb9++AERFRZGVlUVurncfCqM5AhGRc3ksCDIyMmjcuLF7OSQkhPT09ArbTJs2jWHDhjF37lz3zZNSUlIYPXo0w4YN48svv6zRmnwdmiMQETlbrd199Ow7WYwbN46ePXvSsGFDxowZQ1JSEtdeey1jx46lf//+HDhwgJEjR7Jy5Up8fHxqpAbNEYiInMtjPYLw8HAyMjLcy0ePHiUsLMy9PHjwYJo0aYLD4aBXr17s2rWLiIgIBgwYgGEYtGjRgtDQUNLS0mqsJh9dUCYicg6PBUF0dDRJSUkAbN++nfDwcAIDy54VnJOTw3333UdRUREAmzZt4pprrmHZsmUsWLAAgPT0dI4dO0ZERESN1eTrsOtRlSIiZ/HY0FC3bt3o0KEDcXFxGIbBtGnTSExMJCgoiJiYGHr16sXQoUPx9fWlffv2xMbGkpeXx/jx41m1ahXFxcVMnz69xoaF4PQtJjRZLCJSnkfnCMaPH19huW3bM7d/HjVqFKNGjarwfmBgIG+++abH6vF1ao5ARORslrqy2Mdup8RlUuqqN49gEBHxOEsFgR5XKSJyLmsFgUOPqxQROZvFgqDsvkaaJxAROcNSQeBzukegawlERNwsFQSnh4b0uEoRkTMsGQS6A6mIyBnWCgKn5ghERM5mqSDwseusIRGRs1kqCHQdgYjIuawVBO7rCBQEIiKnWSwINEcgInI2iwXB6esINEcgInKaJYNAzyQQETnDYkFwamhI1xGIiLhZKwicmiwWETmbpYJA1xGIiJzLUkFgsxk47YauIxARKcdSQQBl8wQaGhIROcOCQWDT0JCISDmWCwIfh01nDYmIlGO5IPB12HQdgYhIORYMArt6BCIi5VgvCJyaIxARKc9yQeBj19CQiEh5lgsCX6cmi0VEyrNeEOg6AhGRCiwYBJojEBEpz3JB4OOw6RYTIiLlODzZ+MyZM9myZQuGYZCQkEDnzp3d7/Xu3ZvIyEjs9rJbQ8+dO5eIiIjz7lMTynoECgIRkdM8FgQbN25k//79LFmyhD179pCQkMCSJUsqbDN//nwCAgIuap/LpTkCEZGKPDY0lJycTN++fQGIiooiKyuL3NzcGt/nYvk6bHpUpYhIOR4LgoyMDBo3buxeDgkJIT09vcI206ZNY9iwYcydOxfTNKu1z+Xy0S0mREQq8OgcQXmmaVZYHjduHD179qRhw4aMGTOGpKSkC+5TE3wddopLTUpdJnabUePti4jUNx4LgvDwcDIyMtzLR48eJSwszL08ePBg9+tevXqxa9euC+5TE04/rrKoxEUDH3uNti0iUh95bGgoOjra/Vf+9u3bCQ8PJzAwEICcnBzuu+8+ioqKANi0aRPXXHPNefepKb4OPa5SRKQ8j/UIunXrRocOHYiLi8MwDKZNm0ZiYiJBQUHExMTQq1cvhg4diq+vL+3btyc2NhbDMM7Zp6b5OM70CERExMNzBOPHj6+w3LZtW/frUaNGMWrUqAvuU9N8HWXDQTqFVESkjOWuLNbQkIhIRZYNggLdgVREBLBgELjnCHQtgYgIYMEgcM8RqEcgIgJYMQicmiMQESnPekHgnixWj0BEBCwcBLqOQESkjAWDQNcRiIiUZ8Eg0ByBiEh5FgwCnTUkIlKe5YJA1xGIiFRk2SBQj0BEpIzlgsBuM3DaDc0RiIicYrkggLJ5Ap0+KiJSxpJB4OOw6fRREZFTLBkEvg6bhoZERE6xcBCoRyAiApYNArvOGhIROcWSQdDAx05eUYm3yxARqRMsGQTBDZzkFCgIRESgGkGQm5vLTz/9BMDGjRt55513OH78uMcL86QgPwfZBcXeLkNEpE64YBA8+uijHD16lN27dzN79mxCQkKYPHlybdTmMcF+6hGIiJx2wSAoKirixhtvZMWKFdxzzz3ccccdFBYW1kZtHhPs5yA7Xz0CERGoZhAsW7aM5cuXc9ttt5GamkpOTk5t1OYxQX4OCktcurpYRIRqBMG0adPYunUr06dPJzAwkLVr1/Loo4/WRm0eE9zACUCO5glERHBcaIPmzZtz9913c/XVV7Nx40aKi4vp0KFDbdTmMUF+ZR87u6CEJoG+Xq5GRMS7qjVZnJ6efsVNFoN6BCIiYNHJ4qBTQZCdrzOHREQsOVkc3KBsaEg9AhGRi5gsfuqppy56snjmzJkMHTqUuLg4tm7dWuk2zz//PPHx8QBs2LCBHj16EB8fT3x8PDNmzLiIj1J97h6BgkBE5MKTxe3atSMmJoYdO3awa9cuOnbsSLdu3S7Y8MaNG9m/fz9Llixhz549JCQksGTJkgrbpKSksGnTJpxOp3td9+7deeWVVy7ho1RfsN/pHoGGhkRELtgjmDlzJu+88w6maVJQUMDrr7/Oiy++eMGGk5OT6du3LwBRUVFkZWWRm5tbYZtZs2bx2GOPXWLply7Ax4FhoIvKRESoRo9g+/btLF682L38wAMPMGLEiAs2nJGRUeE005CQENLT0wkMDAQgMTGR7t2707Rp0wr7paSkMHr0aLKyshg7dizR0dHV/jDVZbMZBPo6yFaPQETkwkFQUlJCQUEBfn5+AJw8eZLS0ot/updpmu7XmZmZJCYm8vbbb5OWluZe36pVK8aOHUv//v05cOAAI0eOZOXKlfj4+Fz08S5E9xsSESlzwSAYNWoUd9xxB61atcLlcvHzzz8zYcKECzYcHh5ORkaGe/no0aOEhYUB8PXXX3P8+HGGDx9OUVERP//8MzNnziQhIYEBAwYA0KJFC0JDQ0lLS6N58+aX+vmqpDuQioiUuWAQDBgwgFtvvZV9+/ZhGAatWrWqMLlblejoaObNm0dcXBzbt28nPDzcPSwUGxtLbGwsAKmpqUyePJmEhASWLVtGeno69913H+np6Rw7doyIiIjL/IiVK3smgYJAROSCQQDg7+9P+/bt3csjR47k3XffPe8+3bp1o0OHDsTFxWEYBtOmTSMxMZGgoCBiYmIq3ad3796MHz+eVatWUVxczPTp0z0yLARlZw4dyizwSNsiIvVJtYLgbOXH+89n/PjxFZbbtm17zjbNmjVj0aJFAAQGBvLmm29eSkkXLdjPyY+F9fvCOBGRmnBJj6o0DKOm66h1QX4O3WJCRITz9Ahmz55d6Re+aZocOHDAo0XVhiA/J7mFJZimeUUEm4jIpaoyCH71q19VudP53qsvghs4KHWZnCwqJcD3kkbIRESuCFV+A/72t7+tzTpqXfn7DSkIRMTKLmmO4Epw5pkEmicQEWuzbBC4n1Km+w2JiMVVGQQbNmyosFxUVOR+/cEHH3iuolpy5rnF6hGIiLVVGQSvvfZaheX777/f/fqTTz7xXEW15Mxzi9UjEBFrqzIIzr5orPxydS8oq8uC3ZPF6hGIiLVVGQRnn1tffvlKOO9ecwQiImWqPG/S5XJRUFDg/uv/9LLL5cLlctVagZ7i57TjY7dpjkBELK/KIDh06BADBw6sMAx0+hbRV0KPAMouKtMdSEXE6qoMgtWrV9dmHV4R5OfUHIGIWF6VcwTFxcW89NJLFBef+Yt59+7dHn+wfG0K9lOPQESkyiCYPXs2ubm5FYaGWrZsSW5uLq+++mqtFOdpQX5OTRaLiOVVGQTfffcdU6dOrfBgGB8fHyZNmsSXX35ZK8V5WtkcgYaGRMTaqgwCu91e+Q42W4XhovosyNepC8pExPKqDILGjRuzefPmc9avWbOG0NBQjxZVW4L81CMQEanyrKGEhAT++Mc/EhUVRbt27SgtLWXLli0cPnyYBQsW1GaNHhPcwMnJolJKSl047Ja9/56IWFyVQdCyZUuWLl3Kl19+yd69ezEMgxEjRhAdHX3lXEfgvt9QCSEBPhfYWkTkynTeJ7LYbDZ69uxJz549a6ueWhUa5AvA0ZwCBYGIWJalx0Mig/0AOJJV4OVKRES8x9JBEHEqCNKyFQQiYl0KAuBIVqGXKxER8R5LB4GPw0aTAB/SctQjEBHrsnQQQFmvIE1zBCJiYZYPgsiGfhzRHIGIWJjlgyAi2E+TxSJiaZYPgshgPzJyiygqqf9PXRMRuRQeDYKZM2cydOhQ4uLi2Lp1a6XbPP/888THx1/UPjUpsuGZi8pERKzIY0GwceNG9u/fz5IlS3jmmWd45plnztkmJSWFTZs2XdQ+NS1c1xKIiMV5LAiSk5Pp27cvAFFRUWRlZZGbm1thm1mzZvHYY49d1D41LVLXEoiIxXksCDIyMmjcuLF7OSQkhPT0dPdyYmIi3bt3p2nTptXexxPcQaAegYhYVK1NFpd/5GVmZiaJiYnce++91d7HUxr5O/Fx2DQ0JCKWdd67j16O8PBwMjIy3MtHjx4lLCwMgK+//prjx48zfPhwioqK+Pnnn5k5c+Z59/EUwzCIDPbTjedExLI81iOIjo4mKSkJgO3btxMeHk5gYCAAsbGxfPrpp7z//vu8+uqrdOjQgYSEhPPu40mRwbqoTESsy2M9gm7dutGhQwfi4uIwDINp06aRmJhIUFAQMTEx1d6nNkQ09GNramatHEtEpK7xWBAAjB8/vsJy27Ztz9mmWbNmLFq0qMp9akNksC8rswowTfOKefqaiEh1Wf7KYii7zURhiYus/GJvlyIiUusUBJTdeA50CqmIWJOCAD2yUkSsTUGAHlkpItamIADCg32xGXDwRL63SxERqXUKAsDXYadlkwB+TMvxdikiIrVOQXBKm4ggfjyiIBAR61EQnNImMoj9x09ysqjE26WIiNQqBcEp7X4RhGnC7jTP3vZaRKSuURCc0iYyGEDDQyJiOQqCU1qE+OPntLFTQSAiFqMgOMVuM/hVRBA/pmV7uxQRkVqlIChHZw6JiBUpCMppExlERm4RGbl6frGIWIeCoJy2mjAWEQtSEJTTJjIIQBPGImIpCoJywoJ8aRLgw49HNGEsItahIDhLm0hNGIuItSgIztKleSO2H8omu0BPKxMRa1AQnKVP23BKXCZrf0z3dikiIrVCQXCWa1s0JiTAh1U70rxdiohIrVAQnMVuM7itTTj/+TGdklKXt8sREfE4BUEl+rYLJyu/mM37T3i7FBERj1MQVKLnr8Lwsdv4/AcND4nIlU9BUIlAXwc9oprw+Y40TNP0djkiIh6lIKhCTLtw9h07yZ70PG+XIiLiUQqCKvRuFwGgs4dE5IqnIKhC00YNaP+LYD5XEIjIFU5BcB5924Xzzf4THM8r8nYpIiIe49EgmDlzJkOHDiUuLo6tW7dWeO/9999nyJAhxMXFMX36dEzTZMOGDfTo0YP4+Hji4+OZMWOGJ8u7oL7tI3CZ8J+dR71ah4iIJzk81fDGjRvZv38/S5YsYc+ePSQkJLBkyRIA8vPzWb58OYsXL8bpdDJy5Ei+++47ALp3784rr7ziqbIuSserGhIe5MuqnWn8/rpm3i5HRMQjPNYjSE5Opm/fvgBERUWRlZVFbm4uAA0aNGDhwoU4nU7y8/PJzc0lLCzMU6VcMpvNoE+7CNb+mE5hSam3yxER8QiPBUFGRgaNGzd2L4eEhJCeXvFGbn/5y1+IiYkhNjaW5s2bA5CSksLo0aMZNmwYX375pafKq7a+7cLJKyplw97j3i5FRMQjam2yuLILsx544AE+//xzvvjiC7755htatWrF2LFjeeONN5g9ezZTpkyhqMi7E7XRvwzFz2ljxbbDXq1DRMRTPBYE4eHhZGRkuJePHj3qHv7JzMxk06ZNAPj5+dGrVy++/fZbIiIiGDBgAIZh0KJFC0JDQ0lL8+7pm35OO7+9tikfbE5lT3quV2sREfEEjwVBdHQ0SUlJAGzfvp3w8HACAwMBKCkpYdKkSeTllV21+/3339O6dWuWLVvGggULAEhPT+fYsWNERER4qsRq+9PtbWjgtPP0Jz/olhMicsXx2FlD3bp1o0OHDsTFxWEYBtOmTSMxMZGgoCBiYmIYM2YMI0eOxOFw0KZNG/r06UNeXh7jx49n1apVFBcXM336dHx8fDxVYrWFBvrySN9r+PPyHazeeZQ+7bwfTiIiNcUw69GfuKmpqfTp04dVq1bRrFntns5ZVOKi/8vrKHGZJD3aCz+nvVaPLyJyqS703akri6vJx2Hj6UEd2X/sJHOSfvR2OSIiNUZBcBGifxnKyJtasmD9T3y1J+PCO4iI1AMKgos0qX9bWocG8MQHW8kpKPZ2OSIil01BcJH8fRw8P6QLh7PymZz4vc4iEpF6T0FwCbq1aMz4fm3419bDvPPVPm+XIyJyWRQEl2h0ryj6tovgmeU72LxPt58QkfpLQXCJbDaD54d0oWnjBtz/7maS9xzzdkkiIpdEQXAZGjZw8u4fuhMa6Ev8gg28t+Fnb5ckInLRFASXqWWTABIf/jU3XxNKwsffM+2f2ygpdXm7LBGRalMQ1IBgPycLRt3A/+vZmoXJ+xn19kYyT+rxliJSPygIaojdZjBlYHvm3NmZTT+d4I5Xv2T7oSxvlyUickEKghp21/XN+fsDPSgqcfG7179i0df7ydaFZyJShykIPOC6lo3517ibua5lY55cuo1rn/43v3v9S95cu4eDmfneLk9EpAKP3Yba6kIDfVl0341s2necL1MyWLsrnVkrdjJrxU6if9mEMbf+kpuimmAYhrdLFRGLUxB4kN1m0OPqJvS4ugl/ur0N+4/lsey/h1j09X7u/usGujZvxIBOkdz8yzDaRgZhsykURKT2KQhqUcsmAfyxzzX8v15X88E3qbz71T5mfroT2EnzkAbcdV1zYjtG0sBpx24z+EVDP/UYRMTjFARe4Oe0E9+jJfE9WnIkq4Avdqez9L8HeeHfu3jh37vc210dFsDwG1syqOtVhAb6erFiEbmSKQi8LLKhH3dd35y7rm/OgeMn2bz/OKUuyC0o5p9bDjHjXz8w418/EBbkS5uIINpEBtEmIoio8EBahwbQ2N+pXoOIXBYFQR3SPMSf5iH+7uV7oluz/VAWX6Uc48e0HHal5bB4w34Kis9cuRzo6yA00IfGAT7YDYNS0yQs0Jee14TS4+omBDdw4rTbCPJz4LTrJDEROZeCoI7rcFVDOlzV0L1c6jI5cPwkezNy2ZueR+qJfI7lFXEirwgTE5thsONINit/SKvQjmFAkwBfIoJ9iQj2IzzIF38fBz4OG8ENHPyioR9hgX7YbaipClAAAAz3SURBVAY2Axr5+xDZ0I8GTjtHcwrIyC0iMtiPiGBf9UBErjAKgnrGbjNoFRpAq9AAeretfBvTNNl37CTf7j9BQUkpxSUuTpws5mhOAWnZhaRlF/D9wSwKikopLHFRdBH3RgryddA4wIeC4lJKXSYNGzhpHOCDj92GiYnTbiM8yI+wIF+cdgOXaWKa4DLLwijQ10GwnwNfhx2H3cBuM3Dabad+GthtNpy2svUOuw2HzcBhN/Cx2/Bx2HCe+unjKHvPbjNw2GzYDBRQIpdIQXAFMgyD1qEBtA4NqNb2J4tKOJJVQHpOIaWnvrhPnCziSFYBeYWlRDb0pUmAL4ez8tl9NJecghL8nDZshkFmfjEn8oooKTXBgOz8YlKO5pKeU4jLNDEMw/0lbZomxaWee6Kb3R0MFX+eDovT6wwDbIaBzTjzuvxPd5wYhvv16Ywxyh3LZpxp324zsJ9atp3a2DBO/ePMzgZnAss4vc2pdYZ7G6Pc+nLLpzYo/57t1H5G+WOevX912i73eatqo7L2z15XXvlgNiqsP2u7qvYxKt/m7Deratug8raqu8/ZB62yzqraOmf/quuprN3K2r75l6E08vepfOfLoCAQ/H0cXB0WyNVhgR4/VkFxKdkFxRSVuCgpNSlxmZS4zrwudbkoLjUpdZkUl7pO/Sx7XVzqouhUD6aopOy9sn3KfrpcZ9qofH1ZO6YJJiYuF7hME5dZ1osyKVsGTm3DqdcVw6ush1PWZlGJi1KzrO3T/063f3YbJrhXmOXWVdjerLj92W2Zpxo5s1yxfszztO1+v+L+Z9qVum7sbb9kfL82Nd6ugkBqlZ/Tjp/T7u0ypAqmWXlInA4Pyi2feV0uiDizzZmF8i/PDdVKNqsQvmdnlFlVe1W2ddb+5d6tzvEra+NCtZx7zMrbPl+d535yaB3qmT/WFAQi4mYY5Yd4NOdiFTqfUETE4hQEIiIWpyAQEbE4BYGIiMUpCERELE5BICJicfXq9NHS0lIAjhw54uVKRETqj9Pfmae/Q89Wr4IgPT0dgOHDh3u5EhGR+ic9PZ2WLVues94wz76Erg4rKChg27ZthIWFYbfr6lQRkeooLS0lPT2djh074ufnd8779SoIRESk5mmyWETE4urVHMHlmDlzJlu2bMEwDBISEujcubO3S7qg5557jm+++YaSkhIefPBBOnXqxIQJEygtLSUsLIw5c+bg41Pzt6StSQUFBfzP//wPDz/8MDfddFO9qn/ZsmX89a9/xeFwMG7cONq0aVNv6s/Ly2PixIlkZWVRXFzMmDFjCAsLY/r06QC0adOGp556yrtFVmLXrl08/PDD3HPPPYwYMYLDhw9X+jtftmwZCxcuxGazMWTIEO666y5vlw5UXv/kyZMpKSnB4XAwZ84cwsLC6l79pgVs2LDBfOCBB0zTNM2UlBRzyJAhXq7owpKTk83777/fNE3TPH78uHnLLbeYkyZNMj/99FPTNE3z+eefNxcvXuzNEqvlhRdeMH/3u9+ZH330Ub2q//jx4+btt99u5uTkmGlpaebUqVPrVf2LFi0y586da5qmaR45csTs16+fOWLECHPLli2maZrm448/bq5Zs8abJZ4jLy/PHDFihDl16lRz0aJFpmmalf7O8/LyzNtvv93Mzs428/PzzYEDB5onTpzwZummaVZe/4QJE8zly5ebpmmaf/vb38zZs2fXyfotMTSUnJxM3759AYiKiiIrK4vc3FwvV3V+N9xwAy+//DIAwcHB5Ofns2HDBvr06QPAbbfdRnJysjdLvKA9e/aQkpLCrbfeClCv6k9OTuamm24iMDCQ8PBwZsyYUa/qb9y4MZmZmQBkZ2fTqFEjDh486O4J18X6fXx8mD9/PuHh4e51lf3Ot2zZQqdOnQgKCsLPz49u3brx7bffeqtst8rqnzZtGv369QPO/G9SF+u3RBBkZGTQuHFj93JISIj7VNS6ym634+9f9iD7Dz/8kF69epGfn+8eimjSpEmd/wyzZ89m0qRJ7uX6VH9qaioFBQWMHj2au+++m+Tk5HpV/8CBAzl06BAxMTGMGDGCCRMmEBwc7H6/LtbvcDjOOaOlst95RkYGISEh7m3qyn/PldXv7++P3W6ntLSU9957j9/85jd1sn7LzBGUZ9ajE6U+//xzPvzwQ/7v//6P22+/3b2+rn+GpUuX0rVrV5o3b17p+3W9foDMzExeffVVDh06xMiRIys+UKSO1//Pf/6Tq666igULFrBz507GjBlDUFCQ+/26Xn9lqqq5rn+W0tJSJkyYQI8ePbjpppv45JNPKrxfF+q3RBCEh4eTkZHhXj569ChhYWFerKh6vvjiC958803++te/EhQUhL+/PwUFBfj5+ZGWllahC1rXrFmzhgMHDrBmzRqOHDmCj49Pvaq/SZMmXHvttTgcDlq0aEFAQAB2u73e1P/tt99y8803A9C2bVsKCwspKSlxv1/X6z+tsv/PVPbfc9euXb1Y5flNnjyZli1bMnbsWKDy7yNv12+JoaHo6GiSkpIA2L59O+Hh4QQGev75vJcjJyeH5557jrfeeotGjRoB8Otf/9r9OVauXEnPnj29WeJ5vfTSS3z00Ue8//773HXXXTz88MP1qv6bb76Zr7/+GpfLxYkTJzh58mS9qr9ly5Zs2bIFgIMHDxIQEEBUVBSbN28G6n79p1X2O+/SpQvff/892dnZ5OXl8e2333L99dd7udLKLVu2DKfTybhx49zr6mL9lrmgbO7cuWzevBnDMJg2bRpt27b1dknntWTJEubNm0fr1q3d62bNmsXUqVMpLCzkqquu4tlnn8XpdHqxyuqZN28eTZs25eabb2bixIn1pv5//OMffPjhhwA89NBDdOrUqd7Un5eXR0JCAseOHaOkpIRHHnmEsLAw/vd//xeXy0WXLl2YPHmyt8usYNu2bcyePZuDBw/icDiIiIhg7ty5TJo06Zzf+WeffcaCBQswDIMRI0Zwxx13eLv8Sus/duwYvr6+7j88o6KimD59ep2r3zJBICIilbPE0JCIiFRNQSAiYnEKAhERi1MQiIhYnIJARMTiFARyRUhNTeXaa68lPj6+wr/T99u5HPPmzeNvf/vbebdp06YNq1evdi9v2LCBefPmXfIxN2zYUOHccxFPssSVxWINrVu3ZtGiRV45dqtWrXj11Ve55ZZb9PQ8qXcUBHLFmzRpEv7+/uzdu5cTJ07w7LPP0r59exYuXMinn34KQJ8+fXjggQc4ePAgkyZNorS0lKuuuorZs2cDZfeZf/DBB9m3bx9TpkyhV69eFY4RHh5Op06d+Pjjj7nzzjsrvHfjjTeyYcMGAMaNG8fw4cPZuHEjJ06cYP/+/aSmpvLII4/w0UcfcfDgQebPnw9AVlYWY8aM4eDBg8TExDBmzBhSUlJ4+umnMQyDgIAAZs2aRXZ2Nk888QT+/v6MGDGC2267zdO/UrnCaGhILKGkpIR33nmHRx55hNdee40DBw7w8ccfs3jxYhYvXsyKFSv4+eefefHFF7nnnnt47733CA8PZ9u2bUDZDejeeustpk6dyj/+8Y9Kj/Hggw+ycOFCCgoKqlVTVlYWCxYsIDY2lqVLl7pfr1q1CoAff/yR5557jvfff5+PPvqIzMxMZsyYwdNPP83ChQuJjo5m8eLFAOzYsYO5c+cqBOSSqEcgV4yffvqJ+Ph493Lr1q15+umngbJ71gB07dqVuXPnsmPHDrp06YLDUfafQLdu3di5cyc//PADU6ZMAWDChAkArFu3jm7dugEQERFBTk5Opcdv2LAhgwYN4t1336VLly4XrLdTp04AFW6AGBoa6p7X6NixIwEBAUDZrQkOHDjA1q1befLJJwEoKipyt9G8efMKt1oXuRgKArlinG+OwOVyuV8bhoFhGBVu/1tcXIzNZsNut1d6W+DTgXEh8fHx3HnnnbRq1arS94uLiytts/zr08c3DKPCvoZh0KBBA959990K76WmptbZex5J/aChIbGEb775BoDvvvuOqKgo2rVrx3//+19KSkooKSlhy5YttGvXjo4dO/L1118D8PLLL/PVV19d1HF8fX259957efPNN93rDMMgPz+f/Px8duzYUe22fvjhB/Lz8yksLGTPnj20aNGCtm3bsm7dOgCWL19e554yJvWTegRyxTh7aAjgiSeeAKCwsJAHH3yQw4cPM2fOHJo1a8bQoUMZMWIEpmly11130bRpU8aNG8fkyZN57733+MUvfsHYsWPdIVJdgwcP5u2333YvDxs2jCFDhhAVFUWHDh2q3U779u1JSEhg3759xMXFERwczJQpU3jyySeZP38+vr6+PP/883X+satS9+nuo3LFmzRpEv369dNEqkgVNDQkImJx6hGIiFicegQiIhanIBARsTgFgYiIxSkIREQsTkEgImJxCgIREYv7/4WOQK8UT2ZAAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4441 | test accuracy: 0.687\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3854 | test accuracy: 0.697\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.9558 | test accuracy: 0.721\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3927 | test accuracy: 0.761\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1966 | test accuracy: 0.774\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1437 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3020 | test accuracy: 0.825\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8377 | test accuracy: 0.848\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.0616 | test accuracy: 0.845\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.0845 | test accuracy: 0.882\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2547 | test accuracy: 0.919\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1332 | test accuracy: 0.956\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1235 | test accuracy: 0.980\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1769 | test accuracy: 0.983\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0841 | test accuracy: 0.980\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2590 | test accuracy: 0.983\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1105 | test accuracy: 0.983\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0446 | test accuracy: 0.987\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0512 | test accuracy: 0.987\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0244 | test accuracy: 0.983\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0187 | test accuracy: 0.987\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0128 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0211 | test accuracy: 0.983\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0294 | test accuracy: 0.987\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0095 | test accuracy: 0.980\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0034 | test accuracy: 0.980\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0047 | test accuracy: 0.987\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0241 | test accuracy: 0.983\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0063 | test accuracy: 0.983\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0431 | test accuracy: 0.980\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1014 | test accuracy: 0.983\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0914 | test accuracy: 0.983\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0107 | test accuracy: 0.983\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0297 | test accuracy: 0.983\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0091 | test accuracy: 0.983\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1896 | test accuracy: 0.983\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0053 | test accuracy: 0.983\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0669 | test accuracy: 0.983\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0524 | test accuracy: 0.983\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1905 | test accuracy: 0.983\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0038 | test accuracy: 0.983\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0046 | test accuracy: 0.983\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0013 | test accuracy: 0.983\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0441 | test accuracy: 0.983\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1361 | test accuracy: 0.983\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0032 | test accuracy: 0.983\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0023 | test accuracy: 0.983\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0038 | test accuracy: 0.983\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0085 | test accuracy: 0.983\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0046 | test accuracy: 0.983\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0025 | test accuracy: 0.983\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0157 | test accuracy: 0.983\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0034 | test accuracy: 0.983\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0130 | test accuracy: 0.983\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0223 | test accuracy: 0.983\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0131 | test accuracy: 0.983\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0099 | test accuracy: 0.983\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1135 | test accuracy: 0.983\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0072 | test accuracy: 0.983\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0009 | test accuracy: 0.983\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0103 | test accuracy: 0.983\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0032 | test accuracy: 0.983\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0126 | test accuracy: 0.983\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0367 | test accuracy: 0.983\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0075 | test accuracy: 0.983\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0014 | test accuracy: 0.983\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0056 | test accuracy: 0.983\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0335 | test accuracy: 0.983\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0053 | test accuracy: 0.983\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2088 | test accuracy: 0.983\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0010 | test accuracy: 0.983\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0020 | test accuracy: 0.983\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0052 | test accuracy: 0.983\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0152 | test accuracy: 0.983\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0044 | test accuracy: 0.983\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0045 | test accuracy: 0.983\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0230 | test accuracy: 0.983\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0751 | test accuracy: 0.983\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0553 | test accuracy: 0.983\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0389 | test accuracy: 0.983\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0028 | test accuracy: 0.983\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0038 | test accuracy: 0.983\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0116 | test accuracy: 0.983\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0009 | test accuracy: 0.983\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0231 | test accuracy: 0.983\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0080 | test accuracy: 0.983\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0144 | test accuracy: 0.983\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0013 | test accuracy: 0.983\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0026 | test accuracy: 0.983\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0181 | test accuracy: 0.983\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0107 | test accuracy: 0.983\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1618 | test accuracy: 0.983\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0200 | test accuracy: 0.983\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0070 | test accuracy: 0.983\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0015 | test accuracy: 0.983\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0023 | test accuracy: 0.983\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0080 | test accuracy: 0.983\n",
            "total time:  66.55091372599964\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4142 | test accuracy: 0.687\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3605 | test accuracy: 0.694\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.9739 | test accuracy: 0.710\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4074 | test accuracy: 0.774\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2108 | test accuracy: 0.774\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1539 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3147 | test accuracy: 0.832\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8230 | test accuracy: 0.842\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.0714 | test accuracy: 0.842\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1041 | test accuracy: 0.875\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2726 | test accuracy: 0.902\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1661 | test accuracy: 0.936\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1687 | test accuracy: 0.953\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1833 | test accuracy: 0.953\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0988 | test accuracy: 0.976\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2659 | test accuracy: 0.980\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1577 | test accuracy: 0.983\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0594 | test accuracy: 0.987\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0892 | test accuracy: 0.987\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0295 | test accuracy: 0.983\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0236 | test accuracy: 0.987\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0144 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0250 | test accuracy: 0.983\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0551 | test accuracy: 0.987\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0094 | test accuracy: 0.976\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0036 | test accuracy: 0.980\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0089 | test accuracy: 0.983\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0240 | test accuracy: 0.983\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0086 | test accuracy: 0.983\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0311 | test accuracy: 0.980\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1123 | test accuracy: 0.983\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.1028 | test accuracy: 0.983\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0130 | test accuracy: 0.983\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0328 | test accuracy: 0.983\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0136 | test accuracy: 0.983\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1796 | test accuracy: 0.983\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0064 | test accuracy: 0.983\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0575 | test accuracy: 0.980\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0525 | test accuracy: 0.983\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2330 | test accuracy: 0.983\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0054 | test accuracy: 0.983\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0054 | test accuracy: 0.980\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0016 | test accuracy: 0.980\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0516 | test accuracy: 0.980\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1307 | test accuracy: 0.983\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0044 | test accuracy: 0.980\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0029 | test accuracy: 0.983\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0037 | test accuracy: 0.983\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0103 | test accuracy: 0.983\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0054 | test accuracy: 0.980\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0029 | test accuracy: 0.980\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0183 | test accuracy: 0.980\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0040 | test accuracy: 0.983\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0145 | test accuracy: 0.980\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0244 | test accuracy: 0.980\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0176 | test accuracy: 0.980\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0123 | test accuracy: 0.980\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1185 | test accuracy: 0.980\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0080 | test accuracy: 0.980\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0008 | test accuracy: 0.980\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0129 | test accuracy: 0.980\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0033 | test accuracy: 0.980\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0108 | test accuracy: 0.980\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0343 | test accuracy: 0.980\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0064 | test accuracy: 0.980\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0016 | test accuracy: 0.980\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0043 | test accuracy: 0.980\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0343 | test accuracy: 0.980\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0006 | test accuracy: 0.980\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0043 | test accuracy: 0.980\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1860 | test accuracy: 0.980\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0012 | test accuracy: 0.980\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0043 | test accuracy: 0.980\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0022 | test accuracy: 0.980\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0052 | test accuracy: 0.980\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0172 | test accuracy: 0.980\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0086 | test accuracy: 0.980\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0060 | test accuracy: 0.980\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0288 | test accuracy: 0.980\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0751 | test accuracy: 0.980\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0360 | test accuracy: 0.980\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0488 | test accuracy: 0.980\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0035 | test accuracy: 0.980\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0036 | test accuracy: 0.980\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0103 | test accuracy: 0.980\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0007 | test accuracy: 0.980\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0337 | test accuracy: 0.980\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0102 | test accuracy: 0.980\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0169 | test accuracy: 0.980\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0010 | test accuracy: 0.980\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0030 | test accuracy: 0.980\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0008 | test accuracy: 0.980\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0183 | test accuracy: 0.980\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0133 | test accuracy: 0.976\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1284 | test accuracy: 0.980\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0185 | test accuracy: 0.976\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0073 | test accuracy: 0.980\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0021 | test accuracy: 0.976\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0027 | test accuracy: 0.976\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0122 | test accuracy: 0.976\n",
            "total time:  67.2136200519999\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19585323333740234.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.34015536308288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6882425742489952 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19997715950012207.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.35555434226989746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5465470343828202 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078266143798828.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.3483855724334717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4614398130348751 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2077333927154541.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.3513922691345215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4145117895943778 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20024943351745605.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3460667133331299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38796065023967197 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20252346992492676.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3456082344055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3692437823329653 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20132923126220703.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.343212366104126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3592270029442651 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1993415355682373.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3444814682006836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35026682104383194 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016761302947998.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34831809997558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3442153040851865 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1974470615386963.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3393371105194092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33987223633698055 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.198836088180542.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3567516803741455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33580237030982973 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20153498649597168.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3498373031616211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33343011609145573 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20392560958862305.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3452770709991455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3303559226649148 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111520767211914.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35552310943603516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32898382885115485 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21336126327514648.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3617126941680908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3273445687123707 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19613981246948242.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.33849620819091797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.325713986158371 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2163991928100586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36481237411499023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3248677564518792 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21043682098388672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35034656524658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3238177452768598 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1924886703491211.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3310129642486572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3230681508779526 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21352529525756836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3552436828613281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32177218624523707 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20302987098693848.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34744906425476074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32116544502122063 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20184993743896484.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33918261528015137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3204749439443861 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20848941802978516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3518791198730469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3192476319415229 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2045884132385254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3447096347808838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3197360664606094 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20194435119628906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33858633041381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31919414315904887 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20463323593139648.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34401416778564453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3183656075171062 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017529010772705.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34966492652893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31821733457701545 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19372868537902832.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3345606327056885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3177508945975985 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20539236068725586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34418272972106934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.317552746619497 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19800591468811035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34033989906311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3172357252665928 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999666690826416.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3401353359222412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3169397962944848 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21514081954956055.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3609044551849365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31652616092136926 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19855618476867676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33861374855041504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3167564915759223 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1953444480895996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3317410945892334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3160788723400661 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21454548835754395.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36097168922424316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3159838693482535 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19872450828552246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33948373794555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31596780121326445 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19893670082092285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35081982612609863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3157544480902808 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21704626083374023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36610960960388184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31554682297366005 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20250630378723145.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34767603874206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3154038446290152 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20235586166381836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35773611068725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31541662854807717 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21001982688903809.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35327792167663574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3151374625308173 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20169568061828613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34055256843566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3151553622313908 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19976115226745605.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34599828720092773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3150396291698728 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2007122039794922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34096336364746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3150081826107843 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19493556022644043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33536553382873535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31490038207599097 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19903826713562012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3485593795776367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3148774496146611 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19783902168273926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33725738525390625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31481711907046184 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20497345924377441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34581923484802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147266822201865 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19926881790161133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3444862365722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3146085500717163 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19854307174682617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.338390588760376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3144519363130842 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2027268409729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34305787086486816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31467586542878834 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20359468460083008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35523390769958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3145056886332376 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20604848861694336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465240001678467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31431531480380465 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20141077041625977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34626150131225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3142968688692365 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057490348815918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3564887046813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31429044689450947 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19748401641845703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3375859260559082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31437567557607377 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21057510375976562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3535044193267822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142267499651228 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19716167449951172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461577892303467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31422711781093055 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1983790397644043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3398265838623047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140783778258732 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2034766674041748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34975123405456543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141103914805821 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20079970359802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35170531272888184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31412461144583564 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19520998001098633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3355727195739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31414287388324735 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20077133178710938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34319043159484863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140254816838673 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20900940895080566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34920573234558105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.314089417883328 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19482755661010742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33690857887268066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140107031379427 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20133495330810547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34723663330078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140202773468835 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21914267539978027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36016416549682617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31395224119935716 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1985766887664795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.335681676864624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31392537696020945 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19524884223937988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34307050704956055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139832194362368 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20989274978637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3550565242767334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31393562257289886 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20586514472961426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35161685943603516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31392924955913 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21478605270385742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3622918128967285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31381910698754445 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2179877758026123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.363666296005249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31376912551266806 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20627117156982422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546926975250244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138299856867109 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20226764678955078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34786152839660645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137558366571154 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21113967895507812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35228419303894043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31376534657818933 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20551800727844238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3483107089996338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137629819767816 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21146273612976074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36340832710266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31377131342887876 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21325278282165527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3565506935119629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31375993362494875 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20868635177612305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3569328784942627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3138005384377071 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201493501663208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3607218265533447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137159709419523 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2031571865081787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351245641708374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.313659770999636 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2128612995147705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36138010025024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.313718946490969 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20831060409545898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.359389066696167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136985753263746 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.201707124710083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34316444396972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136892978634153 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2067255973815918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35372114181518555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31369802611214775 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20417094230651855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3505234718322754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136654364211219 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20150017738342285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3436305522918701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31364322815622603 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2091507911682129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3571798801422119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.313628209062985 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2130413055419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35708022117614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136591460023608 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20615434646606445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35245347023010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31360248625278475 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20088815689086914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3444850444793701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135916922773634 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20720434188842773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35027289390563965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31360318064689635 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2111678123474121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3608074188232422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31362556304250444 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21036529541015625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3579287528991699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31358516131128583 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21005773544311523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351269006729126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135850676468441 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20419597625732422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3477926254272461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31356058376176016 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048192024230957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34636592864990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31358500931944167 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2140810489654541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3614768981933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135693026440484 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21132707595825195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35897254943847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135665467807225 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21046900749206543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35444140434265137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135236701795033 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21140694618225098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350827693939209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.313527940426554 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20832014083862305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35365986824035645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354426102978844 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21692347526550293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36838340759277344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31353210891996114 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20825958251953125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35383081436157227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31351452810423713 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20750641822814941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35584235191345215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31354985407420566 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20549702644348145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35826802253723145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31353876505579265 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035658359527588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3558781147003174\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135110446384975 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21422863006591797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.357757568359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31352474221161436 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20860886573791504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36171460151672363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135203378541129 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20740389823913574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35506439208984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31350245986666 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20905733108520508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35274600982666016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31350552099091666 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21923232078552246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3609466552734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31351389416626524 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20877432823181152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3519153594970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135138805423464 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055652141571045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34821248054504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31347111889294216 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2131493091583252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.356142520904541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134996899536678 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20867466926574707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35712432861328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.313481929898262 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20375728607177734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3459138870239258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134778814656394 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2180953025817871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3654460906982422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134795261280877 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2101578712463379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35962796211242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31346026105540137 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20549464225769043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3497939109802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31346165793282643 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2140946388244629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3608579635620117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134534699576242 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20343399047851562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34690284729003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134515732526779 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20489144325256348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3552088737487793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134817272424698 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20523691177368164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34928274154663086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134834510939462 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20692777633666992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34624791145324707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31347393138068064 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20288443565368652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35758209228515625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31345540796007426 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2009725570678711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34656476974487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31346149316855837 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5024 | test accuracy: 0.707\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4479 | test accuracy: 0.704\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1697 | test accuracy: 0.744\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4900 | test accuracy: 0.771\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5317 | test accuracy: 0.785\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2955 | test accuracy: 0.788\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3372 | test accuracy: 0.825\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2465 | test accuracy: 0.859\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3176 | test accuracy: 0.875\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6338 | test accuracy: 0.889\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4020 | test accuracy: 0.929\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0706 | test accuracy: 0.953\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1098 | test accuracy: 0.943\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0663 | test accuracy: 0.976\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0364 | test accuracy: 0.980\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0079 | test accuracy: 0.976\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0508 | test accuracy: 0.983\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5980 | test accuracy: 0.983\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0281 | test accuracy: 0.983\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9293 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1420 | test accuracy: 0.987\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0131 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0395 | test accuracy: 0.983\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0109 | test accuracy: 0.983\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0055 | test accuracy: 0.983\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2587 | test accuracy: 0.983\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0040 | test accuracy: 0.987\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0550 | test accuracy: 0.983\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0023 | test accuracy: 0.980\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0005 | test accuracy: 0.983\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0125 | test accuracy: 0.983\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0007 | test accuracy: 0.983\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0255 | test accuracy: 0.983\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0095 | test accuracy: 0.983\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1381 | test accuracy: 0.983\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0189 | test accuracy: 0.983\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0067 | test accuracy: 0.983\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0215 | test accuracy: 0.983\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0200 | test accuracy: 0.983\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0786 | test accuracy: 0.983\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0152 | test accuracy: 0.983\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0081 | test accuracy: 0.983\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0108 | test accuracy: 0.983\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0100 | test accuracy: 0.983\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0090 | test accuracy: 0.983\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0057 | test accuracy: 0.983\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0330 | test accuracy: 0.983\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0004 | test accuracy: 0.983\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2405 | test accuracy: 0.983\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0047 | test accuracy: 0.983\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0014 | test accuracy: 0.983\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0022 | test accuracy: 0.983\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0040 | test accuracy: 0.983\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0062 | test accuracy: 0.983\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0057 | test accuracy: 0.983\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0252 | test accuracy: 0.983\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0246 | test accuracy: 0.983\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0009 | test accuracy: 0.983\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0623 | test accuracy: 0.983\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0020 | test accuracy: 0.983\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0035 | test accuracy: 0.983\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0306 | test accuracy: 0.983\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0514 | test accuracy: 0.983\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0022 | test accuracy: 0.983\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0063 | test accuracy: 0.983\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3670 | test accuracy: 0.983\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0903 | test accuracy: 0.983\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0058 | test accuracy: 0.983\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0920 | test accuracy: 0.983\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0260 | test accuracy: 0.983\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0056 | test accuracy: 0.983\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0012 | test accuracy: 0.983\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0079 | test accuracy: 0.983\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0002 | test accuracy: 0.983\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0013 | test accuracy: 0.983\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0010 | test accuracy: 0.983\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0235 | test accuracy: 0.983\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0022 | test accuracy: 0.983\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0007 | test accuracy: 0.983\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0003 | test accuracy: 0.983\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0074 | test accuracy: 0.983\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2602 | test accuracy: 0.983\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0026 | test accuracy: 0.983\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1608 | test accuracy: 0.983\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0111 | test accuracy: 0.983\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0020 | test accuracy: 0.983\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0068 | test accuracy: 0.983\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0933 | test accuracy: 0.983\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0006 | test accuracy: 0.983\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0118 | test accuracy: 0.983\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0760 | test accuracy: 0.983\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0373 | test accuracy: 0.983\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0044 | test accuracy: 0.983\n",
            "total time:  85.051968795\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5099 | test accuracy: 0.707\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4028 | test accuracy: 0.710\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1582 | test accuracy: 0.778\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4411 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5512 | test accuracy: 0.798\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2931 | test accuracy: 0.815\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4007 | test accuracy: 0.838\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1701 | test accuracy: 0.879\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2589 | test accuracy: 0.882\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5740 | test accuracy: 0.902\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3758 | test accuracy: 0.943\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0538 | test accuracy: 0.953\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0712 | test accuracy: 0.949\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0638 | test accuracy: 0.980\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0327 | test accuracy: 0.980\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0073 | test accuracy: 0.973\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0320 | test accuracy: 0.983\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6663 | test accuracy: 0.980\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0234 | test accuracy: 0.987\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9053 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1284 | test accuracy: 0.983\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0110 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0302 | test accuracy: 0.987\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0108 | test accuracy: 0.980\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0056 | test accuracy: 0.987\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1865 | test accuracy: 0.980\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0047 | test accuracy: 0.983\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0498 | test accuracy: 0.983\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0028 | test accuracy: 0.976\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0005 | test accuracy: 0.980\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0117 | test accuracy: 0.983\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0007 | test accuracy: 0.983\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0238 | test accuracy: 0.983\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0110 | test accuracy: 0.983\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1343 | test accuracy: 0.983\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0181 | test accuracy: 0.983\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0056 | test accuracy: 0.983\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0173 | test accuracy: 0.983\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0125 | test accuracy: 0.983\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0855 | test accuracy: 0.983\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0190 | test accuracy: 0.983\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0087 | test accuracy: 0.983\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0043 | test accuracy: 0.983\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0147 | test accuracy: 0.983\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0075 | test accuracy: 0.983\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0069 | test accuracy: 0.983\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0060 | test accuracy: 0.983\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0413 | test accuracy: 0.983\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0007 | test accuracy: 0.983\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2036 | test accuracy: 0.983\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0066 | test accuracy: 0.983\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0013 | test accuracy: 0.983\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0010 | test accuracy: 0.983\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0036 | test accuracy: 0.983\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0029 | test accuracy: 0.983\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0033 | test accuracy: 0.983\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0076 | test accuracy: 0.983\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0059 | test accuracy: 0.983\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0131 | test accuracy: 0.983\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0294 | test accuracy: 0.983\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0007 | test accuracy: 0.980\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0593 | test accuracy: 0.983\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0016 | test accuracy: 0.983\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0031 | test accuracy: 0.980\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0311 | test accuracy: 0.983\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0490 | test accuracy: 0.980\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0020 | test accuracy: 0.980\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0068 | test accuracy: 0.980\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3333 | test accuracy: 0.980\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0935 | test accuracy: 0.980\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0048 | test accuracy: 0.983\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0044 | test accuracy: 0.980\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1051 | test accuracy: 0.980\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0325 | test accuracy: 0.980\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0059 | test accuracy: 0.980\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0015 | test accuracy: 0.980\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0109 | test accuracy: 0.980\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0004 | test accuracy: 0.980\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0017 | test accuracy: 0.980\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0013 | test accuracy: 0.980\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0252 | test accuracy: 0.980\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0027 | test accuracy: 0.980\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0075 | test accuracy: 0.980\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0007 | test accuracy: 0.980\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0006 | test accuracy: 0.980\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0002 | test accuracy: 0.980\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0099 | test accuracy: 0.980\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2530 | test accuracy: 0.980\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0022 | test accuracy: 0.980\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1496 | test accuracy: 0.980\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0079 | test accuracy: 0.980\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0022 | test accuracy: 0.980\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0084 | test accuracy: 0.980\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0537 | test accuracy: 0.980\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0005 | test accuracy: 0.980\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0102 | test accuracy: 0.980\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0056 | test accuracy: 0.980\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0756 | test accuracy: 0.980\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0285 | test accuracy: 0.980\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0039 | test accuracy: 0.980\n",
            "total time:  67.0859455939999\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19569659233093262.\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epoch took: 0.3427755832672119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.4833434121949332 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20689845085144043.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.35007429122924805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4165629016501563 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2124950885772705.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.35936903953552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.38330873165811813 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19749903678894043.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.33584141731262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3626746041434152 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20193171501159668.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.3496849536895752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.35235924039568217 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21197867393493652.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35181665420532227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34362243797097886 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20261120796203613.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34870433807373047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3381002060004643 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20006656646728516.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3445911407470703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33337846313204084 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2085127830505371.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.35103821754455566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33121846871716637 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20213890075683594.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3418893814086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3287026937518801 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2080078125.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3620755672454834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3263442235333579 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20313096046447754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3444957733154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3248041957616806 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19837641716003418.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3445627689361572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3231363820178168 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20034003257751465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34967947006225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32168336297784533 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2020423412322998.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34569358825683594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.321197156395231 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20771479606628418.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35324931144714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32037399538925715 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20380187034606934.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35852718353271484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.31975488918168204 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20109105110168457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3475041389465332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.31895937706742966 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2162015438079834.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.368086576461792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31860687945570265 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20816993713378906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35726451873779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31782708721501485 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2030487060546875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34372901916503906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3177401615040643 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20513176918029785.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35083913803100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31715858450957707 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21122527122497559.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35675930976867676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31695594148976464 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328688621520996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3506648540496826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3165210310901914 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20044732093811035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34322357177734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31655730179377967 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21006321907043457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3527638912200928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.316254387157304 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20740532875061035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3527870178222656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31607994181769233 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20729422569274902.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3531680107116699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31593993817056926 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20842194557189941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34632110595703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3158695902143206 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982886791229248.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3436267375946045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31543126233986446 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19600272178649902.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3325202465057373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3155510076454708 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20748424530029297.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3486475944519043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31517296603747774 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20583796501159668.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3495299816131592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3152622899838856 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2051103115081787.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3545656204223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3150694830077035 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20734763145446777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3496417999267578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31493978159768243 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20751571655273438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35498857498168945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31484971174171994 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20862102508544922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35242152214050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3146835914679936 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21040725708007812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34965944290161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3147410299096789 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19901251792907715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33776330947875977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3146197425467627 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20450472831726074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34187984466552734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3146174158368792 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21319222450256348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3619422912597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.314596215741975 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20334076881408691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3452584743499756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31443506266389576 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19780325889587402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3453075885772705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31438181613172805 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20221972465515137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343658447265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3143639811447689 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1982119083404541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34391045570373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31427481344767977 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20157337188720703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3498103618621826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3142282379525048 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19743704795837402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34153175354003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3142030724457332 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20150375366210938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.341388463973999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31427061046872823 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19732069969177246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34482336044311523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31405617892742155 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20402193069458008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34675168991088867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31408125928470065 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20328879356384277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34935688972473145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31395068083490646 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20577263832092285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3533480167388916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3140578112431935 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20459771156311035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3495173454284668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31394884160586767 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20140433311462402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3391447067260742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3139589875936508 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19936847686767578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3456423282623291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3139298941407885 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20537495613098145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34943437576293945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31395890968186513 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20265865325927734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34191393852233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3139316669532231 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090435028076172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484513759613037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3138771078416279 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20103788375854492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3430335521697998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.313920555795942 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19910573959350586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34249353408813477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3137882432767323 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20923328399658203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3540074825286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31387400201388765 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20669150352478027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35030627250671387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31376604395253316 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046825885772705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3423943519592285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138286765132632 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2068941593170166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35797834396362305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31379247094903673 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2001039981842041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34351038932800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31377732796328406 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19732356071472168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3371109962463379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137154302426747 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20973849296569824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35143470764160156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31374626074518475 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20370745658874512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34616994857788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137622109481267 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19965171813964844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33643078804016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31369633163724625 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2099771499633789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35332775115966797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.313725415297917 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19859981536865234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.339479923248291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31369060575962066 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20772910118103027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34680604934692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3136421012026923 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20636320114135742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35128164291381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31360516803605215 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2023303508758545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3424043655395508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31364092741693766 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20098519325256348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3379693031311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136129413332258 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21928191184997559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3645033836364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136479011603764 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20322942733764648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428225517272949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136212293590818 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20246362686157227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34373974800109863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136082662003381 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21862006187438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3615753650665283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136456208569663 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21297812461853027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3549327850341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31358592978545596 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19815611839294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33684492111206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136016339063644 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21483302116394043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3553965091705322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136410798345293 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19858002662658691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3376922607421875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31357544149671285 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20274758338928223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35266971588134766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31355968032564435 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2120954990386963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36079978942871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135812793459211 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20264840126037598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34569740295410156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135593226977757 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095484733581543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36165452003479004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135543601853507 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2066817283630371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3514821529388428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135403177567891 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20055317878723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3409423828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135146456105368 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19927763938903809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546128273010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31353409120014736 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20913290977478027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35726189613342285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31351138736520495 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19780421257019043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3383960723876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31350603444235664 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20727872848510742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3635401725769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31352850113596237 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2077195644378662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3492593765258789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31348988286086493 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1980586051940918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33470654487609863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31349484154156276 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.209975004196167.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35329723358154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135338068008423 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20197391510009766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34175777435302734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31347493827342987 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20177459716796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34761500358581543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31349386743136815 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21402859687805176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554701805114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31348437539168766 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20639944076538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34438228607177734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3134811588696071 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20432353019714355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34750795364379883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31349486878940036 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22232413291931152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3736386299133301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31347327189786095 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21120524406433105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527257442474365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3134788291794913 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21595525741577148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3668849468231201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134580143860408 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21354341506958008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3558542728424072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134741685220173 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20438599586486816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34952378273010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134966288294111 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2119743824005127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3556852340698242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31343809749398915 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2135634422302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3546900749206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134541468960898 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20318984985351562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3441951274871826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134715280362538 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2090778350830078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3583838939666748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.313457647391728 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20282912254333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3408830165863037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134557238646916 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20355987548828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34283971786499023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31343805151326315 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20549607276916504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35857439041137695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134764922516687 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19810247421264648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34079909324645996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31340641507080624 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.198591947555542.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3426074981689453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134222618171147 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19656586647033691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3450357913970947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31341553117547716 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20515751838684082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34515953063964844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31341696679592135 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20563411712646484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34689927101135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134342866284507 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20673060417175293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3649587631225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31342841642243524 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20267081260681152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34711694717407227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31343409461634497 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20511841773986816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3538227081298828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134328488792692 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20371699333190918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3576161861419678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134183134351458 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21581339836120605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36162638664245605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134182734148843 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21249604225158691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36046409606933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343078698430743 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2226238250732422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.367612361907959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31341438932078225 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20389771461486816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34864211082458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31341519228049686 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2059938907623291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35337257385253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134291372128895 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2198781967163086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3606884479522705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31340387633868627 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6716 | test accuracy: 0.690\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4011 | test accuracy: 0.754\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5040 | test accuracy: 0.751\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7318 | test accuracy: 0.764\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2679 | test accuracy: 0.778\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2612 | test accuracy: 0.811\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4427 | test accuracy: 0.818\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5244 | test accuracy: 0.859\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2551 | test accuracy: 0.869\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2372 | test accuracy: 0.886\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6480 | test accuracy: 0.902\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0612 | test accuracy: 0.926\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1016 | test accuracy: 0.926\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.1114 | test accuracy: 0.963\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0529 | test accuracy: 0.966\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2924 | test accuracy: 0.963\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0577 | test accuracy: 0.973\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0242 | test accuracy: 0.976\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0154 | test accuracy: 0.973\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0713 | test accuracy: 0.973\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0101 | test accuracy: 0.976\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2154 | test accuracy: 0.970\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.0585 | test accuracy: 0.970\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0439 | test accuracy: 0.970\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0058 | test accuracy: 0.970\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1774 | test accuracy: 0.973\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0251 | test accuracy: 0.966\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0431 | test accuracy: 0.966\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.3619 | test accuracy: 0.966\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0051 | test accuracy: 0.963\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0036 | test accuracy: 0.966\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0013 | test accuracy: 0.970\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0219 | test accuracy: 0.966\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0053 | test accuracy: 0.963\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0023 | test accuracy: 0.963\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0014 | test accuracy: 0.963\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0159 | test accuracy: 0.963\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0147 | test accuracy: 0.963\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0027 | test accuracy: 0.966\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0071 | test accuracy: 0.966\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0032 | test accuracy: 0.966\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0043 | test accuracy: 0.963\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6454 | test accuracy: 0.966\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0030 | test accuracy: 0.966\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0013 | test accuracy: 0.966\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0930 | test accuracy: 0.966\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7800 | test accuracy: 0.966\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0027 | test accuracy: 0.966\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.3780 | test accuracy: 0.963\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0003 | test accuracy: 0.966\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0150 | test accuracy: 0.966\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0214 | test accuracy: 0.963\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0012 | test accuracy: 0.963\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0213 | test accuracy: 0.963\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0068 | test accuracy: 0.963\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0126 | test accuracy: 0.963\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0022 | test accuracy: 0.963\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0547 | test accuracy: 0.963\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0674 | test accuracy: 0.966\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0190 | test accuracy: 0.963\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0096 | test accuracy: 0.966\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0318 | test accuracy: 0.963\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0014 | test accuracy: 0.963\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0032 | test accuracy: 0.963\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0017 | test accuracy: 0.966\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0009 | test accuracy: 0.963\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0014 | test accuracy: 0.966\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0166 | test accuracy: 0.963\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0011 | test accuracy: 0.966\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0137 | test accuracy: 0.966\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0045 | test accuracy: 0.963\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0674 | test accuracy: 0.963\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0066 | test accuracy: 0.966\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0442 | test accuracy: 0.963\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0231 | test accuracy: 0.963\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.9828 | test accuracy: 0.963\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0175 | test accuracy: 0.963\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0015 | test accuracy: 0.963\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8498 | test accuracy: 0.966\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0376 | test accuracy: 0.966\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0046 | test accuracy: 0.966\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0041 | test accuracy: 0.966\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0009 | test accuracy: 0.966\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0266 | test accuracy: 0.966\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.9000 | test accuracy: 0.963\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1428 | test accuracy: 0.966\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0027 | test accuracy: 0.966\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0012 | test accuracy: 0.963\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0041 | test accuracy: 0.963\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0026 | test accuracy: 0.966\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0017 | test accuracy: 0.963\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0088 | test accuracy: 0.966\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0133 | test accuracy: 0.966\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0066 | test accuracy: 0.966\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0422 | test accuracy: 0.963\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0037 | test accuracy: 0.963\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0474 | test accuracy: 0.963\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0011 | test accuracy: 0.963\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0092 | test accuracy: 0.963\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0020 | test accuracy: 0.963\n",
            "total time:  67.07016264899994\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6846 | test accuracy: 0.680\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3904 | test accuracy: 0.741\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5730 | test accuracy: 0.758\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7480 | test accuracy: 0.764\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3148 | test accuracy: 0.778\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3070 | test accuracy: 0.798\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4835 | test accuracy: 0.808\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6205 | test accuracy: 0.815\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2208 | test accuracy: 0.838\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3002 | test accuracy: 0.865\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7827 | test accuracy: 0.892\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.0850 | test accuracy: 0.912\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1193 | test accuracy: 0.906\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.1526 | test accuracy: 0.949\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0737 | test accuracy: 0.966\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2556 | test accuracy: 0.966\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0928 | test accuracy: 0.980\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0321 | test accuracy: 0.970\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0145 | test accuracy: 0.980\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0917 | test accuracy: 0.976\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0108 | test accuracy: 0.976\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2023 | test accuracy: 0.973\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.1253 | test accuracy: 0.970\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0424 | test accuracy: 0.970\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0073 | test accuracy: 0.970\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2365 | test accuracy: 0.973\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0238 | test accuracy: 0.966\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0684 | test accuracy: 0.970\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.3178 | test accuracy: 0.966\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0046 | test accuracy: 0.966\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0036 | test accuracy: 0.966\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0015 | test accuracy: 0.970\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0334 | test accuracy: 0.966\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0063 | test accuracy: 0.966\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0028 | test accuracy: 0.966\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0021 | test accuracy: 0.966\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0270 | test accuracy: 0.966\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0133 | test accuracy: 0.966\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0034 | test accuracy: 0.966\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0090 | test accuracy: 0.966\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0038 | test accuracy: 0.966\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0052 | test accuracy: 0.966\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6226 | test accuracy: 0.966\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0039 | test accuracy: 0.970\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0020 | test accuracy: 0.966\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0913 | test accuracy: 0.966\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7890 | test accuracy: 0.966\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0033 | test accuracy: 0.966\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.3599 | test accuracy: 0.966\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0005 | test accuracy: 0.966\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0115 | test accuracy: 0.966\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0204 | test accuracy: 0.966\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0017 | test accuracy: 0.966\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0210 | test accuracy: 0.966\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0077 | test accuracy: 0.966\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0173 | test accuracy: 0.966\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0019 | test accuracy: 0.966\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0623 | test accuracy: 0.966\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0506 | test accuracy: 0.966\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0185 | test accuracy: 0.966\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0079 | test accuracy: 0.966\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0331 | test accuracy: 0.966\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0018 | test accuracy: 0.966\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0041 | test accuracy: 0.966\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0021 | test accuracy: 0.966\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0017 | test accuracy: 0.966\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0015 | test accuracy: 0.966\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0146 | test accuracy: 0.966\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0015 | test accuracy: 0.966\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0152 | test accuracy: 0.966\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0048 | test accuracy: 0.966\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0621 | test accuracy: 0.966\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0094 | test accuracy: 0.966\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0411 | test accuracy: 0.966\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0224 | test accuracy: 0.966\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.9641 | test accuracy: 0.966\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0209 | test accuracy: 0.963\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0017 | test accuracy: 0.966\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8317 | test accuracy: 0.966\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0495 | test accuracy: 0.970\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0046 | test accuracy: 0.966\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0045 | test accuracy: 0.966\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0011 | test accuracy: 0.966\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0241 | test accuracy: 0.966\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.8868 | test accuracy: 0.966\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1488 | test accuracy: 0.966\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0038 | test accuracy: 0.966\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0015 | test accuracy: 0.966\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0042 | test accuracy: 0.966\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0030 | test accuracy: 0.966\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0015 | test accuracy: 0.966\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0091 | test accuracy: 0.966\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0110 | test accuracy: 0.966\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0082 | test accuracy: 0.963\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0556 | test accuracy: 0.966\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0042 | test accuracy: 0.966\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0646 | test accuracy: 0.963\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0010 | test accuracy: 0.966\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0096 | test accuracy: 0.966\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0019 | test accuracy: 0.966\n",
            "total time:  66.68371775800006\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2016468048095703.\n",
            "\n",
            "  Average training loss: 0.57\n",
            "  Training epoch took: 0.354964017868042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5658908673695155 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2087113857269287.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.3519868850708008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.46786387264728546 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20231366157531738.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.34496498107910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4160473725625447 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20689797401428223.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.35222601890563965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.386960141147886 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20261001586914062.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.34376049041748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3679263323545456 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20331621170043945.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.34191107749938965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3572231237377439 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20684313774108887.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.35915255546569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3480242882456098 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20439648628234863.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.343350887298584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.342560579095568 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19607114791870117.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.34174394607543945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33781981468200684 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21528077125549316.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.36877965927124023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3341026144368308 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1988973617553711.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34079647064208984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33022917125906265 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20835304260253906.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3513681888580322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.328433946626527 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20077919960021973.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34830641746520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32622786717755453 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20736289024353027.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3509945869445801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3252371621983392 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1978907585144043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34514760971069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32395197451114655 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22121644020080566.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3653111457824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32268933696406227 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2018427848815918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34592461585998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32217126318386624 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20766282081604004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3505265712738037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32117329963615965 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2126178741455078.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35503673553466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3207100033760071 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20409154891967773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34816884994506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31936062744685584 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19899201393127441.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3436915874481201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3192034942763192 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.209061861038208.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35193657875061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31853347590991427 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1961665153503418.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33946657180786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31799263060092925 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20944833755493164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3473930358886719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3177714935370854 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20821213722229004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3482553958892822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3178604087659291 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19830751419067383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33646464347839355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31734782457351685 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20449399948120117.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34867286682128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31681253228868755 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2061476707458496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3450045585632324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31687923073768615 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1995542049407959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34264183044433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3164808771439961 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20650076866149902.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34851837158203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3162581477846418 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20713305473327637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34806060791015625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31585400657994406 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2103722095489502.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35321760177612305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3156952444996153 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.202225923538208.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3512766361236572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3155695761953081 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2174208164215088.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35804271697998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3155186678682055 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1988370418548584.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3467259407043457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3152769237756729 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20879316329956055.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35738205909729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3153270159448896 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19587063789367676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.33547306060791016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3153030834027699 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2030472755432129.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3411571979522705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31511613045419967 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20178627967834473.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35068750381469727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3149585195950099 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20477938652038574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34923362731933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31500383019447326 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20345401763916016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3465914726257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31487326409135546 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21448135375976562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36540985107421875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31478350503104074 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19781041145324707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33502721786499023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.314715405021395 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20612573623657227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34973931312561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31465432132993426 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20235967636108398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527822494506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31455119550228117 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19704174995422363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34282445907592773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3144522245441164 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049410343170166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34667277336120605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3143916713339942 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19966816902160645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3515636920928955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3144506560904639 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21008658409118652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35364508628845215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31427079354013715 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20467400550842285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34775757789611816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3143438539334706 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21814417839050293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36105942726135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31426540868622915 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.207655668258667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3560357093811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31425402462482455 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20579910278320312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35003137588500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3142146404300417 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2140660285949707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3573319911956787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141812524625233 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20773744583129883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35332751274108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3141391911676952 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2083275318145752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35561633110046387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31404026831899373 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20980310440063477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537280559539795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140667349100113 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19816160202026367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3421504497528076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31406100179467883 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20443511009216309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34929847717285156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31402279819761003 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21188068389892578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35805797576904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31401026802403587 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20857882499694824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35459065437316895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3139553074325834 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21369719505310059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35566186904907227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3139358592884881 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2083725929260254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36133432388305664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31388296484947203 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20208358764648438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3517160415649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31384366069521225 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2048346996307373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3536393642425537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31386664978095463 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20415472984313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3522059917449951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31384901276656557 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2073523998260498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3503561019897461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.313896438905171 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20193028450012207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3506484031677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.313823806813785 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21012520790100098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3560349941253662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138274086373193 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20569062232971191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3478705883026123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31383834055491855 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20640349388122559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35208678245544434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31382224687508176 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2060091495513916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3460230827331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137460810797555 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2036914825439453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3454456329345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31372747463839395 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2023296356201172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35915541648864746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3137404203414917 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20670676231384277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34876203536987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31373351173741476 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2021341323852539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447551727294922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137423404625484 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20683026313781738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.345003604888916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137058164392199 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20452022552490234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.348297119140625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137152284383774 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20157432556152344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3439905643463135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136751447405134 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2061328887939453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35016775131225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31369189407144277 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19621920585632324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3385915756225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136923977306911 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20156598091125488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457505702972412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31365656980446405 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21394586563110352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3552846908569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136595628091267 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20169615745544434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3425424098968506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31363605260849 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2028944492340088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3420705795288086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31361477630478996 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21150994300842285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3500652313232422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.313614650283541 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19828009605407715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343003511428833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136269326720919 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2014453411102295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34105491638183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136353531054088 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21088337898254395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35395002365112305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135723352432251 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2005002498626709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3414649963378906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136136557374682 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20665693283081055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3458220958709717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135617622307369 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.216111421585083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3575727939605713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135457443339484 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20544862747192383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34668493270874023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135416320392064 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20205116271972656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343874454498291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31355311572551725 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21265363693237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3632533550262451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31355293052537103 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20658349990844727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3527565002441406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135507025888988 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20013189315795898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3394155502319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135304089103426 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21097373962402344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487281799316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135462760925293 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20496034622192383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35030698776245117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135474217789514 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20592451095581055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35802507400512695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31352038085460665 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2017970085144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34729456901550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31353201397827696 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20914745330810547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34973597526550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135156320674079 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19984102249145508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523573875427246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31352634046758926 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20439553260803223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34679484367370605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134983931268964 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20316290855407715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34354710578918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135292572634561 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19314122200012207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3484020233154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135260496820722 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1986076831817627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3404529094696045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31347742676734924 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20301532745361328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461325168609619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134731377874102 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20177555084228516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351975679397583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31348221472331456 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1993699073791504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3393268585205078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134902660335813 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20615196228027344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34984707832336426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348023457186563 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2028369903564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35612988471984863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134688160249165 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20033645629882812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34384775161743164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31345907407147544 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2078843116760254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34682607650756836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134740786893027 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21146368980407715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35047149658203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134527840784618 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20098400115966797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3447279930114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134851694107056 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2055518627166748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34988951683044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31346396718706404 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2199416160583496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3643927574157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134554871491024 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21616005897521973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3572850227355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31346350737980433 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2064344882965088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3493936061859131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31346195765904017 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21121001243591309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3550419807434082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134455463715962 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20641517639160156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34693479537963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134623548814229 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19865655899047852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34006404876708984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31343126382146563 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2129056453704834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3580331802368164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134660282305309 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19890856742858887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.337658166885376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313436484336853 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19947171211242676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3424408435821533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343929384435926 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20766091346740723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.350264310836792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134183555841446 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2049252986907959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34110283851623535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134549072810582 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5136 | test accuracy: 0.724\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5486 | test accuracy: 0.764\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4434 | test accuracy: 0.761\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8252 | test accuracy: 0.768\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3380 | test accuracy: 0.758\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1020 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4275 | test accuracy: 0.811\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4835 | test accuracy: 0.838\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.0794 | test accuracy: 0.845\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1980 | test accuracy: 0.859\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2958 | test accuracy: 0.886\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2917 | test accuracy: 0.896\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2809 | test accuracy: 0.919\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0376 | test accuracy: 0.936\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7929 | test accuracy: 0.956\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1372 | test accuracy: 0.960\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2857 | test accuracy: 0.970\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0117 | test accuracy: 0.973\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0874 | test accuracy: 0.980\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0427 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0350 | test accuracy: 0.976\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1990 | test accuracy: 0.970\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0090 | test accuracy: 0.973\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0988 | test accuracy: 0.976\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0162 | test accuracy: 0.970\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0431 | test accuracy: 0.970\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0129 | test accuracy: 0.976\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0114 | test accuracy: 0.970\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.3626 | test accuracy: 0.973\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0192 | test accuracy: 0.980\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0133 | test accuracy: 0.973\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0058 | test accuracy: 0.973\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0041 | test accuracy: 0.966\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0073 | test accuracy: 0.970\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0065 | test accuracy: 0.966\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0209 | test accuracy: 0.973\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0047 | test accuracy: 0.973\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7822 | test accuracy: 0.970\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2859 | test accuracy: 0.973\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0137 | test accuracy: 0.973\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0060 | test accuracy: 0.973\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0015 | test accuracy: 0.973\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0044 | test accuracy: 0.973\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0119 | test accuracy: 0.973\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3471 | test accuracy: 0.973\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1343 | test accuracy: 0.973\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0017 | test accuracy: 0.973\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0207 | test accuracy: 0.973\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0067 | test accuracy: 0.973\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0016 | test accuracy: 0.973\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0007 | test accuracy: 0.973\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0082 | test accuracy: 0.973\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0062 | test accuracy: 0.973\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1329 | test accuracy: 0.973\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0062 | test accuracy: 0.973\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0017 | test accuracy: 0.973\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0068 | test accuracy: 0.973\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3644 | test accuracy: 0.973\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0079 | test accuracy: 0.973\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0153 | test accuracy: 0.973\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0024 | test accuracy: 0.973\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0012 | test accuracy: 0.973\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1292 | test accuracy: 0.973\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0027 | test accuracy: 0.973\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0022 | test accuracy: 0.973\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0058 | test accuracy: 0.973\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1281 | test accuracy: 0.973\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0603 | test accuracy: 0.973\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1312 | test accuracy: 0.973\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3197 | test accuracy: 0.966\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0020 | test accuracy: 0.973\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0185 | test accuracy: 0.973\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0498 | test accuracy: 0.966\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0094 | test accuracy: 0.966\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0049 | test accuracy: 0.973\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0038 | test accuracy: 0.973\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0120 | test accuracy: 0.973\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0024 | test accuracy: 0.973\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0017 | test accuracy: 0.973\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0043 | test accuracy: 0.970\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6064 | test accuracy: 0.973\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0005 | test accuracy: 0.970\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0231 | test accuracy: 0.973\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0787 | test accuracy: 0.973\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0011 | test accuracy: 0.973\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0039 | test accuracy: 0.973\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0852 | test accuracy: 0.973\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0018 | test accuracy: 0.970\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0018 | test accuracy: 0.973\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0072 | test accuracy: 0.973\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0112 | test accuracy: 0.970\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0030 | test accuracy: 0.973\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0013 | test accuracy: 0.970\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0018 | test accuracy: 0.973\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0533 | test accuracy: 0.973\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0007 | test accuracy: 0.970\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0014 | test accuracy: 0.973\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3668 | test accuracy: 0.973\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0022 | test accuracy: 0.973\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0031 | test accuracy: 0.970\n",
            "total time:  65.85579829500057\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5150 | test accuracy: 0.704\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5334 | test accuracy: 0.741\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4431 | test accuracy: 0.761\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7209 | test accuracy: 0.758\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3190 | test accuracy: 0.768\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1461 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4190 | test accuracy: 0.835\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5772 | test accuracy: 0.832\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.0545 | test accuracy: 0.859\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2063 | test accuracy: 0.862\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2583 | test accuracy: 0.879\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.3390 | test accuracy: 0.892\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2706 | test accuracy: 0.909\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0349 | test accuracy: 0.929\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7318 | test accuracy: 0.956\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1141 | test accuracy: 0.966\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2504 | test accuracy: 0.973\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0123 | test accuracy: 0.966\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0767 | test accuracy: 0.966\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0436 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0312 | test accuracy: 0.970\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2071 | test accuracy: 0.973\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0103 | test accuracy: 0.966\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1630 | test accuracy: 0.973\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0137 | test accuracy: 0.970\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0708 | test accuracy: 0.973\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0133 | test accuracy: 0.973\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0174 | test accuracy: 0.973\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.3305 | test accuracy: 0.966\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0151 | test accuracy: 0.973\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0142 | test accuracy: 0.966\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0046 | test accuracy: 0.970\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0058 | test accuracy: 0.970\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0073 | test accuracy: 0.970\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0056 | test accuracy: 0.970\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0182 | test accuracy: 0.970\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0041 | test accuracy: 0.970\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6461 | test accuracy: 0.970\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3292 | test accuracy: 0.970\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0175 | test accuracy: 0.970\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0078 | test accuracy: 0.970\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0011 | test accuracy: 0.970\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0071 | test accuracy: 0.970\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0132 | test accuracy: 0.970\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3150 | test accuracy: 0.970\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1133 | test accuracy: 0.970\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0021 | test accuracy: 0.970\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0271 | test accuracy: 0.970\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0100 | test accuracy: 0.970\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0014 | test accuracy: 0.970\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0004 | test accuracy: 0.970\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0064 | test accuracy: 0.970\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0049 | test accuracy: 0.970\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0878 | test accuracy: 0.970\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0051 | test accuracy: 0.970\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0019 | test accuracy: 0.970\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0087 | test accuracy: 0.970\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3384 | test accuracy: 0.970\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0090 | test accuracy: 0.970\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0156 | test accuracy: 0.970\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0028 | test accuracy: 0.970\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0013 | test accuracy: 0.970\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0862 | test accuracy: 0.970\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0024 | test accuracy: 0.970\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0023 | test accuracy: 0.970\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0057 | test accuracy: 0.970\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1293 | test accuracy: 0.970\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0710 | test accuracy: 0.970\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1137 | test accuracy: 0.970\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2767 | test accuracy: 0.970\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0023 | test accuracy: 0.970\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0594 | test accuracy: 0.970\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0504 | test accuracy: 0.970\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0076 | test accuracy: 0.970\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0092 | test accuracy: 0.970\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0043 | test accuracy: 0.970\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0122 | test accuracy: 0.970\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0027 | test accuracy: 0.970\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0020 | test accuracy: 0.970\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0056 | test accuracy: 0.970\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.5513 | test accuracy: 0.970\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0005 | test accuracy: 0.970\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0281 | test accuracy: 0.970\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0697 | test accuracy: 0.970\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0009 | test accuracy: 0.970\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0041 | test accuracy: 0.970\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0882 | test accuracy: 0.970\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0017 | test accuracy: 0.970\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0015 | test accuracy: 0.970\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0048 | test accuracy: 0.970\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0091 | test accuracy: 0.970\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0023 | test accuracy: 0.970\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0009 | test accuracy: 0.970\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0015 | test accuracy: 0.970\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0465 | test accuracy: 0.970\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0004 | test accuracy: 0.970\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0014 | test accuracy: 0.970\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2677 | test accuracy: 0.970\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0046 | test accuracy: 0.970\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0017 | test accuracy: 0.970\n",
            "total time:  67.04528955600017\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21466922760009766.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.3596835136413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6895273651395525 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057192325592041.\n",
            "\n",
            "  Average training loss: 0.54\n",
            "  Training epoch took: 0.34312868118286133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5391492068767547 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2143850326538086.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epoch took: 0.3656883239746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.45228206728185927 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21813058853149414.\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epoch took: 0.3612086772918701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.40953136086463926 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21131420135498047.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.3519461154937744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3855975466115134 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20742273330688477.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.3471806049346924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.36932622279439653 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21329092979431152.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.35826659202575684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3574446891035352 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20133662223815918.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.34456920623779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3514299533196858 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20063471794128418.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.35040783882141113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34411236303193227 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2088162899017334.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3513970375061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3394073256424495 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20180463790893555.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.3408849239349365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3366437737430845 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2057340145111084.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3555257320404053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33266572909695763 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2026357650756836.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.34375548362731934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32962443700858524 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20436692237854004.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3459324836730957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32751824259757994 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2046055793762207.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3656632900238037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3270059696265629 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21199274063110352.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.3591752052307129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3253269199814115 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20442962646484375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34664344787597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3234942512852805 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20441293716430664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3564732074737549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3227840793984277 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20887064933776855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35039186477661133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3219778035368238 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21317696571350098.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.364962100982666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32150059597832814 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20973467826843262.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36379313468933105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32069179373128076 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2050328254699707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34856319427490234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32025180714471 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20258164405822754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3450920581817627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3194631827729089 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2153792381286621.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35855841636657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31916050655501227 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19962620735168457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3403153419494629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31840923854282926 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20171880722045898.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3557577133178711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3182651221752167 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21640515327453613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35889458656311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31785565103803365 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20284771919250488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.346355676651001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3172578641346523 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21306514739990234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3604133129119873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3173620785985674 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22089695930480957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36245012283325195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3168168723583221 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20647835731506348.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34745287895202637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3165777053151812 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2022244930267334.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34438347816467285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3164182262761252 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21276640892028809.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35413026809692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3163039705583027 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21083498001098633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35305333137512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3160222636801856 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21286487579345703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3631765842437744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31588854363986424 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21841812133789062.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3604707717895508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3158760632787432 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2035973072052002.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.34615135192871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31580438784190584 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20102477073669434.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.349653959274292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31556733165468487 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19907832145690918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3419456481933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3154564810650689 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2043590545654297.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.3470144271850586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31529029650347573 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20926284790039062.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.36594367027282715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3152159307684217 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20779657363891602.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.35060620307922363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.315150733930724 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20554375648498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35703182220458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.314976162144116 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20805883407592773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36100149154663086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3148249839033399 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2069237232208252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3481743335723877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31480879868779865 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20641779899597168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35219740867614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31487762204238345 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20883607864379883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3507955074310303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3145957091024944 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20035123825073242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449826240539551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147103237254279 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20035052299499512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3400759696960449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31453279767717635 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21621465682983398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35631394386291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31440343729087283 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19982099533081055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3404567241668701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3144781925848552 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20237112045288086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34525299072265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31447988024779727 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21116280555725098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35622262954711914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31447029965264456 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20695161819458008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35572290420532227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31430498915059224 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21224451065063477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3573024272918701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31420621573925017 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2179734706878662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35835814476013184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31429806394236426 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2019026279449463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34795451164245605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3141969416822706 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20070552825927734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3418543338775635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3141847648790905 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21224379539489746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35359740257263184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31410336451871057 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19942426681518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3455240726470947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31412551999092103 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20876216888427734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34935665130615234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31411973110267094 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21501421928405762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3552875518798828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31405154168605803 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20843291282653809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3564431667327881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31396135900701794 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20200085639953613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3428008556365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139726770775659 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20980262756347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3623008728027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140135679926191 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20676302909851074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.348268985748291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31392708378178735 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20391154289245605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36017346382141113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139552129166467 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19889068603515625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33789563179016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139151862689427 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20537376403808594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34865403175354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138726992266519 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20416641235351562.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3585624694824219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31384341503892627 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2042844295501709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35306620597839355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31389418201787134 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20636820793151855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35071253776550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31382741842951095 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21084237098693848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3601515293121338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138125747442245 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20687508583068848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3507664203643799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138432277100427 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19774365425109863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.33919405937194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31375625261238643 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19718623161315918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3449678421020508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137652903795242 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20259428024291992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34691309928894043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137505837849208 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20211124420166016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3444709777832031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137267981256757 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21061182022094727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3536715507507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137401785169329 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20990681648254395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3559134006500244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137193718126842 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2121875286102295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35335755348205566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31367866694927216 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21555542945861816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3635990619659424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137173333338329 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20495080947875977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487379550933838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136738253491265 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20554447174072266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34935855865478516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136724697692054 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21179914474487305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3558619022369385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31370935014316015 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2053821086883545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34809398651123047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136683246919087 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20151090621948242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34392642974853516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31367780906813486 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21495819091796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3653867244720459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136375014271055 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20632672309875488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.349658727645874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31360414241041457 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19922351837158203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3405270576477051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31366168941770284 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21892881393432617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3636493682861328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136080141578402 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2019796371459961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34340906143188477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135877881731306 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20056581497192383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34622788429260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136007113116128 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2019824981689453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3440256118774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31364269895212993 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20028924942016602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3438899517059326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31356295772961207 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20532941818237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3591177463531494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135708715234484 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21031785011291504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35695934295654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31354064004761834 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20604658126831055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34897470474243164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135893600327628 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20211577415466309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3487405776977539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31354891061782836 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20786809921264648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.346710205078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135377266577312 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20485997200012207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3464677333831787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135584941932133 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19778680801391602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3457043170928955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31353646218776704 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2024059295654297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34327268600463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354060215609414 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20270442962646484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3446176052093506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135505659239633 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20190763473510742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3523564338684082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134794580084937 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20530056953430176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34807848930358887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.313526622738157 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19750618934631348.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.343691349029541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135108057941709 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21052289009094238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3678112030029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.313494838987078 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2012491226196289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3472721576690674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135146379470825 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20893359184265137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35069870948791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135250287396567 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.218735933303833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36585521697998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134857782295772 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21412038803100586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36259031295776367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31348812367234913 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20860552787780762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.351914644241333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134986511298588 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.22340178489685059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.37163639068603516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31348749910082135 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2093353271484375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3535490036010742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31347342899867464 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2040553092956543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35305333137512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31348146455628534 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2159590721130371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3562192916870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134774727480752 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2032022476196289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.34268999099731445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134851796286447 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2128760814666748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3692128658294678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31346625302519115 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2095015048980713.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3521742820739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31348153182438443 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.1999208927154541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3406798839569092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31348026267119816 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20570826530456543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3554360866546631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.313472004873412 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20622515678405762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.35045528411865234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134598233870098 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2037487030029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3461747169494629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344063878059386 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.20935940742492676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.36588382720947266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134560163531985 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21085095405578613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3525364398956299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31345034199101585 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2084951400756836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3537588119506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134514391422272 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.21026849746704102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.3587069511413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134391541991915 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9163 | test accuracy: 0.721\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3263 | test accuracy: 0.751\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4424 | test accuracy: 0.754\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8890 | test accuracy: 0.761\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3769 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3092 | test accuracy: 0.781\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6500 | test accuracy: 0.781\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5734 | test accuracy: 0.818\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.8525 | test accuracy: 0.838\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4535 | test accuracy: 0.859\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2556 | test accuracy: 0.859\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1216 | test accuracy: 0.879\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1497 | test accuracy: 0.906\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3203 | test accuracy: 0.926\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.3114 | test accuracy: 0.943\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0642 | test accuracy: 0.963\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0188 | test accuracy: 0.970\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0510 | test accuracy: 0.966\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0318 | test accuracy: 0.970\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0955 | test accuracy: 0.973\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1801 | test accuracy: 0.970\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1018 | test accuracy: 0.963\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2670 | test accuracy: 0.970\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5827 | test accuracy: 0.963\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0126 | test accuracy: 0.970\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0270 | test accuracy: 0.960\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0138 | test accuracy: 0.970\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0152 | test accuracy: 0.970\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0045 | test accuracy: 0.960\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0141 | test accuracy: 0.966\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0044 | test accuracy: 0.970\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0050 | test accuracy: 0.966\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0743 | test accuracy: 0.966\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0062 | test accuracy: 0.966\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0287 | test accuracy: 0.966\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1055 | test accuracy: 0.970\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0209 | test accuracy: 0.966\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2560 | test accuracy: 0.966\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0018 | test accuracy: 0.966\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0074 | test accuracy: 0.966\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0164 | test accuracy: 0.966\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0327 | test accuracy: 0.966\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0097 | test accuracy: 0.966\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1050 | test accuracy: 0.966\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1047 | test accuracy: 0.966\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.3375 | test accuracy: 0.970\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1529 | test accuracy: 0.966\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0055 | test accuracy: 0.966\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0398 | test accuracy: 0.966\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0144 | test accuracy: 0.970\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0016 | test accuracy: 0.970\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0018 | test accuracy: 0.970\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0055 | test accuracy: 0.970\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1202 | test accuracy: 0.970\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0016 | test accuracy: 0.970\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0122 | test accuracy: 0.970\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0082 | test accuracy: 0.970\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0037 | test accuracy: 0.970\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0011 | test accuracy: 0.966\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0067 | test accuracy: 0.970\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0176 | test accuracy: 0.970\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3898 | test accuracy: 0.970\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0024 | test accuracy: 0.970\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0051 | test accuracy: 0.970\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0099 | test accuracy: 0.970\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0631 | test accuracy: 0.970\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0023 | test accuracy: 0.970\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0932 | test accuracy: 0.970\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0018 | test accuracy: 0.966\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0714 | test accuracy: 0.966\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0053 | test accuracy: 0.966\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0096 | test accuracy: 0.966\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0058 | test accuracy: 0.966\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0003 | test accuracy: 0.973\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0601 | test accuracy: 0.966\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0349 | test accuracy: 0.966\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0360 | test accuracy: 0.966\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0036 | test accuracy: 0.966\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2084 | test accuracy: 0.966\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2115 | test accuracy: 0.973\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0018 | test accuracy: 0.973\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0019 | test accuracy: 0.966\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0047 | test accuracy: 0.966\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0007 | test accuracy: 0.970\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0126 | test accuracy: 0.970\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0675 | test accuracy: 0.970\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0069 | test accuracy: 0.970\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0013 | test accuracy: 0.970\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0233 | test accuracy: 0.970\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0010 | test accuracy: 0.970\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0024 | test accuracy: 0.970\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0525 | test accuracy: 0.970\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0021 | test accuracy: 0.966\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0037 | test accuracy: 0.970\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1438 | test accuracy: 0.970\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0038 | test accuracy: 0.970\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3816 | test accuracy: 0.970\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0007 | test accuracy: 0.970\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0087 | test accuracy: 0.970\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0291 | test accuracy: 0.970\n",
            "total time:  66.50603613699968\n",
            "Connected to a GPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9087 | test accuracy: 0.694\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3479 | test accuracy: 0.754\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4646 | test accuracy: 0.754\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.9131 | test accuracy: 0.754\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3852 | test accuracy: 0.781\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3117 | test accuracy: 0.781\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5775 | test accuracy: 0.798\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5097 | test accuracy: 0.832\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.8854 | test accuracy: 0.869\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3583 | test accuracy: 0.875\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2444 | test accuracy: 0.892\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1086 | test accuracy: 0.906\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1123 | test accuracy: 0.929\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2304 | test accuracy: 0.939\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2747 | test accuracy: 0.973\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0542 | test accuracy: 0.970\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0120 | test accuracy: 0.973\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0405 | test accuracy: 0.973\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0252 | test accuracy: 0.973\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0957 | test accuracy: 0.973\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1500 | test accuracy: 0.973\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0965 | test accuracy: 0.970\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2924 | test accuracy: 0.973\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6857 | test accuracy: 0.966\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0090 | test accuracy: 0.970\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0227 | test accuracy: 0.976\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0139 | test accuracy: 0.970\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0111 | test accuracy: 0.973\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0040 | test accuracy: 0.973\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0115 | test accuracy: 0.966\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0045 | test accuracy: 0.966\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0042 | test accuracy: 0.966\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0841 | test accuracy: 0.966\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0058 | test accuracy: 0.966\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0281 | test accuracy: 0.966\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0770 | test accuracy: 0.970\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0114 | test accuracy: 0.970\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2827 | test accuracy: 0.966\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0019 | test accuracy: 0.966\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0065 | test accuracy: 0.970\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0126 | test accuracy: 0.970\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0305 | test accuracy: 0.966\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0093 | test accuracy: 0.966\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1041 | test accuracy: 0.966\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1222 | test accuracy: 0.966\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.3391 | test accuracy: 0.973\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1670 | test accuracy: 0.970\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0034 | test accuracy: 0.970\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0348 | test accuracy: 0.966\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0138 | test accuracy: 0.970\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0018 | test accuracy: 0.970\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0013 | test accuracy: 0.970\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0071 | test accuracy: 0.970\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1275 | test accuracy: 0.970\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0025 | test accuracy: 0.973\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0124 | test accuracy: 0.970\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0092 | test accuracy: 0.970\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0041 | test accuracy: 0.970\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0014 | test accuracy: 0.973\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0055 | test accuracy: 0.973\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0140 | test accuracy: 0.973\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4338 | test accuracy: 0.973\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0018 | test accuracy: 0.970\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0048 | test accuracy: 0.973\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0071 | test accuracy: 0.973\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0413 | test accuracy: 0.973\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0027 | test accuracy: 0.973\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0787 | test accuracy: 0.973\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0009 | test accuracy: 0.970\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0865 | test accuracy: 0.970\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0047 | test accuracy: 0.973\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0099 | test accuracy: 0.973\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0041 | test accuracy: 0.973\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0006 | test accuracy: 0.973\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0589 | test accuracy: 0.973\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0366 | test accuracy: 0.973\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0420 | test accuracy: 0.973\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0037 | test accuracy: 0.973\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1523 | test accuracy: 0.973\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2216 | test accuracy: 0.973\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0036 | test accuracy: 0.973\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0015 | test accuracy: 0.973\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0048 | test accuracy: 0.973\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0006 | test accuracy: 0.973\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0113 | test accuracy: 0.973\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0574 | test accuracy: 0.973\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0053 | test accuracy: 0.973\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0021 | test accuracy: 0.973\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0205 | test accuracy: 0.973\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0014 | test accuracy: 0.973\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0027 | test accuracy: 0.973\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0431 | test accuracy: 0.973\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0019 | test accuracy: 0.973\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0042 | test accuracy: 0.973\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1014 | test accuracy: 0.973\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0021 | test accuracy: 0.973\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3193 | test accuracy: 0.973\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0008 | test accuracy: 0.973\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0076 | test accuracy: 0.973\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0347 | test accuracy: 0.973\n",
            "total time:  67.09413456499988\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.19173932075500488.\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epoch took: 0.3408019542694092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.49038039999348776 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "rnn_arr = rnn_arr1 + rnn_arr2 + rnn_arr3 + rnn_arr4\n",
        "nrnn_arr = nrnn_arr1 + nrnn_arr2 + nrnn_arr3 + nrnn_arr4\n",
        "seed_arr = seed_arr1 + seed_arr2 + seed_arr3 + seed_arr4\n",
        "method_arr = method_arr1 + method_arr2 + method_arr3 + method_arr4\n",
        "specify_arr = specify_arr1 + specify_arr2 + specify_arr3 + specify_arr4\n",
        "\n",
        "df1 = pd.DataFrame(list(zip(rnn_arr, nrnn_arr, seed_arr, method_arr, specify_arr)), columns = [\"RNN Accuracy\", \"NRNN Accuracy\", \"Seed\", \"Method\", \"Specify\"])\n",
        "df1\n",
        "df1.to_csv('out.csv')  "
      ],
      "metadata": {
        "id": "fr3APVezQQ70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df1 = pd.DataFrame(list(zip(rnn_arr1, nrnn_arr1, lstm_arr1, seed_arr1, method_arr1, specify_arr1)))\n",
        "df1"
      ],
      "metadata": {
        "id": "YwLvpjdvVA-L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}