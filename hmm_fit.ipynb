{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hmm-fit.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px3RmzMhFa-A",
        "outputId": "3210657b-96ee-4ad4-f8af-59eea30417fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.7/dist-packages (0.2.7)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.21.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install hmmlearn\n",
        "from hmmlearn import hmm\n",
        "from torch.distributions import uniform\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
        "import datetime as dt\n",
        "import time\n",
        "import random\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Datasets using HMM\n",
        "\n",
        "def random_transmat(n_states):\n",
        "    matrix = np.random.rand(n_states, n_states)\n",
        "    return matrix/matrix.sum(axis=1)[:,None]\n",
        "\n",
        "def random_startprob(n_states):\n",
        "    startprob = np.random.rand(n_states)\n",
        "    return startprob/startprob.sum()\n",
        "\n",
        "def random_means(n_features):\n",
        "    return np.random.randint(5, size=(n_features,n_features))\n",
        "\n",
        "def generate_hmm(n_states, n_features , length):\n",
        "    #GENERATING A MODEL\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = random_startprob(n_states)\n",
        "    model.transmat_ = random_transmat(n_states)\n",
        "\n",
        "    model.means_ = random_means(n_features)\n",
        "    model.covars_ = np.tile(np.identity(n_features), (n_features, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "    dataset=[]\n",
        "    states = []\n",
        "\n",
        "    \n",
        "    X, Z = model.sample(length)\n",
        "    dataset.append(np.array(X))\n",
        "    states.append(Z)\n",
        "\n",
        "    dataset = np.stack(dataset)\n",
        "    \n",
        "    return np.array(X), np.array(Z)\n",
        "\n",
        "def sliding_windows(dataset, Z, window_length = 10):\n",
        "    lis = []\n",
        "    targets=[]\n",
        "\n",
        "    window_length = 10\n",
        "    if len(dataset) >= window_length+1:\n",
        "        for i in range(0, len(dataset)-window_length, 1):\n",
        "\n",
        "            x_window = dataset[i:i+window_length, :]\n",
        "            z_window = Z[i:i+window_length]\n",
        "\n",
        "            lis.append(x_window)\n",
        "            targets.append(z_window[-1])\n",
        "    return np.array(lis), np.array(targets)\n",
        "\n",
        "def generate_time_dependent_flip(length, startprob, transmat):\n",
        "    #GENERATING A MODEL\n",
        "\n",
        "\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = startprob\n",
        "    model.transmat_ = transmat\n",
        "\n",
        "    #this doesn't actually matter for us\n",
        "    model.means_ = np.array([[0.0, 0.0], \n",
        "                             [5.0, 10.0]])\n",
        "    model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "\n",
        "\n",
        "    X, Z = model.sample(length)\n",
        "\n",
        "    \n",
        "    return Z"
      ],
      "metadata": {
        "id": "xpW126KEFfM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Injecting Noise into Labels\n",
        "\n",
        "#Given a flip_mask, flip an input\n",
        "def flip(array, flip_mask):\n",
        "    flipped_array = np.logical_xor(array, flip_mask, out=array)\n",
        "    return flipped_array\n",
        "\n",
        "#Class Independent / Time Independent\n",
        "def flip_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Dependent / Time Independent\n",
        "def flip_labels_class(array, flip_probability_0, flip_probability_1):\n",
        "    flip_mask = []\n",
        "    for elem in array:\n",
        "        if elem == 0:\n",
        "            to_flip = np.random.binomial(1, flip_probability_0, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "        else:\n",
        "            to_flip = np.random.binomial(1, flip_probability_1, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "            \n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Independent / Time Dependent\n",
        "def flip_labels_time(array, startprob, transmat):\n",
        "    flip_mask = generate_time_dependent_flip(len(array), startprob, transmat)[0]\n",
        "\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "\n",
        "#Class Dependent / Time Dependent\n",
        "#This can be achieved by careful design of the transition matrix (transmat)"
      ],
      "metadata": {
        "id": "OgyRemBtNY8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset,Z = generate_hmm(2,3,100)"
      ],
      "metadata": {
        "id": "oW2zggR4NdQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmIkYpDuNgTw",
        "outputId": "8cdd0741-3063-4aa5-df5b-20072a35b335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startprob = random_startprob(2)\n",
        "transmat = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])"
      ],
      "metadata": {
        "id": "PXjUWBM5NjW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                     flip_probability= None, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=None, transmat=None):\n",
        "    \n",
        "    #Generate Data\n",
        "    dataset, states_true = generate_hmm(n_states, n_features , length)\n",
        "    \n",
        "    if method == \"basic\":\n",
        "        states_flipped = (flip_labels_basic(states_true, flip_probability))\n",
        "        \n",
        "    elif method == \"class\":\n",
        "        states_flipped = (flip_labels_class(states_true, flip_probability_0, flip_probability_1))\n",
        "        \n",
        "    elif method == \"time\":\n",
        "        states_flipped = (flip_labels_time(states_true, startprob, transmat))\n",
        "    \n",
        "    #RESHAPE INTO WINDOWS\n",
        "    dataset_windows, states_true = sliding_windows(dataset, states_true, window_length= window_length)\n",
        "    \n",
        "    #RESHAPE INTO WINDOWS\n",
        "    _, states_flipped = sliding_windows(dataset, states_flipped, window_length= window_length)\n",
        "    \n",
        "    x_train = dataset_windows[:int(train_ratio*len(dataset_windows)),:,:]\n",
        "    x_test = dataset_windows[int(train_ratio*len(dataset_windows)):,:,:]\n",
        "    \n",
        "   \n",
        "    #Flip The Labels according to method\n",
        "    \n",
        "    \n",
        "    y_train_true = states_true[:int(train_ratio*len(dataset_windows))]\n",
        "    y_test_true = states_true[int(train_ratio*len(dataset_windows)):]\n",
        "    \n",
        "    y_train_flipped = np.array(states_flipped[:int(train_ratio*len(dataset_windows))])\n",
        "    y_test_flipped = np.array(states_flipped[int(train_ratio*len(dataset_windows)):])\n",
        "    \n",
        "    \n",
        "    return x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped"
      ],
      "metadata": {
        "id": "1q2hceLfNoDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "OphpChUuPAo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, feature_size, n_state, hidden_size=128, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
        "                 seed=random.seed('2021')):\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_state = n_state\n",
        "        self.seed = seed\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.rnn_type = rnn\n",
        "        self.regres = regres\n",
        "        self.return_all = return_all\n",
        "        \n",
        "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "\n",
        "        self.regressor = nn.Sequential(nn.BatchNorm1d(num_features=self.hidden_size),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Dropout(0.3),\n",
        "                                       nn.Linear(self.hidden_size, self.n_state),\n",
        "                                       nn.Softmax(-1))\n",
        "\n",
        "    def forward(self, input, past_state=None, **kwargs):\n",
        "        input = input.to(self.device)\n",
        "        self.rnn.to(self.device)\n",
        "        self.regressor.to(self.device)\n",
        "        if not past_state:\n",
        "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
        "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            all_encodings, encoding = self.rnn(input, past_state)\n",
        "        else:\n",
        "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
        "        \n",
        "        if self.regres:\n",
        "            if not self.return_all:\n",
        "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
        "            else:\n",
        "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
        "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
        "        else:\n",
        "            return encoding.view(encoding.shape[1], -1)\n",
        "        \n",
        "        \n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
        "            \n",
        "def save_ckpt(generator_model, output_dir, data):\n",
        "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
        "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
        "    os.makedirs(check_pt_dir, exist_ok=True)\n",
        "    torch.save(generator_model.state_dict(), fname)\n",
        "    \n",
        "def get_accuracy(model, loader):\n",
        "    \n",
        "    correct, total = 0, 0\n",
        "    for xs, ts in loader:\n",
        "        xs.to(device)\n",
        "        ts.to(device)\n",
        "        zs = model(xs)\n",
        "        \n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        ts = torch.argmax(ts, 1)\n",
        "\n",
        "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
        "        total += int(ts.shape[0])\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "g_bwMeNQSvCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, n_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    history = dict(train=[], val=[])\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
        "    \n",
        "    best_loss = 10000.0\n",
        "    \n",
        "    n=0\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t0 = time.time()\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "        train_losses=[]\n",
        "        model = model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = (time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input= batch[0].to(device)\n",
        "            b_target =  batch[1].to(device)\n",
        "            iters.append(n)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(b_input)\n",
        "\n",
        "            target = torch.argmax(b_target, 1)\n",
        "\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if n % 10 == 0:\n",
        "                iters_sub.append(n)\n",
        "                \n",
        "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
        "                #print(get_accuracy(model, train_dataloader))\n",
        "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "        training_time = (time.time() - t0)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        val_losses = []\n",
        "        model = model.eval()\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "    \n",
        "        history['train'].append(train_loss)\n",
        "        \n",
        "        print(f'Epoch {epoch}: train loss {train_loss} ')\n",
        "    plt.style.use('seaborn-white')\n",
        "    plt.plot(history['train'])\n",
        "\n",
        "    plt.title('LSTM  Training Curves')\n",
        "    plt.ylabel('CE Loss')\n",
        "    plt.xlabel('Epoch Number')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    plt.show()\n",
        "    \n",
        "    return model.eval(), history\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    #INFERENCE ON TEST SET\n",
        "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
        "    \n",
        "    y_test = y_test.numpy()\n",
        "    y_pred = predictions.cpu().numpy()\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    #print(accuracy)\n",
        "    #print(f1)\n",
        "    #print(precision)\n",
        "    return accuracy\n",
        "\n",
        "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
        "  train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train.reshape(-1, 1)))\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "  x_test = torch.flip(torch.tensor(x_test, dtype=torch.float), [1,2])\n",
        "  y_test = torch.tensor(y_test, dtype=torch.int)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = LSTMClassifier(3, #num features \n",
        "                  2, #num classes,\n",
        "                  seed = seed,\n",
        "                  rnn=\"GRU\" #rnn type    \n",
        "            )\n",
        "\n",
        "  model, history = train_model(model, train_loader, 128, 5e-4)\n",
        "  acc = evaluate_model(model, x_test, y_test)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "ZfHKnAlSTDiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f6HsUdvpPGMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='mnist', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.0005, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# get dataset\n",
        "#==============================================================================\n",
        "\n",
        "def driver(seed, train, test, add_noise=0, mult_noise=0, nrnn = False):\n",
        "  \n",
        "  model = NoisyRNN(input_dim=int(3), output_classes=2, n_units=args.n_units, \n",
        "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "                add_noise=add_noise, mult_noise=mult_noise).to(device)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)        \n",
        "  noise = torch.randn(1,693,10,3).float()\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # set random seed to reproduce the work\n",
        "  #==============================================================================\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # Model summary\n",
        "  #==============================================================================\n",
        "  print(model)    \n",
        "  print('**** Setup ****')\n",
        "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "  print('************')    \n",
        "    \n",
        "\n",
        "  if args.optimizer == 'SGD':\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "  elif  args.optimizer == 'Adam':\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  else:\n",
        "      print(\"Unexpected optimizer!\")\n",
        "      raise \n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # training and testing\n",
        "  count = 0\n",
        "  loss_hist = []\n",
        "  test_acc = []\n",
        "\n",
        "  t0 = timeit.default_timer()\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      lossaccum = 0\n",
        "      \n",
        "      for step, (x, y) in enumerate(train):\n",
        "          count += 1\n",
        "          \n",
        "          # Reshape data for recurrent unit\n",
        "          inputs = Variable(x.view(-1, 10, int(3))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "          targets = Variable(y).to(device)\n",
        "\n",
        "                  \n",
        "          # send data to recurrent unit    \n",
        "          output = model(inputs, mode='train')\n",
        "          loss = loss_func(output, targets.long())\n",
        "          \n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()          \n",
        "          \n",
        "          if args.gclip != 0.0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "              \n",
        "          optimizer.step() # update weights\n",
        "          lossaccum += loss.item()\n",
        "\n",
        "          if args.model == 'test':\n",
        "              D = model.W.weight.data.cpu().numpy()  \n",
        "              u, s, v = np.linalg.svd(D, 0)\n",
        "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "      loss_hist.append(lossaccum)    \n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          total_num = 0\n",
        "          for data, target in test: \n",
        "              data, target = data.to(device), target.to(device)               \n",
        "              output = model(data.view(-1, 10, int(3)))                  \n",
        "              \n",
        "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "              #print(output.shape)\n",
        "              #print(pred.shape)\n",
        "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "              total_num += len(data)\n",
        "          \n",
        "          accuracy = correct / total_num\n",
        "          test_acc.append(accuracy)\n",
        "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "      if nrnn == True:\n",
        "          B = model.B.data.cpu().numpy()            \n",
        "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "          A = 0.5 * (A + A.T)\n",
        "          e, _ = np.linalg.eig(A)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "          \n",
        "          C = model.C.data.cpu().numpy()            \n",
        "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "          e, _ = np.linalg.eig(W)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "              \n",
        "              \n",
        "\n",
        "      # schedule learning rate decay    \n",
        "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "  print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "\n",
        "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "  pickle.dump(data,f)\n",
        "  f.close()\n",
        "\n",
        "  return max(test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbqUW26PJZz",
        "outputId": "7b2333df-8451-4c1d-fe46-55fbdbfc9d23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [1, 2, 3]"
      ],
      "metadata": {
        "id": "Of8YS2vvcNL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"No noise\"\n",
        "flip_probability_0 = 0.1\n",
        "flip_probability_1 = 0.2\n",
        "rnn_arr1 = []\n",
        "nrnn_arr1 = []\n",
        "lstm_arr1 = []\n",
        "seed_arr1 = []\n",
        "method_arr1 = []\n",
        "specify_arr1 = []\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method = \"class\", \n",
        "                     flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1, startprob=None, transmat=None)\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_true))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_true))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=25, shuffle=True)\n",
        "\n",
        "for seed in seeds:\n",
        "  acc = driver(seed, train_loader, test_loader)\n",
        "  acc2 = driver(seed, train_loader, test_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "  acc3 = lstm_driver(seed, x_test, y_test_true, x_train, y_train_true)\n",
        "  rnn_arr1.append(acc)\n",
        "  nrnn_arr1.append(acc2)\n",
        "  lstm_arr1.append(acc3)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(None)"
      ],
      "metadata": {
        "id": "0KIIDEL9fsrd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3c4560ab-cd09-40c4-c7ca-81a99616414d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7366 | test accuracy: 0.552\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7792 | test accuracy: 0.552\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6315 | test accuracy: 0.451\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6959 | test accuracy: 0.566\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6724 | test accuracy: 0.481\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7338 | test accuracy: 0.552\n",
            "Epoch:  6 Iteration:  490 | train loss: 1.0005 | test accuracy: 0.552\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5804 | test accuracy: 0.495\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6514 | test accuracy: 0.586\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5423 | test accuracy: 0.582\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6819 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7165 | test accuracy: 0.599\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6393 | test accuracy: 0.616\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5134 | test accuracy: 0.623\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7182 | test accuracy: 0.589\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6860 | test accuracy: 0.623\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6970 | test accuracy: 0.643\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5504 | test accuracy: 0.640\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.4961 | test accuracy: 0.673\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5832 | test accuracy: 0.653\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5676 | test accuracy: 0.704\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.3901 | test accuracy: 0.717\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.9043 | test accuracy: 0.717\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6242 | test accuracy: 0.710\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.8387 | test accuracy: 0.724\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8285 | test accuracy: 0.754\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.8296 | test accuracy: 0.741\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.3207 | test accuracy: 0.727\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.9315 | test accuracy: 0.771\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3998 | test accuracy: 0.764\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.9623 | test accuracy: 0.788\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.8270 | test accuracy: 0.778\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7136 | test accuracy: 0.778\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.3038 | test accuracy: 0.778\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.3230 | test accuracy: 0.778\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5613 | test accuracy: 0.778\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.3936 | test accuracy: 0.768\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6516 | test accuracy: 0.768\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3986 | test accuracy: 0.774\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.3596 | test accuracy: 0.771\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7232 | test accuracy: 0.771\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3485 | test accuracy: 0.774\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.4515 | test accuracy: 0.774\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6260 | test accuracy: 0.774\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.0007 | test accuracy: 0.774\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2849 | test accuracy: 0.774\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3250 | test accuracy: 0.781\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5853 | test accuracy: 0.774\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.4667 | test accuracy: 0.778\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5581 | test accuracy: 0.781\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2563 | test accuracy: 0.781\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6365 | test accuracy: 0.778\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7589 | test accuracy: 0.785\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6471 | test accuracy: 0.781\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.9285 | test accuracy: 0.788\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2859 | test accuracy: 0.774\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5610 | test accuracy: 0.788\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3714 | test accuracy: 0.785\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.3514 | test accuracy: 0.788\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.3477 | test accuracy: 0.785\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3824 | test accuracy: 0.785\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7477 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6440 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2856 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7534 | test accuracy: 0.788\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6015 | test accuracy: 0.781\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3926 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5474 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5803 | test accuracy: 0.785\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.8650 | test accuracy: 0.788\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5146 | test accuracy: 0.788\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.8929 | test accuracy: 0.785\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.4085 | test accuracy: 0.788\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.9063 | test accuracy: 0.785\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.2591 | test accuracy: 0.788\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6450 | test accuracy: 0.785\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8452 | test accuracy: 0.785\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3203 | test accuracy: 0.788\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2951 | test accuracy: 0.788\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.4321 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4416 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.3019 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3092 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6689 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.9060 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4091 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.3071 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2890 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.5328 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 1.0549 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.5052 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.9445 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.3262 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.3622 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4194 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2194 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2841 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.5127 | test accuracy: 0.795\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2378 | test accuracy: 0.795\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6683 | test accuracy: 0.795\n",
            "total time:  46.237280844\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9217 | test accuracy: 0.545\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7257 | test accuracy: 0.465\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4944 | test accuracy: 0.461\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7769 | test accuracy: 0.559\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7418 | test accuracy: 0.576\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7993 | test accuracy: 0.576\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7576 | test accuracy: 0.559\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8234 | test accuracy: 0.569\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5958 | test accuracy: 0.609\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6588 | test accuracy: 0.572\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.9216 | test accuracy: 0.593\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6712 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5633 | test accuracy: 0.582\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5628 | test accuracy: 0.620\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5451 | test accuracy: 0.599\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.9254 | test accuracy: 0.616\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.8168 | test accuracy: 0.609\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8852 | test accuracy: 0.623\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5293 | test accuracy: 0.626\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6325 | test accuracy: 0.643\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6056 | test accuracy: 0.640\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7307 | test accuracy: 0.663\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.8607 | test accuracy: 0.663\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7392 | test accuracy: 0.613\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6485 | test accuracy: 0.616\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7684 | test accuracy: 0.640\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4725 | test accuracy: 0.626\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7048 | test accuracy: 0.636\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.0054 | test accuracy: 0.653\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3408 | test accuracy: 0.643\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.9395 | test accuracy: 0.690\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7766 | test accuracy: 0.690\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6747 | test accuracy: 0.684\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.9152 | test accuracy: 0.697\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5449 | test accuracy: 0.694\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6526 | test accuracy: 0.690\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5265 | test accuracy: 0.697\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6756 | test accuracy: 0.694\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4399 | test accuracy: 0.700\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7900 | test accuracy: 0.690\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.4872 | test accuracy: 0.700\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7629 | test accuracy: 0.700\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.5735 | test accuracy: 0.700\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.5831 | test accuracy: 0.704\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7834 | test accuracy: 0.697\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.9970 | test accuracy: 0.710\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7327 | test accuracy: 0.704\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6646 | test accuracy: 0.700\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5058 | test accuracy: 0.694\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5356 | test accuracy: 0.700\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7675 | test accuracy: 0.707\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.4683 | test accuracy: 0.710\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5902 | test accuracy: 0.707\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6312 | test accuracy: 0.700\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5143 | test accuracy: 0.714\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0341 | test accuracy: 0.717\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5105 | test accuracy: 0.714\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7583 | test accuracy: 0.714\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6892 | test accuracy: 0.707\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4674 | test accuracy: 0.707\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.5957 | test accuracy: 0.707\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4960 | test accuracy: 0.714\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.5915 | test accuracy: 0.717\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7694 | test accuracy: 0.714\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.3930 | test accuracy: 0.721\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7982 | test accuracy: 0.721\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7167 | test accuracy: 0.721\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7752 | test accuracy: 0.721\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7130 | test accuracy: 0.721\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7090 | test accuracy: 0.724\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5244 | test accuracy: 0.721\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.5107 | test accuracy: 0.724\n",
            "Epoch:  72 Iteration:  5110 | train loss: 1.0005 | test accuracy: 0.717\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.5242 | test accuracy: 0.724\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6334 | test accuracy: 0.721\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.4734 | test accuracy: 0.717\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.5253 | test accuracy: 0.717\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7904 | test accuracy: 0.717\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.4957 | test accuracy: 0.717\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.5950 | test accuracy: 0.717\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.9681 | test accuracy: 0.731\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6397 | test accuracy: 0.724\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2879 | test accuracy: 0.717\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.4185 | test accuracy: 0.717\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6266 | test accuracy: 0.724\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7044 | test accuracy: 0.721\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4424 | test accuracy: 0.724\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.9665 | test accuracy: 0.724\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.5838 | test accuracy: 0.727\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4474 | test accuracy: 0.731\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.8884 | test accuracy: 0.737\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6618 | test accuracy: 0.724\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7388 | test accuracy: 0.727\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6602 | test accuracy: 0.731\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6913 | test accuracy: 0.734\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4703 | test accuracy: 0.744\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7228 | test accuracy: 0.741\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6143 | test accuracy: 0.741\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7778 | test accuracy: 0.734\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.5223 | test accuracy: 0.741\n",
            "total time:  35.63641827000038\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2764928340911865.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0.47784948348999023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5205185677324022 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25468897819519043.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.4372236728668213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.444404302750315 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25551414489746094.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.436948299407959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.3976813354662487 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2608640193939209.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4390392303466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3734282697950091 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24406170845031738.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4224848747253418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.360676571726799 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26293444633483887.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44281673431396484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34961468279361724 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24866342544555664.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42478060722351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34265370666980743 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25922322273254395.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45160841941833496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3384058730942862 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25910305976867676.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43932271003723145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3341737359762192 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544591426849365.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42765259742736816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3317730188369751 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534348964691162.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42085862159729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3288838343960898 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24579238891601562.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42340564727783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32671175045626505 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26340484619140625.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4488508701324463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3257338792085648 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24983954429626465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4286041259765625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32356148191860745 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504570484161377.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4359457492828369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32370694535119193 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2522127628326416.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4320065975189209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3215467733996255 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2492992877960205.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43004608154296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32065289744309017 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609848976135254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43601226806640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32038676057543075 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2445077896118164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4222583770751953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31968233159610204 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26578688621520996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44651055335998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.319378895844732 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25846219062805176.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4456498622894287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31841460040637426 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646493911743164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45354723930358887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31803010957581657 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24585819244384766.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43010854721069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3178189107349941 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27711057662963867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46079015731811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31755917753492086 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637298107147217.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4443168640136719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.317048561998776 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24931120872497559.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44190430641174316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3170071120773043 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623610496520996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4474046230316162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31670277033533367 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26114583015441895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4387969970703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31655164744172776 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531294822692871.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4413797855377197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3160689158099038 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25411343574523926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.431199312210083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3157579439026969 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24793124198913574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4277617931365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31566617574010575 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2729043960571289.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4424617290496826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3156540551355907 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24466657638549805.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.427016019821167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31549969272954126 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721703052520752.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4474976062774658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3152400834219796 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2403576374053955.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41786670684814453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3152235265289034 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2495429515838623.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43332743644714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3150542621101652 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24444866180419922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42836475372314453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31508417640413555 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25768065452575684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487419128417969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31487988361290525 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25342464447021484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4404163360595703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31478075512817927 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24544143676757812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4297938346862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3148353674582073 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607297897338867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4346904754638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31481345551354545 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24646258354187012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4310598373413086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3147285435880933 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24382400512695312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42824816703796387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31454747659819465 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503988742828369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4400177001953125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3144909015723637 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25559091567993164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4326317310333252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3145034372806549 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26639509201049805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4752488136291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31441408693790435 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27387452125549316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4635136127471924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31439761647156306 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2585785388946533.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43628382682800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3143714542899813 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26493096351623535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45444655418395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31439700637544904 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25531864166259766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44644618034362793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3142275320632117 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26096248626708984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43965721130371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31414680140359064 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536299228668213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43662071228027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3142453602382115 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24365448951721191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4151139259338379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3142166376113892 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24727892875671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42452502250671387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31420698165893557 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512342929840088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42625856399536133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3141396032912391 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25924158096313477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44392967224121094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31406770476273127 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250943660736084.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44269585609436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140624114445278 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25894689559936523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44138193130493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31398129803793773 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565925121307373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42931485176086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140054157802037 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.8199074268341064.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 1.0030806064605713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31405180096626284 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3108820915222168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.9560456275939941\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138501205614635 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3026869297027588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49410176277160645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138832858630589 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.8086309432983398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.9935636520385742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138449170759746 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26888394355773926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44377565383911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31386861077376776 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26219630241394043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44930386543273926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138647024120603 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2608063220977783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44376564025878906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31381685180323465 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523953914642334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4443526268005371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3138263911008835 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27283215522766113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4594566822052002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31377732498305183 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2526581287384033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4322245121002197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31383878205503735 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26346707344055176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4373915195465088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31387648412159513 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2486097812652588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42508602142333984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137699442250388 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575833797454834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44582462310791016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137711720807212 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2549703121185303.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4290616512298584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31370350675923486 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24277091026306152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4199070930480957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3137448902641024 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516365051269531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4410548210144043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136850667851312 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25527310371398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4316272735595703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31367068759032657 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27136707305908203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4511227607727051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136639390672956 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548553943634033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323246479034424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136778026819229 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27553486824035645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46492695808410645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136367861713682 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24959158897399902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4227466583251953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136610201426915 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539241313934326.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44955968856811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136789355959211 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572672367095947.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43840694427490234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31364590610776627 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24442434310913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4132351875305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136722620044436 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26473259925842285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4431040287017822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136077080454145 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24897980690002441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4273824691772461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.313610965864999 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2782406806945801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47103190422058105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136074036359787 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25435805320739746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44077491760253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31359865708010537 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27953433990478516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4765944480895996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136119991540909 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618587017059326.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4458444118499756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135858505964279 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596931457519531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46427226066589355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135453177349908 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512791156768799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4304964542388916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31358868479728697 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25160813331604004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43215513229370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31354509166308814 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2549784183502197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4296760559082031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135355170283999 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23968791961669922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42658329010009766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135445880038398 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603332996368408.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45171499252319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135001970188958 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2530856132507324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4383087158203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.313510337471962 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2451925277709961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4345080852508545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135042348078319 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25300002098083496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43213319778442383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135319347892489 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23536038398742676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4091489315032959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135030278137752 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25383615493774414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4319272041320801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135232640164239 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24995946884155273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4248237609863281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135224474327905 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25024890899658203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.435469388961792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31351011778627125 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24112796783447266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.40842700004577637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31351436291422163 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23947906494140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42987704277038574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134880721569061 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561345100402832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44252586364746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134868643113545 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23778486251831055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4126467704772949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.313492528455598 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607133388519287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43613171577453613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31348611627306255 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24624133110046387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42397427558898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31348014644214084 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26111388206481934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4513545036315918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31345747794423784 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24222922325134277.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4153709411621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31347179583140783 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26821327209472656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46362972259521484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134472842727389 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25629663467407227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4326605796813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134844205209187 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562077045440674.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.444504976272583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134590187243053 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2767641544342041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45287609100341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31346155575343543 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2451915740966797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4276468753814697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31344439770494187 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2478041648864746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4338419437408447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134366754974638 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.243208646774292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4256131649017334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134392201900482 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575507164001465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4346122741699219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134792523724692 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521176338195801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42610883712768555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31342943012714386 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23988699913024902.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4093790054321289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31343033143452237 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23835110664367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.421550989151001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343028332505907 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509794235229492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323580265045166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134337902069092 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2444915771484375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4164102077484131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31342089814799173 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24747371673583984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41663122177124023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134212212903159 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2397294044494629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4254879951477051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31344068092959265 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561466693878174.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43014025688171387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134304357426507 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24626708030700684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42987585067749023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134130431073053 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24549055099487305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4245309829711914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31342983160700116 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6630 | test accuracy: 0.552\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7012 | test accuracy: 0.556\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6221 | test accuracy: 0.552\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5998 | test accuracy: 0.589\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5939 | test accuracy: 0.582\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6509 | test accuracy: 0.596\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5931 | test accuracy: 0.613\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5724 | test accuracy: 0.572\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6826 | test accuracy: 0.623\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7159 | test accuracy: 0.633\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7600 | test accuracy: 0.650\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6963 | test accuracy: 0.646\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.4968 | test accuracy: 0.660\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6440 | test accuracy: 0.690\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6985 | test accuracy: 0.697\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5649 | test accuracy: 0.697\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4175 | test accuracy: 0.663\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.3694 | test accuracy: 0.741\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2352 | test accuracy: 0.721\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2789 | test accuracy: 0.754\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3393 | test accuracy: 0.768\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.3850 | test accuracy: 0.734\n",
            "Epoch:  22 Iteration:  1610 | train loss: 1.0020 | test accuracy: 0.761\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6639 | test accuracy: 0.788\n",
            "Epoch:  24 Iteration:  1750 | train loss: 1.1390 | test accuracy: 0.788\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7139 | test accuracy: 0.798\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.8668 | test accuracy: 0.811\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.4670 | test accuracy: 0.811\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.3665 | test accuracy: 0.808\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2693 | test accuracy: 0.808\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.3646 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6808 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6160 | test accuracy: 0.808\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5706 | test accuracy: 0.808\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5271 | test accuracy: 0.808\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3133 | test accuracy: 0.808\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5262 | test accuracy: 0.808\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2566 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7155 | test accuracy: 0.808\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7121 | test accuracy: 0.808\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2562 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1600 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2651 | test accuracy: 0.801\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1973 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2403 | test accuracy: 0.811\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2296 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7330 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3074 | test accuracy: 0.808\n",
            "Epoch:  48 Iteration:  3430 | train loss: 1.3277 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8988 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6236 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.4613 | test accuracy: 0.808\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6837 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4308 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2177 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.9984 | test accuracy: 0.808\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6969 | test accuracy: 0.808\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.4717 | test accuracy: 0.808\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1887 | test accuracy: 0.808\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5498 | test accuracy: 0.801\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.0720 | test accuracy: 0.808\n",
            "Epoch:  61 Iteration:  4340 | train loss: 1.0770 | test accuracy: 0.808\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.9839 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.8933 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 1.1991 | test accuracy: 0.808\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2639 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.4810 | test accuracy: 0.808\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.4450 | test accuracy: 0.808\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1757 | test accuracy: 0.808\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5690 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1569 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3187 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.9715 | test accuracy: 0.808\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8825 | test accuracy: 0.811\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4118 | test accuracy: 0.808\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2846 | test accuracy: 0.811\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7652 | test accuracy: 0.808\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2273 | test accuracy: 0.811\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5948 | test accuracy: 0.808\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1842 | test accuracy: 0.811\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3613 | test accuracy: 0.811\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.9343 | test accuracy: 0.811\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7375 | test accuracy: 0.808\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.5578 | test accuracy: 0.808\n",
            "Epoch:  84 Iteration:  5950 | train loss: 1.3818 | test accuracy: 0.811\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2984 | test accuracy: 0.811\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.5380 | test accuracy: 0.808\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2190 | test accuracy: 0.811\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1514 | test accuracy: 0.811\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2547 | test accuracy: 0.811\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7959 | test accuracy: 0.811\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.3504 | test accuracy: 0.811\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.5276 | test accuracy: 0.811\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1734 | test accuracy: 0.811\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.5813 | test accuracy: 0.811\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.5493 | test accuracy: 0.811\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3027 | test accuracy: 0.811\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2205 | test accuracy: 0.811\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2519 | test accuracy: 0.811\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.3499 | test accuracy: 0.811\n",
            "total time:  34.46579043799966\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5762 | test accuracy: 0.552\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7149 | test accuracy: 0.552\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7280 | test accuracy: 0.579\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6229 | test accuracy: 0.552\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5547 | test accuracy: 0.552\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8130 | test accuracy: 0.596\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5534 | test accuracy: 0.579\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6547 | test accuracy: 0.606\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7540 | test accuracy: 0.613\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.8522 | test accuracy: 0.582\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5048 | test accuracy: 0.599\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6370 | test accuracy: 0.609\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5341 | test accuracy: 0.579\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5037 | test accuracy: 0.582\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6333 | test accuracy: 0.599\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.9441 | test accuracy: 0.643\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5795 | test accuracy: 0.589\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8485 | test accuracy: 0.606\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.4476 | test accuracy: 0.667\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7694 | test accuracy: 0.687\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6346 | test accuracy: 0.680\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7480 | test accuracy: 0.626\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.8349 | test accuracy: 0.687\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5308 | test accuracy: 0.714\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6130 | test accuracy: 0.694\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7874 | test accuracy: 0.721\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4851 | test accuracy: 0.721\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6063 | test accuracy: 0.731\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6551 | test accuracy: 0.741\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.4246 | test accuracy: 0.737\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.8100 | test accuracy: 0.761\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5498 | test accuracy: 0.761\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8950 | test accuracy: 0.761\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.4261 | test accuracy: 0.761\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5715 | test accuracy: 0.764\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7654 | test accuracy: 0.761\n",
            "Epoch:  36 Iteration:  2590 | train loss: 1.0592 | test accuracy: 0.764\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6177 | test accuracy: 0.764\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6636 | test accuracy: 0.768\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.3942 | test accuracy: 0.764\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6815 | test accuracy: 0.768\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6202 | test accuracy: 0.771\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6021 | test accuracy: 0.771\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4792 | test accuracy: 0.774\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6076 | test accuracy: 0.771\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.8549 | test accuracy: 0.774\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4078 | test accuracy: 0.774\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3506 | test accuracy: 0.774\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.4659 | test accuracy: 0.774\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.4434 | test accuracy: 0.774\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.4168 | test accuracy: 0.778\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6900 | test accuracy: 0.778\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.8181 | test accuracy: 0.778\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.3377 | test accuracy: 0.778\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2988 | test accuracy: 0.781\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6761 | test accuracy: 0.785\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.3878 | test accuracy: 0.781\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7198 | test accuracy: 0.785\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.3898 | test accuracy: 0.785\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4598 | test accuracy: 0.785\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3418 | test accuracy: 0.785\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4210 | test accuracy: 0.788\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.4569 | test accuracy: 0.788\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5755 | test accuracy: 0.788\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2882 | test accuracy: 0.788\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5898 | test accuracy: 0.785\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2729 | test accuracy: 0.795\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3770 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5640 | test accuracy: 0.798\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3579 | test accuracy: 0.781\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.4520 | test accuracy: 0.785\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3953 | test accuracy: 0.798\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.4406 | test accuracy: 0.795\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.4845 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5946 | test accuracy: 0.795\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6667 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.5151 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.9768 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5719 | test accuracy: 0.795\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3707 | test accuracy: 0.795\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2882 | test accuracy: 0.795\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.8275 | test accuracy: 0.795\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5641 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.8535 | test accuracy: 0.795\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2514 | test accuracy: 0.795\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.5655 | test accuracy: 0.795\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4293 | test accuracy: 0.795\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.4337 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.5815 | test accuracy: 0.795\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7000 | test accuracy: 0.795\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6905 | test accuracy: 0.795\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7767 | test accuracy: 0.795\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7445 | test accuracy: 0.801\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.3538 | test accuracy: 0.798\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4392 | test accuracy: 0.798\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7202 | test accuracy: 0.801\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6632 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.8848 | test accuracy: 0.798\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6554 | test accuracy: 0.798\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.3089 | test accuracy: 0.801\n",
            "total time:  35.57552973100019\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25472307205200195.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.4483480453491211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6816773959568568 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26199889183044434.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.44258618354797363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5492410578898022 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671043872833252.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.44973254203796387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4636094697884151 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27086591720581055.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4521491527557373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4215295902320317 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25327467918395996.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.44771552085876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38906737055097307 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27652621269226074.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.46466779708862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3743439372096743 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694211006164551.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.458066463470459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36223571598529813 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27451014518737793.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.47202515602111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3521195126431329 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25977182388305664.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.46247410774230957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3463151948792594 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.4126322269439697.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.6090888977050781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34218075828892847 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26874876022338867.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4519166946411133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33805148899555204 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26784610748291016.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46489691734313965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3353855137314115 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678337097167969.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44438695907592773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3316162888492857 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2656979560852051.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.45586633682250977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.328943566765104 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2640204429626465.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43701624870300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3282700253384454 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250568151473999.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4315769672393799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3260907288108553 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266404390335083.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4401736259460449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32503454855510167 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25701475143432617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4328327178955078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3242034937654223 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630155086517334.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4499068260192871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32288630519594463 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24655771255493164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4214167594909668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3223067875419344 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25884008407592773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44922876358032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32115807703563143 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678978443145752.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44746923446655273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3212969937494823 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26524877548217773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4461660385131836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3204194771392005 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682483196258545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45549750328063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31941280790737697 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682332992553711.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45990777015686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31901164948940275 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583630084991455.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44669198989868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31870668062141966 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2841453552246094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49639344215393066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3185739368200302 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269939661026001.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4683971405029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3181734838656017 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3949427604675293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5799188613891602\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31769431063107084 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591738700866699.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.564563512802124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31783407500811983 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651536464691162.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45351576805114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3169872100864138 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25455760955810547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44260096549987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3169821377311434 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562832832336426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44699883460998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3168755863394056 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26843762397766113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4449291229248047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3168862121445792 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26361966133117676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4543774127960205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31638156856809346 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2695152759552002.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45248937606811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3163192787340709 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2683396339416504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45322513580322266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3158824503421783 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25139713287353516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4317007064819336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3158076243741172 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25229859352111816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4339029788970947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31586450934410093 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.256878137588501.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43761229515075684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3155754655599594 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523484230041504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4387331008911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31565999048096793 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25696372985839844.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43936657905578613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31550603083201817 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24396777153015137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41468095779418945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3151600535426821 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25612854957580566.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4442262649536133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3151345602103642 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26896095275878906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4429631233215332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31526430376938414 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592926025390625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4560556411743164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31510336995124816 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662088871002197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46059465408325195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3148160253252302 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24761629104614258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42247533798217773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3149113833904266 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28084611892700195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47298693656921387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31467549162251607 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24335360527038574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4121527671813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31468751345361984 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25437211990356445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4325077533721924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146838984319142 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26071953773498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.447033166885376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3145639938967569 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24709725379943848.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4436070919036865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3145881290946688 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25462937355041504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43590474128723145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144346190350396 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24451208114624023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4285440444946289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3144374821867262 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643415927886963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45691466331481934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3144009671040944 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24094724655151367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4085996150970459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31439707194055827 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25141310691833496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43035078048706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.314249747140067 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26633262634277344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4416770935058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142193828310285 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24726629257202148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.438185453414917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31423123691763194 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25678062438964844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4338874816894531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31419613105910166 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24785852432250977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4234127998352051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31415456788880486 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26963019371032715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4484903812408447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141666382551193 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25063538551330566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42397022247314453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140207495008196 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568018436431885.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44344115257263184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31414725610188077 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25652217864990234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4314231872558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3141728652375085 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25382089614868164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45061206817626953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139506838151387 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2635922431945801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44989657402038574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31394862064293455 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25753283500671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46250176429748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31396539466721674 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26390671730041504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44786763191223145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138973172221865 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25641489028930664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45421528816223145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31391672577176777 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2906229496002197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49211573600769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31387913141931806 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671654224395752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45828890800476074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31383841165474485 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27734899520874023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46666836738586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138264609234674 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25946831703186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4392414093017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31384557996477397 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2687392234802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4721381664276123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31395466242517744 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610478401184082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.445019006729126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3138329769883837 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628185749053955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45740509033203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31380631838526046 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631852626800537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4533114433288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31374184787273407 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25591516494750977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4399290084838867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.313763456259455 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2802255153656006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4699869155883789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137607114655631 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597842216491699.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.443835973739624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137156592948096 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2747960090637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45630407333374023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136627848659243 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2431046962738037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41930437088012695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137840113469533 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2599794864654541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4501626491546631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136811933347157 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2624385356903076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372069835662842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3137255289724895 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24753761291503906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.435624361038208\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136903213603156 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2706279754638672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4489619731903076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136853882244655 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607150077819824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44480037689208984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.313660272530147 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619001865386963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.441425085067749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.313697532245091 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2537057399749756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43443775177001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136481681040355 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628016471862793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45807385444641113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136728865759713 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24171876907348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.425429105758667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31367243613515583 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2787511348724365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47744202613830566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136353658778327 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2660210132598877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4516439437866211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136131397315434 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572486400604248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45720767974853516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135751792362758 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2740752696990967.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4571549892425537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358978876045773 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2660558223724365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48439764976501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31360337606498173 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27701497077941895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46356868743896484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31359691534723555 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25274109840393066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44292712211608887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135823007140841 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2697176933288574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47330355644226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135887635605676 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26068925857543945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4478902816772461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31357205297265733 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2634913921356201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4588441848754883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31361644991806575 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26479578018188477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4450247287750244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135497676474707 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25775909423828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43062448501586914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.313562730380467 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25594472885131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4342679977416992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135407762868064 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532222270965576.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4462454319000244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135225491864341 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671487331390381.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4463160037994385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135085791349411 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25092506408691406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4235658645629883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135267483336585 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24374604225158691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4427154064178467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134968429803848 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504889965057373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4415600299835205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135363199881145 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27282166481018066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4559614658355713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31353678831032344 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2810845375061035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47321200370788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135023785488946 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2810940742492676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4855654239654541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134959489107132 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27108311653137207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4566628932952881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135017105511257 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2851259708404541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4721395969390869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31349202053887504 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688453197479248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44704580307006836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134831641401563 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25728845596313477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.436140775680542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31345957858221873 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2672109603881836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45786213874816895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134806845869337 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25401735305786133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323112964630127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134924258504595 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2567620277404785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4381237030029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134782637868609 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2589397430419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4366898536682129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134686078344073 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27098512649536133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4601123332977295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31348115588937486 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26015591621398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43351221084594727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344297996589116 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571561336517334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4452683925628662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313467909182821 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2502880096435547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43148350715637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134694380419595 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595527172088623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44120144844055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31344492392880574 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25017428398132324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4294564723968506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134596479790551 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6662 | test accuracy: 0.556\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8003 | test accuracy: 0.549\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6914 | test accuracy: 0.545\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6973 | test accuracy: 0.569\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7807 | test accuracy: 0.556\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5814 | test accuracy: 0.589\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6704 | test accuracy: 0.606\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8434 | test accuracy: 0.589\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6053 | test accuracy: 0.599\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7492 | test accuracy: 0.620\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6839 | test accuracy: 0.623\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6956 | test accuracy: 0.599\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6780 | test accuracy: 0.640\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5034 | test accuracy: 0.640\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7129 | test accuracy: 0.636\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7373 | test accuracy: 0.593\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6431 | test accuracy: 0.640\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.9190 | test accuracy: 0.626\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5441 | test accuracy: 0.640\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6211 | test accuracy: 0.690\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6954 | test accuracy: 0.673\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5856 | test accuracy: 0.684\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5904 | test accuracy: 0.714\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5576 | test accuracy: 0.650\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2403 | test accuracy: 0.721\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.4895 | test accuracy: 0.727\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.3523 | test accuracy: 0.734\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2949 | test accuracy: 0.741\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.0534 | test accuracy: 0.734\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7149 | test accuracy: 0.751\n",
            "Epoch:  30 Iteration:  2170 | train loss: 1.1218 | test accuracy: 0.778\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2745 | test accuracy: 0.764\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.4740 | test accuracy: 0.758\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2655 | test accuracy: 0.761\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.4262 | test accuracy: 0.758\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7598 | test accuracy: 0.761\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5154 | test accuracy: 0.761\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.3111 | test accuracy: 0.758\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4632 | test accuracy: 0.758\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.3817 | test accuracy: 0.761\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.4456 | test accuracy: 0.764\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4885 | test accuracy: 0.758\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.5600 | test accuracy: 0.764\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4151 | test accuracy: 0.764\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5096 | test accuracy: 0.761\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4731 | test accuracy: 0.764\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4729 | test accuracy: 0.764\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7902 | test accuracy: 0.761\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2828 | test accuracy: 0.768\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2732 | test accuracy: 0.771\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5006 | test accuracy: 0.771\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2586 | test accuracy: 0.771\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.9145 | test accuracy: 0.778\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4159 | test accuracy: 0.771\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7635 | test accuracy: 0.774\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.4364 | test accuracy: 0.774\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.9554 | test accuracy: 0.774\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2647 | test accuracy: 0.778\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.4521 | test accuracy: 0.768\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4247 | test accuracy: 0.771\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3571 | test accuracy: 0.771\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4973 | test accuracy: 0.778\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7943 | test accuracy: 0.778\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2969 | test accuracy: 0.788\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2709 | test accuracy: 0.778\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6081 | test accuracy: 0.788\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.4079 | test accuracy: 0.788\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7958 | test accuracy: 0.781\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.8213 | test accuracy: 0.788\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5827 | test accuracy: 0.785\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5733 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3993 | test accuracy: 0.788\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2753 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8055 | test accuracy: 0.788\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.3263 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6843 | test accuracy: 0.788\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.4432 | test accuracy: 0.788\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1789 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8524 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.4219 | test accuracy: 0.788\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7393 | test accuracy: 0.788\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.4156 | test accuracy: 0.788\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3453 | test accuracy: 0.788\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.3215 | test accuracy: 0.788\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.5940 | test accuracy: 0.788\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.8721 | test accuracy: 0.788\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6761 | test accuracy: 0.788\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.4090 | test accuracy: 0.788\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.5202 | test accuracy: 0.788\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2557 | test accuracy: 0.788\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.3914 | test accuracy: 0.798\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.4739 | test accuracy: 0.788\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1328 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7404 | test accuracy: 0.788\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1495 | test accuracy: 0.801\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4175 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3856 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2579 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.3823 | test accuracy: 0.795\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.4386 | test accuracy: 0.791\n",
            "total time:  31.54150806000007\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4706 | test accuracy: 0.549\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.9146 | test accuracy: 0.542\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5543 | test accuracy: 0.556\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5983 | test accuracy: 0.552\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6241 | test accuracy: 0.532\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5897 | test accuracy: 0.505\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8696 | test accuracy: 0.559\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5807 | test accuracy: 0.589\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5608 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7252 | test accuracy: 0.556\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8792 | test accuracy: 0.579\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6218 | test accuracy: 0.556\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7008 | test accuracy: 0.616\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5539 | test accuracy: 0.556\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5831 | test accuracy: 0.636\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6272 | test accuracy: 0.620\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6329 | test accuracy: 0.626\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7682 | test accuracy: 0.626\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.8244 | test accuracy: 0.620\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9470 | test accuracy: 0.576\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.8611 | test accuracy: 0.569\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7128 | test accuracy: 0.640\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6006 | test accuracy: 0.620\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7418 | test accuracy: 0.630\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5938 | test accuracy: 0.643\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6513 | test accuracy: 0.694\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.5063 | test accuracy: 0.673\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6043 | test accuracy: 0.721\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.8361 | test accuracy: 0.714\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6260 | test accuracy: 0.667\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.5591 | test accuracy: 0.663\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 1.0617 | test accuracy: 0.680\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6049 | test accuracy: 0.687\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7458 | test accuracy: 0.687\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5353 | test accuracy: 0.687\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5561 | test accuracy: 0.684\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6765 | test accuracy: 0.697\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6802 | test accuracy: 0.690\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6428 | test accuracy: 0.690\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6926 | test accuracy: 0.694\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.5398 | test accuracy: 0.687\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5710 | test accuracy: 0.707\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.4441 | test accuracy: 0.690\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3338 | test accuracy: 0.694\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6581 | test accuracy: 0.700\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4702 | test accuracy: 0.700\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3376 | test accuracy: 0.700\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5629 | test accuracy: 0.704\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7810 | test accuracy: 0.710\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.4850 | test accuracy: 0.710\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5222 | test accuracy: 0.710\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.3915 | test accuracy: 0.714\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5832 | test accuracy: 0.707\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.5352 | test accuracy: 0.707\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5593 | test accuracy: 0.710\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6988 | test accuracy: 0.707\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5837 | test accuracy: 0.714\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.5355 | test accuracy: 0.707\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.5843 | test accuracy: 0.707\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4139 | test accuracy: 0.714\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.4551 | test accuracy: 0.710\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7471 | test accuracy: 0.714\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7538 | test accuracy: 0.710\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6351 | test accuracy: 0.707\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.4820 | test accuracy: 0.714\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.4588 | test accuracy: 0.717\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.4105 | test accuracy: 0.710\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5533 | test accuracy: 0.714\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7309 | test accuracy: 0.717\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.8134 | test accuracy: 0.721\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5868 | test accuracy: 0.717\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6971 | test accuracy: 0.717\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6493 | test accuracy: 0.717\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6169 | test accuracy: 0.727\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6518 | test accuracy: 0.724\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.3821 | test accuracy: 0.727\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3746 | test accuracy: 0.721\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6165 | test accuracy: 0.721\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3893 | test accuracy: 0.727\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.8777 | test accuracy: 0.721\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3775 | test accuracy: 0.721\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.8703 | test accuracy: 0.721\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7877 | test accuracy: 0.721\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7282 | test accuracy: 0.721\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.4776 | test accuracy: 0.734\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.3844 | test accuracy: 0.734\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4558 | test accuracy: 0.727\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.8287 | test accuracy: 0.727\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.5122 | test accuracy: 0.731\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.3919 | test accuracy: 0.731\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.8479 | test accuracy: 0.731\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7768 | test accuracy: 0.734\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7284 | test accuracy: 0.747\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.3357 | test accuracy: 0.731\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.5758 | test accuracy: 0.737\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2867 | test accuracy: 0.737\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.5249 | test accuracy: 0.734\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3435 | test accuracy: 0.737\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.3340 | test accuracy: 0.744\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.4179 | test accuracy: 0.741\n",
            "total time:  35.47132039100006\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26113295555114746.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.44660139083862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7019835489136832 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595844268798828.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.43621253967285156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5570267579385213 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548999786376953.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4528183937072754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.47047303659575324 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24295806884765625.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4142305850982666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.42318018249103 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598459720611572.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.4386584758758545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39372331031731195 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544975280761719.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.43146276473999023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37491007660116465 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25255727767944336.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4308454990386963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3637005750622068 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2551746368408203.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.43799829483032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35612716036183495 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24209952354431152.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4157528877258301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34766861924103326 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2556798458099365.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45443058013916016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34234365820884705 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2517998218536377.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4324343204498291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33871732311589375 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578315734863281.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4376945495605469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3350318789482117 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26354146003723145.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44495224952697754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33302287672247205 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26810789108276367.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4591493606567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32994155968938554 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25484371185302734.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4273064136505127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.328260971392904 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667064666748047.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4731760025024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.326221182516643 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647058963775635.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4633064270019531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3254553488322667 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2533583641052246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43590235710144043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3247766362769263 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694230079650879.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4708828926086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32321657623563493 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26157689094543457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45276927947998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3219548110451017 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26392102241516113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4480738639831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3214133756501334 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615699768066406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44322824478149414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32109754170690263 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563810348510742.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4318726062774658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3202998906373978 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26193904876708984.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4480617046356201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31992069610527585 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583029270172119.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4480757713317871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3195714384317398 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24658465385437012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4311258792877197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3188567723546709 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24657082557678223.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43154120445251465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.318779547725405 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590203285217285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43765687942504883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.318047377041408 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25357913970947266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43158817291259766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31740342038018365 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24845504760742188.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4250800609588623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31757847539016176 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26069021224975586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.436328649520874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.317573583977563 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24599409103393555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.442401647567749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3172504987035479 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2553896903991699.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4467740058898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3168030317340578 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615172863006592.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4465365409851074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3165980075086866 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250307559967041.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43390870094299316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3162290407078607 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738921642303467.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45832204818725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3165507631642478 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623565196990967.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.444197416305542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31616800512586324 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25713396072387695.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4408717155456543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31598336696624757 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2783682346343994.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4686307907104492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31579738770212445 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562861442565918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4385559558868408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3155258527823857 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2677783966064453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4512207508087158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3153843654053552 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514946460723877.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44078850746154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31530886037009104 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2468860149383545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43361568450927734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31541249581745695 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25211334228515625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42795777320861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3153031310864857 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25445055961608887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4295017719268799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.315112128002303 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26971983909606934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4569118022918701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149662162576403 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2526392936706543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4438903331756592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31498279571533205 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25058507919311523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4314594268798828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3148794846875327 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25250911712646484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43872594833374023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3148942193814686 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24927330017089844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4367485046386719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31467598846980505 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24846935272216797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42479515075683594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31470488820757186 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25574398040771484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43137431144714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31454872403826034 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25432848930358887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43721485137939453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3145203356231962 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24640965461730957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42955946922302246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3145119019917079 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26207637786865234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4491603374481201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3144595618758883 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25528502464294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44112515449523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31438528384481157 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26080965995788574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4361865520477295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31427752843924933 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2526075839996338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4290809631347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31450004960809436 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26990723609924316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4618215560913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142593728644507 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557237148284912.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.438262939453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141685434750148 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26688432693481445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4652433395385742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141690390450614 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2706129550933838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45749783515930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141380880560194 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25090718269348145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44254469871520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31416671914713723 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25159192085266113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359619617462158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31421796679496766 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2473745346069336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4210014343261719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140682003327778 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578928470611572.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4371790885925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140097894838878 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.253110408782959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317495822906494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31398692216191976 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25716614723205566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44373154640197754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139927540506635 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25702619552612305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4736611843109131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139845699071884 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2567451000213623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4679737091064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31395043432712555 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28253984451293945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47060155868530273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139445892402104 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25284242630004883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44506335258483887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138790526560375 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24808907508850098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4252746105194092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138469810996737 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2415454387664795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41355299949645996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31388537458011084 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25873565673828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4350554943084717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31377139474664417 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2612030506134033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.434751033782959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31384635823113577 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23815083503723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4349644184112549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31375983570303234 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25326085090637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4352550506591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31381561969007765 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548189163208008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4354119300842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137996345758438 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27243471145629883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4572148323059082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31376314120633264 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2635233402252197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44626712799072266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31372910823140826 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27935171127319336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4767000675201416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137028843164444 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26510024070739746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46116185188293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3138119216476168 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29677844047546387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47972655296325684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137563722474234 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545506954193115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4396235942840576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136809034006936 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.300173282623291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4956951141357422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31368877589702604 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25399303436279297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4347352981567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136268424136298 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2581956386566162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46246933937072754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.313660734466144 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26312708854675293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44129467010498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3137018029178892 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26010847091674805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4506664276123047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31368144963468825 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26767659187316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44579195976257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31369574325425287 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601494789123535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43717527389526367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136596760579518 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561624050140381.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43971776962280273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136392129319055 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2517430782318115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4273040294647217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31358531202588763 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592020034790039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4642527103424072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31361049711704253 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2506370544433594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4236164093017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135995090007782 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2349531650543213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.40987467765808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31360228402273993 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2570803165435791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4312324523925781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135966611760003 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24223971366882324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4145028591156006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31359384443078725 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2436201572418213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43195390701293945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31357667786734444 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24999165534973145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4415559768676758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31359483471938543 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26116156578063965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4573802947998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135959378310612 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649352550506592.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45329952239990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31358299936567036 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25833582878112793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4571192264556885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31357510983943937 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27898120880126953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4673001766204834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31356232932635714 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2830228805541992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47464513778686523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135740216289248 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26093077659606934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4376511573791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.313515054328101 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622652053833008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4416642189025879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31355225018092564 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26008105278015137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4540441036224365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135535291263035 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24986958503723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43343520164489746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31352209150791166 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2703826427459717.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4508805274963379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135283406291689 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2560100555419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42936015129089355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31349208652973176 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25598645210266113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4287393093109131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135047916855131 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666459083557129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4492518901824951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134913227387837 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2558627128601074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4472074508666992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31353123017719814 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2798013687133789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46876072883605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31349921694823674 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583334445953369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4764673709869385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347511836460656 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2852201461791992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5122296810150146\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31348588381494796 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27750182151794434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4970831871032715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.313459198815482 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2912404537200928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4885578155517578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134884838547025 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26496362686157227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45664429664611816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31346644716603417 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26645565032958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45957517623901367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31345834902354647 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2684774398803711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45174169540405273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134872283254351 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762451171875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46855688095092773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134724280663899 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2796304225921631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47661614418029785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31346082644803186 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28586792945861816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49465036392211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31346879856927057 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2994272708892822.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4992237091064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31347713087286266 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2790064811706543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4672095775604248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134608234677996 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"basic\"\n",
        "flip_probabilities = [0.1, 0.5, 0.9]\n",
        "rnn_arr2 = []\n",
        "nrnn_arr2 = []\n",
        "lstm_arr2 = []\n",
        "seed_arr2 = []\n",
        "method_arr2 = []\n",
        "specify_arr2 = []\n",
        "\n",
        "\n",
        "for flip_probability in flip_probabilities:\n",
        "\n",
        "  x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped= generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                      flip_probability, flip_probability_0=None, flip_probability_1=None,\n",
        "                      startprob=None, transmat=None)\n",
        "  \n",
        "  basic_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "  basic_train_flipped_loader = data_utils.DataLoader(basic_train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "  basic_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "  basic_test_flipped_loader = data_utils.DataLoader(basic_test_flipped, batch_size=25, shuffle=True)\n",
        "\n",
        "  for seed in seeds:\n",
        "    acc = driver(seed, basic_train_flipped_loader, basic_test_flipped_loader)\n",
        "    acc2 = driver(seed, basic_train_flipped_loader, basic_test_flipped_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "    acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
        "    rnn_arr2.append(acc)\n",
        "    nrnn_arr2.append(acc2)\n",
        "    lstm_arr2.append(acc3)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(flip_probability)"
      ],
      "metadata": {
        "id": "8Wy7bjBuWxp5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00f87ee4-f036-4210-dc3d-5956ca63527f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6879 | test accuracy: 0.438\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6755 | test accuracy: 0.478\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6074 | test accuracy: 0.478\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6926 | test accuracy: 0.455\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7469 | test accuracy: 0.461\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8061 | test accuracy: 0.522\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7352 | test accuracy: 0.522\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7046 | test accuracy: 0.468\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7197 | test accuracy: 0.481\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5989 | test accuracy: 0.441\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8366 | test accuracy: 0.522\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6960 | test accuracy: 0.471\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7433 | test accuracy: 0.441\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6762 | test accuracy: 0.488\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7464 | test accuracy: 0.468\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7120 | test accuracy: 0.468\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6464 | test accuracy: 0.488\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6944 | test accuracy: 0.471\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7051 | test accuracy: 0.519\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6865 | test accuracy: 0.468\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7784 | test accuracy: 0.465\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6097 | test accuracy: 0.455\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6826 | test accuracy: 0.461\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6366 | test accuracy: 0.505\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7122 | test accuracy: 0.444\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6181 | test accuracy: 0.478\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6332 | test accuracy: 0.481\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7206 | test accuracy: 0.461\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7244 | test accuracy: 0.451\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7288 | test accuracy: 0.451\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6384 | test accuracy: 0.458\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6568 | test accuracy: 0.478\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6484 | test accuracy: 0.478\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6794 | test accuracy: 0.434\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6489 | test accuracy: 0.451\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6930 | test accuracy: 0.441\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6342 | test accuracy: 0.451\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7180 | test accuracy: 0.434\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6839 | test accuracy: 0.481\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6647 | test accuracy: 0.458\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6562 | test accuracy: 0.444\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7040 | test accuracy: 0.448\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6144 | test accuracy: 0.481\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7526 | test accuracy: 0.441\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6823 | test accuracy: 0.448\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7544 | test accuracy: 0.481\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7056 | test accuracy: 0.471\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7555 | test accuracy: 0.475\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6545 | test accuracy: 0.471\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7296 | test accuracy: 0.444\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6821 | test accuracy: 0.441\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6813 | test accuracy: 0.481\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7097 | test accuracy: 0.448\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6802 | test accuracy: 0.475\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6670 | test accuracy: 0.448\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6966 | test accuracy: 0.475\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7531 | test accuracy: 0.471\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7059 | test accuracy: 0.475\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7038 | test accuracy: 0.475\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7181 | test accuracy: 0.455\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7086 | test accuracy: 0.475\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6381 | test accuracy: 0.458\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6831 | test accuracy: 0.481\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6715 | test accuracy: 0.451\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6651 | test accuracy: 0.455\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5805 | test accuracy: 0.485\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7189 | test accuracy: 0.448\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7429 | test accuracy: 0.478\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6366 | test accuracy: 0.475\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6404 | test accuracy: 0.475\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7242 | test accuracy: 0.461\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6959 | test accuracy: 0.461\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6762 | test accuracy: 0.471\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7023 | test accuracy: 0.475\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6474 | test accuracy: 0.478\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6827 | test accuracy: 0.448\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7411 | test accuracy: 0.451\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7147 | test accuracy: 0.478\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6303 | test accuracy: 0.478\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6558 | test accuracy: 0.478\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6382 | test accuracy: 0.451\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6779 | test accuracy: 0.478\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7193 | test accuracy: 0.465\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7625 | test accuracy: 0.475\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6086 | test accuracy: 0.455\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7581 | test accuracy: 0.475\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7124 | test accuracy: 0.475\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6447 | test accuracy: 0.475\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6616 | test accuracy: 0.455\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6644 | test accuracy: 0.475\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6973 | test accuracy: 0.475\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6969 | test accuracy: 0.478\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6369 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7024 | test accuracy: 0.478\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6801 | test accuracy: 0.478\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7206 | test accuracy: 0.478\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6594 | test accuracy: 0.485\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7817 | test accuracy: 0.485\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6895 | test accuracy: 0.478\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6141 | test accuracy: 0.475\n",
            "total time:  32.45463922499994\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.1547 | test accuracy: 0.515\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6874 | test accuracy: 0.498\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8980 | test accuracy: 0.485\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7801 | test accuracy: 0.478\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7713 | test accuracy: 0.475\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7749 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6219 | test accuracy: 0.481\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8045 | test accuracy: 0.502\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6301 | test accuracy: 0.465\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7378 | test accuracy: 0.468\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7656 | test accuracy: 0.485\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7254 | test accuracy: 0.431\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7772 | test accuracy: 0.478\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6625 | test accuracy: 0.502\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7644 | test accuracy: 0.451\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8040 | test accuracy: 0.475\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7055 | test accuracy: 0.458\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7516 | test accuracy: 0.519\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6374 | test accuracy: 0.505\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6804 | test accuracy: 0.488\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7793 | test accuracy: 0.502\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7234 | test accuracy: 0.522\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6849 | test accuracy: 0.495\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.8358 | test accuracy: 0.488\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6138 | test accuracy: 0.455\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6964 | test accuracy: 0.481\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6368 | test accuracy: 0.468\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6989 | test accuracy: 0.498\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7468 | test accuracy: 0.471\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7629 | test accuracy: 0.471\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6214 | test accuracy: 0.471\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6750 | test accuracy: 0.471\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7813 | test accuracy: 0.488\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5782 | test accuracy: 0.492\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7966 | test accuracy: 0.468\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6802 | test accuracy: 0.455\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6918 | test accuracy: 0.478\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7114 | test accuracy: 0.468\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6712 | test accuracy: 0.495\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6361 | test accuracy: 0.465\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.8211 | test accuracy: 0.475\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5799 | test accuracy: 0.468\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7624 | test accuracy: 0.468\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7017 | test accuracy: 0.468\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6711 | test accuracy: 0.468\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7027 | test accuracy: 0.468\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7017 | test accuracy: 0.468\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7179 | test accuracy: 0.448\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6736 | test accuracy: 0.461\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6411 | test accuracy: 0.441\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7431 | test accuracy: 0.485\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6887 | test accuracy: 0.481\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6714 | test accuracy: 0.488\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6340 | test accuracy: 0.492\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6767 | test accuracy: 0.465\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6437 | test accuracy: 0.451\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7276 | test accuracy: 0.455\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7497 | test accuracy: 0.455\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7316 | test accuracy: 0.461\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6802 | test accuracy: 0.461\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7364 | test accuracy: 0.465\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7421 | test accuracy: 0.448\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6422 | test accuracy: 0.461\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6066 | test accuracy: 0.465\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6881 | test accuracy: 0.448\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7596 | test accuracy: 0.455\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6329 | test accuracy: 0.461\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6080 | test accuracy: 0.468\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6746 | test accuracy: 0.461\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6638 | test accuracy: 0.468\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6534 | test accuracy: 0.465\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7230 | test accuracy: 0.465\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6435 | test accuracy: 0.478\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7295 | test accuracy: 0.448\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7971 | test accuracy: 0.451\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6253 | test accuracy: 0.468\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6063 | test accuracy: 0.444\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7613 | test accuracy: 0.461\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6777 | test accuracy: 0.448\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6832 | test accuracy: 0.458\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6260 | test accuracy: 0.468\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6226 | test accuracy: 0.451\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6378 | test accuracy: 0.441\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.5397 | test accuracy: 0.441\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6337 | test accuracy: 0.488\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6641 | test accuracy: 0.441\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6946 | test accuracy: 0.444\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6774 | test accuracy: 0.451\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6752 | test accuracy: 0.468\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6394 | test accuracy: 0.461\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6974 | test accuracy: 0.465\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6573 | test accuracy: 0.471\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6882 | test accuracy: 0.458\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6138 | test accuracy: 0.458\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6912 | test accuracy: 0.455\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6492 | test accuracy: 0.461\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7982 | test accuracy: 0.451\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6711 | test accuracy: 0.465\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7174 | test accuracy: 0.444\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7481 | test accuracy: 0.455\n",
            "total time:  35.74370072500005\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2576584815979004.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 0.4654052257537842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5278662775244032 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2606813907623291.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.458601713180542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4407155262572425 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2763350009918213.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4768967628479004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.39986023008823396 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586705684661865.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4501662254333496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.37422984285014016 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2703068256378174.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.46700143814086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3608370759657451 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2660503387451172.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4545891284942627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34988039135932925 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27585530281066895.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46712350845336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34233681857585907 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2779843807220459.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45725178718566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3378299815314157 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25585079193115234.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44823575019836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3342546888760158 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27182793617248535.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4548656940460205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3308829013790403 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24457144737243652.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4275634288787842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32783759917531696 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25733041763305664.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4515702724456787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3261376508644649 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24815702438354492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4261934757232666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3248278375182833 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25278306007385254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44252800941467285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32377498958792006 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26532745361328125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4579653739929199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3223818242549896 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2723221778869629.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47026681900024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3210889560835702 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.283710241317749.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4701497554779053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32040122151374817 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.264967679977417.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4485328197479248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32037081164973125 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555656433105469.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4508323669433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3191608646086284 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25936436653137207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4436953067779541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3188974210194179 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.258098840713501.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45914530754089355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3182091661861965 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26590418815612793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45214366912841797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31796054456915174 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2584667205810547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45586180686950684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.317590742451804 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26993393898010254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46668124198913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.316887754201889 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3057279586791992.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48809313774108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31690046531813487 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2650146484375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4533560276031494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3166734712464469 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623269557952881.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46364736557006836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3164574316569737 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2915077209472656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4777669906616211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.316381511092186 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28029823303222656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.510329008102417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31594919519765036 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2876763343811035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4760856628417969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3158522571836199 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28202223777770996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4644174575805664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31563762170927867 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2768247127532959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47118139266967773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3156879297324589 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527141571044922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4369535446166992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31538054559912 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609846591949463.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43665528297424316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31540529131889344 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25833559036254883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4454054832458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3152546286582947 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618880271911621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4479522705078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3149024465254375 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24979281425476074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42655158042907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31497152107102533 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.262876033782959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4488527774810791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3150191677468164 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583920955657959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44455504417419434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3147633871861866 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25516653060913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4348781108856201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3146494460957391 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2519986629486084.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4431931972503662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3146485115800585 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26215577125549316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4434957504272461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31465850940772466 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24994158744812012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4351320266723633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3143807853971209 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2747838497161865.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46604228019714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3143966670547213 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25577473640441895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389455318450928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31456455418041773 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25909900665283203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46250438690185547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31432479577405115 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2979731559753418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49521493911743164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3145007405962263 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25165462493896484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44631409645080566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3142929026058742 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2741427421569824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46478962898254395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31417855407510487 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24901318550109863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4428534507751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3142034134694508 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25603508949279785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43877720832824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31423612790448324 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25603246688842773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43648529052734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31413822216647014 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27675724029541016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4778780937194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31411625104291097 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25713205337524414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44461822509765625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141056133168084 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266451358795166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4526841640472412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3140075479234968 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27930212020874023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4725770950317383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3139648961169379 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24264836311340332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4285569190979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140329497201102 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539644241333008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.426652193069458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31398004336016516 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24444317817687988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41673994064331055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31393416013036457 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2673609256744385.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44339895248413086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3139283963612148 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618386745452881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4408397674560547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138409299509866 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26482057571411133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4386258125305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138671781335558 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25934267044067383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4357302188873291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31390086880752016 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2440779209136963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4322190284729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31386471348149436 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622356414794922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4482383728027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3137639262846538 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25710391998291016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43914246559143066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137681032930102 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28823423385620117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47933411598205566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137614735535213 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26165342330932617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.446230411529541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137297821896417 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2929995059967041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4901719093322754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31374872497149875 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2684895992279053.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45462942123413086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31374955347606115 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27536630630493164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4829246997833252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137351470334189 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555272579193115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44175219535827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31382043276514326 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26810598373413086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4868912696838379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31370639630726405 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27724289894104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4613170623779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3136802694627217 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26320767402648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46155214309692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136866096939359 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682631015777588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45316362380981445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136769086122513 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25617098808288574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4391298294067383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136395692825317 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28565502166748047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4720423221588135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31367369294166564 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603874206542969.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4502716064453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136083760431835 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2744171619415283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4541597366333008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3135782901729856 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25968194007873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317607879638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31361927049500604 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2585103511810303.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.453571081161499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3135991317885263 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738378047943115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.469071626663208\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.313597611444337 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2639806270599365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4575023651123047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31356325703007837 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26457929611206055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4443979263305664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31360459072249275 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713189125061035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46570444107055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31358944943972994 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27204108238220215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4659757614135742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135746645075934 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27953672409057617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48282337188720703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31355730380330765 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2883734703063965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4839057922363281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31358162888458796 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25942039489746094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4591243267059326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135701860700335 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2860260009765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4754188060760498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31358261023248946 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577528953552246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4476158618927002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135466686316899 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26915764808654785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46059107780456543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135447915111269 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25567007064819336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.430556058883667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135530037539346 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2529420852661133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4430971145629883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135360087667193 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26235103607177734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44449901580810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135265303509576 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26667141914367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46963071823120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135274623121534 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3049488067626953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49410009384155273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135275283030101 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25940370559692383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4466545581817627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31352676919528416 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2763252258300781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45195531845092773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31350760119301935 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25926923751831055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43917059898376465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135004809924534 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2459700107574463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43286609649658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134811499289104 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2478940486907959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42518019676208496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31349930252347674 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604794502258301.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44248461723327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31349738580839975 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26564955711364746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4584028720855713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31348191627434324 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2927513122558594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4855082035064697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31344747841358184 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26587677001953125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4540083408355713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134959016527448 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25614500045776367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4427945613861084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31349058278969355 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2748739719390869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4609088897705078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31350037625857763 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605147361755371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.465787410736084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31347219688551764 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604210376739502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4496898651123047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.313476351755006 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637794017791748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4690427780151367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31345547309943605 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2906641960144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4987654685974121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134536900690624 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26653432846069336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4727630615234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347108142716545 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26938533782958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45810770988464355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134223963533129 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611386775970459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43219542503356934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31344434533800397 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2614922523498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4407663345336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134233794042042 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26021242141723633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4536552429199219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134452917746135 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28313755989074707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47383975982666016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31344224768025536 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2988772392272949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49351978302001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31343783097607747 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25434398651123047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44091010093688965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343704972948344 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694587707519531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4565267562866211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134459137916565 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25384998321533203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317359924316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31343235884393966 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2584867477416992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359133243560791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343869652066914 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25643348693847656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360320568084717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134259649685451 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26911258697509766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45438694953918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.313430769102914 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26337170600891113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4367702007293701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31341521739959716 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562251091003418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44551897048950195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31341383755207064 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7307 | test accuracy: 0.478\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7804 | test accuracy: 0.488\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7124 | test accuracy: 0.475\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6761 | test accuracy: 0.525\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6577 | test accuracy: 0.515\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7164 | test accuracy: 0.468\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6617 | test accuracy: 0.455\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7548 | test accuracy: 0.448\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7240 | test accuracy: 0.502\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7561 | test accuracy: 0.465\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6881 | test accuracy: 0.505\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6735 | test accuracy: 0.465\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7006 | test accuracy: 0.468\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6482 | test accuracy: 0.471\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6829 | test accuracy: 0.458\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6479 | test accuracy: 0.458\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7163 | test accuracy: 0.458\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6561 | test accuracy: 0.505\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6795 | test accuracy: 0.512\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8073 | test accuracy: 0.468\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7625 | test accuracy: 0.505\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7043 | test accuracy: 0.455\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6591 | test accuracy: 0.481\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6917 | test accuracy: 0.471\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7022 | test accuracy: 0.495\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7750 | test accuracy: 0.502\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7735 | test accuracy: 0.448\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5819 | test accuracy: 0.465\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6859 | test accuracy: 0.461\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7077 | test accuracy: 0.478\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6468 | test accuracy: 0.465\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7892 | test accuracy: 0.461\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6950 | test accuracy: 0.458\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6810 | test accuracy: 0.455\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6984 | test accuracy: 0.441\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6480 | test accuracy: 0.441\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7682 | test accuracy: 0.438\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6496 | test accuracy: 0.481\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6337 | test accuracy: 0.471\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6223 | test accuracy: 0.485\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7293 | test accuracy: 0.458\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6575 | test accuracy: 0.475\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6909 | test accuracy: 0.485\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6522 | test accuracy: 0.485\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7291 | test accuracy: 0.481\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6747 | test accuracy: 0.488\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6474 | test accuracy: 0.468\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6760 | test accuracy: 0.475\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6630 | test accuracy: 0.475\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6936 | test accuracy: 0.475\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6257 | test accuracy: 0.481\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6504 | test accuracy: 0.475\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6784 | test accuracy: 0.465\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7529 | test accuracy: 0.451\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7122 | test accuracy: 0.468\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6703 | test accuracy: 0.465\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6551 | test accuracy: 0.475\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6970 | test accuracy: 0.471\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6896 | test accuracy: 0.471\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6881 | test accuracy: 0.475\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7265 | test accuracy: 0.485\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6965 | test accuracy: 0.485\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7360 | test accuracy: 0.468\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6370 | test accuracy: 0.481\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6794 | test accuracy: 0.478\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6826 | test accuracy: 0.478\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6648 | test accuracy: 0.475\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7302 | test accuracy: 0.478\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6705 | test accuracy: 0.485\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6585 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7096 | test accuracy: 0.441\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6927 | test accuracy: 0.451\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7465 | test accuracy: 0.478\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7414 | test accuracy: 0.475\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6855 | test accuracy: 0.461\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6315 | test accuracy: 0.488\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.5686 | test accuracy: 0.481\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6976 | test accuracy: 0.485\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6261 | test accuracy: 0.455\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6799 | test accuracy: 0.471\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6234 | test accuracy: 0.475\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6650 | test accuracy: 0.478\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6299 | test accuracy: 0.468\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6808 | test accuracy: 0.471\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7476 | test accuracy: 0.475\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6939 | test accuracy: 0.481\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7089 | test accuracy: 0.465\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6872 | test accuracy: 0.455\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6530 | test accuracy: 0.492\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7578 | test accuracy: 0.478\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7502 | test accuracy: 0.468\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6389 | test accuracy: 0.465\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6117 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6238 | test accuracy: 0.434\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6942 | test accuracy: 0.481\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6520 | test accuracy: 0.455\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6621 | test accuracy: 0.475\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6687 | test accuracy: 0.485\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.5807 | test accuracy: 0.478\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7115 | test accuracy: 0.478\n",
            "total time:  31.94138121600008\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8019 | test accuracy: 0.478\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7076 | test accuracy: 0.465\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7790 | test accuracy: 0.522\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8926 | test accuracy: 0.522\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9314 | test accuracy: 0.458\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6417 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8382 | test accuracy: 0.455\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6030 | test accuracy: 0.478\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6413 | test accuracy: 0.481\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6015 | test accuracy: 0.468\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8326 | test accuracy: 0.485\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6040 | test accuracy: 0.451\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8376 | test accuracy: 0.471\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6735 | test accuracy: 0.481\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4376 | test accuracy: 0.492\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7185 | test accuracy: 0.525\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.8090 | test accuracy: 0.451\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7868 | test accuracy: 0.471\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.8244 | test accuracy: 0.461\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7326 | test accuracy: 0.461\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7284 | test accuracy: 0.519\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7006 | test accuracy: 0.451\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6909 | test accuracy: 0.448\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7918 | test accuracy: 0.508\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6799 | test accuracy: 0.438\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6885 | test accuracy: 0.478\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6966 | test accuracy: 0.478\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6858 | test accuracy: 0.475\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7567 | test accuracy: 0.441\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7004 | test accuracy: 0.478\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6609 | test accuracy: 0.481\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7349 | test accuracy: 0.478\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6906 | test accuracy: 0.461\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6459 | test accuracy: 0.458\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7025 | test accuracy: 0.451\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7551 | test accuracy: 0.455\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6903 | test accuracy: 0.465\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6876 | test accuracy: 0.451\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6333 | test accuracy: 0.468\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6983 | test accuracy: 0.468\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7651 | test accuracy: 0.465\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6541 | test accuracy: 0.468\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7396 | test accuracy: 0.465\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7026 | test accuracy: 0.448\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6779 | test accuracy: 0.448\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7838 | test accuracy: 0.461\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6357 | test accuracy: 0.461\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7123 | test accuracy: 0.471\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6845 | test accuracy: 0.468\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6940 | test accuracy: 0.468\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6483 | test accuracy: 0.451\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6649 | test accuracy: 0.455\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6479 | test accuracy: 0.461\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7069 | test accuracy: 0.471\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6851 | test accuracy: 0.468\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6730 | test accuracy: 0.468\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6324 | test accuracy: 0.471\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6231 | test accuracy: 0.438\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7200 | test accuracy: 0.438\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7266 | test accuracy: 0.451\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6583 | test accuracy: 0.451\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6308 | test accuracy: 0.461\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6730 | test accuracy: 0.465\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7233 | test accuracy: 0.475\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6456 | test accuracy: 0.448\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6889 | test accuracy: 0.468\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7560 | test accuracy: 0.468\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6007 | test accuracy: 0.465\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7177 | test accuracy: 0.468\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6032 | test accuracy: 0.458\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6502 | test accuracy: 0.461\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7558 | test accuracy: 0.461\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7433 | test accuracy: 0.458\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7265 | test accuracy: 0.458\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6771 | test accuracy: 0.465\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7038 | test accuracy: 0.465\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7006 | test accuracy: 0.468\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7071 | test accuracy: 0.465\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7150 | test accuracy: 0.465\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6734 | test accuracy: 0.444\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7124 | test accuracy: 0.458\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7766 | test accuracy: 0.448\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7925 | test accuracy: 0.455\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6622 | test accuracy: 0.451\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6599 | test accuracy: 0.465\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7177 | test accuracy: 0.438\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7334 | test accuracy: 0.458\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7196 | test accuracy: 0.458\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6238 | test accuracy: 0.465\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6841 | test accuracy: 0.458\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6122 | test accuracy: 0.465\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6835 | test accuracy: 0.461\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6763 | test accuracy: 0.468\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6503 | test accuracy: 0.461\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7824 | test accuracy: 0.465\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7623 | test accuracy: 0.461\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6754 | test accuracy: 0.461\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6783 | test accuracy: 0.458\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7380 | test accuracy: 0.451\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6051 | test accuracy: 0.461\n",
            "total time:  35.385943850999865\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541937828063965.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.44808292388916016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6807390979358128 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542459964752197.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4312732219696045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5500095810209001 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24130678176879883.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.41866087913513184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.465504989027977 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25686216354370117.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44632411003112793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.421043901358332 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25385475158691406.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.4328460693359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3882476879017694 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25964832305908203.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.44558000564575195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37423124015331266 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534909248352051.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.43945956230163574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3602739134005138 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26188063621520996.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44930338859558105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3519589479480471 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615244388580322.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44387340545654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.345725005865097 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2722330093383789.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46715831756591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3393844557659967 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27402281761169434.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.458052396774292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3371980173247201 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698042392730713.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4743976593017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3340065232345036 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26662111282348633.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.45149922370910645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33029802015849524 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26999521255493164.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4663069248199463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32864923306873867 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27921581268310547.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46559786796569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3275979731764112 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24723291397094727.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4262096881866455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32568529077938624 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542226314544678.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4435007572174072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3243953785726002 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2501332759857178.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43425607681274414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32320343554019926 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719242572784424.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5757546424865723\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32193251252174376 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.4784111976623535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.8720228672027588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32149523624352044 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.33768439292907715.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.670081377029419\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32111456266471317 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.4240989685058594.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.70029616355896\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32001631855964663 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30188965797424316.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5379288196563721\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3192869586603982 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3215198516845703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5417323112487793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31885530735765183 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2888662815093994.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4930274486541748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31872051571096693 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29446959495544434.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5208585262298584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3179620917354311 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2983553409576416.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49952220916748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31766523463385443 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30576515197753906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.528733491897583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3175108905349459 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30066943168640137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5225775241851807\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3173591021980558 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.32859015464782715.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5519611835479736\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31709686049393243 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30555057525634766.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5211124420166016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3167173802852631 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3021254539489746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5272996425628662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3165397328989846 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29694533348083496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5152592658996582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31628930228097096 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3056302070617676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5236852169036865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3162864885159901 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29566097259521484.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5241248607635498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31605420793805805 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3079705238342285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5480284690856934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31569718292781285 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.35831451416015625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.645653486251831\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31578823413167684 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.38097095489501953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.6608612537384033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3155062347650528 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3640735149383545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.6363162994384766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3152963148696082 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24656200408935547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4277486801147461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3152176908084324 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26052069664001465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.443432092666626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31523921872888294 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26564574241638184.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4457261562347412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31500475874968936 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2476191520690918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4221162796020508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.315001339997564 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559325695037842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4410724639892578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3148520618677139 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507956027984619.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4279332160949707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3148719506604331 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25170326232910156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44576239585876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31473893310342516 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25537919998168945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4417994022369385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3147333596433912 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25507354736328125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4373304843902588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147032580205372 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2734057903289795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4666755199432373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3145554052931922 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25798821449279785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4468982219696045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3145032499517713 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28842926025390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46523451805114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31444905144827706 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2496190071105957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4272575378417969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3144060586180006 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619197368621826.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45898938179016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144395777157375 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646903991699219.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4428699016571045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31430893668106624 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514073848724365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43628644943237305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3142267618860517 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2556746006011963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43361830711364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143095280442919 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25954461097717285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43804287910461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31415002856935775 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25655078887939453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4379301071166992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31419571723256795 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25063228607177734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4266340732574463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3140591868332454 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597687244415283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487464427947998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31406168001038687 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25061821937561035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4299917221069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31403075286320276 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25930285453796387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4532136917114258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3140429973602295 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25577664375305176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4303398132324219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141254765646798 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2470536231994629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4221174716949463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139642234359469 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259260892868042.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43088698387145996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3139714253800256 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516207695007324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4332869052886963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139043854815619 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2518889904022217.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44341063499450684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3138830644743783 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24562501907348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42186903953552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139083083186831 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24877238273620605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4446072578430176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138282145772662 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2584397792816162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4516599178314209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.313873228430748 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25752973556518555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45045995712280273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31383606025150845 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29752087593078613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4838094711303711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31377597749233244 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2722163200378418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47861266136169434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138163379260472 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2693603038787842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4519336223602295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31379977592400143 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27121996879577637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4510500431060791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137838751077652 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26666855812072754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4747755527496338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31378470531531744 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651045322418213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45322155952453613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3138099150998252 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26917266845703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.464550256729126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137923278978893 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24529790878295898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317800998687744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137059130838939 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27589845657348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.461669921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31372566308294025 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27501869201660156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45414280891418457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137052259274891 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24896001815795898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43601465225219727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31368895300797056 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2743206024169922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4594120979309082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136531391314098 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24741172790527344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4259452819824219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136897487299783 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2453458309173584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4293043613433838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136159168822425 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596442699432373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44262266159057617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136382362672261 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2522623538970947.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42076897621154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136243909597397 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24781274795532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42308545112609863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136636699948992 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23308897018432617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41150951385498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136107108422688 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25398683547973633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43076086044311523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136361756495067 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24602150917053223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4326193332672119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31361335601125445 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24618148803710938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43119168281555176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.313588970048087 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25415706634521484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44441914558410645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136145412921906 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26558995246887207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44616198539733887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31359605406011853 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2585437297821045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4448580741882324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31356061952454706 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29099488258361816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48479580879211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135545854057584 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26204514503479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44080305099487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135597552571978 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26057004928588867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4652516841888428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135357128722327 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269366979598999.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46220898628234863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135711465563093 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2644953727722168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4504659175872803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31357053816318514 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25791454315185547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44268250465393066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135281596864973 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25742435455322266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43907761573791504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135706543922424 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26993799209594727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47531843185424805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135459729603359 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2916083335876465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46878695487976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31355419754981995 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25374412536621094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.434035062789917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135236416544233 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24989533424377441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4221820831298828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31357070675918036 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2443830966949463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4178624153137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135226398706436 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24251627922058105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4216904640197754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135101914405823 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24297595024108887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4161372184753418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135299976382937 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24409198760986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4214496612548828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31351049627576555 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604806423187256.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43750834465026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135499881846564 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24776101112365723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4213716983795166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134860622031348 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24541115760803223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42561888694763184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31349228918552396 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24481987953186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42471933364868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134764220033373 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24248361587524414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4224402904510498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134960114955902 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25901317596435547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4462928771972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31347452614988597 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25322914123535156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4405860900878906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31346672475337983 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667417526245117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4622154235839844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31347161063126155 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.273728609085083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45652151107788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347626064504897 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26225709915161133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45563483238220215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31347250044345853 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652602195739746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44598889350891113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134466290473938 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25386643409729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.459216833114624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31346733144351413 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27382659912109375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4538557529449463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134603296007429 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2522616386413574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43129849433898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134499579668045 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24460363388061523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42444753646850586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134448856115341 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25098109245300293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42296385765075684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134510913065502 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607688903808594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45058107376098633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.313456255197525 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2558465003967285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43522047996520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134356818028859 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7253 | test accuracy: 0.522\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6901 | test accuracy: 0.478\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7629 | test accuracy: 0.478\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7148 | test accuracy: 0.411\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7977 | test accuracy: 0.478\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6706 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6619 | test accuracy: 0.495\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6871 | test accuracy: 0.492\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7533 | test accuracy: 0.421\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7950 | test accuracy: 0.424\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6567 | test accuracy: 0.458\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7363 | test accuracy: 0.434\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7829 | test accuracy: 0.451\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6827 | test accuracy: 0.468\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7729 | test accuracy: 0.522\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6683 | test accuracy: 0.448\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7286 | test accuracy: 0.434\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7695 | test accuracy: 0.508\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7593 | test accuracy: 0.522\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6518 | test accuracy: 0.515\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6487 | test accuracy: 0.522\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7458 | test accuracy: 0.485\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5813 | test accuracy: 0.468\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6819 | test accuracy: 0.461\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6402 | test accuracy: 0.448\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7062 | test accuracy: 0.475\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6745 | test accuracy: 0.458\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7151 | test accuracy: 0.458\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6606 | test accuracy: 0.478\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7112 | test accuracy: 0.465\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7039 | test accuracy: 0.525\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6622 | test accuracy: 0.481\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6651 | test accuracy: 0.475\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7365 | test accuracy: 0.451\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6871 | test accuracy: 0.455\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6822 | test accuracy: 0.458\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7056 | test accuracy: 0.448\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6795 | test accuracy: 0.451\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6684 | test accuracy: 0.468\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7590 | test accuracy: 0.451\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6689 | test accuracy: 0.465\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7102 | test accuracy: 0.468\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6474 | test accuracy: 0.444\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6846 | test accuracy: 0.441\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6397 | test accuracy: 0.444\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6944 | test accuracy: 0.481\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6747 | test accuracy: 0.448\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6600 | test accuracy: 0.465\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6768 | test accuracy: 0.444\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6478 | test accuracy: 0.448\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7053 | test accuracy: 0.451\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6510 | test accuracy: 0.448\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7259 | test accuracy: 0.471\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7007 | test accuracy: 0.461\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7304 | test accuracy: 0.448\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7511 | test accuracy: 0.458\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6477 | test accuracy: 0.468\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6826 | test accuracy: 0.448\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7322 | test accuracy: 0.451\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7352 | test accuracy: 0.458\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7054 | test accuracy: 0.471\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7067 | test accuracy: 0.458\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7271 | test accuracy: 0.465\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6503 | test accuracy: 0.465\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6879 | test accuracy: 0.448\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6930 | test accuracy: 0.448\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6761 | test accuracy: 0.478\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6723 | test accuracy: 0.451\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6851 | test accuracy: 0.451\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7569 | test accuracy: 0.451\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6837 | test accuracy: 0.448\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7081 | test accuracy: 0.455\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6457 | test accuracy: 0.458\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7331 | test accuracy: 0.458\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6804 | test accuracy: 0.458\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7175 | test accuracy: 0.455\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6496 | test accuracy: 0.444\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7005 | test accuracy: 0.448\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6393 | test accuracy: 0.448\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6791 | test accuracy: 0.448\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.5640 | test accuracy: 0.444\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6611 | test accuracy: 0.448\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6480 | test accuracy: 0.448\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6584 | test accuracy: 0.471\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6743 | test accuracy: 0.448\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.8046 | test accuracy: 0.448\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6583 | test accuracy: 0.455\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7140 | test accuracy: 0.448\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6876 | test accuracy: 0.455\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6950 | test accuracy: 0.444\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6550 | test accuracy: 0.461\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6992 | test accuracy: 0.451\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6888 | test accuracy: 0.455\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6591 | test accuracy: 0.458\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6879 | test accuracy: 0.458\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6504 | test accuracy: 0.455\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6900 | test accuracy: 0.451\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7087 | test accuracy: 0.438\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6928 | test accuracy: 0.455\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6872 | test accuracy: 0.461\n",
            "total time:  30.986430515000393\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9474 | test accuracy: 0.522\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7733 | test accuracy: 0.522\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.9077 | test accuracy: 0.478\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7586 | test accuracy: 0.522\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6975 | test accuracy: 0.478\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7878 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7363 | test accuracy: 0.495\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7560 | test accuracy: 0.468\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5322 | test accuracy: 0.478\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6498 | test accuracy: 0.478\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6988 | test accuracy: 0.471\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7729 | test accuracy: 0.505\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6720 | test accuracy: 0.512\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.8054 | test accuracy: 0.451\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.8025 | test accuracy: 0.485\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6593 | test accuracy: 0.475\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6278 | test accuracy: 0.481\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7185 | test accuracy: 0.492\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6796 | test accuracy: 0.481\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6380 | test accuracy: 0.515\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6682 | test accuracy: 0.448\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7716 | test accuracy: 0.478\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6818 | test accuracy: 0.455\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7091 | test accuracy: 0.488\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7511 | test accuracy: 0.461\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6126 | test accuracy: 0.488\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.5992 | test accuracy: 0.475\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6362 | test accuracy: 0.458\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6666 | test accuracy: 0.485\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6883 | test accuracy: 0.492\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7603 | test accuracy: 0.478\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6891 | test accuracy: 0.468\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6250 | test accuracy: 0.468\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7335 | test accuracy: 0.471\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6569 | test accuracy: 0.478\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6899 | test accuracy: 0.475\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5799 | test accuracy: 0.455\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6550 | test accuracy: 0.458\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6318 | test accuracy: 0.465\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5968 | test accuracy: 0.465\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7845 | test accuracy: 0.468\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6817 | test accuracy: 0.461\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7984 | test accuracy: 0.461\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6733 | test accuracy: 0.468\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6955 | test accuracy: 0.468\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6909 | test accuracy: 0.468\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7090 | test accuracy: 0.475\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7310 | test accuracy: 0.438\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7356 | test accuracy: 0.471\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7005 | test accuracy: 0.441\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6801 | test accuracy: 0.441\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6925 | test accuracy: 0.451\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6261 | test accuracy: 0.451\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6671 | test accuracy: 0.451\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6497 | test accuracy: 0.458\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6713 | test accuracy: 0.468\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7343 | test accuracy: 0.478\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6512 | test accuracy: 0.455\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6924 | test accuracy: 0.451\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6510 | test accuracy: 0.441\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6279 | test accuracy: 0.448\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6814 | test accuracy: 0.444\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6692 | test accuracy: 0.451\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6883 | test accuracy: 0.444\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7602 | test accuracy: 0.448\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6739 | test accuracy: 0.444\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6867 | test accuracy: 0.458\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6834 | test accuracy: 0.455\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6797 | test accuracy: 0.455\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6497 | test accuracy: 0.455\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7056 | test accuracy: 0.451\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6973 | test accuracy: 0.461\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7756 | test accuracy: 0.444\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6799 | test accuracy: 0.458\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7183 | test accuracy: 0.455\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7533 | test accuracy: 0.451\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6949 | test accuracy: 0.455\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6554 | test accuracy: 0.448\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7096 | test accuracy: 0.461\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7311 | test accuracy: 0.441\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6582 | test accuracy: 0.458\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6363 | test accuracy: 0.438\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6406 | test accuracy: 0.455\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7197 | test accuracy: 0.458\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6515 | test accuracy: 0.451\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6638 | test accuracy: 0.451\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7095 | test accuracy: 0.451\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7162 | test accuracy: 0.458\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6775 | test accuracy: 0.444\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6811 | test accuracy: 0.458\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6765 | test accuracy: 0.465\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6884 | test accuracy: 0.444\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7298 | test accuracy: 0.448\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6966 | test accuracy: 0.455\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7106 | test accuracy: 0.451\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7337 | test accuracy: 0.461\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7198 | test accuracy: 0.441\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7340 | test accuracy: 0.465\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7698 | test accuracy: 0.465\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7177 | test accuracy: 0.478\n",
            "total time:  34.83623480500046\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24527645111083984.\n",
            "\n",
            "  Average training loss: 0.71\n",
            "  Training epoch took: 0.4353780746459961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7055780904633658 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661142349243164.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.45235300064086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5579448401927948 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2602832317352295.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.43918371200561523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4705292480332511 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26597094535827637.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44930315017700195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4215831526688167 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26873302459716797.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4471430778503418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.395745621408735 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2441234588623047.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.42862367630004883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37705433453832354 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23722219467163086.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4147319793701172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36279291766030447 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.244384765625.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4152085781097412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.353288836138589 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2585024833679199.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.439453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34740851308618276 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25487852096557617.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43073177337646484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34224475281579153 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25308752059936523.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42786526679992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3373796569449561 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542390823364258.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43671560287475586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3351602000849588 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2540137767791748.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4331552982330322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33156962863036565 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2463066577911377.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43105626106262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32882620309080396 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25131678581237793.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4300713539123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3274997191769736 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26117610931396484.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44284772872924805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.326466920546123 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609984874725342.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4338994026184082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32505195225988115 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26758265495300293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4526174068450928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3240627769912992 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.270801305770874.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4534645080566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.322415663089071 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26380133628845215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44100379943847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3221263212817056 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25107622146606445.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44238734245300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32099841492516656 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26838064193725586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4668893814086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.320471282516207 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733776569366455.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4629249572753906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3200614963259016 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627439498901367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4595482349395752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3195000448397228 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25971388816833496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4388158321380615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31898991550718037 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26912426948547363.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45279455184936523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31830306819507054 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619485855102539.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4541349411010742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3177348984139306 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251906156539917.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4464714527130127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31795898122446875 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2919790744781494.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4873332977294922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31744988645826067 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27193140983581543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4621438980102539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3171333325760705 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28107643127441406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4820222854614258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3168180674314499 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26287174224853516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4707212448120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31669445506164007 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26882195472717285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.454021692276001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31665238440036775 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688479423522949.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4648318290710449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.316420521906444 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26306962966918945.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4422433376312256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.316068662915911 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2669515609741211.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44269633293151855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.316099859561239 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2657196521759033.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4461534023284912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31572418085166387 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25725364685058594.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43849921226501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31574085141931263 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25480151176452637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43901872634887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3154779106378555 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27519679069519043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4736168384552002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3153971752950123 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25197935104370117.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43106555938720703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3152085768324988 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2716958522796631.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4476175308227539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3153941959142685 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25566840171813965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43578147888183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3150811868054526 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777426242828369.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4725351333618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3150274540696825 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596616744995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4427454471588135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31494352306638446 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25140857696533203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45189595222473145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149809143372944 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2752516269683838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4591083526611328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31490116928304945 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25478219985961914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4529294967651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31463736338274817 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26876354217529297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47450733184814453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3145968168973923 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.258683443069458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4387953281402588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31456622055598665 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603433132171631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44066762924194336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3144792186362403 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24076294898986816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4136338233947754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31456330461161475 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27468156814575195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4559323787689209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144927646432604 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24265027046203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41431450843811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31425900544439045 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25461316108703613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.448718786239624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143027710063117 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25255823135375977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4281187057495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3144199856689998 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24310064315795898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4161348342895508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31422171763011386 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26462388038635254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4455552101135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142978016819273 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541484832763672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401390552520752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31421754232474736 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2903738021850586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4880545139312744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142476222344807 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25946497917175293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4423503875732422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31416621506214143 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2692902088165283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46974921226501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31401666956288476 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27255773544311523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4801499843597412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31406987011432647 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2727944850921631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4870290756225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31396580338478086 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2635512351989746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44811105728149414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31406733053071156 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26975488662719727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4691147804260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31395768863814216 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2813107967376709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4777491092681885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31393707777772634 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2723367214202881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4648313522338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3140291064977646 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2759222984313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4681847095489502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139712908438274 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682163715362549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46216392517089844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139312952756882 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27184629440307617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45288944244384766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138254744665963 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24786710739135742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42907118797302246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31386139265128543 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666206359863281.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4521024227142334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31381920278072356 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26374244689941406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4457700252532959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138169573886054 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616305351257324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4426991939544678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31381293705531527 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28252625465393066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4655148983001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137747023786817 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2699286937713623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4617886543273926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31379553633076807 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2782301902770996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46465134620666504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31375861167907715 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26802849769592285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45864129066467285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137392371892929 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.273043155670166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45397329330444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31372877444539754 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2606699466705322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44971537590026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313740787761552 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25414204597473145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46177172660827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137101364987237 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25479674339294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4467613697052002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31376907016549793 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2650771141052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4631061553955078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136653393507004 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26099562644958496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4527592658996582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31369014935834066 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.296130895614624.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48888349533081055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31368968742234365 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28203654289245605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47066283226013184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.313712602853775 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29564952850341797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48240089416503906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31364897276674 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2681095600128174.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45660901069641113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31366276144981386 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26957273483276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4614102840423584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136777596814292 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27915096282958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4700794219970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31363442071846553 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26636457443237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45966625213623047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.313610137786184 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2543323040008545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43691444396972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31362441991056716 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26979780197143555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47330451011657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135756824697767 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.265380859375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46388888359069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31361025401524134 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25849199295043945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4425051212310791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31360641036714826 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28736257553100586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4864542484283447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358720447335925 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26714277267456055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45275282859802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31362251469067165 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534520626068115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359734058380127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357406590666087 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610301971435547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44214749336242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135640203952789 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266552209854126.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4449331760406494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135623216629028 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2569160461425781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4430720806121826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31353365991796767 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25863122940063477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4475278854370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.313565189072064 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26374125480651855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4410560131072998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31357401907444 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2519388198852539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42475366592407227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31351737933499474 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2599976062774658.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44236254692077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135269863264901 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25940990447998047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44269752502441406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31352740100451876 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682921886444092.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4545743465423584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31354718932083675 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595043182373047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45700621604919434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31353741032736643 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2809336185455322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47452521324157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135162217276437 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24914765357971191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43239378929138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134792481149946 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2641928195953369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45622920989990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31352367273398807 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2712061405181885.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46611833572387695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134984735931669 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27813148498535156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4775824546813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134926778929574 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2538423538208008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43269968032836914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31349678805896214 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25524210929870605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43547773361206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31347509111676897 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516002655029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4468879699707031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31346786192485265 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24859094619750977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4298093318939209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3135045260190964 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250964879989624.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43477678298950195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347773245402744 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24645495414733887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4168510437011719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31345383482319966 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534365653991699.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360191822052002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345029430730004 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532079219818115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42722177505493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134661832026073 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25121521949768066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42421889305114746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.313454395532608 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2391185760498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4103565216064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134676273380007 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25566840171813965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43786001205444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31347991611276355 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24764776229858398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41997861862182617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134547199521746 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25154948234558105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42853569984436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31346551605633327 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616415023803711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44583582878112793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31347575613430567 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7642 | test accuracy: 0.465\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7569 | test accuracy: 0.465\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7515 | test accuracy: 0.465\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7392 | test accuracy: 0.465\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8345 | test accuracy: 0.465\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6848 | test accuracy: 0.475\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6432 | test accuracy: 0.481\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6635 | test accuracy: 0.475\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6408 | test accuracy: 0.468\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7184 | test accuracy: 0.471\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7205 | test accuracy: 0.468\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8180 | test accuracy: 0.522\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6627 | test accuracy: 0.448\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6725 | test accuracy: 0.481\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7079 | test accuracy: 0.471\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7131 | test accuracy: 0.475\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6605 | test accuracy: 0.465\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7031 | test accuracy: 0.475\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6709 | test accuracy: 0.465\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6415 | test accuracy: 0.471\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6548 | test accuracy: 0.451\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6660 | test accuracy: 0.455\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7051 | test accuracy: 0.475\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6860 | test accuracy: 0.475\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7356 | test accuracy: 0.468\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6520 | test accuracy: 0.468\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6519 | test accuracy: 0.468\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6869 | test accuracy: 0.475\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6539 | test accuracy: 0.475\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6743 | test accuracy: 0.465\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6878 | test accuracy: 0.478\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7199 | test accuracy: 0.478\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6767 | test accuracy: 0.471\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6827 | test accuracy: 0.458\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7093 | test accuracy: 0.455\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6613 | test accuracy: 0.465\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6688 | test accuracy: 0.465\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6577 | test accuracy: 0.471\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6728 | test accuracy: 0.468\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7038 | test accuracy: 0.458\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7423 | test accuracy: 0.465\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7294 | test accuracy: 0.458\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6851 | test accuracy: 0.458\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7139 | test accuracy: 0.465\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6927 | test accuracy: 0.465\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6599 | test accuracy: 0.458\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6538 | test accuracy: 0.468\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6317 | test accuracy: 0.458\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7449 | test accuracy: 0.465\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6705 | test accuracy: 0.458\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6917 | test accuracy: 0.468\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7124 | test accuracy: 0.471\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6338 | test accuracy: 0.458\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7289 | test accuracy: 0.451\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6411 | test accuracy: 0.461\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7478 | test accuracy: 0.468\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6797 | test accuracy: 0.461\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6703 | test accuracy: 0.468\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6636 | test accuracy: 0.458\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6498 | test accuracy: 0.458\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6694 | test accuracy: 0.461\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6890 | test accuracy: 0.471\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6550 | test accuracy: 0.468\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6903 | test accuracy: 0.455\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6970 | test accuracy: 0.465\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7248 | test accuracy: 0.468\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6952 | test accuracy: 0.471\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7853 | test accuracy: 0.471\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5842 | test accuracy: 0.461\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7249 | test accuracy: 0.468\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6711 | test accuracy: 0.458\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6615 | test accuracy: 0.455\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6787 | test accuracy: 0.478\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7722 | test accuracy: 0.465\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6568 | test accuracy: 0.458\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6336 | test accuracy: 0.468\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7077 | test accuracy: 0.458\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7588 | test accuracy: 0.481\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7241 | test accuracy: 0.458\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.5836 | test accuracy: 0.461\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6677 | test accuracy: 0.458\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6875 | test accuracy: 0.458\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.8072 | test accuracy: 0.458\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6379 | test accuracy: 0.478\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6689 | test accuracy: 0.461\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7089 | test accuracy: 0.458\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7362 | test accuracy: 0.468\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7178 | test accuracy: 0.458\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6755 | test accuracy: 0.471\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7050 | test accuracy: 0.458\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6386 | test accuracy: 0.471\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6883 | test accuracy: 0.471\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7045 | test accuracy: 0.471\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6544 | test accuracy: 0.461\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7067 | test accuracy: 0.461\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7149 | test accuracy: 0.458\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6450 | test accuracy: 0.465\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6805 | test accuracy: 0.465\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6267 | test accuracy: 0.461\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6078 | test accuracy: 0.461\n",
            "total time:  31.411586839999472\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5078 | test accuracy: 0.485\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5177 | test accuracy: 0.535\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5626 | test accuracy: 0.539\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5435 | test accuracy: 0.522\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7058 | test accuracy: 0.485\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7376 | test accuracy: 0.535\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5200 | test accuracy: 0.481\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6610 | test accuracy: 0.461\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6599 | test accuracy: 0.495\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4756 | test accuracy: 0.478\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8050 | test accuracy: 0.468\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6390 | test accuracy: 0.512\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6379 | test accuracy: 0.485\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7065 | test accuracy: 0.495\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.9006 | test accuracy: 0.465\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6030 | test accuracy: 0.465\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5247 | test accuracy: 0.492\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6423 | test accuracy: 0.495\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7487 | test accuracy: 0.471\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5821 | test accuracy: 0.465\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7407 | test accuracy: 0.488\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7511 | test accuracy: 0.481\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7031 | test accuracy: 0.498\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7309 | test accuracy: 0.475\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5253 | test accuracy: 0.505\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6882 | test accuracy: 0.512\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6510 | test accuracy: 0.481\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7580 | test accuracy: 0.498\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6398 | test accuracy: 0.498\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8197 | test accuracy: 0.492\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6303 | test accuracy: 0.481\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6032 | test accuracy: 0.471\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6820 | test accuracy: 0.478\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7360 | test accuracy: 0.495\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6973 | test accuracy: 0.502\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7444 | test accuracy: 0.495\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6530 | test accuracy: 0.481\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7415 | test accuracy: 0.498\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7154 | test accuracy: 0.495\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5774 | test accuracy: 0.498\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6983 | test accuracy: 0.498\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6776 | test accuracy: 0.502\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7772 | test accuracy: 0.495\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7034 | test accuracy: 0.485\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6599 | test accuracy: 0.495\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6795 | test accuracy: 0.498\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7012 | test accuracy: 0.488\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6753 | test accuracy: 0.502\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7532 | test accuracy: 0.485\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7281 | test accuracy: 0.498\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6956 | test accuracy: 0.505\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.5904 | test accuracy: 0.498\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5943 | test accuracy: 0.502\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6060 | test accuracy: 0.498\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6825 | test accuracy: 0.492\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7373 | test accuracy: 0.498\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6591 | test accuracy: 0.498\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6026 | test accuracy: 0.481\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7464 | test accuracy: 0.505\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7808 | test accuracy: 0.492\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6470 | test accuracy: 0.498\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7415 | test accuracy: 0.502\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6741 | test accuracy: 0.498\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6595 | test accuracy: 0.492\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5958 | test accuracy: 0.498\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7011 | test accuracy: 0.498\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6833 | test accuracy: 0.495\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6783 | test accuracy: 0.495\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7469 | test accuracy: 0.505\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7023 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7397 | test accuracy: 0.488\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7270 | test accuracy: 0.488\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6486 | test accuracy: 0.485\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8047 | test accuracy: 0.488\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7025 | test accuracy: 0.488\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7399 | test accuracy: 0.495\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7433 | test accuracy: 0.498\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7121 | test accuracy: 0.471\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7051 | test accuracy: 0.488\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6702 | test accuracy: 0.471\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7629 | test accuracy: 0.492\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6976 | test accuracy: 0.495\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7328 | test accuracy: 0.488\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7293 | test accuracy: 0.498\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6842 | test accuracy: 0.495\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6454 | test accuracy: 0.495\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6798 | test accuracy: 0.488\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7162 | test accuracy: 0.502\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7306 | test accuracy: 0.492\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7006 | test accuracy: 0.481\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6701 | test accuracy: 0.481\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7516 | test accuracy: 0.492\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.8566 | test accuracy: 0.468\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7034 | test accuracy: 0.478\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6674 | test accuracy: 0.465\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7181 | test accuracy: 0.468\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6275 | test accuracy: 0.492\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6976 | test accuracy: 0.492\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7848 | test accuracy: 0.495\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7333 | test accuracy: 0.485\n",
            "total time:  35.647375974999704\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24857449531555176.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 0.4347379207611084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5279873294489724 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2602500915527344.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.4595036506652832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.44229133214269367 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2689497470855713.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.46444249153137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4014523297548294 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738025188446045.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.46888041496276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3762203859431403 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2691612243652344.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4549846649169922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36177963401590074 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661607265472412.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4556248188018799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.35085987618991304 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.270646333694458.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4657938480377197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3446005450827735 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25001978874206543.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4357643127441406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3392247515065329 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24803709983825684.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4268615245819092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3347762640033449 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26796793937683105.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46895408630371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3317905272756304 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.277801513671875.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4639699459075928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32919250428676605 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2659926414489746.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4501991271972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3275603643485478 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2731497287750244.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4649055004119873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32585251075880867 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575969696044922.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43864965438842773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3248575908797128 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26157593727111816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44321680068969727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3235074613775526 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25011610984802246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44090890884399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.322056109564645 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2784128189086914.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4691951274871826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3209471195936203 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654709815979004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4605565071105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32093271400247303 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593860626220703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45062923431396484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3198212078639439 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28010010719299316.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45829248428344727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3193093623433794 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.256488561630249.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.439774751663208\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31870291616235463 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26984739303588867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44979143142700195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31814549735614234 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26809191703796387.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44937634468078613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31791726095335826 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25629186630249023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4321894645690918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3174962192773819 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24274706840515137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4232761859893799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3169900723866054 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2550392150878906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4361298084259033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31665424874850684 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2553219795227051.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43336987495422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.316461643576622 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2614922523498535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4486722946166992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31653030386992864 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.264667272567749.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4519233703613281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3164758873837335 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26781606674194336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46204257011413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31646389748368947 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26587486267089844.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4475064277648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31602016133921484 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25490665435791016.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4676346778869629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3156542211771011 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2705051898956299.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4625699520111084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31541334433215007 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630167007446289.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46231722831726074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31552460874829974 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26825380325317383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4558391571044922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31539872459002904 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544734477996826.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4339115619659424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3152052674974714 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2743825912475586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4611797332763672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31500923037528994 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671194076538086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4467775821685791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.315059175661632 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2642827033996582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45408201217651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3149839575801577 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2711639404296875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45751500129699707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31470228050436294 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25524401664733887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4470944404602051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31491053104400635 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26458740234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44559717178344727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31465144029685427 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2526736259460449.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45026278495788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3144846009356635 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618215084075928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44085073471069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3144921852009637 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25380682945251465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4303250312805176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31441742479801177 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2740335464477539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4540863037109375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3143939333302634 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25475287437438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43184328079223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3144504508801869 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586543560028076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44953441619873047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31444496044090814 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26625847816467285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44533658027648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3142362296581268 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603764533996582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46486973762512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31426270391259875 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2723972797393799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4807896614074707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31423185425145284 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27001261711120605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4781219959259033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.314174251471247 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27745842933654785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46538352966308594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141974261828831 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26148557662963867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4619779586791992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31410118511744906 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2939586639404297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4795529842376709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3141344674995967 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24979400634765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4305610656738281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3139625127826418 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27186155319213867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46096158027648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31411574908665246 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254638671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4354686737060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31402599598680225 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26830387115478516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45110130310058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139194543872561 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26316022872924805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44771480560302734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3139158138207027 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26896190643310547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46310973167419434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3139129425798144 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611985206604004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44446301460266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3139676719903946 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2580106258392334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4336528778076172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139268832547324 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2797703742980957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4636693000793457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138916143349239 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25876665115356445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4425547122955322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31381993208612713 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2701869010925293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4557812213897705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138712857450758 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611846923828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44335293769836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31385619470051357 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26521825790405273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.451404333114624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31378221384116584 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.252119779586792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43157052993774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31371345264571054 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24581122398376465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4273054599761963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31380767694541384 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2660517692565918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.447770357131958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31379215845039915 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572171688079834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4454982280731201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.313776524577822 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27336907386779785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46022558212280273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137227020093373 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28250598907470703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.478100061416626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31372937389782496 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27422404289245605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46172571182250977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31371556307588305 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25849485397338867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44931459426879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31371079087257386 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27167415618896484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4639253616333008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136682493346078 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595193386077881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4393284320831299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137143624680383 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666480541229248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4507167339324951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31364618454660687 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646951675415039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4456369876861572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31366669280188425 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637045383453369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.446012020111084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136167594364711 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629678249359131.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44666385650634766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3135956121342523 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27274060249328613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.466217041015625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31364542373589105 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26410603523254395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4677164554595947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136209407023021 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2705495357513428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45259571075439453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136281541415623 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26532578468322754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45902228355407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31354909794671193 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26215648651123047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44583868980407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135658621788025 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25197601318359375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42171216011047363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31354462504386904 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25365495681762695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.426255464553833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31359776173319137 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26204872131347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4469127655029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135523945093155 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654244899749756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44320249557495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135745244366782 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571094036102295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4404926300048828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31360932333128794 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25486302375793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43799495697021484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135822245052883 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24731898307800293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4242255687713623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135680305106299 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26542067527770996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44271278381347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31352688883032115 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2636404037475586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44878339767456055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135510729891913 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26591038703918457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4573690891265869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31349656667028153 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26346588134765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4447007179260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135714079652514 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2771925926208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47374582290649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135425009897777 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700212001800537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4678809642791748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3134976246527263 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26128554344177246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4529557228088379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31354090784277233 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2543950080871582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4466862678527832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135457149573735 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2670857906341553.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4510035514831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31349668077060155 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25882673263549805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4421422481536865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31352415212563106 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26761746406555176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4444136619567871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134905023234231 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26038122177124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4437873363494873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.313452935218811 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26241350173950195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4527125358581543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31347503534385135 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645399570465088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43961668014526367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134885434593473 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24655985832214355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.430722713470459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31348839913095744 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25353074073791504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4318721294403076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134926770414625 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25177693367004395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4308812618255615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31349328288010186 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2543933391571045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4226400852203369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31346422135829927 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627389430999756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4473433494567871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134915275233133 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25719428062438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43564343452453613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134556906563895 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2723677158355713.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4471406936645508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31344791991370063 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2455887794494629.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4302635192871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31346688568592074 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2644989490509033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.455371618270874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134494296142033 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2714264392852783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4764242172241211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134436615875789 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2891981601715088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5000565052032471\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134520160300391 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28537845611572266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48329877853393555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.313428464957646 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28554821014404297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4861946105957031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134370820862906 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645587921142578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.448671817779541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134378514119557 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2832345962524414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48238539695739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134307656969343 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2826552391052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4721839427947998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31342570909432005 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25940942764282227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4429287910461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134258078677314 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561030387878418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4358348846435547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134249951158251 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24471354484558105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43700718879699707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134434700012207 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2711308002471924.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4654417037963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31342708127839225 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7066 | test accuracy: 0.451\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7385 | test accuracy: 0.532\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6844 | test accuracy: 0.471\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7435 | test accuracy: 0.471\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7281 | test accuracy: 0.485\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6333 | test accuracy: 0.465\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6995 | test accuracy: 0.465\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6579 | test accuracy: 0.535\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7043 | test accuracy: 0.492\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6859 | test accuracy: 0.478\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6585 | test accuracy: 0.481\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7264 | test accuracy: 0.465\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6244 | test accuracy: 0.502\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7085 | test accuracy: 0.481\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6351 | test accuracy: 0.481\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7039 | test accuracy: 0.458\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6903 | test accuracy: 0.492\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6834 | test accuracy: 0.529\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7150 | test accuracy: 0.478\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6872 | test accuracy: 0.495\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6856 | test accuracy: 0.492\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7296 | test accuracy: 0.498\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6758 | test accuracy: 0.485\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7250 | test accuracy: 0.458\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6854 | test accuracy: 0.481\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6703 | test accuracy: 0.468\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7113 | test accuracy: 0.492\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6470 | test accuracy: 0.495\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6615 | test accuracy: 0.508\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6304 | test accuracy: 0.498\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7966 | test accuracy: 0.485\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6564 | test accuracy: 0.478\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6429 | test accuracy: 0.481\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7041 | test accuracy: 0.485\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7191 | test accuracy: 0.485\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6679 | test accuracy: 0.492\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7112 | test accuracy: 0.485\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7292 | test accuracy: 0.481\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7380 | test accuracy: 0.481\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6417 | test accuracy: 0.481\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6903 | test accuracy: 0.478\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7408 | test accuracy: 0.485\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7419 | test accuracy: 0.488\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7153 | test accuracy: 0.478\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6852 | test accuracy: 0.478\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6672 | test accuracy: 0.478\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7661 | test accuracy: 0.481\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6449 | test accuracy: 0.481\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6490 | test accuracy: 0.488\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6529 | test accuracy: 0.481\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6221 | test accuracy: 0.485\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6944 | test accuracy: 0.478\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6551 | test accuracy: 0.488\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6989 | test accuracy: 0.481\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7231 | test accuracy: 0.485\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6116 | test accuracy: 0.478\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6369 | test accuracy: 0.485\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6953 | test accuracy: 0.481\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6971 | test accuracy: 0.481\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7550 | test accuracy: 0.492\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6271 | test accuracy: 0.471\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6911 | test accuracy: 0.478\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6515 | test accuracy: 0.478\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7392 | test accuracy: 0.468\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7004 | test accuracy: 0.492\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7252 | test accuracy: 0.478\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7395 | test accuracy: 0.492\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7291 | test accuracy: 0.495\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.8101 | test accuracy: 0.495\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6525 | test accuracy: 0.478\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6596 | test accuracy: 0.478\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6375 | test accuracy: 0.492\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6959 | test accuracy: 0.492\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6416 | test accuracy: 0.471\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6695 | test accuracy: 0.495\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6805 | test accuracy: 0.485\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6969 | test accuracy: 0.478\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6880 | test accuracy: 0.475\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7101 | test accuracy: 0.478\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7094 | test accuracy: 0.481\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7548 | test accuracy: 0.485\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6437 | test accuracy: 0.475\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7053 | test accuracy: 0.495\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7116 | test accuracy: 0.481\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6032 | test accuracy: 0.495\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7334 | test accuracy: 0.471\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7270 | test accuracy: 0.492\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7422 | test accuracy: 0.478\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7297 | test accuracy: 0.475\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6865 | test accuracy: 0.475\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6993 | test accuracy: 0.468\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6861 | test accuracy: 0.488\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6267 | test accuracy: 0.492\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7149 | test accuracy: 0.485\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7433 | test accuracy: 0.488\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7714 | test accuracy: 0.475\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6535 | test accuracy: 0.495\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7296 | test accuracy: 0.492\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6036 | test accuracy: 0.481\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6790 | test accuracy: 0.495\n",
            "total time:  31.41168425199976\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8584 | test accuracy: 0.535\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6765 | test accuracy: 0.529\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7821 | test accuracy: 0.502\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7483 | test accuracy: 0.468\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8962 | test accuracy: 0.488\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6338 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.9239 | test accuracy: 0.465\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8853 | test accuracy: 0.498\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7960 | test accuracy: 0.481\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6618 | test accuracy: 0.468\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8707 | test accuracy: 0.465\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7299 | test accuracy: 0.465\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8597 | test accuracy: 0.465\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7831 | test accuracy: 0.465\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7850 | test accuracy: 0.468\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7745 | test accuracy: 0.455\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5511 | test accuracy: 0.458\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7335 | test accuracy: 0.451\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6543 | test accuracy: 0.448\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8495 | test accuracy: 0.505\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5296 | test accuracy: 0.475\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7322 | test accuracy: 0.471\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6429 | test accuracy: 0.471\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7400 | test accuracy: 0.481\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6811 | test accuracy: 0.458\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7045 | test accuracy: 0.475\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7412 | test accuracy: 0.448\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7045 | test accuracy: 0.461\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7519 | test accuracy: 0.471\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7598 | test accuracy: 0.468\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7381 | test accuracy: 0.471\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6988 | test accuracy: 0.492\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7213 | test accuracy: 0.468\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7029 | test accuracy: 0.475\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6901 | test accuracy: 0.461\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7204 | test accuracy: 0.465\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6544 | test accuracy: 0.478\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7032 | test accuracy: 0.468\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7023 | test accuracy: 0.465\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6545 | test accuracy: 0.492\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6949 | test accuracy: 0.481\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6042 | test accuracy: 0.492\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6619 | test accuracy: 0.481\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6505 | test accuracy: 0.492\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6859 | test accuracy: 0.488\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7381 | test accuracy: 0.495\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6535 | test accuracy: 0.488\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6747 | test accuracy: 0.495\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7034 | test accuracy: 0.481\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6161 | test accuracy: 0.488\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6495 | test accuracy: 0.485\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6860 | test accuracy: 0.495\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7170 | test accuracy: 0.485\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7006 | test accuracy: 0.492\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7002 | test accuracy: 0.485\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6637 | test accuracy: 0.492\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7415 | test accuracy: 0.485\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6820 | test accuracy: 0.481\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6283 | test accuracy: 0.485\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6897 | test accuracy: 0.485\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7065 | test accuracy: 0.488\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6382 | test accuracy: 0.478\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7232 | test accuracy: 0.478\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7029 | test accuracy: 0.492\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7704 | test accuracy: 0.485\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6115 | test accuracy: 0.488\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6942 | test accuracy: 0.485\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7003 | test accuracy: 0.485\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7495 | test accuracy: 0.481\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6662 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7179 | test accuracy: 0.465\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7124 | test accuracy: 0.485\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7248 | test accuracy: 0.488\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7673 | test accuracy: 0.488\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6283 | test accuracy: 0.485\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7699 | test accuracy: 0.488\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7370 | test accuracy: 0.492\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.5838 | test accuracy: 0.485\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7302 | test accuracy: 0.485\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6572 | test accuracy: 0.488\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6451 | test accuracy: 0.481\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6637 | test accuracy: 0.485\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6892 | test accuracy: 0.492\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6243 | test accuracy: 0.485\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7452 | test accuracy: 0.485\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7408 | test accuracy: 0.492\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6215 | test accuracy: 0.451\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7408 | test accuracy: 0.488\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6357 | test accuracy: 0.455\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.5991 | test accuracy: 0.458\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6092 | test accuracy: 0.488\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7733 | test accuracy: 0.492\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7362 | test accuracy: 0.488\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6613 | test accuracy: 0.468\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6620 | test accuracy: 0.471\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6740 | test accuracy: 0.488\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6299 | test accuracy: 0.488\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6858 | test accuracy: 0.488\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7131 | test accuracy: 0.488\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6799 | test accuracy: 0.461\n",
            "total time:  36.159953824000695\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503385543823242.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.44222307205200195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6884294697216579 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2489912509918213.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4389617443084717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5490435889789036 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26853251457214355.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4453561305999756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46512312165328434 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.272125244140625.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.45982837677001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.42114626339503697 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26915836334228516.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.45543646812438965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38975539803504944 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26462483406066895.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.440873384475708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3757827699184418 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25613927841186523.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.45488905906677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3621615822826113 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25816798210144043.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44337034225463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35450514129229954 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2665529251098633.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4437704086303711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3484921395778656 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24167847633361816.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4326767921447754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34282166702406747 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26307106018066406.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.44916343688964844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3396566974265235 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26937246322631836.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45422959327697754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3350039128746305 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645747661590576.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44202446937561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3319725351674216 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25592994689941406.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.434558629989624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3297436535358429 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27014994621276855.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4468653202056885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32851755023002627 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604072093963623.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44739413261413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3266120480639594 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24965763092041016.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43801188468933105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32598611755030493 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2664327621459961.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4544408321380615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3243180215358734 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26234865188598633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4422273635864258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3232450463942119 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2801189422607422.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4835023880004883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32185428653444564 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27642154693603516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47585391998291016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3216343526329313 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632737159729004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45696353912353516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3209484479257039 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555227279663086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4381282329559326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.32014663985797337 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561910152435303.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4407939910888672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3197887122631073 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601163387298584.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4487495422363281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31927243726594107 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2560868263244629.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45198822021484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3186728549855096 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27068185806274414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4871494770050049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31826828249863215 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25666332244873047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4409472942352295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.317924126131194 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2817254066467285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47377896308898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31766106826918467 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25241708755493164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4332606792449951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3172079294919968 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27187371253967285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4531524181365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3169142493179866 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25763463973999023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4301412105560303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.316900920016425 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24981927871704102.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4352080821990967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3166470970426287 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2692892551422119.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4528188705444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3164133237940924 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24708199501037598.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4281797409057617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31635314992495944 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254101037979126.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4373183250427246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31632967378411975 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25203609466552734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43812108039855957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31594805845192503 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28650832176208496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4652059078216553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31588833544935496 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615640163421631.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4466087818145752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31557576188019343 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27278852462768555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4733271598815918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31554243224007744 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2816479206085205.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4595472812652588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31555102935859136 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26304149627685547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48337364196777344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31526828365666526 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2668919563293457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45211315155029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31513238719531467 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24462580680847168.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44144511222839355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31511164350169046 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2726168632507324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4612772464752197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31525670077119555 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25264573097229004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43825769424438477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3148972830602101 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627701759338379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44199037551879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31489607010568893 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24666762351989746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4260118007659912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147494401250567 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26929450035095215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46258544921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3148249294076647 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613961696624756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4423079490661621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3147388811622347 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583484649658203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4473867416381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31465604049818857 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544417381286621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43288707733154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31455678045749663 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2519657611846924.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43120789527893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144972860813141 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2702779769897461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45093250274658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144464177744729 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25326967239379883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43412208557128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3144902620996748 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27735376358032227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4642767906188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31449953487941196 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2525961399078369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43844008445739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3143132231065205 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2680971622467041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46697545051574707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31428904831409454 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25695300102233887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360983371734619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142417256321226 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2580561637878418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45120954513549805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142866181475776 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26598477363586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4532339572906494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141803903239114 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26726818084716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4646291732788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141131882156645 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.265484094619751.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.448319673538208\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31408192217350006 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2585315704345703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4731473922729492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.314064616390637 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2853217124938965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4714827537536621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140951744147709 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562887668609619.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43825507164001465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140537772859846 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643623352050781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45963358879089355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3140394287449973 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25336599349975586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4305131435394287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3140657910278865 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521042823791504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43303585052490234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31397090000765665 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678096294403076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4445509910583496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31400972519602094 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25496721267700195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4482240676879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139135241508484 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26805830001831055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.458057165145874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.313981300166675 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593269348144531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44452476501464844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3139261267014912 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2903404235839844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48629069328308105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31388145472322193 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.253798246383667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45072388648986816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138400563171932 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24704623222351074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4388101100921631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31386891348021373 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531249523162842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360036849975586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3138407430478505 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616465091705322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44025230407714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137720176151821 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2735874652862549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45488405227661133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.313749007667814 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26068782806396484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44315505027770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3138113754136222 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2677943706512451.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45023179054260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137794349874769 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26795101165771484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45934128761291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137244697128023 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2567768096923828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45271778106689453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31372576909405847 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27074646949768066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.486431360244751\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31378124271120345 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721598148345947.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46993255615234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.313712255018098 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645077705383301.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4698493480682373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31376139393874575 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2779815196990967.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46705031394958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136769358600889 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26255035400390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44034337997436523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31372295788356236 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2639787197113037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45481109619140625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136565374476569 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27400922775268555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4531266689300537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3137291018451963 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26105809211730957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4446439743041992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136651826756341 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26041579246520996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4482865333557129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31360011526516507 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24965286254882812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4329512119293213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136287991489683 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2505183219909668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4400153160095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136498361825943 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627298831939697.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.458820104598999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31356917108808247 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629818916320801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44727134704589844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136307362999235 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623162269592285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44218993186950684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31358068798269545 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510089874267578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4309718608856201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31362545362540656 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26262879371643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43665003776550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31354434405054366 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2624073028564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44120264053344727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31361737038408005 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646000385284424.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45464491844177246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31356758305004667 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557353973388672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44417572021484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135700000183923 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26064062118530273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4333665370941162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135438527379717 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26742100715637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4502840042114258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135925429207938 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25644755363464355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4393932819366455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135659124170031 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688007354736328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44930458068847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31354647406509945 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694566249847412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46682000160217285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31351899036339353 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24081015586853027.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42548704147338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135323541504996 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27292490005493164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4614894390106201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31353229710033964 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24974822998046875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42213869094848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135284662246704 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25147557258605957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44325733184814453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135440766811371 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2639124393463135.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4464092254638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135062302861895 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25873589515686035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4436759948730469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135274248463767 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25325489044189453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43442702293395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347782100949967 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24469304084777832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4346959590911865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135319215910775 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261063814163208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44429612159729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31349898150988986 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24104619026184082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41037988662719727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31350264464105876 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539033889770508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4442257881164551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134746483394078 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2439424991607666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41396570205688477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347041385514396 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26141977310180664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4443244934082031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134597050292151 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25350189208984375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4222123622894287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31348426171711513 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2439098358154297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4251675605773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31348635171140943 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25260186195373535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43847084045410156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3135039014475686 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527179718017578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42494773864746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31345741493361334 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26573657989501953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44553136825561523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134611236197608 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25513792037963867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4296300411224365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134412739958082 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619500160217285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4526698589324951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134710418326514 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2638356685638428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45383429527282715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134467316525323 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7679 | test accuracy: 0.475\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6024 | test accuracy: 0.465\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7298 | test accuracy: 0.535\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6954 | test accuracy: 0.519\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6778 | test accuracy: 0.481\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7479 | test accuracy: 0.461\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6753 | test accuracy: 0.515\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7075 | test accuracy: 0.475\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6900 | test accuracy: 0.451\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6773 | test accuracy: 0.485\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7376 | test accuracy: 0.488\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7030 | test accuracy: 0.444\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6832 | test accuracy: 0.478\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6787 | test accuracy: 0.508\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6578 | test accuracy: 0.532\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7805 | test accuracy: 0.481\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6305 | test accuracy: 0.465\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6636 | test accuracy: 0.495\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6783 | test accuracy: 0.468\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7283 | test accuracy: 0.498\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7060 | test accuracy: 0.498\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6601 | test accuracy: 0.498\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6817 | test accuracy: 0.471\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6020 | test accuracy: 0.471\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7002 | test accuracy: 0.498\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7025 | test accuracy: 0.498\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7373 | test accuracy: 0.485\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7315 | test accuracy: 0.488\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6343 | test accuracy: 0.471\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6694 | test accuracy: 0.485\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6091 | test accuracy: 0.485\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6855 | test accuracy: 0.475\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6944 | test accuracy: 0.492\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6946 | test accuracy: 0.492\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7285 | test accuracy: 0.495\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6193 | test accuracy: 0.485\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6925 | test accuracy: 0.485\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7592 | test accuracy: 0.498\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6558 | test accuracy: 0.481\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6965 | test accuracy: 0.481\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6853 | test accuracy: 0.485\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7422 | test accuracy: 0.495\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7024 | test accuracy: 0.488\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6889 | test accuracy: 0.485\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6674 | test accuracy: 0.485\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7710 | test accuracy: 0.485\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7348 | test accuracy: 0.485\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6769 | test accuracy: 0.488\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6806 | test accuracy: 0.505\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6627 | test accuracy: 0.492\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5862 | test accuracy: 0.492\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6328 | test accuracy: 0.502\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6939 | test accuracy: 0.488\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6787 | test accuracy: 0.498\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6654 | test accuracy: 0.488\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6122 | test accuracy: 0.495\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7267 | test accuracy: 0.485\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7083 | test accuracy: 0.488\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6733 | test accuracy: 0.492\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6570 | test accuracy: 0.492\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6824 | test accuracy: 0.485\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6657 | test accuracy: 0.488\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7367 | test accuracy: 0.488\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6432 | test accuracy: 0.485\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6402 | test accuracy: 0.492\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6186 | test accuracy: 0.495\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7102 | test accuracy: 0.508\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6186 | test accuracy: 0.492\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7030 | test accuracy: 0.485\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6521 | test accuracy: 0.481\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6800 | test accuracy: 0.485\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7404 | test accuracy: 0.495\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7025 | test accuracy: 0.481\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7210 | test accuracy: 0.492\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7024 | test accuracy: 0.492\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6923 | test accuracy: 0.495\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6983 | test accuracy: 0.478\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7268 | test accuracy: 0.478\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6851 | test accuracy: 0.485\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7253 | test accuracy: 0.485\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7245 | test accuracy: 0.502\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7590 | test accuracy: 0.485\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6871 | test accuracy: 0.485\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6759 | test accuracy: 0.481\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7447 | test accuracy: 0.485\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6848 | test accuracy: 0.481\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6770 | test accuracy: 0.485\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7404 | test accuracy: 0.498\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6174 | test accuracy: 0.498\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6489 | test accuracy: 0.481\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7136 | test accuracy: 0.498\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6697 | test accuracy: 0.502\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7419 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6731 | test accuracy: 0.481\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7166 | test accuracy: 0.471\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7395 | test accuracy: 0.481\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7329 | test accuracy: 0.475\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7719 | test accuracy: 0.475\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6873 | test accuracy: 0.475\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6969 | test accuracy: 0.478\n",
            "total time:  31.885826339000232\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8148 | test accuracy: 0.475\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8257 | test accuracy: 0.468\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6448 | test accuracy: 0.465\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5740 | test accuracy: 0.455\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8086 | test accuracy: 0.471\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7705 | test accuracy: 0.465\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6504 | test accuracy: 0.465\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7782 | test accuracy: 0.485\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5674 | test accuracy: 0.465\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7152 | test accuracy: 0.515\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8339 | test accuracy: 0.478\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6616 | test accuracy: 0.481\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6232 | test accuracy: 0.505\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7373 | test accuracy: 0.488\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.8123 | test accuracy: 0.508\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6537 | test accuracy: 0.488\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7027 | test accuracy: 0.498\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7363 | test accuracy: 0.495\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6730 | test accuracy: 0.481\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7164 | test accuracy: 0.488\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6743 | test accuracy: 0.465\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7107 | test accuracy: 0.461\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6890 | test accuracy: 0.465\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6918 | test accuracy: 0.478\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6991 | test accuracy: 0.488\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6425 | test accuracy: 0.478\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6073 | test accuracy: 0.461\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6620 | test accuracy: 0.465\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6446 | test accuracy: 0.505\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6785 | test accuracy: 0.485\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6177 | test accuracy: 0.465\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6336 | test accuracy: 0.448\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6282 | test accuracy: 0.461\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7096 | test accuracy: 0.468\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7197 | test accuracy: 0.468\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6710 | test accuracy: 0.468\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6531 | test accuracy: 0.471\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6945 | test accuracy: 0.468\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7821 | test accuracy: 0.471\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7471 | test accuracy: 0.468\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.5847 | test accuracy: 0.468\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6965 | test accuracy: 0.471\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6714 | test accuracy: 0.465\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6870 | test accuracy: 0.471\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7336 | test accuracy: 0.471\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6120 | test accuracy: 0.471\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7451 | test accuracy: 0.478\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6374 | test accuracy: 0.465\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6601 | test accuracy: 0.461\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6100 | test accuracy: 0.465\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6970 | test accuracy: 0.478\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6836 | test accuracy: 0.461\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6559 | test accuracy: 0.471\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6454 | test accuracy: 0.471\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5950 | test accuracy: 0.471\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6908 | test accuracy: 0.478\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7715 | test accuracy: 0.471\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6880 | test accuracy: 0.468\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7765 | test accuracy: 0.465\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7800 | test accuracy: 0.471\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7489 | test accuracy: 0.475\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7029 | test accuracy: 0.465\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6956 | test accuracy: 0.465\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7426 | test accuracy: 0.475\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6875 | test accuracy: 0.465\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6289 | test accuracy: 0.471\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6806 | test accuracy: 0.488\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6908 | test accuracy: 0.485\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7621 | test accuracy: 0.488\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6063 | test accuracy: 0.481\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7456 | test accuracy: 0.478\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6884 | test accuracy: 0.468\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7225 | test accuracy: 0.478\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6669 | test accuracy: 0.488\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6767 | test accuracy: 0.488\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6640 | test accuracy: 0.478\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7623 | test accuracy: 0.471\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6933 | test accuracy: 0.475\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7233 | test accuracy: 0.471\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6803 | test accuracy: 0.475\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6474 | test accuracy: 0.471\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6601 | test accuracy: 0.485\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6638 | test accuracy: 0.468\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6927 | test accuracy: 0.471\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7022 | test accuracy: 0.468\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6547 | test accuracy: 0.465\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7570 | test accuracy: 0.471\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7120 | test accuracy: 0.471\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6524 | test accuracy: 0.481\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7514 | test accuracy: 0.481\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7236 | test accuracy: 0.468\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7099 | test accuracy: 0.481\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7094 | test accuracy: 0.475\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7178 | test accuracy: 0.485\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7043 | test accuracy: 0.485\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6813 | test accuracy: 0.471\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7071 | test accuracy: 0.471\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6878 | test accuracy: 0.475\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7817 | test accuracy: 0.475\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6462 | test accuracy: 0.475\n",
            "total time:  37.31329319499946\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629055976867676.\n",
            "\n",
            "  Average training loss: 0.71\n",
            "  Training epoch took: 0.45624780654907227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7066172506128039 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2771427631378174.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.46872615814208984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5611413508653641 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29146766662597656.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.48344874382019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.47372560117925916 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541372776031494.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44718074798583984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4220944745200021 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28829145431518555.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4804959297180176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39785380491188593 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2650320529937744.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.45677900314331055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3795854410954884 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27539539337158203.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4866211414337158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36636294424533844 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643918991088867.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4488239288330078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35651594655854363 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27942514419555664.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4754188060760498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3482489790235247 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2734980583190918.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.44790053367614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34414483862263817 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630889415740967.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4609558582305908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.34074537200587135 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27696657180786133.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46752333641052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33599586146218435 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27122950553894043.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46094417572021484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3336872743708747 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2763659954071045.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4582490921020508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3313008078507015 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627990245819092.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46370887756347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32885185735566275 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2805609703063965.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4708220958709717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32696461294378554 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651824951171875.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4627721309661865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32607731010232655 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2722625732421875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4595954418182373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32425106550965993 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2594740390777588.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44159436225891113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3236306288412639 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28031492233276367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48088932037353516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3224700591393879 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27075839042663574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4619588851928711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3214390469448907 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2782292366027832.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47901344299316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3210911967924663 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27059316635131836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46349096298217773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.32069993785449435 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28648996353149414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4826655387878418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.32037513468946727 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2779884338378906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4669363498687744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31972176347460063 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2850794792175293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4837946891784668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31912079751491546 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2801170349121094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47287869453430176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.318424545441355 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.264406681060791.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46984338760375977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3186126568487712 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27947068214416504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.485515832901001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31813916180815016 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28038597106933594.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49419212341308594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31764379952635086 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26456570625305176.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45557069778442383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3173729687929153 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661454677581787.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4541487693786621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31708707298551286 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28392648696899414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4794456958770752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3170580199786595 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26410412788391113.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4475698471069336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3166829322065626 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.272019624710083.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46283745765686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3164744896548135 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25782084465026855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43451714515686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31630796364375524 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2818624973297119.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47362637519836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3160362443753651 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671780586242676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44617342948913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3161576679774693 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27259397506713867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4731485843658447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31590271123817987 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777836322784424.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.469454288482666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31591853286538807 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27711939811706543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48438119888305664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31548164061137607 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28225183486938477.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47165393829345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31561376069273267 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645888328552246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45512866973876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31530097808156693 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28025102615356445.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4781217575073242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31526224868638175 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28549790382385254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4908759593963623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3151894348008292 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27646636962890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4645063877105713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31495842507907323 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25798749923706055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.458965539932251\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31488201107297625 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29688096046447754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4938228130340576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3148803595985685 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269303560256958.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4573981761932373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31470818477017537 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2937757968902588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48214006423950195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3147510635001319 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25784945487976074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44899940490722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31464025889124186 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2775444984436035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4779367446899414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3146061509847641 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26614975929260254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.460705041885376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3145451873540878 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2779686450958252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5091588497161865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31460513642856053 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26644420623779297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4546022415161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31445160593305316 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26323509216308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45137977600097656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3144879617861339 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738070487976074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45638608932495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31440499084336415 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26334667205810547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44518327713012695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31428238706929346 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29250168800354004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4804840087890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3143071221453803 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26447629928588867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.446974515914917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.314238612140928 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592284679412842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4541809558868408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31429311079638345 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555105686187744.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4366436004638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3142255582979747 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577178478240967.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4624309539794922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.314201392020498 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27597880363464355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46558165550231934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31409122134957995 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.272935152053833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4638712406158447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3141467958688736 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28092026710510254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47414159774780273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140531301498413 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28369665145874023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48067641258239746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31402207357542855 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2783944606781006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47237205505371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3140541762113571 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2796797752380371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4692997932434082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.314000414950507 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27364516258239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4591352939605713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3140108287334442 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2723817825317383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4732346534729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31389825088637213 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28470420837402344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47263598442077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138857764857156 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2687816619873047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46702075004577637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31386278825146813 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2747797966003418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4701406955718994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138853247676577 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25626707077026367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44489026069641113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138591161796025 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26563334465026855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4496161937713623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138244811977659 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26618528366088867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45943689346313477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.313892040508134 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2664525508880615.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4505891799926758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.313858551638467 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26644468307495117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4546225070953369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3138695546558925 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698023319244385.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.458773136138916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31381618934018274 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721221446990967.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4517498016357422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31380033067294527 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627866268157959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44525861740112305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31382263132504057 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2526881694793701.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4398529529571533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31378052532672884 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25151872634887695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4489784240722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31373946368694305 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2686169147491455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46074557304382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137866939817156 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682037353515625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4507732391357422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31374948535646713 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27507948875427246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4677085876464844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31374390508447375 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2799537181854248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4695546627044678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31369379545961107 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.293410062789917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49007248878479004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31365789004734584 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616863250732422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4531540870666504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136896414416177 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27818989753723145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4628629684448242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136741318872997 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2488868236541748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4386262893676758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136405148676464 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2683689594268799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46814918518066406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31366749831608365 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713203430175781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45694422721862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31365597333226886 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527437210083008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4383840560913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136628244604383 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.258223295211792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4311659336090088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135873143162046 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27004337310791016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4713292121887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135718439306532 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27536964416503906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.451282262802124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31364230470997945 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510206699371338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4311256408691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135854286806924 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615530490875244.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487035274505615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31357422173023225 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23960232734680176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42230868339538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135932675429753 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25592923164367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4336388111114502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135754372392382 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2537975311279297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43131232261657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135935344866344 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2580592632293701.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44713807106018066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3136306383780071 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26200246810913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43615293502807617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135641779218401 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25427889823913574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4356849193572998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135443244661604 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2689199447631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4595212936401367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135543976511274 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25308942794799805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4421095848083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135240656988961 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2574729919433594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43735647201538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31363478473254613 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578270435333252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43631458282470703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31350491898400445 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2854187488555908.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4767603874206543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135214592729296 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26805853843688965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4491293430328369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135481434209006 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27617311477661133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46561193466186523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134972197668893 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516155242919922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4257643222808838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347963980266025 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259387731552124.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4521775245666504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134845967803683 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2600998878479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44005465507507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3135387510061264 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542989253997803.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44563841819763184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347824675696234 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2617189884185791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44669151306152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31351011267730167 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2554149627685547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4363877773284912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134851757969175 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28952550888061523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4761025905609131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134976748909269 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643728256225586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44445109367370605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134675140891756 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26998043060302734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48369288444519043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31349728533199855 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27086377143859863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4619908332824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134642950126103 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2639937400817871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4728536605834961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31348211169242857 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777729034423828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4705829620361328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345420096601756 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651240825653076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4664435386657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31349725680691853 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.292356014251709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48054027557373047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134771372590746 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26122593879699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45116114616394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31344994263989584 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6177 | test accuracy: 0.498\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7243 | test accuracy: 0.492\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6967 | test accuracy: 0.485\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6916 | test accuracy: 0.471\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7462 | test accuracy: 0.492\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7108 | test accuracy: 0.468\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7187 | test accuracy: 0.465\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7145 | test accuracy: 0.502\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5913 | test accuracy: 0.495\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5552 | test accuracy: 0.498\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6452 | test accuracy: 0.495\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7300 | test accuracy: 0.498\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7497 | test accuracy: 0.492\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6392 | test accuracy: 0.502\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7632 | test accuracy: 0.498\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6963 | test accuracy: 0.505\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6436 | test accuracy: 0.481\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6680 | test accuracy: 0.502\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6316 | test accuracy: 0.478\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6756 | test accuracy: 0.478\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6698 | test accuracy: 0.458\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7168 | test accuracy: 0.512\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6777 | test accuracy: 0.505\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6774 | test accuracy: 0.468\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6674 | test accuracy: 0.458\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6395 | test accuracy: 0.478\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7089 | test accuracy: 0.458\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7306 | test accuracy: 0.519\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6361 | test accuracy: 0.505\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6426 | test accuracy: 0.468\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7423 | test accuracy: 0.515\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6821 | test accuracy: 0.525\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7277 | test accuracy: 0.522\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7168 | test accuracy: 0.505\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6992 | test accuracy: 0.508\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6466 | test accuracy: 0.508\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7234 | test accuracy: 0.525\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6677 | test accuracy: 0.525\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7153 | test accuracy: 0.512\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7543 | test accuracy: 0.512\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.8077 | test accuracy: 0.508\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6769 | test accuracy: 0.512\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6895 | test accuracy: 0.505\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7052 | test accuracy: 0.502\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7842 | test accuracy: 0.522\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6838 | test accuracy: 0.508\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6945 | test accuracy: 0.529\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6156 | test accuracy: 0.502\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5957 | test accuracy: 0.525\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6370 | test accuracy: 0.525\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6412 | test accuracy: 0.498\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6457 | test accuracy: 0.505\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7715 | test accuracy: 0.529\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6907 | test accuracy: 0.505\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6137 | test accuracy: 0.502\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6893 | test accuracy: 0.505\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6929 | test accuracy: 0.512\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6761 | test accuracy: 0.519\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6332 | test accuracy: 0.525\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5939 | test accuracy: 0.529\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7071 | test accuracy: 0.512\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6872 | test accuracy: 0.519\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6604 | test accuracy: 0.519\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7103 | test accuracy: 0.522\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6221 | test accuracy: 0.502\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6057 | test accuracy: 0.498\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7380 | test accuracy: 0.519\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6286 | test accuracy: 0.522\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7181 | test accuracy: 0.522\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7249 | test accuracy: 0.519\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6306 | test accuracy: 0.515\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6762 | test accuracy: 0.502\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7438 | test accuracy: 0.529\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6775 | test accuracy: 0.502\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6203 | test accuracy: 0.502\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6635 | test accuracy: 0.512\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6311 | test accuracy: 0.495\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.5935 | test accuracy: 0.519\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7067 | test accuracy: 0.498\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7478 | test accuracy: 0.525\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6853 | test accuracy: 0.495\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7612 | test accuracy: 0.522\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6758 | test accuracy: 0.529\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7470 | test accuracy: 0.502\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7290 | test accuracy: 0.508\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6952 | test accuracy: 0.525\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6512 | test accuracy: 0.512\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6854 | test accuracy: 0.498\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6813 | test accuracy: 0.515\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6397 | test accuracy: 0.515\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6564 | test accuracy: 0.512\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7425 | test accuracy: 0.512\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6763 | test accuracy: 0.502\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6468 | test accuracy: 0.515\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6928 | test accuracy: 0.512\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6622 | test accuracy: 0.522\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6630 | test accuracy: 0.519\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7336 | test accuracy: 0.525\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6385 | test accuracy: 0.505\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7747 | test accuracy: 0.515\n",
            "total time:  31.39646313599951\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9560 | test accuracy: 0.502\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4585 | test accuracy: 0.502\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6205 | test accuracy: 0.502\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6529 | test accuracy: 0.461\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9494 | test accuracy: 0.502\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7295 | test accuracy: 0.505\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.9035 | test accuracy: 0.525\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7317 | test accuracy: 0.471\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6512 | test accuracy: 0.525\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.8120 | test accuracy: 0.485\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6427 | test accuracy: 0.475\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6819 | test accuracy: 0.495\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6028 | test accuracy: 0.498\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6738 | test accuracy: 0.498\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5576 | test accuracy: 0.481\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5869 | test accuracy: 0.495\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7470 | test accuracy: 0.529\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7698 | test accuracy: 0.471\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7817 | test accuracy: 0.492\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6662 | test accuracy: 0.529\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7129 | test accuracy: 0.515\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6701 | test accuracy: 0.505\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6249 | test accuracy: 0.451\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6849 | test accuracy: 0.492\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6729 | test accuracy: 0.505\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7446 | test accuracy: 0.455\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7099 | test accuracy: 0.502\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7130 | test accuracy: 0.492\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6505 | test accuracy: 0.505\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7276 | test accuracy: 0.498\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6603 | test accuracy: 0.495\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7023 | test accuracy: 0.502\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6997 | test accuracy: 0.498\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6679 | test accuracy: 0.495\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6131 | test accuracy: 0.488\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6504 | test accuracy: 0.505\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6749 | test accuracy: 0.515\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6402 | test accuracy: 0.495\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7151 | test accuracy: 0.512\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5976 | test accuracy: 0.515\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6159 | test accuracy: 0.508\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6616 | test accuracy: 0.498\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6229 | test accuracy: 0.498\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7127 | test accuracy: 0.508\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8287 | test accuracy: 0.502\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7923 | test accuracy: 0.502\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7039 | test accuracy: 0.502\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6840 | test accuracy: 0.515\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6944 | test accuracy: 0.498\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6227 | test accuracy: 0.508\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6590 | test accuracy: 0.495\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7958 | test accuracy: 0.505\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7536 | test accuracy: 0.485\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6772 | test accuracy: 0.502\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6476 | test accuracy: 0.498\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6519 | test accuracy: 0.498\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6825 | test accuracy: 0.512\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7539 | test accuracy: 0.498\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6308 | test accuracy: 0.502\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6718 | test accuracy: 0.515\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.5915 | test accuracy: 0.498\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7932 | test accuracy: 0.495\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7475 | test accuracy: 0.488\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6928 | test accuracy: 0.495\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6557 | test accuracy: 0.502\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6975 | test accuracy: 0.508\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7089 | test accuracy: 0.498\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6272 | test accuracy: 0.505\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6708 | test accuracy: 0.508\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7799 | test accuracy: 0.505\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.8057 | test accuracy: 0.512\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7184 | test accuracy: 0.508\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7483 | test accuracy: 0.515\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7415 | test accuracy: 0.492\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6073 | test accuracy: 0.512\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6237 | test accuracy: 0.505\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7581 | test accuracy: 0.488\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6993 | test accuracy: 0.508\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7002 | test accuracy: 0.505\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6488 | test accuracy: 0.508\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6133 | test accuracy: 0.498\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6688 | test accuracy: 0.508\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7356 | test accuracy: 0.508\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6460 | test accuracy: 0.502\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6740 | test accuracy: 0.498\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7227 | test accuracy: 0.495\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6146 | test accuracy: 0.508\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6508 | test accuracy: 0.508\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7452 | test accuracy: 0.495\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6368 | test accuracy: 0.508\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6703 | test accuracy: 0.508\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6730 | test accuracy: 0.498\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6405 | test accuracy: 0.512\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7614 | test accuracy: 0.495\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6782 | test accuracy: 0.502\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7398 | test accuracy: 0.505\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7950 | test accuracy: 0.508\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7233 | test accuracy: 0.498\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6975 | test accuracy: 0.498\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7980 | test accuracy: 0.515\n",
            "total time:  35.75985691400001\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2742648124694824.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0.4627056121826172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5238870893205915 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719757556915283.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.46141600608825684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.44329553076199124 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26147913932800293.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.43777036666870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4007274355207171 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26796865463256836.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4477694034576416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.37459977439471653 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2584564685821533.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4481210708618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.35960462604250226 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762894630432129.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4586949348449707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34969080175672257 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25584864616394043.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43839025497436523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3430985574211393 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2718665599822998.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46274375915527344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3388328777892249 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25876307487487793.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4457893371582031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3347093220267977 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2739384174346924.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46523427963256836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33117770424910953 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27895641326904297.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46802401542663574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32765495564256397 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28152036666870117.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.48536205291748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3268602873597826 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27410221099853516.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.460369348526001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3254573392016547 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25688624382019043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45337414741516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3233466148376465 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26581645011901855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.457014799118042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32262678572109765 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26044273376464844.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4689066410064697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.321636067543711 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662951946258545.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45676612854003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32127564166273387 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2481064796447754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42156338691711426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32030529720442635 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28075075149536133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4660775661468506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3196967452764511 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2655162811279297.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44430994987487793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3190984585455486 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2589714527130127.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4424855709075928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3187971119369779 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2588155269622803.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43370985984802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31785270614283423 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25411081314086914.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4512307643890381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3176851234265736 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251662015914917.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4330298900604248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3173838334424155 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26810741424560547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4487152099609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31695055450711934 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27887487411499023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46541619300842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31693818398884366 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26323962211608887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4577980041503906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31639592477253503 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2773294448852539.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4578423500061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3163364197526659 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25774049758911133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4328603744506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3161250229392733 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25679802894592285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45423269271850586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31580626581396376 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25875234603881836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4453873634338379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3158142992428371 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26887989044189453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4741635322570801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3156858580453055 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28084254264831543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4720175266265869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31544125931603567 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2675926685333252.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47315406799316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31520241371222907 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28388500213623047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4693748950958252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3152076474257878 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25958847999572754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.459625244140625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3150858006307057 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29778122901916504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4896876811981201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31501767465046476 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628657817840576.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44733214378356934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3149743203605924 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26946115493774414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45444488525390625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31505591784204756 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24706792831420898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4262080192565918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31489446886948175 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27344536781311035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46442437171936035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31470894174916403 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2703399658203125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45288968086242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31457002673830303 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27037882804870605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4732983112335205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31456205121108466 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2798304557800293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4658968448638916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.314537228005273 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2767372131347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46346473693847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31452892082078115 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26531004905700684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.450115442276001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31442842738968985 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733762264251709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4590568542480469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3144275554588863 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269376277923584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45778417587280273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3142438782112939 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593050003051758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44603896141052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3141718008688518 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555429935455322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4433586597442627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3142441259963172 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559812068939209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4295170307159424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3142899828297751 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24786591529846191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4268343448638916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3141785651445389 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24914979934692383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42216920852661133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3140928030014038 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25054264068603516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317610263824463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3140982734305518 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26282548904418945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4385247230529785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3141156528677259 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509911060333252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44059133529663086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140697377068656 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25589776039123535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4466242790222168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31400594157832007 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24866652488708496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4294271469116211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3140222851719175 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2750880718231201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45949697494506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.313993210877691 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26824402809143066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4398617744445801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.313904601761273 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504565715789795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42968010902404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.313852059841156 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2685995101928711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44303011894226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31394377265657697 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24705243110656738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43340158462524414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138145442519869 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2600252628326416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.457674503326416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.313906272820064 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572329044342041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359321594238281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138302070753915 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2567875385284424.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44694995880126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138184883764812 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2656056880950928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45910143852233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31378238414015086 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27825093269348145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4723367691040039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31374079031603674 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678697109222412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.452899694442749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31374745752130234 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25752830505371094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4412071704864502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137898755925042 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27987027168273926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4654655456542969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31378535883767267 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630338668823242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45807504653930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31379651938165937 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26523518562316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46111583709716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137171038559505 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682380676269531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4677152633666992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31370767099516733 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619442939758301.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4757544994354248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31367924341133663 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26088595390319824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45983242988586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.313702050277165 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2804248332977295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46167874336242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31370792984962464 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524733543395996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44215893745422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136703772204263 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27135229110717773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45616817474365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31371623618262157 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25772643089294434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4386098384857178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136325729744775 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26743197441101074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45047426223754883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31365115514823366 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577524185180664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4481358528137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136264153889247 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26356005668640137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4381375312805176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31358444477830616 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521498203277588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4293670654296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3135924394641604 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2672412395477295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45148611068725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31360489640917094 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25031137466430664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4273507595062256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136602546487536 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661426067352295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4512288570404053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31355017934526713 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26940226554870605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4524829387664795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31356001709188736 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25793027877807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4540386199951172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31362898775509424 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28498244285583496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46994733810424805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31357708403042384 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27231669425964355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46338677406311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31357144883700777 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26593875885009766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4548628330230713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135706744023732 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762606143951416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4640350341796875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135644848857607 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27422165870666504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4726743698120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31351914065224784 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27758073806762695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4862325191497803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31355042840753283 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28799915313720703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4873173236846924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135253297431128 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2909212112426758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4953019618988037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31351125623498644 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29666781425476074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5057988166809082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.313510246362005 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2759816646575928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4669032096862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31354074222700934 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2791779041290283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4824671745300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135199419089726 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2828996181488037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4685065746307373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31353357647146496 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2669198513031006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48226451873779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31351961961814334 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2703850269317627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46205973625183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135085037776402 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26656413078308105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4664309024810791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31347433796950747 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2705247402191162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4617629051208496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31349145557199204 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2530848979949951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45903921127319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31348733348505836 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2992732524871826.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49080991744995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31347607118742804 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2765994071960449.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.463458776473999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134921784911837 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2817654609680176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47550487518310547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134947989668165 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26828813552856445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44793081283569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134620738880975 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2820470333099365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4707977771759033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134787380695343 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26758551597595215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46795010566711426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31347491315432957 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29210424423217773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4791231155395508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31347989780562263 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2884707450866699.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4781785011291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31341659469263894 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679295539855957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4924490451812744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134550601243973 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590062618255615.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44647884368896484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134439068181174 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605884075164795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4561755657196045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31344989325319017 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26319384574890137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47690749168395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31345373264380866 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25686192512512207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46168088912963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.313459883417402 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29962587356567383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4936058521270752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31342777865273613 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2641453742980957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.481442928314209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134366146155766 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2810373306274414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4693617820739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31342117488384247 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26186275482177734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45054197311401367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134339681693486 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2814168930053711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4695472717285156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134489276579448 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2650284767150879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45020246505737305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134204741035189 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26988720893859863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46818971633911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134354851075581 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26275134086608887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44365978240966797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134262021098818 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2727985382080078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4638864994049072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134219855070114 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6741 | test accuracy: 0.461\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7111 | test accuracy: 0.485\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7117 | test accuracy: 0.488\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7993 | test accuracy: 0.502\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7605 | test accuracy: 0.465\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6936 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7357 | test accuracy: 0.458\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8449 | test accuracy: 0.485\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6757 | test accuracy: 0.475\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7132 | test accuracy: 0.481\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7347 | test accuracy: 0.498\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7057 | test accuracy: 0.475\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6662 | test accuracy: 0.488\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7145 | test accuracy: 0.485\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6056 | test accuracy: 0.502\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7263 | test accuracy: 0.505\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7571 | test accuracy: 0.502\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7367 | test accuracy: 0.512\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6677 | test accuracy: 0.461\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6883 | test accuracy: 0.508\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6241 | test accuracy: 0.468\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6966 | test accuracy: 0.488\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6519 | test accuracy: 0.515\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6144 | test accuracy: 0.498\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6080 | test accuracy: 0.525\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7638 | test accuracy: 0.458\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7208 | test accuracy: 0.465\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7250 | test accuracy: 0.522\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6641 | test accuracy: 0.532\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8164 | test accuracy: 0.525\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6230 | test accuracy: 0.539\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6930 | test accuracy: 0.519\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6650 | test accuracy: 0.505\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6295 | test accuracy: 0.515\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5656 | test accuracy: 0.532\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7280 | test accuracy: 0.508\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7354 | test accuracy: 0.525\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6919 | test accuracy: 0.535\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7545 | test accuracy: 0.535\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6867 | test accuracy: 0.525\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6210 | test accuracy: 0.532\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7092 | test accuracy: 0.529\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7479 | test accuracy: 0.532\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6721 | test accuracy: 0.529\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7098 | test accuracy: 0.532\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7053 | test accuracy: 0.529\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7331 | test accuracy: 0.529\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6927 | test accuracy: 0.532\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7073 | test accuracy: 0.529\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7485 | test accuracy: 0.508\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7138 | test accuracy: 0.535\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7364 | test accuracy: 0.505\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6201 | test accuracy: 0.532\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6985 | test accuracy: 0.532\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6037 | test accuracy: 0.539\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6917 | test accuracy: 0.529\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6584 | test accuracy: 0.535\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7183 | test accuracy: 0.542\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6967 | test accuracy: 0.522\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7383 | test accuracy: 0.532\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6862 | test accuracy: 0.532\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6184 | test accuracy: 0.515\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7346 | test accuracy: 0.515\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6943 | test accuracy: 0.529\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6330 | test accuracy: 0.522\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6822 | test accuracy: 0.532\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6745 | test accuracy: 0.522\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6948 | test accuracy: 0.519\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6510 | test accuracy: 0.535\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6858 | test accuracy: 0.532\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6059 | test accuracy: 0.529\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6099 | test accuracy: 0.535\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7388 | test accuracy: 0.532\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6689 | test accuracy: 0.532\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6701 | test accuracy: 0.525\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7619 | test accuracy: 0.515\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7300 | test accuracy: 0.508\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7611 | test accuracy: 0.512\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6462 | test accuracy: 0.535\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6510 | test accuracy: 0.529\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6707 | test accuracy: 0.529\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6571 | test accuracy: 0.522\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6558 | test accuracy: 0.532\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6769 | test accuracy: 0.512\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7258 | test accuracy: 0.535\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6639 | test accuracy: 0.535\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7259 | test accuracy: 0.532\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6428 | test accuracy: 0.508\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7029 | test accuracy: 0.508\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7139 | test accuracy: 0.525\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6912 | test accuracy: 0.539\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7369 | test accuracy: 0.519\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6639 | test accuracy: 0.519\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7149 | test accuracy: 0.529\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6085 | test accuracy: 0.522\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7039 | test accuracy: 0.512\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6756 | test accuracy: 0.539\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6960 | test accuracy: 0.539\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7402 | test accuracy: 0.539\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6901 | test accuracy: 0.512\n",
            "total time:  33.20150445499985\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6750 | test accuracy: 0.508\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5888 | test accuracy: 0.502\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5759 | test accuracy: 0.495\n",
            "Epoch:  3 Iteration:  280 | train loss: 1.0244 | test accuracy: 0.492\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5867 | test accuracy: 0.519\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6806 | test accuracy: 0.478\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6130 | test accuracy: 0.515\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8808 | test accuracy: 0.475\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7548 | test accuracy: 0.498\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7566 | test accuracy: 0.458\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6811 | test accuracy: 0.481\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7273 | test accuracy: 0.485\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8113 | test accuracy: 0.508\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7689 | test accuracy: 0.488\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7404 | test accuracy: 0.488\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5619 | test accuracy: 0.495\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6727 | test accuracy: 0.478\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6908 | test accuracy: 0.495\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6430 | test accuracy: 0.502\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7324 | test accuracy: 0.488\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7314 | test accuracy: 0.488\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7084 | test accuracy: 0.498\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7379 | test accuracy: 0.458\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7313 | test accuracy: 0.444\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6877 | test accuracy: 0.522\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5793 | test accuracy: 0.485\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6587 | test accuracy: 0.471\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6801 | test accuracy: 0.508\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6532 | test accuracy: 0.485\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6855 | test accuracy: 0.481\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7376 | test accuracy: 0.488\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6266 | test accuracy: 0.475\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6412 | test accuracy: 0.458\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7193 | test accuracy: 0.465\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6773 | test accuracy: 0.492\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7495 | test accuracy: 0.492\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7078 | test accuracy: 0.488\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7123 | test accuracy: 0.468\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6627 | test accuracy: 0.498\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6947 | test accuracy: 0.495\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7832 | test accuracy: 0.498\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7288 | test accuracy: 0.492\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6385 | test accuracy: 0.498\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7106 | test accuracy: 0.475\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7625 | test accuracy: 0.498\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6731 | test accuracy: 0.495\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6187 | test accuracy: 0.498\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7451 | test accuracy: 0.495\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7517 | test accuracy: 0.492\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7446 | test accuracy: 0.498\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6098 | test accuracy: 0.475\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7090 | test accuracy: 0.465\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7627 | test accuracy: 0.492\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7447 | test accuracy: 0.488\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6717 | test accuracy: 0.485\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7008 | test accuracy: 0.488\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6862 | test accuracy: 0.488\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6199 | test accuracy: 0.495\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7557 | test accuracy: 0.492\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6692 | test accuracy: 0.485\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7494 | test accuracy: 0.492\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6887 | test accuracy: 0.485\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6764 | test accuracy: 0.485\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6622 | test accuracy: 0.478\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6721 | test accuracy: 0.478\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6861 | test accuracy: 0.481\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6731 | test accuracy: 0.485\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6538 | test accuracy: 0.488\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6638 | test accuracy: 0.485\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6523 | test accuracy: 0.488\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5846 | test accuracy: 0.485\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7108 | test accuracy: 0.485\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6312 | test accuracy: 0.492\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6947 | test accuracy: 0.485\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7547 | test accuracy: 0.485\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6760 | test accuracy: 0.492\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7092 | test accuracy: 0.488\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6790 | test accuracy: 0.498\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5973 | test accuracy: 0.485\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7285 | test accuracy: 0.498\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7496 | test accuracy: 0.485\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7304 | test accuracy: 0.492\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7455 | test accuracy: 0.488\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6974 | test accuracy: 0.485\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6665 | test accuracy: 0.502\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7831 | test accuracy: 0.485\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6841 | test accuracy: 0.498\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7317 | test accuracy: 0.492\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6788 | test accuracy: 0.492\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8425 | test accuracy: 0.492\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7931 | test accuracy: 0.488\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7735 | test accuracy: 0.492\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7738 | test accuracy: 0.492\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6822 | test accuracy: 0.498\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7831 | test accuracy: 0.492\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7284 | test accuracy: 0.502\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6413 | test accuracy: 0.488\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7107 | test accuracy: 0.492\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7079 | test accuracy: 0.485\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7351 | test accuracy: 0.495\n",
            "total time:  37.77098534800007\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26219940185546875.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.4619903564453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6839614595685687 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30446553230285645.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.495861291885376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5480303870780128 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26604151725769043.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.45360851287841797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46153163526739394 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2747523784637451.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.46187520027160645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41879732012748716 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25110912322998047.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.43840765953063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.388763056908335 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27112889289855957.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.48604846000671387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37524317162377496 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27011561393737793.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4660806655883789\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3633539855480194 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269317626953125.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4694523811340332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35203940144607 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26486945152282715.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4527461528778076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34695306462900977 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27721071243286133.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4771111011505127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34176092616149356 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2790191173553467.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4591257572174072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3361438376562936 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27025747299194336.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4854755401611328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3342895908015115 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2842545509338379.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4721698760986328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3310711609465735 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27304744720458984.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46448683738708496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32972853694643295 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2730600833892822.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4607565402984619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32867874332836694 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25719189643859863.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4479212760925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3262141942977905 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28430891036987305.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4812924861907959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3249917196375983 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652297019958496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46201133728027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.324038741844041 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28896522521972656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48943233489990234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32277014766420636 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29397082328796387.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48532748222351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3219358082328524 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26785707473754883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46837806701660156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3213596756969179 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591848373413086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43765807151794434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3206691929272243 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26758599281311035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4486105442047119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3196518178497042 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25652647018432617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.434084415435791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3194893730538232 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671051025390625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48050808906555176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3188846958535058 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2728738784790039.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.457230806350708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31892878115177153 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2560441493988037.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44481825828552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3181120106152126 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26004815101623535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43960142135620117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31812168955802916 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25577569007873535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4457056522369385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31761934374059947 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269428014755249.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4876737594604492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3174174555710384 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27737951278686523.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46995115280151367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3171791042600359 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27664732933044434.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4870262145996094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3168248312813895 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26972055435180664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.467836856842041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3168612109763282 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2854480743408203.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.492847204208374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31630108526774814 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27102017402648926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4604055881500244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3162264836685998 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27329516410827637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4599936008453369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31597486436367034 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2937452793121338.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4861714839935303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3160214177199772 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.267864465713501.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46242856979370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31583498162882667 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2835657596588135.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46718811988830566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3155182242393494 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26072120666503906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4667184352874756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3154955834150314 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2725367546081543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4497079849243164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3154545898948397 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632570266723633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45855212211608887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31522142801965986 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2657005786895752.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4624638557434082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31531456879207065 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26578521728515625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46637916564941406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3151486533028739 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2800877094268799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5060155391693115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31496969376291545 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26728010177612305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4586632251739502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149536809750966 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26396965980529785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44621992111206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31486661902495794 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27021241188049316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45522594451904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3148497934852328 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27272963523864746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45783114433288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3147835522890091 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27428770065307617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46227335929870605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3147886548723493 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28069424629211426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5000123977661133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31470887235232764 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3060920238494873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5255265235900879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31444989229951587 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27844762802124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4834165573120117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3145673372915813 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2883796691894531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4821813106536865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31446575352123807 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29256296157836914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48641347885131836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31441841551235744 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26468658447265625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45807409286499023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3142899283340999 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26031494140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45737600326538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31417838292462485 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2752659320831299.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47333359718322754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142335708652224 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26714134216308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4564831256866455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142209734235491 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27318525314331055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4623124599456787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31428187915257044 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2740206718444824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4667201042175293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140771610396249 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26534175872802734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44661903381347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31415306414876665 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609858512878418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4479494094848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.314205619267055 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26652026176452637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45297741889953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3141226883445467 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25321030616760254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44252443313598633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31396986331258503 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2736477851867676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4631991386413574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140478083065578 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26065492630004883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45771098136901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31400135670389445 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27855896949768066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4690845012664795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139732015984399 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631652355194092.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44478487968444824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139226245028632 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27745819091796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47571301460266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138664126396179 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2701106071472168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4550769329071045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31391124427318573 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2779970169067383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47021961212158203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31386317270142694 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27414441108703613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45921945571899414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138216074023928 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2878854274749756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4795043468475342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31393892168998716 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2392282485961914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4118959903717041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138559967279434 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24303483963012695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4374833106994629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138606969799314 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28024816513061523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4749743938446045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31378191454069954 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.253676176071167.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4518623352050781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.313803305370467 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2758183479309082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4631989002227783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31376138712678636 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26550745964050293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4425313472747803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137568920850754 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713918685913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45725178718566895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31375920687402997 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632730007171631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4382331371307373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31372408866882323 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671799659729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4648888111114502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137148082256317 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25960373878479004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.451491117477417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31376380154064726 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.258256196975708.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45354223251342773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.313679187638419 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762634754180908.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46158909797668457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31367994632039753 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24134159088134766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4151628017425537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136657404048102 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616889476776123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4310431480407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31365542880126407 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2502739429473877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42957544326782227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136332907846996 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.271683931350708.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4659450054168701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136501205818994 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2644655704498291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4460732936859131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136413880756923 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26489925384521484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.457749605178833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136413940361568 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25330233573913574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42075347900390625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136483886412212 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24203205108642578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4104738235473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31357953803879873 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2759110927581787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4664437770843506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.313629423720496 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2554028034210205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4434020519256592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135770027126585 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3017585277557373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49004673957824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135774288858686 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2505760192871094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4395005702972412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31357960402965546 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28954648971557617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4833252429962158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31359085781233653 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26767516136169434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45659565925598145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31358453035354616 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2640523910522461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46387743949890137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31356603928974697 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563636302947998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43450927734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31356591965471 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667109966278076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4663548469543457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135329042162214 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27394986152648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4510469436645508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31357826377664294 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26020097732543945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4450368881225586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31353568434715273 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738983631134033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45660877227783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31359736153057644 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613258361816406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44478535652160645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31354071072169715 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25686001777648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43662452697753906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31352314310414453 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545499801635742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43180346488952637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31354370840958184 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26978540420532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46228480339050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135481080838612 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26033878326416016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43494725227355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31350043416023254 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559208869934082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4312407970428467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31350375626768384 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27158284187316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4565451145172119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31348558025700707 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25059938430786133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43593645095825195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134969472885132 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28020691871643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47061753273010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134787099702018 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.273329496383667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46857786178588867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31350025406905585 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26034998893737793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4475398063659668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3135143701519285 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26227521896362305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44185924530029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31348581569535394 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2644979953765869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49713587760925293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134518993752343 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24905061721801758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43820858001708984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31346853375434874 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27317357063293457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4582855701446533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134603632347924 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26865363121032715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4510984420776367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134989959853036 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700083255767822.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46307897567749023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31347660848072595 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27530622482299805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4575979709625244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134602976696832 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26651525497436523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4500772953033447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134827107191086 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645263671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.458895206451416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31347337918622153 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733311653137207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.483656644821167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134507170745305 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30783843994140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.545773983001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31345182742391314 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5511 | test accuracy: 0.502\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6656 | test accuracy: 0.512\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8259 | test accuracy: 0.529\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7001 | test accuracy: 0.481\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7065 | test accuracy: 0.549\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6912 | test accuracy: 0.488\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7145 | test accuracy: 0.498\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7240 | test accuracy: 0.498\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7441 | test accuracy: 0.519\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6543 | test accuracy: 0.512\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6635 | test accuracy: 0.471\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7966 | test accuracy: 0.492\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6657 | test accuracy: 0.535\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6308 | test accuracy: 0.549\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6812 | test accuracy: 0.468\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6737 | test accuracy: 0.539\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6227 | test accuracy: 0.508\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6484 | test accuracy: 0.542\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7424 | test accuracy: 0.522\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6963 | test accuracy: 0.515\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.9233 | test accuracy: 0.502\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7081 | test accuracy: 0.522\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7084 | test accuracy: 0.519\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7414 | test accuracy: 0.478\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6660 | test accuracy: 0.519\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6472 | test accuracy: 0.522\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7344 | test accuracy: 0.539\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6941 | test accuracy: 0.471\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7640 | test accuracy: 0.532\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7775 | test accuracy: 0.532\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6718 | test accuracy: 0.481\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7170 | test accuracy: 0.475\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6755 | test accuracy: 0.512\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7267 | test accuracy: 0.515\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6379 | test accuracy: 0.529\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6691 | test accuracy: 0.525\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7155 | test accuracy: 0.529\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6919 | test accuracy: 0.529\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6991 | test accuracy: 0.522\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6789 | test accuracy: 0.532\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6372 | test accuracy: 0.529\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7849 | test accuracy: 0.525\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7191 | test accuracy: 0.532\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7069 | test accuracy: 0.522\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6802 | test accuracy: 0.529\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7191 | test accuracy: 0.529\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7021 | test accuracy: 0.519\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7298 | test accuracy: 0.525\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7396 | test accuracy: 0.515\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7549 | test accuracy: 0.529\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7086 | test accuracy: 0.525\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6526 | test accuracy: 0.522\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6298 | test accuracy: 0.508\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7526 | test accuracy: 0.525\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7334 | test accuracy: 0.525\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7789 | test accuracy: 0.522\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7301 | test accuracy: 0.535\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7246 | test accuracy: 0.525\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7535 | test accuracy: 0.519\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6593 | test accuracy: 0.519\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6110 | test accuracy: 0.512\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6895 | test accuracy: 0.519\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7191 | test accuracy: 0.515\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6596 | test accuracy: 0.535\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6960 | test accuracy: 0.535\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6706 | test accuracy: 0.519\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6976 | test accuracy: 0.515\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7152 | test accuracy: 0.522\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6254 | test accuracy: 0.519\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7249 | test accuracy: 0.529\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6259 | test accuracy: 0.519\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6525 | test accuracy: 0.532\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7606 | test accuracy: 0.532\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6932 | test accuracy: 0.529\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6350 | test accuracy: 0.512\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7227 | test accuracy: 0.525\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6513 | test accuracy: 0.522\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6275 | test accuracy: 0.529\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7025 | test accuracy: 0.519\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7234 | test accuracy: 0.519\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6876 | test accuracy: 0.525\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6134 | test accuracy: 0.519\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6104 | test accuracy: 0.519\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7081 | test accuracy: 0.535\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6124 | test accuracy: 0.525\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6867 | test accuracy: 0.515\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6751 | test accuracy: 0.515\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6011 | test accuracy: 0.512\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6983 | test accuracy: 0.532\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6801 | test accuracy: 0.515\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6923 | test accuracy: 0.519\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6680 | test accuracy: 0.519\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7179 | test accuracy: 0.522\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6924 | test accuracy: 0.515\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7003 | test accuracy: 0.515\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6837 | test accuracy: 0.512\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6438 | test accuracy: 0.522\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6505 | test accuracy: 0.535\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6980 | test accuracy: 0.519\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6312 | test accuracy: 0.515\n",
            "total time:  32.38292092699976\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6411 | test accuracy: 0.502\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7586 | test accuracy: 0.481\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.9202 | test accuracy: 0.478\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6874 | test accuracy: 0.468\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9589 | test accuracy: 0.502\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7090 | test accuracy: 0.492\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7504 | test accuracy: 0.465\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7934 | test accuracy: 0.498\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7134 | test accuracy: 0.458\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7734 | test accuracy: 0.458\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7740 | test accuracy: 0.444\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6455 | test accuracy: 0.444\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6810 | test accuracy: 0.455\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7242 | test accuracy: 0.468\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5934 | test accuracy: 0.495\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7056 | test accuracy: 0.434\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5876 | test accuracy: 0.512\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7503 | test accuracy: 0.475\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6768 | test accuracy: 0.498\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8015 | test accuracy: 0.465\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6873 | test accuracy: 0.428\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6502 | test accuracy: 0.471\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6390 | test accuracy: 0.495\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5661 | test accuracy: 0.492\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7843 | test accuracy: 0.481\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6353 | test accuracy: 0.481\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6121 | test accuracy: 0.448\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7604 | test accuracy: 0.485\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6684 | test accuracy: 0.458\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7101 | test accuracy: 0.468\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6033 | test accuracy: 0.478\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7036 | test accuracy: 0.465\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7752 | test accuracy: 0.478\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6701 | test accuracy: 0.475\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7475 | test accuracy: 0.458\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7359 | test accuracy: 0.458\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7220 | test accuracy: 0.458\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7595 | test accuracy: 0.434\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7138 | test accuracy: 0.458\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6845 | test accuracy: 0.438\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6857 | test accuracy: 0.428\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7131 | test accuracy: 0.434\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7349 | test accuracy: 0.461\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7859 | test accuracy: 0.438\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6052 | test accuracy: 0.434\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6921 | test accuracy: 0.434\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6503 | test accuracy: 0.438\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7109 | test accuracy: 0.465\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6978 | test accuracy: 0.444\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5711 | test accuracy: 0.438\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.7483 | test accuracy: 0.461\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6198 | test accuracy: 0.434\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6871 | test accuracy: 0.451\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6939 | test accuracy: 0.451\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6104 | test accuracy: 0.451\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7029 | test accuracy: 0.451\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6999 | test accuracy: 0.465\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6439 | test accuracy: 0.461\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7366 | test accuracy: 0.465\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6891 | test accuracy: 0.458\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.5609 | test accuracy: 0.431\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6411 | test accuracy: 0.461\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7211 | test accuracy: 0.458\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6560 | test accuracy: 0.461\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7598 | test accuracy: 0.461\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7676 | test accuracy: 0.465\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7027 | test accuracy: 0.458\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7213 | test accuracy: 0.458\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6559 | test accuracy: 0.461\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6010 | test accuracy: 0.458\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.7455 | test accuracy: 0.468\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6686 | test accuracy: 0.455\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6785 | test accuracy: 0.458\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6887 | test accuracy: 0.458\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6207 | test accuracy: 0.461\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7740 | test accuracy: 0.481\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7819 | test accuracy: 0.458\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.5977 | test accuracy: 0.458\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7276 | test accuracy: 0.471\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6542 | test accuracy: 0.455\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6753 | test accuracy: 0.438\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7936 | test accuracy: 0.471\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6662 | test accuracy: 0.431\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6690 | test accuracy: 0.458\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6614 | test accuracy: 0.434\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6456 | test accuracy: 0.455\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6644 | test accuracy: 0.468\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7320 | test accuracy: 0.465\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7060 | test accuracy: 0.465\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6258 | test accuracy: 0.451\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6749 | test accuracy: 0.468\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7594 | test accuracy: 0.471\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7335 | test accuracy: 0.471\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7200 | test accuracy: 0.461\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7307 | test accuracy: 0.458\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7539 | test accuracy: 0.451\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6723 | test accuracy: 0.431\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6369 | test accuracy: 0.461\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6926 | test accuracy: 0.458\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7667 | test accuracy: 0.458\n",
            "total time:  36.44201010500001\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25716495513916016.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.4660911560058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7003034676824297 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2842133045196533.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.485248327255249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5605157149689538 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28844380378723145.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.48884129524230957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.47222791825022015 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30210041999816895.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4982798099517822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4231359818152019 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26195192337036133.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4560070037841797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3967266359499523 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2837374210357666.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.46794843673706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37826815502984185 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582085132598877.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.446364164352417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36427190474101473 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2729313373565674.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4582242965698242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3543186353785651 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26480674743652344.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4437904357910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3488813489675522 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26794886589050293.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4706759452819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34341305920055937 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27259159088134766.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46134161949157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3399413649524961 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2670717239379883.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4461028575897217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3365657708474568 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26601743698120117.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4576904773712158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3334778717585972 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2781960964202881.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4711129665374756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.33096605709620885 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2799861431121826.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4660835266113281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3284840617861067 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27463316917419434.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4739999771118164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.327157495703016 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2854599952697754.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4786536693572998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3257692647831781 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2743988037109375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47513628005981445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32425670666354045 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29175710678100586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48899245262145996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3235578792435782 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25812244415283203.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4440598487854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3221448025533131 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26343297958374023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46587347984313965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3215415818350656 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2763066291809082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4673755168914795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32099725561482567 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27661919593811035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46710848808288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.32001138627529147 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25798916816711426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4518013000488281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3199674150773457 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2819206714630127.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46292924880981445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.319327523452895 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27065181732177734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45250940322875977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31912333156381334 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652316093444824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4531829357147217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3181785694190434 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666656970977783.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4617009162902832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31806082682950154 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26829028129577637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4632604122161865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3177419705050332 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2573068141937256.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4538307189941406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31755532920360563 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25069594383239746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4264092445373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31726626072611125 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254749059677124.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42755675315856934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3170899518898555 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2674686908721924.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44639086723327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31651246760572704 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27339625358581543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4582550525665283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31656747800963264 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2906801700592041.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4789774417877197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3165863458599363 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678346633911133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47299718856811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31621140241622925 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2717761993408203.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45943522453308105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31603027667318073 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27283668518066406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4519200325012207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3158012126173292 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555999755859375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44455504417419434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3158470949956349 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27224278450012207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4927084445953369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3155265327010836 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2924177646636963.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49237656593322754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3155503145286015 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2676546573638916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4512488842010498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31544580587318966 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507615089416504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44090747833251953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31541217650685993 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26439785957336426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4515376091003418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.315365640606199 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696988582611084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45541906356811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3150646678039006 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.274960994720459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46597862243652344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31510351768561773 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2744874954223633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4616560935974121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31497681822095597 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542688846588135.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44025301933288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31487721502780913 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25699663162231445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4381256103515625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31478172370365687 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607150077819824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45561671257019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31481112241744996 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28001832962036133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4890587329864502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31452019555228095 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2695763111114502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45980334281921387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3145658986909049 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2770698070526123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4819300174713135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31466195966516225 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26861047744750977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4552934169769287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31456312452043805 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27387404441833496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46182727813720703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31444155148097447 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26579880714416504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4494190216064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31441430577210017 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27003908157348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4663364887237549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31424917578697203 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777576446533203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4703032970428467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3144571419273104 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26170992851257324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4652249813079834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142806644950594 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27959156036376953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46483469009399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141913171325411 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24663758277893066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43608999252319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31428878818239486 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2804908752441406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4722423553466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3142028842653547 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643740177154541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45672178268432617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141324715954917 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28610897064208984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48090219497680664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140395700931549 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27733349800109863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4664328098297119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31413441513265883 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26842641830444336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4634125232696533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31411264751638684 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547752857208252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43778371810913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3140738261597497 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25593113899230957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45081639289855957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3140121681349618 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2697031497955322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4497036933898926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31394878923892977 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25112485885620117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45151829719543457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31390516800539836 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2715306282043457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45364975929260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139632833855493 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2481536865234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4218571186065674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31387670253004346 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2695591449737549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45142674446105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138628125190735 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2702016830444336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4611854553222656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31384131950991495 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2757270336151123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4622340202331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3139287207807813 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2570180892944336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43962883949279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138631560972759 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25615596771240234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45071887969970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31382191479206084 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26154375076293945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43848252296447754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31381390009607585 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26764798164367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4736673831939697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3138695499726704 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27685093879699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46016478538513184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31381421642644064 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557694911956787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4702281951904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31378389171191623 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30245113372802734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5033235549926758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137085361140115 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28235459327697754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4813547134399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137289536850793 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29290270805358887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47786879539489746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31378930466515675 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26318979263305664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4467885494232178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31368255061762673 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27111220359802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4525618553161621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3137041368654796 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2505161762237549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4276139736175537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136876514979771 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2699863910675049.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4714944362640381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31369039714336394 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26923465728759766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45439791679382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136436487947192 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26396656036376953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46073460578918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136894975389753 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25715112686157227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43477702140808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31367755276816234 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25318312644958496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44214367866516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31364804123129164 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26407933235168457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44062066078186035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136232193027224 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25576162338256836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360051155090332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136527576616832 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27641773223876953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45537662506103516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136022384677615 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25151681900024414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42513084411621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31359975848879135 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578268051147461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4543492794036865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136188281433923 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.248460054397583.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4241492748260498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135915815830231 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618565559387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45256781578063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135851204395294 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2703127861022949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47393083572387695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135888589279992 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27248263359069824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4697847366333008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135310939380101 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2793445587158203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45954179763793945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135528892278671 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2624051570892334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45921802520751953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135366635663169 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28761982917785645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47496557235717773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31354557062898364 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25876331329345703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4482386112213135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31353466255324225 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2807455062866211.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48667311668395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3136239213602883 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646050453186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4598877429962158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31352821971688954 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26697206497192383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4530630111694336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31349431659494126 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539706230163574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4327819347381592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31353226346629004 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26391005516052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4511544704437256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31353492992264886 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28144192695617676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47481656074523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31349089699132104 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266437292098999.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4757528305053711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351032555103303 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2900733947753906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47336649894714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31349935276167734 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2587900161743164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4513413906097412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31352326571941375 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2747783660888672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45926785469055176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135081282683781 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544822692871094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44025349617004395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134928435087204 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26437902450561523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45696282386779785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347415958132063 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24932050704956055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4357590675354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3135093407971518 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2802414894104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4761683940887451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134787163564137 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26041603088378906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.432628870010376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31347880959510804 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643258571624756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4469492435455322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134758791753224 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27763843536376953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4642062187194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31346755836691176 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27925944328308105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4759387969970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31347303347928185 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2675464153289795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4732189178466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134846325431551 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27787280082702637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49314188957214355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134503385850361 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2708134651184082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45801305770874023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134667886154992 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26308393478393555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4611842632293701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31347265115806033 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2681546211242676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45774173736572266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31348073737961907 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"class\"\n",
        "rnn_arr3 = []\n",
        "nrnn_arr3 = []\n",
        "lstm_arr3 = []\n",
        "seed_arr3 = []\n",
        "method_arr3 = []\n",
        "specify_arr3 = []\n",
        "\n",
        "flip_probabilities_0 = [0.2, 0.8]\n",
        "flip_probabilities_1 = [0.2, 0.8]\n",
        "\n",
        "for flip_probability_0 in flip_probabilities_0:\n",
        "  for flip_probability_1 in flip_probabilities_1:\n",
        "\n",
        "    x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                        flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1,\n",
        "                        startprob=None, transmat=None)\n",
        "\n",
        "    class_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "    class_train_flipped_loader = data_utils.DataLoader(class_train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    class_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "    class_test_flipped_loader = data_utils.DataLoader(class_test_flipped, batch_size=25, shuffle=True)\n",
        "\n",
        "    for seed in seeds:\n",
        "      acc = driver(seed, class_train_flipped_loader, class_test_flipped_loader)\n",
        "      acc2 = driver(seed, class_train_flipped_loader, class_test_flipped_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "      acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
        "      rnn_arr3.append(acc)\n",
        "      nrnn_arr3.append(acc2)\n",
        "      lstm_arr3.append(acc3)\n",
        "      seed_arr3.append(seed)\n",
        "      method_arr3.append(method)\n",
        "      specify_arr3.append((flip_probability_0, flip_probability_1))"
      ],
      "metadata": {
        "id": "KIEQl0-6i6ia",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7674730e-8588-4974-fd7c-2445e941a9a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7135 | test accuracy: 0.572\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6912 | test accuracy: 0.522\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4555 | test accuracy: 0.556\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6106 | test accuracy: 0.636\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8136 | test accuracy: 0.630\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5047 | test accuracy: 0.646\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4639 | test accuracy: 0.542\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7182 | test accuracy: 0.609\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3737 | test accuracy: 0.626\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4503 | test accuracy: 0.660\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5729 | test accuracy: 0.640\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.9834 | test accuracy: 0.636\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5099 | test accuracy: 0.684\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7224 | test accuracy: 0.657\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5514 | test accuracy: 0.697\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5929 | test accuracy: 0.717\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3111 | test accuracy: 0.734\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5801 | test accuracy: 0.744\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2904 | test accuracy: 0.747\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4856 | test accuracy: 0.754\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2174 | test accuracy: 0.768\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6635 | test accuracy: 0.785\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.8750 | test accuracy: 0.778\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6799 | test accuracy: 0.781\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2590 | test accuracy: 0.764\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8206 | test accuracy: 0.778\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1902 | test accuracy: 0.774\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.4644 | test accuracy: 0.768\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7363 | test accuracy: 0.778\n",
            "Epoch:  29 Iteration:  2100 | train loss: 1.2370 | test accuracy: 0.774\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7622 | test accuracy: 0.778\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2404 | test accuracy: 0.781\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2884 | test accuracy: 0.781\n",
            "Epoch:  33 Iteration:  2380 | train loss: 1.1145 | test accuracy: 0.781\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.4307 | test accuracy: 0.774\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7687 | test accuracy: 0.771\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2598 | test accuracy: 0.781\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1599 | test accuracy: 0.774\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.0185 | test accuracy: 0.774\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5615 | test accuracy: 0.771\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6104 | test accuracy: 0.774\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1830 | test accuracy: 0.768\n",
            "Epoch:  42 Iteration:  3010 | train loss: 1.1517 | test accuracy: 0.771\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7439 | test accuracy: 0.778\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.7047 | test accuracy: 0.774\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2070 | test accuracy: 0.778\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2892 | test accuracy: 0.774\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.9023 | test accuracy: 0.771\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6107 | test accuracy: 0.771\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7263 | test accuracy: 0.771\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6989 | test accuracy: 0.771\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6191 | test accuracy: 0.771\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.8306 | test accuracy: 0.768\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4553 | test accuracy: 0.774\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5278 | test accuracy: 0.774\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1401 | test accuracy: 0.771\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2648 | test accuracy: 0.768\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.0626 | test accuracy: 0.774\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.5888 | test accuracy: 0.768\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7096 | test accuracy: 0.771\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7312 | test accuracy: 0.774\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4738 | test accuracy: 0.771\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3255 | test accuracy: 0.771\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5433 | test accuracy: 0.768\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7850 | test accuracy: 0.771\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2565 | test accuracy: 0.771\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2514 | test accuracy: 0.771\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3169 | test accuracy: 0.771\n",
            "Epoch:  68 Iteration:  4830 | train loss: 1.1329 | test accuracy: 0.771\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6385 | test accuracy: 0.768\n",
            "Epoch:  70 Iteration:  4970 | train loss: 1.0509 | test accuracy: 0.768\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2188 | test accuracy: 0.768\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2895 | test accuracy: 0.768\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2005 | test accuracy: 0.768\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7084 | test accuracy: 0.768\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.3370 | test accuracy: 0.768\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1821 | test accuracy: 0.768\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.9044 | test accuracy: 0.768\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5916 | test accuracy: 0.771\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1914 | test accuracy: 0.768\n",
            "Epoch:  80 Iteration:  5670 | train loss: 1.1484 | test accuracy: 0.768\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.3045 | test accuracy: 0.768\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2333 | test accuracy: 0.768\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2482 | test accuracy: 0.771\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2395 | test accuracy: 0.768\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6747 | test accuracy: 0.771\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.3497 | test accuracy: 0.768\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2313 | test accuracy: 0.768\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.0331 | test accuracy: 0.768\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4241 | test accuracy: 0.768\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2206 | test accuracy: 0.768\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6416 | test accuracy: 0.768\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1637 | test accuracy: 0.768\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6170 | test accuracy: 0.768\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1260 | test accuracy: 0.768\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.5224 | test accuracy: 0.768\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1967 | test accuracy: 0.768\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3437 | test accuracy: 0.768\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2200 | test accuracy: 0.768\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.5352 | test accuracy: 0.768\n",
            "total time:  32.67605718300001\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8842 | test accuracy: 0.552\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4323 | test accuracy: 0.488\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6038 | test accuracy: 0.569\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7764 | test accuracy: 0.556\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8126 | test accuracy: 0.552\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6800 | test accuracy: 0.646\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6346 | test accuracy: 0.599\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7972 | test accuracy: 0.613\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6682 | test accuracy: 0.626\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6454 | test accuracy: 0.633\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6361 | test accuracy: 0.613\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5348 | test accuracy: 0.653\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5419 | test accuracy: 0.640\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5284 | test accuracy: 0.596\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7588 | test accuracy: 0.640\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7210 | test accuracy: 0.667\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6043 | test accuracy: 0.667\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5193 | test accuracy: 0.636\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.8452 | test accuracy: 0.640\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6017 | test accuracy: 0.660\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6099 | test accuracy: 0.643\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.7265 | test accuracy: 0.663\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6361 | test accuracy: 0.663\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7598 | test accuracy: 0.694\n",
            "Epoch:  24 Iteration:  1750 | train loss: 1.0739 | test accuracy: 0.673\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.4333 | test accuracy: 0.721\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7900 | test accuracy: 0.710\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7010 | test accuracy: 0.741\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.5556 | test accuracy: 0.737\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.5255 | test accuracy: 0.751\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2939 | test accuracy: 0.761\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.3954 | test accuracy: 0.764\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5105 | test accuracy: 0.761\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6626 | test accuracy: 0.761\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6585 | test accuracy: 0.761\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.9075 | test accuracy: 0.761\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7114 | test accuracy: 0.764\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6797 | test accuracy: 0.771\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5209 | test accuracy: 0.771\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6647 | test accuracy: 0.771\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7827 | test accuracy: 0.774\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.5759 | test accuracy: 0.771\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.3187 | test accuracy: 0.774\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4884 | test accuracy: 0.774\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.1844 | test accuracy: 0.774\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4589 | test accuracy: 0.774\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.5527 | test accuracy: 0.774\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.8410 | test accuracy: 0.774\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.4144 | test accuracy: 0.778\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7241 | test accuracy: 0.774\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6223 | test accuracy: 0.778\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.5191 | test accuracy: 0.771\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6294 | test accuracy: 0.774\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.8255 | test accuracy: 0.778\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4483 | test accuracy: 0.774\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7936 | test accuracy: 0.774\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7420 | test accuracy: 0.778\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.4467 | test accuracy: 0.778\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.9209 | test accuracy: 0.778\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2721 | test accuracy: 0.778\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.4283 | test accuracy: 0.778\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2398 | test accuracy: 0.778\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3524 | test accuracy: 0.778\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7907 | test accuracy: 0.778\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.8652 | test accuracy: 0.778\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.4533 | test accuracy: 0.778\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7045 | test accuracy: 0.774\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3540 | test accuracy: 0.774\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.4261 | test accuracy: 0.774\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3751 | test accuracy: 0.778\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2343 | test accuracy: 0.778\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7052 | test accuracy: 0.778\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2951 | test accuracy: 0.778\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.4124 | test accuracy: 0.778\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7260 | test accuracy: 0.778\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7106 | test accuracy: 0.778\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2276 | test accuracy: 0.778\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1399 | test accuracy: 0.778\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7862 | test accuracy: 0.778\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6342 | test accuracy: 0.778\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3761 | test accuracy: 0.778\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.4649 | test accuracy: 0.778\n",
            "Epoch:  82 Iteration:  5810 | train loss: 1.3455 | test accuracy: 0.778\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2700 | test accuracy: 0.778\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6905 | test accuracy: 0.778\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7391 | test accuracy: 0.778\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4907 | test accuracy: 0.778\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5486 | test accuracy: 0.781\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2859 | test accuracy: 0.778\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2506 | test accuracy: 0.778\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.8262 | test accuracy: 0.778\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2139 | test accuracy: 0.781\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.3246 | test accuracy: 0.781\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6739 | test accuracy: 0.781\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.3073 | test accuracy: 0.781\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4461 | test accuracy: 0.781\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2803 | test accuracy: 0.781\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.9561 | test accuracy: 0.781\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.3367 | test accuracy: 0.781\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.8478 | test accuracy: 0.781\n",
            "total time:  36.74602278400016\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2528340816497803.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0.4304475784301758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5227017696414675 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2660195827484131.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.4576263427734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.440413675563676 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2736377716064453.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4475076198577881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.3984722656863076 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2491168975830078.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4394400119781494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.373499396443367 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2710998058319092.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4498898983001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.35908298109258924 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25246334075927734.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44359350204467773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.34930329195090704 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596440315246582.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4484562873840332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3426221651690347 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25165534019470215.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42840099334716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33773180970123834 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667384147644043.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4629092216491699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3338574081659317 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504570484161377.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42447495460510254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3304627720798765 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25374722480773926.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43509531021118164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3279901329960142 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661464214324951.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44484567642211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32725780052798137 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23814725875854492.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42134857177734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32489690142018457 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2486581802368164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43422889709472656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32338287489754813 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24696922302246094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4321126937866211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3222826770373753 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26277852058410645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4520251750946045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3216609346015113 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2839469909667969.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46907734870910645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32024967542716437 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26755857467651367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45203351974487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3206831863948277 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598452568054199.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44680356979370117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31956397209848675 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.275890588760376.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45537734031677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31844316380364557 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2627573013305664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4383559226989746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3184628989015307 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25128841400146484.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4474208354949951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.317881087745939 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254072904586792.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4410738945007324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31774790585041046 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24717926979064941.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4229731559753418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3171748859541757 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557792663574219.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4355611801147461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31663815506867 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2576422691345215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43444204330444336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3165873536041805 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546408176422119.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4413924217224121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31628473315920147 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2675342559814453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44503235816955566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3163452071802957 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2533090114593506.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4503164291381836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3157140212399619 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25778698921203613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4484379291534424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3157304746764047 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24799370765686035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4425172805786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3154933299337115 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2570650577545166.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4381279945373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3155836330992835 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26035165786743164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43901658058166504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3152468127863748 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24910330772399902.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42992305755615234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3153848350048065 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565011978149414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42789196968078613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3151122889348439 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25623607635498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.445162296295166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.314953766976084 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2570335865020752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4442594051361084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3149553405387061 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24499988555908203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4324915409088135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3147721869604928 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26720428466796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45227980613708496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3147757032087871 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26242661476135254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4526247978210449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3144764163664409 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548363208770752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4385559558868408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3147177010774612 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2635679244995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4530818462371826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31453078985214233 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26176905632019043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4452362060546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3145441681146622 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24604105949401855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43144917488098145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.314414872441973 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2549865245819092.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44422101974487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31440416744777133 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25005030632019043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4265902042388916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.314389654142516 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266157865524292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44669580459594727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3143231404679162 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24686908721923828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4252350330352783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31422406392438074 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2581367492675781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4433152675628662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31420912529741013 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25433897972106934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44455456733703613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31415478076253617 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2606995105743408.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45444536209106445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3141335053103311 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27102160453796387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46415090560913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31413183340004514 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24214959144592285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4335806369781494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31409285707133155 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25690364837646484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43619370460510254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3140184032065528 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24594330787658691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4285097122192383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3140077697379248 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2656228542327881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.453594446182251\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140567183494568 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25818419456481934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43343210220336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31399407982826233 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24894261360168457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4316444396972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3139747002295085 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24447178840637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4202263355255127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139261083943503 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24269604682922363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4168820381164551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31389234789780207 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593104839324951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4342384338378906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31391166874340604 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24400734901428223.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4238157272338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31385206963334766 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2675659656524658.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4603855609893799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31383487582206726 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25429797172546387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4322197437286377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31383751588208336 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28183579444885254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5040831565856934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138564131089619 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2617838382720947.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4420812129974365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137893080711365 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261763334274292.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45866870880126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137675008603505 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26993393898010254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4627208709716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31371797706399646 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25812339782714844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4482080936431885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31379765016692024 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700648307800293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4514808654785156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137533264500754 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24750065803527832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42181825637817383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31371647672993797 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.264312744140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4488413333892822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31372252106666565 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.258455753326416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4407925605773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137535648686545 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2451612949371338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45595288276672363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3137206222329821 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2483816146850586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42615723609924316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136626090322222 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250624418258667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44440579414367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31366143354347775 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2550647258758545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43090009689331055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31366709087576183 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25922250747680664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43519043922424316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31364087547574726 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26540112495422363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4453442096710205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136184760502407 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24095821380615234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41321778297424316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31366890328271047 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523319721221924.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4422426223754883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3135915215526308 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25777649879455566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4361124038696289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136083313396999 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2501342296600342.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44595813751220703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136335585798536 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26563382148742676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45955801010131836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31359744497707914 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2505016326904297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43088579177856445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31359669310706 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28868579864501953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.482086181640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31357278525829313 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25862765312194824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4630110263824463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31358948264803205 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2891261577606201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.481250524520874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136192585740771 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25948596000671387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44878554344177246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31363176703453066 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662484645843506.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47216296195983887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135594027382987 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618141174316406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44975733757019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31356555138315473 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26468658447265625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4634850025177002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135304961885725 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26548337936401367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45515894889831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31353476600987573 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2564122676849365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4443380832672119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135206976107189 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26874279975891113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45842981338500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31353061071463995 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25431323051452637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389045238494873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31350878519671305 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2785336971282959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4950277805328369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135330932480948 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30315542221069336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5225112438201904\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31352991461753843 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3277552127838135.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5309691429138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135146426303046 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2732386589050293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47025394439697266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31352037021092005 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2746388912200928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4535708427429199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31349300358976634 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593395709991455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44881772994995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135116973093578 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622566223144531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4541795253753662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3134870001247951 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2495133876800537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4251575469970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135032632521221 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619786262512207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44923830032348633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31348010684762684 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2743382453918457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46201539039611816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3134697096688407 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27263593673706055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47095322608947754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31346837835652486 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27967119216918945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47272372245788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31349224661077774 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29260969161987305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4952206611633301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134965590068272 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29982709884643555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48844385147094727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31346232295036314 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613959312438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4613075256347656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134830240692411 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.277050256729126.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47756266593933105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134617711816515 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613999843597412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.449735164642334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31347355246543884 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26845383644104004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45958971977233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31344754908766065 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26968860626220703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46663451194763184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134398196424757 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27277064323425293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47078990936279297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31343533439295634 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27495646476745605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46645069122314453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134443713085992 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27182841300964355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4661853313446045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31344453735010963 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26671338081359863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.452939510345459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134398643459593 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.270749568939209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46678948402404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31344047571931566 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26843714714050293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4501492977142334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134341154779707 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2576124668121338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4367034435272217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31342900565692355 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2580568790435791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44307804107666016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134233687605177 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26398754119873047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4750349521636963\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134259556021009 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503972053527832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4304924011230469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134266061442239 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25501298904418945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4387040138244629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134283815111433 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592177391052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.441547155380249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31341040006705695 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2642934322357178.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45090174674987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31341441486563 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7656 | test accuracy: 0.461\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6696 | test accuracy: 0.562\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8735 | test accuracy: 0.582\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7154 | test accuracy: 0.633\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.4648 | test accuracy: 0.606\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5564 | test accuracy: 0.657\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8041 | test accuracy: 0.643\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4663 | test accuracy: 0.616\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.8203 | test accuracy: 0.643\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7031 | test accuracy: 0.646\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5555 | test accuracy: 0.684\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5204 | test accuracy: 0.687\n",
            "Epoch:  12 Iteration:  910 | train loss: 1.0273 | test accuracy: 0.700\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.4499 | test accuracy: 0.717\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7407 | test accuracy: 0.717\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7693 | test accuracy: 0.751\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6319 | test accuracy: 0.731\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.5533 | test accuracy: 0.724\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5529 | test accuracy: 0.774\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6229 | test accuracy: 0.778\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2096 | test accuracy: 0.778\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5114 | test accuracy: 0.778\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2677 | test accuracy: 0.781\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2514 | test accuracy: 0.781\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2596 | test accuracy: 0.778\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5554 | test accuracy: 0.778\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2250 | test accuracy: 0.778\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5875 | test accuracy: 0.785\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2474 | test accuracy: 0.778\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3186 | test accuracy: 0.778\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6991 | test accuracy: 0.778\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7737 | test accuracy: 0.778\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2359 | test accuracy: 0.778\n",
            "Epoch:  33 Iteration:  2380 | train loss: 1.1010 | test accuracy: 0.778\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5826 | test accuracy: 0.778\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6681 | test accuracy: 0.778\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2588 | test accuracy: 0.778\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.3618 | test accuracy: 0.778\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5797 | test accuracy: 0.778\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2836 | test accuracy: 0.778\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.5110 | test accuracy: 0.778\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.2452 | test accuracy: 0.778\n",
            "Epoch:  42 Iteration:  3010 | train loss: 1.4029 | test accuracy: 0.778\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6639 | test accuracy: 0.774\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6459 | test accuracy: 0.778\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1801 | test accuracy: 0.774\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3018 | test accuracy: 0.778\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2093 | test accuracy: 0.778\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1721 | test accuracy: 0.778\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.3423 | test accuracy: 0.774\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2930 | test accuracy: 0.774\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2702 | test accuracy: 0.774\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1807 | test accuracy: 0.774\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2632 | test accuracy: 0.778\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6278 | test accuracy: 0.778\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2710 | test accuracy: 0.774\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5847 | test accuracy: 0.774\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7783 | test accuracy: 0.774\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1936 | test accuracy: 0.774\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6989 | test accuracy: 0.774\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7988 | test accuracy: 0.774\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3034 | test accuracy: 0.774\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2447 | test accuracy: 0.774\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2262 | test accuracy: 0.774\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2787 | test accuracy: 0.774\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1847 | test accuracy: 0.774\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.4557 | test accuracy: 0.774\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7244 | test accuracy: 0.774\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2375 | test accuracy: 0.774\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7260 | test accuracy: 0.774\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2400 | test accuracy: 0.774\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.9326 | test accuracy: 0.774\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2954 | test accuracy: 0.774\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6398 | test accuracy: 0.774\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.2741 | test accuracy: 0.774\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.4190 | test accuracy: 0.774\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1731 | test accuracy: 0.774\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1661 | test accuracy: 0.774\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2883 | test accuracy: 0.774\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1690 | test accuracy: 0.774\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3273 | test accuracy: 0.774\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.0468 | test accuracy: 0.774\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1699 | test accuracy: 0.774\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2026 | test accuracy: 0.774\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1665 | test accuracy: 0.774\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2709 | test accuracy: 0.774\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2853 | test accuracy: 0.774\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5422 | test accuracy: 0.774\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.0285 | test accuracy: 0.774\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4134 | test accuracy: 0.774\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7820 | test accuracy: 0.774\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.3753 | test accuracy: 0.774\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1621 | test accuracy: 0.774\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1979 | test accuracy: 0.774\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1750 | test accuracy: 0.778\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2526 | test accuracy: 0.778\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7562 | test accuracy: 0.774\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3159 | test accuracy: 0.774\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1867 | test accuracy: 0.774\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2169 | test accuracy: 0.778\n",
            "total time:  31.18801914800042\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8486 | test accuracy: 0.455\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7690 | test accuracy: 0.556\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6765 | test accuracy: 0.653\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6455 | test accuracy: 0.562\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.9470 | test accuracy: 0.566\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6303 | test accuracy: 0.556\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.9821 | test accuracy: 0.606\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6584 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7146 | test accuracy: 0.630\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5984 | test accuracy: 0.630\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7490 | test accuracy: 0.633\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6035 | test accuracy: 0.626\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5452 | test accuracy: 0.640\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7324 | test accuracy: 0.623\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.9605 | test accuracy: 0.636\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5939 | test accuracy: 0.657\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7579 | test accuracy: 0.650\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6060 | test accuracy: 0.630\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6103 | test accuracy: 0.657\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5879 | test accuracy: 0.620\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.9047 | test accuracy: 0.653\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.8061 | test accuracy: 0.636\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6728 | test accuracy: 0.663\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6610 | test accuracy: 0.697\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.9832 | test accuracy: 0.687\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6218 | test accuracy: 0.714\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.7082 | test accuracy: 0.663\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5813 | test accuracy: 0.734\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.5913 | test accuracy: 0.737\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7795 | test accuracy: 0.744\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.5493 | test accuracy: 0.754\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.7131 | test accuracy: 0.758\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2534 | test accuracy: 0.754\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.3957 | test accuracy: 0.751\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6259 | test accuracy: 0.754\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3616 | test accuracy: 0.754\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.3873 | test accuracy: 0.754\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.4398 | test accuracy: 0.754\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3912 | test accuracy: 0.754\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5492 | test accuracy: 0.758\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.8871 | test accuracy: 0.754\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4027 | test accuracy: 0.758\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6224 | test accuracy: 0.761\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7108 | test accuracy: 0.761\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3325 | test accuracy: 0.761\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4869 | test accuracy: 0.758\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7160 | test accuracy: 0.758\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7811 | test accuracy: 0.758\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.4235 | test accuracy: 0.758\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.7625 | test accuracy: 0.764\n",
            "Epoch:  50 Iteration:  3570 | train loss: 1.4029 | test accuracy: 0.764\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.3134 | test accuracy: 0.764\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7566 | test accuracy: 0.764\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4541 | test accuracy: 0.764\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.3275 | test accuracy: 0.771\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.9063 | test accuracy: 0.768\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.3501 | test accuracy: 0.768\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2846 | test accuracy: 0.764\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6119 | test accuracy: 0.764\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4847 | test accuracy: 0.771\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2497 | test accuracy: 0.771\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4153 | test accuracy: 0.771\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3298 | test accuracy: 0.771\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.4992 | test accuracy: 0.771\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.3553 | test accuracy: 0.768\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6612 | test accuracy: 0.768\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7887 | test accuracy: 0.768\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.4399 | test accuracy: 0.764\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5011 | test accuracy: 0.768\n",
            "Epoch:  69 Iteration:  4900 | train loss: 1.2009 | test accuracy: 0.774\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6536 | test accuracy: 0.771\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7187 | test accuracy: 0.774\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3961 | test accuracy: 0.774\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6574 | test accuracy: 0.774\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7570 | test accuracy: 0.778\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.3664 | test accuracy: 0.778\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.4332 | test accuracy: 0.778\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6325 | test accuracy: 0.778\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.4222 | test accuracy: 0.781\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3647 | test accuracy: 0.781\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4827 | test accuracy: 0.781\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5093 | test accuracy: 0.781\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5048 | test accuracy: 0.785\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6961 | test accuracy: 0.785\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7259 | test accuracy: 0.785\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7302 | test accuracy: 0.781\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4968 | test accuracy: 0.785\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2851 | test accuracy: 0.781\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.5961 | test accuracy: 0.785\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8489 | test accuracy: 0.781\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2705 | test accuracy: 0.785\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2950 | test accuracy: 0.785\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.5931 | test accuracy: 0.778\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7075 | test accuracy: 0.778\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6976 | test accuracy: 0.778\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6546 | test accuracy: 0.781\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3197 | test accuracy: 0.778\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.9158 | test accuracy: 0.778\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7927 | test accuracy: 0.778\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2877 | test accuracy: 0.778\n",
            "total time:  35.98792820700055\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26789259910583496.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.4486992359161377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6834367828709739 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2697899341583252.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4575984477996826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5501835065228599 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26085782051086426.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.43575406074523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46421811282634734 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25981616973876953.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4471251964569092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.417984111394201 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2704582214355469.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.4751605987548828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38751732196126665 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.267547607421875.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4653811454772949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3730476519891194 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27642011642456055.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.46523356437683105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3588040313550404 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557106018066406.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.45581603050231934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3505947142839432 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2714424133300781.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.46282124519348145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34536757937499457 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25202417373657227.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43029260635375977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3407360485621861 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26423144340515137.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4525902271270752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33596161689077103 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2625746726989746.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44208598136901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33250507967812676 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27251744270324707.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4477043151855469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33084213222776143 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2574801445007324.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43941640853881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32898642633642466 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25609493255615234.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4493985176086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32719410743032185 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26001667976379395.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44469690322875977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32575280538627077 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24808430671691895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4326972961425781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32419973271233693 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2822554111480713.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46541428565979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3235301724502018 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26326990127563477.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4505124092102051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32176193382058826 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25430917739868164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43742990493774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32159627165113175 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26719045639038086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4496757984161377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32047763296536036 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2501859664916992.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43265366554260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31971928562436785 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25318455696105957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43845152854919434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.319588155405862 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251845121383667.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4430990219116211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3186902735914503 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596416473388672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4405393600463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31870590022632056 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25074005126953125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43128228187561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3180678542171206 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.248077392578125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42778921127319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31775319703987664 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25873351097106934.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4463834762573242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31758844213826315 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575719356536865.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4368159770965576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3172821934734072 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24834156036376953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4337425231933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3171088478394917 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2708008289337158.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4567074775695801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3168428489140102 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251920223236084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42513585090637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.316337365763528 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26505613327026367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4519031047821045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31630887176309314 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24975204467773438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4296872615814209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31602333613804406 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26938557624816895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44937944412231445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31566692122391293 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24672722816467285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4214134216308594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31584056700978963 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24189066886901855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43105316162109375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3156229483229773 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591259479522705.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44092321395874023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3158047786780766 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590174674987793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43737006187438965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3153732648917607 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2707221508026123.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4581313133239746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3152584616627012 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251309871673584.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4249763488769531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3153337461607797 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2612607479095459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4423346519470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3150350387607302 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24566221237182617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4243896007537842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31515532646860395 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24243903160095215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42720580101013184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.314861358489309 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26623106002807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44948267936706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3148996327604566 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2478647232055664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41873908042907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31491299016135077 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577543258666992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4363994598388672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31468615106173925 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27141857147216797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45913004875183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31459202340670994 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2518596649169922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42815256118774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3147678588117872 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25534534454345703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4289836883544922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31449225800377983 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24856829643249512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4413285255432129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3144109696149826 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261765718460083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44129323959350586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31448292306491304 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24788856506347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42191457748413086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144033019031797 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27680373191833496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45333027839660645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31438088246754237 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548067569732666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317507743835449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143355633531298 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26368069648742676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4413485527038574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3142751059361866 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2552299499511719.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43164992332458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.314247772523335 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592296600341797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45129895210266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31418334671429227 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25438690185546875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4310622215270996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31411550981657843 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24452424049377441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4154093265533447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141553099666323 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269883394241333.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44704365730285645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140565318720681 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24400949478149414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4180588722229004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31405230249677385 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583613395690918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43255186080932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140482268163136 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24456119537353516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4211602210998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139673671552113 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25293612480163574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44298648834228516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31400435864925386 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24180102348327637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4185984134674072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31397306025028227 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532680034637451.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42286062240600586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31395957342215947 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535226345062256.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4256148338317871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31390937524182455 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24769330024719238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41901135444641113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3138939312526158 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24999046325683594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43667173385620117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31388216486998965 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24261736869812012.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4193570613861084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31388513573578425 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25539469718933105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4388453960418701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31382889321872165 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2699465751647949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4513671398162842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.313837103332792 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605016231536865.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4428901672363281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31378713973930905 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2715764045715332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45590877532958984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31376550027302336 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24690794944763184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4332752227783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138304629496166 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2820723056793213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4783060550689697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137841556753431 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2556724548339844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43628835678100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137881066117968 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25722289085388184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4474303722381592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137362480163574 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514774799346924.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42350172996520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137426593473979 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24747872352600098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4270288944244385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137472139937537 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2480485439300537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4196658134460449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31373069158622197 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24676036834716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44067859649658203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31365253797599246 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548940181732178.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44382405281066895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137089993272509 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24690461158752441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4179508686065674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31371343433856963 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24485468864440918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42237162590026855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31366444144930156 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25884199142456055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43282437324523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136074926171984 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24018120765686035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42420458793640137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31367198697158266 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622213363647461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44544172286987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31367757788726264 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609233856201172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4408719539642334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31365008311612264 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25241541862487793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4365723133087158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31363196969032286 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24118280410766602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41825294494628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136248001030513 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23951339721679688.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42517685890197754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135976812669209 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25913333892822266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42957544326782227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136157206126622 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24518609046936035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4261131286621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135986881596701 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662019729614258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44292235374450684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135679547275816 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531590461730957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.437650203704834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135810707296644 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25333476066589355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4406445026397705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135832339525223 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25125837326049805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43580079078674316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31358631934438436 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2640242576599121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4483160972595215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.313577909554754 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26622533798217773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44145846366882324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31359267490250725 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2495591640472412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43651556968688965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31356221948351176 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2617514133453369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43203163146972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135609209537506 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2480182647705078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4275193214416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31352834446089606 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25313806533813477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43520569801330566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31350236875670295 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24393701553344727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4210507869720459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135216487305505 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25893521308898926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4473533630371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135617217847279 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2775750160217285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45424556732177734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31351008457796914 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2552628517150879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43657493591308594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135166202272688 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578599452972412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323453903198242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31349874820028034 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23982834815979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41425347328186035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135287148611886 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24454998970031738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41730380058288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31349576456206185 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24184918403625488.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.409473180770874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31350499136107307 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2472398281097412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4199860095977783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31348714317594256 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24922966957092285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41771650314331055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135136038064957 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24524283409118652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4229111671447754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3135175347328186 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2347252368927002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43059587478637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347380663667407 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24610328674316406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42037296295166016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134570526225226 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2327888011932373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4079859256744385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31344006657600404 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2573864459991455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43338656425476074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.313504141994885 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24425601959228516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42218661308288574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134447527783258 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2487499713897705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4426999092102051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.313456529378891 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26060986518859863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4370853900909424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.313479551247188 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24137258529663086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4201679229736328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31349096383367264 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24971556663513184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4404106140136719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134531693799155 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565939426422119.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4383213520050049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134464059557234 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630443572998047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4472076892852783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134635720934187 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2538893222808838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4254322052001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134461522102356 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8231 | test accuracy: 0.562\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5772 | test accuracy: 0.569\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6503 | test accuracy: 0.613\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4396 | test accuracy: 0.620\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8693 | test accuracy: 0.640\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8098 | test accuracy: 0.640\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6357 | test accuracy: 0.616\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6350 | test accuracy: 0.677\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6489 | test accuracy: 0.643\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7921 | test accuracy: 0.657\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4259 | test accuracy: 0.657\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4348 | test accuracy: 0.680\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.4326 | test accuracy: 0.687\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6620 | test accuracy: 0.704\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6257 | test accuracy: 0.737\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6130 | test accuracy: 0.734\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7107 | test accuracy: 0.727\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2840 | test accuracy: 0.758\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7044 | test accuracy: 0.771\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7528 | test accuracy: 0.710\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.8848 | test accuracy: 0.764\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.8251 | test accuracy: 0.774\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.7908 | test accuracy: 0.781\n",
            "Epoch:  23 Iteration:  1680 | train loss: 1.0590 | test accuracy: 0.778\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3067 | test accuracy: 0.771\n",
            "Epoch:  25 Iteration:  1820 | train loss: 1.4932 | test accuracy: 0.774\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2906 | test accuracy: 0.781\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2581 | test accuracy: 0.778\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.5996 | test accuracy: 0.778\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6653 | test accuracy: 0.768\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2907 | test accuracy: 0.771\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5840 | test accuracy: 0.781\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6321 | test accuracy: 0.778\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5998 | test accuracy: 0.778\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.3319 | test accuracy: 0.778\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2324 | test accuracy: 0.781\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2068 | test accuracy: 0.778\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.1148 | test accuracy: 0.781\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.1571 | test accuracy: 0.781\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2532 | test accuracy: 0.781\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.5593 | test accuracy: 0.781\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1606 | test accuracy: 0.781\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.3404 | test accuracy: 0.781\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3632 | test accuracy: 0.781\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2197 | test accuracy: 0.781\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6053 | test accuracy: 0.781\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3175 | test accuracy: 0.778\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5261 | test accuracy: 0.781\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2225 | test accuracy: 0.778\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2351 | test accuracy: 0.781\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6948 | test accuracy: 0.781\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7043 | test accuracy: 0.781\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.3295 | test accuracy: 0.781\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1409 | test accuracy: 0.778\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.2091 | test accuracy: 0.778\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7536 | test accuracy: 0.781\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2841 | test accuracy: 0.781\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.5416 | test accuracy: 0.778\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.3068 | test accuracy: 0.778\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2338 | test accuracy: 0.778\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3302 | test accuracy: 0.781\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4504 | test accuracy: 0.778\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.4763 | test accuracy: 0.781\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6548 | test accuracy: 0.778\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2176 | test accuracy: 0.778\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3650 | test accuracy: 0.778\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2696 | test accuracy: 0.778\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2840 | test accuracy: 0.778\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7605 | test accuracy: 0.781\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7805 | test accuracy: 0.778\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6932 | test accuracy: 0.778\n",
            "Epoch:  71 Iteration:  5040 | train loss: 1.2494 | test accuracy: 0.778\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.5166 | test accuracy: 0.781\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.4466 | test accuracy: 0.778\n",
            "Epoch:  74 Iteration:  5250 | train loss: 1.0534 | test accuracy: 0.778\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6549 | test accuracy: 0.778\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7054 | test accuracy: 0.778\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.4363 | test accuracy: 0.778\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5979 | test accuracy: 0.778\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1862 | test accuracy: 0.778\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2861 | test accuracy: 0.778\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6720 | test accuracy: 0.778\n",
            "Epoch:  82 Iteration:  5810 | train loss: 1.1286 | test accuracy: 0.778\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6778 | test accuracy: 0.778\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7951 | test accuracy: 0.778\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4126 | test accuracy: 0.778\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.3849 | test accuracy: 0.778\n",
            "Epoch:  87 Iteration:  6160 | train loss: 1.1261 | test accuracy: 0.778\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2305 | test accuracy: 0.778\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2214 | test accuracy: 0.778\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.5766 | test accuracy: 0.771\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.8202 | test accuracy: 0.774\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7966 | test accuracy: 0.771\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.0968 | test accuracy: 0.774\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.4028 | test accuracy: 0.771\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2202 | test accuracy: 0.778\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3281 | test accuracy: 0.774\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7909 | test accuracy: 0.778\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.0115 | test accuracy: 0.774\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7335 | test accuracy: 0.778\n",
            "total time:  31.645194804999846\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8769 | test accuracy: 0.556\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6898 | test accuracy: 0.556\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6085 | test accuracy: 0.552\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5649 | test accuracy: 0.630\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.4449 | test accuracy: 0.552\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6321 | test accuracy: 0.609\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6034 | test accuracy: 0.653\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5569 | test accuracy: 0.646\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4742 | test accuracy: 0.643\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6634 | test accuracy: 0.630\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5628 | test accuracy: 0.646\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6489 | test accuracy: 0.640\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6366 | test accuracy: 0.643\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5964 | test accuracy: 0.640\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4471 | test accuracy: 0.653\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.5460 | test accuracy: 0.660\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6196 | test accuracy: 0.657\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8511 | test accuracy: 0.670\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.4848 | test accuracy: 0.687\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4525 | test accuracy: 0.694\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7008 | test accuracy: 0.704\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5539 | test accuracy: 0.717\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6013 | test accuracy: 0.721\n",
            "Epoch:  23 Iteration:  1680 | train loss: 1.2770 | test accuracy: 0.721\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7712 | test accuracy: 0.714\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7159 | test accuracy: 0.744\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.9029 | test accuracy: 0.751\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2782 | test accuracy: 0.761\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.4099 | test accuracy: 0.761\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6068 | test accuracy: 0.781\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.3426 | test accuracy: 0.774\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5278 | test accuracy: 0.778\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7575 | test accuracy: 0.781\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.8215 | test accuracy: 0.781\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7897 | test accuracy: 0.785\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.8167 | test accuracy: 0.785\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.9428 | test accuracy: 0.781\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.3683 | test accuracy: 0.785\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4903 | test accuracy: 0.781\n",
            "Epoch:  39 Iteration:  2800 | train loss: 1.0980 | test accuracy: 0.781\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.9064 | test accuracy: 0.781\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7813 | test accuracy: 0.781\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.4161 | test accuracy: 0.781\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6040 | test accuracy: 0.781\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5437 | test accuracy: 0.781\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.5066 | test accuracy: 0.781\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7289 | test accuracy: 0.781\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3261 | test accuracy: 0.781\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2126 | test accuracy: 0.781\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5130 | test accuracy: 0.781\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6319 | test accuracy: 0.781\n",
            "Epoch:  51 Iteration:  3640 | train loss: 1.2363 | test accuracy: 0.781\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.3105 | test accuracy: 0.781\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.5938 | test accuracy: 0.781\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.3069 | test accuracy: 0.785\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1844 | test accuracy: 0.785\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7949 | test accuracy: 0.785\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3621 | test accuracy: 0.785\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2894 | test accuracy: 0.785\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.3880 | test accuracy: 0.785\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.4176 | test accuracy: 0.781\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7236 | test accuracy: 0.781\n",
            "Epoch:  62 Iteration:  4410 | train loss: 1.0335 | test accuracy: 0.785\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2109 | test accuracy: 0.785\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.8311 | test accuracy: 0.781\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3131 | test accuracy: 0.785\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2722 | test accuracy: 0.781\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5591 | test accuracy: 0.781\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3327 | test accuracy: 0.781\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.9802 | test accuracy: 0.781\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.3401 | test accuracy: 0.781\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3326 | test accuracy: 0.781\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3292 | test accuracy: 0.781\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.3290 | test accuracy: 0.781\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5262 | test accuracy: 0.781\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2946 | test accuracy: 0.781\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6031 | test accuracy: 0.781\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.5562 | test accuracy: 0.781\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.4361 | test accuracy: 0.781\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3145 | test accuracy: 0.781\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.2856 | test accuracy: 0.781\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.9812 | test accuracy: 0.781\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6304 | test accuracy: 0.781\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2569 | test accuracy: 0.781\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.5085 | test accuracy: 0.781\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.8845 | test accuracy: 0.781\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.8902 | test accuracy: 0.781\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.3599 | test accuracy: 0.781\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.8768 | test accuracy: 0.785\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4307 | test accuracy: 0.785\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9226 | test accuracy: 0.785\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.5837 | test accuracy: 0.785\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.5809 | test accuracy: 0.785\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.3465 | test accuracy: 0.785\n",
            "Epoch:  94 Iteration:  6650 | train loss: 1.2365 | test accuracy: 0.785\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7404 | test accuracy: 0.785\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4690 | test accuracy: 0.785\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6290 | test accuracy: 0.785\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.9651 | test accuracy: 0.785\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2730 | test accuracy: 0.785\n",
            "total time:  36.755570321999585\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26969289779663086.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.4828979969024658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7018154885087694 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696990966796875.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.465773344039917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5594862546239581 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26338624954223633.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4593636989593506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4737654903105327 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25310659408569336.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44266796112060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.42402698738234385 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26350975036621094.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.438812255859375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39336360905851636 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26314854621887207.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.44405436515808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3753037576164518 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616405487060547.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4519972801208496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36238564252853395 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616915702819824.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4402644634246826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3548412523099354 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25669264793395996.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.452481746673584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34668079997812 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25338101387023926.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.44538187980651855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34210740668433054 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575840950012207.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4425811767578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3378413685730525 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563316822052002.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4418163299560547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33403036424091886 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2657482624053955.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4477195739746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3313167721033096 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26181912422180176.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.441516637802124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32988712319305963 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2752218246459961.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.461942195892334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32790723613330297 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611522674560547.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43938469886779785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32638459035328454 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25006985664367676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4401581287384033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3247847829546247 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25830721855163574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4578719139099121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3244393587112427 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631247043609619.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4549868106842041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3233968555927277 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565798759460449.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45510387420654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3221742936543056 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2686493396759033.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4638495445251465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3215874454804829 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2730402946472168.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45761966705322266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32093634818281447 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.262650728225708.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44301581382751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3192013757569449 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25653958320617676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4335956573486328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31933531846318924 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733638286590576.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45479750633239746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3186503354992185 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2540738582611084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44338417053222656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3186622853790011 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545616626739502.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44618844985961914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31854506390435355 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26087188720703125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45338940620422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31748284101486207 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541618347167969.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.444796085357666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3171034442526954 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29561543464660645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47450971603393555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3172330175127302 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.264697790145874.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4548187255859375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3168124037129538 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26193690299987793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4354586601257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31677292627947673 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25872278213500977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4411191940307617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3164009954248156 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618448734283447.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4465510845184326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31635988269533427 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630186080932617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4500393867492676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31578413333211625 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26308107376098633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.448352575302124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3161126664706639 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536165714263916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43042683601379395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31588069498538973 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26912665367126465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4506807327270508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31560504479067664 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24777650833129883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4167003631591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31560494346278056 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629063129425049.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.449176549911499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31550844950335366 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25452613830566406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.434842586517334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31529793739318845 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593729496002197.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4636876583099365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3152287346976144 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2742185592651367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4713146686553955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3151473479611533 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2639436721801758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46253061294555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3152114651032856 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2804231643676758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4662137031555176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3149433216878346 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25302886962890625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4367842674255371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149378555161612 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26983022689819336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44916772842407227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3147448514189039 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25211668014526367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42479705810546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147446581295558 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259458065032959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44103097915649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3145707990442004 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632737159729004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4425816535949707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3145369091204235 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24858593940734863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43282628059387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31462646552494594 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628939151763916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4454197883605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31436598002910615 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509315013885498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43079376220703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144202504839216 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29102230072021484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4687473773956299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143902455057417 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25455260276794434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43210744857788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143096136195319 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616589069366455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487631320953369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143454700708389 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24442720413208008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4159674644470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142160300697599 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532675266265869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4498119354248047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31429238617420197 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2564222812652588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4340798854827881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3143569030932018 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2530546188354492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43690061569213867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.314217666217259 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26418304443359375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44461989402770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31419402062892915 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26819372177124023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44791293144226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141276116882052 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26671719551086426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4586012363433838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31405199936458045 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510256767272949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4267086982727051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140631411756788 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2753152847290039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4849247932434082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3139757569347109 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539863586425781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4350700378417969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139316452401025 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26226043701171875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4630467891693115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139710762671062 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25401997566223145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44512486457824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31398874819278716 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24468040466308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43822646141052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31395169964858466 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2668612003326416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44750356674194336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139277824333736 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2515981197357178.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4419832229614258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138845801353455 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27321577072143555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46363162994384766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138169024671827 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24937653541564941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4252297878265381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138700617211206 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26035237312316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45382189750671387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138023146561214 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24796223640441895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42180323600769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138834361519132 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23414850234985352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4133641719818115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31380953405584605 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25055718421936035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43012475967407227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31372507342270445 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24987125396728516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45836877822875977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137948738677161 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26099610328674316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44361090660095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31375266569001337 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24556350708007812.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42541003227233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31370584964752196 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.248701810836792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43625617027282715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136450895241329 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23912525177001953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4152684211730957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137240150145122 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2386937141418457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4136042594909668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31370117238589695 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25994038581848145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4412248134613037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137354816709246 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2556326389312744.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43709754943847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31366862314088007 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2764303684234619.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4594085216522217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31371394182954515 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24113035202026367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42455101013183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31372063415391105 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568659782409668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4393348693847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31364461353846956 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2422480583190918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41872215270996094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136363536119461 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25420260429382324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46771931648254395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31364216719354904 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2809298038482666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46236419677734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31364500267165046 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2540163993835449.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4364054203033447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136704657758985 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555811405181885.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42856764793395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136031218937465 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2478179931640625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.430436372756958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136773841721671 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2677159309387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45648646354675293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31359296781676155 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586667537689209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4386327266693115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135943672486714 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26062917709350586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4699227809906006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135834114892142 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631371021270752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.438352108001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135494036333902 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25594639778137207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4413266181945801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135699612753732 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545945644378662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4331223964691162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135820005621229 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24193882942199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4307901859283447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31356793344020845 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24878644943237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45017218589782715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31357100180217196 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25582051277160645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44078850746154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31355644081320083 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24625802040100098.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42523908615112305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135465353727341 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616539001464844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43906331062316895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31355304419994356 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652151584625244.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44591188430786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31356299008641925 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25232505798339844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4256722927093506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135069519281387 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2500946521759033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4366774559020996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31351793450968607 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24514985084533691.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42925548553466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135363038097109 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25020647048950195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47156715393066406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135256737470627 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24364161491394043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4205589294433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135071188211441 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25626134872436523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42896413803100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31354003548622134 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2638101577758789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44599485397338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135113264833178 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2675180435180664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46194005012512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347692566258567 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26068687438964844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4454314708709717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31350060999393464 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546689510345459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44582390785217285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31347933879920414 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25611114501953125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4429464340209961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134531719344003 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563211917877197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4439399242401123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134908148220607 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535734176635742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4370853900909424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347657782690863 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592952251434326.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4559292793273926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31345892633710587 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738673686981201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47236132621765137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134650847741536 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2574474811553955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4511687755584717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134738023791994 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2681281566619873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4454166889190674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134862980672291 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25878238677978516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43125176429748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134745806455612 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2579762935638428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44824719429016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345673756940023 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25031185150146484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4325275421142578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.313467310156141 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250887393951416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42928290367126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31346525166715894 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733476161956787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47014641761779785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31345087672982896 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.2639 | test accuracy: 0.791\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2623 | test accuracy: 0.791\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2936 | test accuracy: 0.791\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7283 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2505 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7155 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6637 | test accuracy: 0.791\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7152 | test accuracy: 0.791\n",
            "Epoch:  8 Iteration:  630 | train loss: 1.0437 | test accuracy: 0.791\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6908 | test accuracy: 0.791\n",
            "Epoch:  10 Iteration:  770 | train loss: 1.0377 | test accuracy: 0.791\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2891 | test accuracy: 0.791\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7148 | test accuracy: 0.791\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5198 | test accuracy: 0.791\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7676 | test accuracy: 0.791\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2745 | test accuracy: 0.791\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1812 | test accuracy: 0.791\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6608 | test accuracy: 0.791\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6037 | test accuracy: 0.791\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1837 | test accuracy: 0.791\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2318 | test accuracy: 0.791\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6719 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2344 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1821 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1835 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2750 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1574 | test accuracy: 0.791\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2406 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2244 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2084 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.9789 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5616 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.5915 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6049 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6680 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5952 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1913 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5216 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5601 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.7331 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2471 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1835 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1567 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.5262 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2106 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2267 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7439 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.5465 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7417 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2805 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2079 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1905 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2094 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2909 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2186 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6876 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2290 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6617 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7121 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6408 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2914 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6980 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6236 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.8828 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1984 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3350 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1718 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3707 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2629 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2252 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5400 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2097 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1891 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2301 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6275 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2085 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2757 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3126 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1590 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1650 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4184 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.3167 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1373 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7566 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7965 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2737 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.5269 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1533 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1312 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2204 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2177 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1527 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 1.0020 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.1919 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2330 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6874 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6432 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2218 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6294 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6565 | test accuracy: 0.791\n",
            "total time:  31.68546832500033\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.1868 | test accuracy: 0.791\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7218 | test accuracy: 0.791\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2085 | test accuracy: 0.791\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2361 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3410 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 1.0530 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5105 | test accuracy: 0.791\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7113 | test accuracy: 0.791\n",
            "Epoch:  8 Iteration:  630 | train loss: 1.2012 | test accuracy: 0.791\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6577 | test accuracy: 0.791\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6366 | test accuracy: 0.791\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5666 | test accuracy: 0.791\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2430 | test accuracy: 0.791\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2638 | test accuracy: 0.791\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2643 | test accuracy: 0.791\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2462 | test accuracy: 0.791\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6736 | test accuracy: 0.791\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2043 | test accuracy: 0.791\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2608 | test accuracy: 0.791\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6125 | test accuracy: 0.791\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5867 | test accuracy: 0.791\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2598 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2175 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5103 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2553 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2152 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 1.1317 | test accuracy: 0.791\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2302 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6676 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6606 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.9431 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2907 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6862 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2904 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 1.0448 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6367 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6671 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5466 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2203 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 1.0363 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 1.2199 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.2254 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 1.0752 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3379 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8440 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6837 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7685 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2910 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6362 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.1789 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2229 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2306 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2136 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2593 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2127 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1882 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.9521 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7650 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6565 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2920 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.8368 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2419 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.7401 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3397 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2300 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7868 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6188 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2141 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7687 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7066 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.3064 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.4991 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7176 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2743 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6321 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6647 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.3108 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7041 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7887 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6615 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7853 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.9127 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2290 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.2169 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7024 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2846 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2246 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2451 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7985 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2728 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6854 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1931 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.3033 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.2729 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1855 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2294 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2004 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 1.1820 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.5744 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.8178 | test accuracy: 0.791\n",
            "total time:  35.19826581699999\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2370460033416748.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 0.41148900985717773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5255042608295168 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23678970336914062.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.41330409049987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.44088646130902426 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2717854976654053.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.44589924812316895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.40043056309223174 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24423003196716309.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.41493916511535645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.37450454320226395 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2543647289276123.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4372880458831787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36109873226710726 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25205373764038086.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4343721866607666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3500889778137207 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2500755786895752.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42754578590393066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34404539082731517 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583482265472412.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43824315071105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33847293428012304 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24910640716552734.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43108057975769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33470622897148133 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.253507137298584.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4206044673919678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.332112580537796 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24648118019104004.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43146610260009766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3289504221507481 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2874910831451416.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4721803665161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32648282391684397 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2385871410369873.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4169597625732422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3254769891500473 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26516127586364746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44153666496276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32379218510219027 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2533869743347168.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4251728057861328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32315856005464283 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.4255385398864746.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.824275016784668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32199888016496386 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.6593492031097412.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 1.0657267570495605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32085562731538503 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24617910385131836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42665791511535645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32031180943761556 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539231777191162.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43801426887512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.319664551956313 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24403142929077148.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4197196960449219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31913225225039893 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2500908374786377.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42655396461486816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31877838713782175 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2567901611328125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4468953609466553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3184084245136806 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2437744140625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41144800186157227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3177178668124335 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2363896369934082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4206516742706299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.317558228969574 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23695826530456543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4182770252227783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31711672374180383 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24090051651000977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41230082511901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31695539695875985 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25194716453552246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4291973114013672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31659972752843585 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24406075477600098.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42311859130859375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3164012815271105 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24790430068969727.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41863274574279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3163325126682009 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25264692306518555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4315052032470703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31604056443486894 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24251604080200195.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4257848262786865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31564040907791685 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531130313873291.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4527473449707031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3156214045626777 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24697303771972656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42452192306518555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31531938910484314 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2540006637573242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4409801959991455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31531382969447547 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29039931297302246.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47478723526000977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3154329299926758 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568700313568115.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4402761459350586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3151881618159158 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2501516342163086.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.433089017868042\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3150075427123478 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24524998664855957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4306216239929199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31527819463184903 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23476171493530273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41443657875061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3149193904229573 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24657654762268066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4229457378387451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3149361742394311 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25432372093200684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4361429214477539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31489035189151765 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2344827651977539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41382575035095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3147473624774388 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24464011192321777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42562341690063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31462084565843856 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24375414848327637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4138636589050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.314542418718338 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24564361572265625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42649126052856445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3145636924675533 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507305145263672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4257378578186035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3144702204636165 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2349083423614502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4122793674468994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31437733130795614 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24415278434753418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4241034984588623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3143656926495688 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24448776245117188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4138336181640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3143008602516992 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23589849472045898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4229276180267334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31430231503077916 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637813091278076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45105695724487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3142535064901624 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25447940826416016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4381542205810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3141187540122441 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27138710021972656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47675395011901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31414604144436975 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28595685958862305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4814643859863281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141039878129959 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2792789936065674.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47466135025024414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31406202018260954 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.245560884475708.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4295542240142822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31407390236854554 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2496936321258545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.438800573348999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31409064403602055 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25955748558044434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43817710876464844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31404413410595483 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23313355445861816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42417359352111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31409102039677755 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524864673614502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42746877670288086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31394689551421573 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26302146911621094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4371180534362793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.313881602031844 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26456117630004883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4461958408355713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3139062753745488 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2529304027557373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4383664131164551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138856828212738 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24379444122314453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43085384368896484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138145284993308 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539856433868408.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42475104331970215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31382752869810376 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23563146591186523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4221763610839844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31386533975601194 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629883289337158.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487617015838623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137960970401764 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24946856498718262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4306159019470215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137753290789468 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27312445640563965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4517176151275635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137863499777658 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2560708522796631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4304525852203369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138347791773932 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2556793689727783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4468870162963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31374173419816154 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694051265716553.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4531571865081787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31372671382767814 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512996196746826.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4404783248901367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31374518147536684 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2795999050140381.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46628403663635254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31371655464172366 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619783878326416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46022582054138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31375229358673096 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26949024200439453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4440724849700928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136853477784565 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23821806907653809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41370415687561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136639484337398 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645130157470703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4656105041503906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31365956578935894 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25594210624694824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4371929168701172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31367915613310676 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25143980979919434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4482724666595459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31367324122360773 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254802942276001.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4311833381652832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3136377372911998 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24418187141418457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42171335220336914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31363149327891215 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2540719509124756.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4416208267211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136236501591546 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24716448783874512.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4266798496246338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3135884250913348 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2570805549621582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4519052505493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135726098503385 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27812671661376953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45769810676574707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31355479317052026 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25290632247924805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4572112560272217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135769192661558 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27361202239990234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4559907913208008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31356547261987416 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582252025604248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4387173652648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136027238198689 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630751132965088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4315798282623291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31354463143008093 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24305319786071777.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4198482036590576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31358617884772166 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3841538429260254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5605964660644531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135586585317339 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23889875411987305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5444133281707764\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3135447089161192 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610483169555664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360544681549072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31353791228362493 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3715031147003174.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5542194843292236\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31351654614721025 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26270294189453125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4336686134338379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135409495660237 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25784826278686523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384281635284424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135218790599278 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25956106185913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45970702171325684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135254157441003 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628188133239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44556379318237305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135266593524388 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26531338691711426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4470691680908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31352794894150326 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568550109863281.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4354424476623535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31352011901991705 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27013444900512695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4549229145050049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135332844087056 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27571558952331543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45185017585754395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31351338369505743 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26745128631591797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4527475833892822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134895009653909 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531294822692871.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4414641857147217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31349301849092753 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24454784393310547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4187319278717041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31348035037517546 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25763750076293945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43929362297058105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134803899696895 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25444531440734863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4300971031188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134680811847959 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24719929695129395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.430361270904541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134851132120405 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631876468658447.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44931507110595703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31346693294388905 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26166844367980957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4481682777404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31346943761621204 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547159194946289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45582103729248047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31348216235637666 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25959110260009766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4332268238067627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134758165904454 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.255812406539917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46816563606262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134558358362743 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24769115447998047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4246490001678467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134518257209233 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25287818908691406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44518280029296875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31343808131558554 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25863027572631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372289180755615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134481234209878 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25284862518310547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.434403657913208\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134417895759855 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26227259635925293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44638729095458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31342060693672724 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24895524978637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4329957962036133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134156435728073 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2538580894470215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4464747905731201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345643741743906 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2453758716583252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43674492835998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344161672251564 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251415491104126.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4261767864227295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134296068123409 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514524459838867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43152666091918945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31341214392866407 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24239087104797363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4187328815460205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134297741310937 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562894821166992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44414806365966797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31344634251935144 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24564886093139648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42238736152648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342703316892895 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503950595855713.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42348432540893555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31341205622468676 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3202 | test accuracy: 0.791\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2398 | test accuracy: 0.791\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2457 | test accuracy: 0.791\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2395 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2112 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 1.1833 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7447 | test accuracy: 0.791\n",
            "Epoch:  7 Iteration:  560 | train loss: 1.0860 | test accuracy: 0.791\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6451 | test accuracy: 0.791\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2203 | test accuracy: 0.791\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2045 | test accuracy: 0.791\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2336 | test accuracy: 0.791\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2540 | test accuracy: 0.791\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2174 | test accuracy: 0.791\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2175 | test accuracy: 0.791\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6692 | test accuracy: 0.791\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2230 | test accuracy: 0.791\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6588 | test accuracy: 0.791\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7253 | test accuracy: 0.791\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2744 | test accuracy: 0.791\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7334 | test accuracy: 0.791\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2121 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2075 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2617 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2131 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2212 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6821 | test accuracy: 0.791\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7365 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.9161 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 1.3526 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6362 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 1.0805 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2446 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6419 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2506 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1785 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1746 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2303 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3009 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.8415 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2334 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.7723 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6187 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2169 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6824 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.5444 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2274 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2975 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5537 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2861 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2225 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2635 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1924 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1637 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6573 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6413 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2159 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6730 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2112 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1568 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2306 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2625 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2489 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6318 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5420 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2723 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2345 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5948 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6176 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 1.1304 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2507 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6199 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2176 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6311 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.2950 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.0464 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7084 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.9879 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2905 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6796 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7503 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2729 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2433 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.3044 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2420 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.3225 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6930 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6328 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6217 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2506 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7474 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2561 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.5480 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7110 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1912 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2597 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.8085 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7980 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6667 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2649 | test accuracy: 0.791\n",
            "total time:  31.2906428590004\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3424 | test accuracy: 0.791\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7341 | test accuracy: 0.791\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6744 | test accuracy: 0.791\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3055 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3441 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2207 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2996 | test accuracy: 0.791\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2951 | test accuracy: 0.791\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4470 | test accuracy: 0.791\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2237 | test accuracy: 0.791\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5625 | test accuracy: 0.791\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7831 | test accuracy: 0.791\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2981 | test accuracy: 0.791\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2097 | test accuracy: 0.791\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.9583 | test accuracy: 0.791\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2892 | test accuracy: 0.791\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7584 | test accuracy: 0.791\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2306 | test accuracy: 0.791\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.1888 | test accuracy: 0.791\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2770 | test accuracy: 0.791\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5282 | test accuracy: 0.791\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6957 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6717 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2551 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3113 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6981 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2699 | test accuracy: 0.791\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8391 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2851 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7166 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2962 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2431 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6094 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5942 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 1.2549 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2413 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2914 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.0236 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7531 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5636 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2246 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8536 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2287 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6196 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8704 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2808 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2212 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1989 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7342 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.1076 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2548 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2929 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.1805 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6704 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2130 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6106 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6078 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2751 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6962 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5723 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1739 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2509 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2640 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2866 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2431 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2970 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3082 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2185 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 1.1196 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2217 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2271 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6815 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.5771 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 1.1823 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 1.4988 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1833 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1774 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2497 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2582 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.5646 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6432 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6607 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2475 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7083 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7261 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2311 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2070 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7568 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.1708 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6509 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.3113 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2306 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6792 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.2414 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2350 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.1637 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2372 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.9460 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2824 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2288 | test accuracy: 0.791\n",
            "total time:  36.18219583999962\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26663661003112793.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.45223283767700195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6824916728905269 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2718625068664551.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4656093120574951\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5542334973812103 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26752185821533203.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4513716697692871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4674946916954858 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542457580566406.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44825077056884766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.42172804134232655 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2720327377319336.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.456895112991333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3927812674215862 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591094970703125.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.44048380851745605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.376063796877861 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27533936500549316.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.47880005836486816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36368422550814494 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637474536895752.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.45049262046813965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35334727253232684 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2749295234680176.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.461223840713501\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.346658348611423 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590785026550293.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.435654878616333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3422232057367052 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2584054470062256.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4362678527832031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33804995119571685 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622833251953125.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.44132375717163086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33541068775313243 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561349868774414.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43941259384155273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3320107123681477 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25791001319885254.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4563162326812744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32999388107231686 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24671101570129395.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4271273612976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32823715720857893 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24350714683532715.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.41556382179260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32687203926699504 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24862384796142578.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42351675033569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3252356665475028 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.246246337890625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4246079921722412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32417661121913366 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2769646644592285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4846644401550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32265478670597075 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3042464256286621.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5220470428466797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3223457281078611 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27699851989746094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4575810432434082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3212174534797668 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2703709602355957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46050453186035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3209260663815907 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26322054862976074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45363616943359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3205932008368628 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610969543457031.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4452378749847412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.319796262894358 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545197010040283.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4311509132385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3188982201474054 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605435848236084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45259523391723633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3185034245252609 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2452390193939209.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42983293533325195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31842381613595144 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25507020950317383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.426724910736084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3179295927286148 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25488996505737305.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4324929714202881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31748524393354144 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24067187309265137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4226353168487549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31725257635116577 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25037574768066406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4331488609313965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31723078744752065 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.253528356552124.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4321408271789551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31686093892369954 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26192450523376465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44428110122680664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3163689502647945 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24025440216064453.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41722869873046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3167992549283164 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568321228027344.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42598772048950195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31634192381586346 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25513482093811035.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4323766231536865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31584139679159434 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24769997596740723.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4294164180755615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31599909365177153 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26046180725097656.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44097208976745605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31576838621071407 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2423865795135498.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41124987602233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31545353404113224 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577180862426758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43332695960998535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3154147424868175 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2526707649230957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4411482810974121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31552941373416354 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25154709815979004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43563389778137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3151467910834721 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2588534355163574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44267964363098145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31505930253437586 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24801421165466309.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43686962127685547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31504940135138376 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27774643898010254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4621772766113281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3152198599917548 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541971206665039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4363734722137451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31489025907857077 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2805912494659424.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46695375442504883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31466665182794845 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2501413822174072.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4249591827392578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31488018929958345 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.255049467086792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4300844669342041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31478104037897925 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24626374244689941.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42472124099731445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3146328295980181 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2470417022705078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42941713333129883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3145719762359347 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26845598220825195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44686222076416016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31458460646016256 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25119805335998535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42392992973327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144317188433238 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2550046443939209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.428985595703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143709370068141 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2612009048461914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44081974029541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31435987310750146 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24207448959350586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42784738540649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143271186522075 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2543513774871826.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.436370849609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142860152891704 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2564518451690674.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44007444381713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31423013848917825 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2697024345397949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4506545066833496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31413997539452143 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25571560859680176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43053102493286133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.314152792096138 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2689809799194336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4529235363006592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31414690954344615 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24576854705810547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4151322841644287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31409566487584795 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2474348545074463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43560075759887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141149218593325 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571587562561035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42830538749694824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31406219686780656 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2460031509399414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42439913749694824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.313990073970386 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2660818099975586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4456815719604492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31395822806017737 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512838840484619.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43462586402893066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.313957508121218 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28429365158081055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46767616271972656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31392192670277186 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563953399658203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43296051025390625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31392485414232524 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568392753601074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45075464248657227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139669358730316 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24831175804138184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4338717460632324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139591417142323 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24503183364868164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42487668991088867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138227249894823 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514007091522217.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.424879789352417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31385694444179535 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25559186935424805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43395352363586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31382577547005247 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2710890769958496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45447754859924316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31386623212269377 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25368165969848633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42733311653137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31380235850811006 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25681161880493164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4480569362640381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31383137490068164 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24383234977722168.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4130287170410156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137709434543337 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24748587608337402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44446873664855957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.313722796525274 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623739242553711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44323086738586426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137459439890725 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23983144760131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4138336181640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137622918401446 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25344061851501465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4297008514404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31371839897973197 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24883127212524414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41872739791870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31366687587329317 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25742316246032715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401073455810547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31365263036319185 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24641823768615723.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4166228771209717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31366105931145805 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24062466621398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.421980619430542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136998027563095 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25902819633483887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4339144229888916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31369737897600447 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26434993743896484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4560842514038086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136726583753313 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.263535737991333.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4459102153778076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135966054030827 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26078009605407715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4562225341796875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.313661316888673 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619130611419678.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44827890396118164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31361408148493086 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2537558078765869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42571496963500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.313581805569785 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613053321838379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4503030776977539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31360002372946055 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26280879974365234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4422929286956787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31363278244222914 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26174497604370117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44397735595703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31358520942074913 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25647759437561035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42761659622192383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135974683931896 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532949447631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4324839115142822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135873419897897 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24880051612854004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43759894371032715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135540685483387 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2472248077392578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4237496852874756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31362057541097915 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25577354431152344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43261218070983887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31359958733831134 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26302003860473633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4362170696258545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31354240349360873 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2579030990600586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43766164779663086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135595913444247 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2587850093841553.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44008517265319824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31353717148303983 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565758228302002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43889665603637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135643392801285 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25046467781066895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487934112548828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135210518326078 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2490828037261963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4241368770599365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135274129254477 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24839544296264648.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4214057922363281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135375631707055 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591884136199951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4316284656524658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135245067732675 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25124287605285645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4282824993133545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135449899094445 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572755813598633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44309115409851074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135064951011113 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577064037322998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4400522708892822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134927042893001 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26781535148620605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44824767112731934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351294091769627 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24165558815002441.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4264039993286133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135292670556477 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24803948402404785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4190089702606201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134946921042034 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24848723411560059.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42179203033447266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31349656539303916 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2520637512207031.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43051576614379883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31348778520311626 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25194454193115234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4394567012786865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134791135787964 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26091980934143066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4405841827392578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31347385942935946 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24105501174926758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4155466556549072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31345045949731554 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2633359432220459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4397542476654053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31348746972424646 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25493907928466797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42710351943969727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31346054204872675 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546873092651367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42801856994628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31347695078168597 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25966358184814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44015026092529297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134882433073861 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24072551727294922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42185044288635254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134409487247467 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25469279289245605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4315502643585205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31342871146542683 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23386883735656738.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4144775867462158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31345828132969994 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25753188133239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43660521507263184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134577529770987 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25348639488220215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4262576103210449\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134534648486546 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3220 | test accuracy: 0.791\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2455 | test accuracy: 0.791\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2814 | test accuracy: 0.791\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6448 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7223 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6528 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7182 | test accuracy: 0.791\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7275 | test accuracy: 0.791\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6792 | test accuracy: 0.791\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2909 | test accuracy: 0.791\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6692 | test accuracy: 0.791\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7240 | test accuracy: 0.791\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6723 | test accuracy: 0.791\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3289 | test accuracy: 0.791\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2036 | test accuracy: 0.791\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2237 | test accuracy: 0.791\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2466 | test accuracy: 0.791\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2342 | test accuracy: 0.791\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5277 | test accuracy: 0.791\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5824 | test accuracy: 0.791\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6386 | test accuracy: 0.791\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2698 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2383 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6599 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5983 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7468 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2521 | test accuracy: 0.791\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6597 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.1079 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3105 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2952 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5360 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2657 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2780 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2146 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2166 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.7116 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1765 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.1350 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.8533 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2241 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1837 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.8560 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 1.0527 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2109 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1996 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4845 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6448 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.5860 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 1.0772 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.1812 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2607 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2179 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1970 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1604 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2906 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5631 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.4096 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2229 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2760 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7702 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6941 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3698 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2392 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.3051 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1628 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5836 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2138 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7180 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6236 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.4680 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1790 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 1.0749 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1980 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1537 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2270 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 1.2644 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.5874 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.9517 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6253 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1816 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2241 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1991 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.5250 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.3164 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1892 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.9820 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5871 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.5656 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 1.0152 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9192 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6178 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6110 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.3177 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1923 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 1.2263 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.0883 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.9607 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2083 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7435 | test accuracy: 0.791\n",
            "total time:  31.690356611000425\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4073 | test accuracy: 0.795\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7637 | test accuracy: 0.791\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.3907 | test accuracy: 0.791\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7414 | test accuracy: 0.791\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2450 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6067 | test accuracy: 0.791\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8447 | test accuracy: 0.791\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2981 | test accuracy: 0.791\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1970 | test accuracy: 0.791\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.8190 | test accuracy: 0.791\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7520 | test accuracy: 0.791\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2339 | test accuracy: 0.791\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2061 | test accuracy: 0.791\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1989 | test accuracy: 0.791\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.1807 | test accuracy: 0.791\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2294 | test accuracy: 0.791\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6043 | test accuracy: 0.791\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8123 | test accuracy: 0.791\n",
            "Epoch:  18 Iteration:  1330 | train loss: 1.4065 | test accuracy: 0.791\n",
            "Epoch:  19 Iteration:  1400 | train loss: 1.2464 | test accuracy: 0.791\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2811 | test accuracy: 0.791\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.0890 | test accuracy: 0.791\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2750 | test accuracy: 0.791\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5652 | test accuracy: 0.791\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6440 | test accuracy: 0.791\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2835 | test accuracy: 0.791\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2421 | test accuracy: 0.791\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2521 | test accuracy: 0.791\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2979 | test accuracy: 0.791\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8106 | test accuracy: 0.791\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2287 | test accuracy: 0.791\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 1.2730 | test accuracy: 0.791\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.2670 | test accuracy: 0.791\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2548 | test accuracy: 0.791\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5875 | test accuracy: 0.791\n",
            "Epoch:  35 Iteration:  2520 | train loss: 1.0282 | test accuracy: 0.791\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2116 | test accuracy: 0.791\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.0138 | test accuracy: 0.791\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6877 | test accuracy: 0.791\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6423 | test accuracy: 0.791\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.8309 | test accuracy: 0.791\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.2915 | test accuracy: 0.791\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.5641 | test accuracy: 0.791\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7124 | test accuracy: 0.791\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6036 | test accuracy: 0.791\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6431 | test accuracy: 0.791\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2746 | test accuracy: 0.791\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6687 | test accuracy: 0.791\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2556 | test accuracy: 0.791\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2573 | test accuracy: 0.791\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2710 | test accuracy: 0.791\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.7143 | test accuracy: 0.791\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6794 | test accuracy: 0.791\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2156 | test accuracy: 0.791\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.0844 | test accuracy: 0.791\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2782 | test accuracy: 0.791\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6154 | test accuracy: 0.791\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1998 | test accuracy: 0.791\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2946 | test accuracy: 0.791\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2396 | test accuracy: 0.791\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2698 | test accuracy: 0.791\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2757 | test accuracy: 0.791\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6725 | test accuracy: 0.791\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2436 | test accuracy: 0.791\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.8494 | test accuracy: 0.791\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.0682 | test accuracy: 0.791\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2884 | test accuracy: 0.791\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2801 | test accuracy: 0.791\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2171 | test accuracy: 0.791\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6172 | test accuracy: 0.791\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2057 | test accuracy: 0.791\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6373 | test accuracy: 0.791\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6210 | test accuracy: 0.791\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.5808 | test accuracy: 0.791\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6060 | test accuracy: 0.791\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.2006 | test accuracy: 0.791\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7542 | test accuracy: 0.791\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6688 | test accuracy: 0.791\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2792 | test accuracy: 0.791\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7225 | test accuracy: 0.791\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3597 | test accuracy: 0.791\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.1107 | test accuracy: 0.791\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1939 | test accuracy: 0.791\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.5873 | test accuracy: 0.791\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2305 | test accuracy: 0.791\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2088 | test accuracy: 0.791\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2583 | test accuracy: 0.791\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2608 | test accuracy: 0.791\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.0303 | test accuracy: 0.791\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.9461 | test accuracy: 0.791\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2612 | test accuracy: 0.791\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2115 | test accuracy: 0.791\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2423 | test accuracy: 0.791\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7644 | test accuracy: 0.791\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2495 | test accuracy: 0.791\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2974 | test accuracy: 0.791\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2336 | test accuracy: 0.791\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2209 | test accuracy: 0.791\n",
            "Epoch:  98 Iteration:  6930 | train loss: 1.0360 | test accuracy: 0.791\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2208 | test accuracy: 0.791\n",
            "total time:  36.4071795789996\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24960756301879883.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.4380345344543457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7031227282115391 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25727367401123047.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.4266841411590576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5602909369128091 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24486088752746582.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4245927333831787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.47114393285342626 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609851360321045.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.448380708694458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4213685218776975 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547781467437744.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.439591646194458\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39452572124344965 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25785017013549805.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.4317317008972168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37737044564315253 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2517275810241699.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.42526793479919434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3651993462017604 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24628448486328125.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.43140149116516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35522723538534984 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25433349609375.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4429507255554199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3485338751758848 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28115129470825195.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.47507786750793457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3438732998711722 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586081027984619.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4463062286376953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3398529746702739 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777740955352783.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4746432304382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3359333898339953 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25633811950683594.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44370484352111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3333046500171934 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25649261474609375.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44075775146484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3306658983230591 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2709221839904785.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4514913558959961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32928511074611116 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269561767578125.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.452953577041626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32757847266537804 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27023983001708984.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46429014205932617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32572991592543465 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2584652900695801.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43596577644348145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32438614623887196 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24913763999938965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42268800735473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32330351173877714 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2452986240386963.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.435760498046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32282226255961827 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24475479125976562.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41709089279174805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32203790971211027 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25385499000549316.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43224143981933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32135752354349406 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2782785892486572.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46477699279785156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.320632946065494 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631216049194336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44264721870422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3196193035159792 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26104092597961426.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4332592487335205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31972945247377665 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24230575561523438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41015625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3195919449840273 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2525758743286133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.439363956451416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31846041125910624 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2500123977661133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4321601390838623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3183204833950315 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25420284271240234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4191575050354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31747022696903776 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25412964820861816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43233704566955566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3177953617913382 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568080425262451.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4315659999847412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31721277109214235 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604231834411621.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4517197608947754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31716667669160026 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2706716060638428.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47361254692077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31693990187985555 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2585868835449219.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46167659759521484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3166162043809891 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2782478332519531.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4704246520996094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3165025987795421 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25615787506103516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4388608932495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31628471740654535 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611231803894043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4508085250854492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31575055164950233 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25165891647338867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4288160800933838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31579517466681345 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721548080444336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44940948486328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3159232586622238 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2406325340270996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4155087471008301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31568461656570435 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507140636444092.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4352381229400635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31544177745069774 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27589845657348633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.454207181930542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31533477434090207 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2469005584716797.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41802167892456055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31542596008096424 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643771171569824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4366312026977539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31514046149594444 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25522351264953613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4252479076385498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3150854557752609 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2579152584075928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43750667572021484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31491906983511786 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24675321578979492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4277153015136719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31496270043509345 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2530786991119385.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4362912178039551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147992287363325 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25438642501831055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4274754524230957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31481185257434846 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25988197326660156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4350125789642334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31466406754084997 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26941752433776855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45270752906799316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146850883960724 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2502121925354004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42833852767944336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31461491073880876 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2749443054199219.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4571259021759033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31462387910911016 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637183666229248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45065999031066895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144969714539392 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26253676414489746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4568490982055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143767190831048 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2621622085571289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44507336616516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31453192489487786 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26503705978393555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4739677906036377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3143989758832114 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29408884048461914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4811370372772217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31438252883298057 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26193881034851074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44905543327331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31432233325072695 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2907423973083496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4704742431640625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31423324729715074 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541382312774658.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44150543212890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31419481549944195 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25589442253112793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.432584285736084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141595674412591 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598259449005127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44829297065734863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.314126222048487 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251359224319458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44464588165283203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31416050408567703 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2550175189971924.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4643371105194092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3141796967812947 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649369239807129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4589810371398926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31402829119137354 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26250290870666504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4358830451965332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31398209886891504 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24877357482910156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43366408348083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31400593859808784 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26688599586486816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44271326065063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31394069109644207 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24834609031677246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.429229736328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31392530160290855 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539389133453369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44887638092041016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31398641467094424 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2498331069946289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42298078536987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31389544563634053 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559037208557129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401669502258301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.313930259006364 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603006362915039.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4362654685974121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138623309986932 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2474837303161621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43012356758117676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138491664613996 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26157450675964355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43743205070495605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138835685593741 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24953413009643555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4285869598388672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31377712275300707 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2670001983642578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45471906661987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31371064739567894 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24958109855651855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4306206703186035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137634251798902 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27636146545410156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4583258628845215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137340098619461 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26831746101379395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4588313102722168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137374499014446 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661724090576172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45017123222351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137813644749778 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2658097743988037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44410252571105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3138063907623291 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26003289222717285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43996644020080566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137027242353984 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2520897388458252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.435927152633667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137224188872746 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2438209056854248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4145371913909912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31369640231132506 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25415825843811035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4291505813598633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136808557169778 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25670838356018066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42806529998779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136806070804596 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26127147674560547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44684529304504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31369257526738303 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26093578338623047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317898750305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136485585144588 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546525001525879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44077467918395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136168088231768 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2599780559539795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4535806179046631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136568729366575 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2379920482635498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4142472743988037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31362379108156474 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24797916412353516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42861461639404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31361577723707473 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27062392234802246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44537925720214844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136231635298048 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24820995330810547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42516636848449707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31360881158283777 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2579171657562256.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4363408088684082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136262983083725 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661168575286865.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487299919128418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135973479066576 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2455277442932129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4442460536956787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31361392991883413 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25463151931762695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4366190433502197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135712270225797 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682619094848633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44394731521606445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135824305670602 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24976873397827148.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4343831539154053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31355005162102834 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2614927291870117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.436053991317749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135777792760304 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25424718856811523.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.430283784866333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31353173170770915 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25583624839782715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4438893795013428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135537828717913 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559847831726074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372560977935791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135586649179459 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26439833641052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4485018253326416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135094382933208 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542717456817627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4269120693206787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135251462459564 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2621743679046631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.436326265335083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135677818741117 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25493836402893066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42366671562194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135313012770244 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24970126152038574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4165921211242676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31351532893521444 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24181461334228516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4153635501861572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351326193128315 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2367701530456543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4040510654449463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31350580879620144 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25124335289001465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4271881580352783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31352905801364356 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24272894859313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4140493869781494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134927417550768 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2564692497253418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43097949028015137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134971129042762 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24657487869262695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42232322692871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134665208203452 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2537846565246582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4312162399291992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3135063018117632 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24977588653564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43123936653137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31349505398954663 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25717926025390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4312856197357178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134624911206109 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25696802139282227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4391617774963379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134945158447538 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26214051246643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4429190158843994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31346902293818335 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26657533645629883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45791149139404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134797147342137 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2717258930206299.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4591245651245117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31346828809806276 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24816465377807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4492504596710205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31344774535724096 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26775431632995605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4463541507720947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134397200175694 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2474384307861328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4190199375152588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31347412168979644 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27647900581359863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.489016056060791\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31345352700778417 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.2039 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6443 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6979 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6164 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2683 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1862 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7248 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2061 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7259 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6242 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2452 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1964 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7789 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7722 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6815 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 1.0461 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6947 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1949 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6828 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2209 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.7262 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2405 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2106 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1833 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1983 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2269 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1416 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7284 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.7194 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1853 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1902 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2322 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6990 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2396 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 1.1008 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1959 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2235 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2096 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2106 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6939 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.1707 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6737 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7563 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2249 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5643 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7393 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.2158 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1978 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2196 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2182 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2040 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1813 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6259 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.5803 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2193 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.7434 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.1677 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2359 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2738 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1970 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 1.3245 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1803 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2514 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7072 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.5536 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5529 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2152 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6462 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1868 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1715 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2662 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.5155 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6002 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2270 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.8051 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1842 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2099 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6598 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7877 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7317 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 1.2852 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6996 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2323 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.1597 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2560 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2539 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7199 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 1.2813 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.2680 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6224 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2082 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1705 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6898 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6800 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2156 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6464 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1948 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2260 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1900 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6807 | test accuracy: 0.805\n",
            "total time:  31.750433135000094\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4173 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2365 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2921 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 1.5696 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1509 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.1314 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2812 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2982 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2520 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2283 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7321 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6762 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5366 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2086 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5467 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8114 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6921 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6119 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2153 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1995 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2536 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2319 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2737 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2485 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6168 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2169 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1739 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1729 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2114 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2641 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2089 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6839 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8162 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6995 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.8169 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7442 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1662 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2728 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.7111 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.8126 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7100 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3260 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2068 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6585 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8311 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2412 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1806 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6964 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7036 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5971 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6026 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2042 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1778 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2512 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 1.9347 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1981 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2237 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2315 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1759 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7000 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1999 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2022 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2247 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2452 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6073 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2144 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1796 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2346 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.8397 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2267 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2813 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1966 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6475 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.7874 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7659 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6633 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 1.3046 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6863 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1969 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2300 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7167 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7518 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2189 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.0287 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1997 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2003 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1994 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7712 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1823 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.1845 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2102 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2219 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1448 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.2414 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1920 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.2565 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6822 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2599 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7329 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7639 | test accuracy: 0.805\n",
            "total time:  35.51760900699992\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719712257385254.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0.4680593013763428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5182184278964996 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25855064392089844.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.438295841217041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4418851439441953 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2750835418701172.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4639871120452881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.39767660413469585 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2553272247314453.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.44034671783447266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.37286685492311206 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2511928081512451.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.43409180641174316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3612144704375948 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25916242599487305.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4526851177215576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3507644010441644 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25614333152770996.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4359934329986572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3418616537536894 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26557111740112305.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4483528137207031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33765136556965963 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26812052726745605.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.45191216468811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.33385171251637596 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2693469524383545.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4548966884613037\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33083045440060754 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688014507293701.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4459836483001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32818159503596167 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26879382133483887.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4488523006439209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3269417954342706 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254070520401001.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4532308578491211\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3249974472182138 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25272321701049805.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4370439052581787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3238448794398989 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25476503372192383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43350648880004883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32248521064008984 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643744945526123.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44440698623657227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32186786234378817 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503194808959961.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42984938621520996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32067948068891255 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647984027862549.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.440814733505249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32020922303199767 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609262466430664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4346611499786377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31935686596802304 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24421072006225586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43521642684936523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31856545848505835 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2589702606201172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4446067810058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31869359655039653 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26706981658935547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4657406806945801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31760862682546886 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2756187915802002.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.474423885345459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3173740897859846 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2741262912750244.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46536779403686523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3172942991767611 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2812082767486572.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4800288677215576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3170188844203949 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2658419609069824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4439551830291748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3167328945228032 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25800085067749023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4458472728729248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31617978342941827 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27228689193725586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4580411911010742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31601211300918036 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24773478507995605.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43575143814086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31599318725722175 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25750184059143066.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43360471725463867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3158991341079984 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24997448921203613.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42174291610717773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3156091949769429 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27501988410949707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45352792739868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31552975731236593 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536041736602783.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4263739585876465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3153558773653848 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24773693084716797.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4235866069793701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3154951721429825 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266646146774292.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44194459915161133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3151751309633255 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25028419494628906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4332914352416992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3150274485349655 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613551616668701.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43668627738952637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31506237600530895 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24250435829162598.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41860055923461914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31493608781269616 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2732722759246826.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4560372829437256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3147588563816888 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598402500152588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4320535659790039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3146716207265854 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26969242095947266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4499399662017822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31472356958048686 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2670621871948242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4395253658294678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3145306489297322 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2469344139099121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43877601623535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.314451602101326 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25313878059387207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4265434741973877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3143203284059252 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25066065788269043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4302396774291992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31452713736466 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2929103374481201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48198819160461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3144293176276343 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647240161895752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4446101188659668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3142989720617022 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2879161834716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46374058723449707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31425835447652 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25619053840637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43570733070373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.314295408981187 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2485501766204834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44947218894958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3141970906938825 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260448694229126.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.437633752822876\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3142094016075134 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565336227416992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4522206783294678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31412817239761354 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509334087371826.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42614316940307617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141810187271663 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25000572204589844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42824411392211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141430365187781 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2741875648498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45801758766174316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31402256573949544 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25915050506591797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4282197952270508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140384610210146 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25206995010375977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4358046054840088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31406513707978384 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24922513961791992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42117857933044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3139885153089251 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24200797080993652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41892170906066895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139255791902542 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25556182861328125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4245123863220215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31387844383716584 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2469160556793213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42862653732299805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31387369419847216 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647366523742676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44064927101135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3138574119125094 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24109911918640137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4050252437591553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138678759336472 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24465036392211914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42717790603637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31385558715888434 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24801969528198242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41834163665771484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31383610580648696 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24712395668029785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4278755187988281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31375713901860375 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2554597854614258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43355226516723633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31384281047752927 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25002026557922363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43270325660705566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31368566538606374 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25331926345825195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.434359073638916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31370626517704553 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2826418876647949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4677412509918213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31383670483316695 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2665979862213135.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.461698055267334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31371163172381267 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26584577560424805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4441492557525635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138023942708969 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536001205444336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44576525688171387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31370831387383596 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643086910247803.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44016361236572266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3136559384209769 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24639225006103516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4462132453918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3137440992253167 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2665255069732666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45311951637268066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136674812861851 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25202512741088867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4252502918243408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31366147909845626 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613658905029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4560730457305908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136477168117251 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521357536315918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4251396656036377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136297847543444 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671694755554199.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.445004940032959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136246642896107 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696354389190674.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4509451389312744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31362714682306564 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26189756393432617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4451127052307129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31362319120338983 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2689800262451172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44530534744262695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136315724679402 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25029540061950684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43160462379455566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3135837163243975 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577834129333496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4444448947906494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136159841503416 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26033544540405273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4437410831451416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135832961116518 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27451300621032715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46681761741638184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31354484004633765 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514820098876953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43041229248046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135752716234752 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.263369083404541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44757747650146484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31357456743717194 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2783534526824951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4625277519226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31355739406176975 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25371217727661133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45415425300598145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.313589956504958 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2684180736541748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44725823402404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31355911365577155 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2566640377044678.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4514806270599365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31357520307813375 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.255845308303833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44249534606933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135296131883349 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25527215003967285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43969130516052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31352246531418393 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26181650161743164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487903118133545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31352354884147643 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260087251663208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4358813762664795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31353025691849845 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24505066871643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4111013412475586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135233504431588 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2704482078552246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44385814666748047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31346852225916727 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24936771392822266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4355151653289795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135343585695539 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26488685607910156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4556601047515869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31350564530917574 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25463056564331055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4340555667877197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31349322753293174 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25865650177001953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372100830078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135162962334497 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25622987747192383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44040751457214355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31348514556884766 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27056097984313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4591045379638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134678074291774 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2659761905670166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44772911071777344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31347564969744 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622103691101074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43711352348327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135011251483645 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26230716705322266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4444427490234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31348956440176284 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541317939758301.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44418883323669434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31348212489059996 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25682902336120605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43953466415405273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31349137127399446 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25874829292297363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44720458984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31348671061652045 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25231313705444336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43230390548706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134587390082223 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26636266708374023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44808459281921387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31347601456301555 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590656280517578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43990325927734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134471331323896 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24822521209716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4405786991119385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31346035599708555 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27884912490844727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46538496017456055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31343151543821607 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577207088470459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4524359703063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134197060550962 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2642853260040283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4535832405090332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134618486676897 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260880708694458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4393126964569092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31343404948711395 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26232051849365234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4456453323364258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134427330323628 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26293492317199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44274306297302246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31343692541122437 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610492706298828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44096875190734863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134314605167934 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2790999412536621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4676787853240967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31343762235982076 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2669980525970459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4605834484100342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134359700339181 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269930362701416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4666171073913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31342339260237556 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25206589698791504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4247703552246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134177101509912 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25644969940185547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43709397315979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134048581123352 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524831295013428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4336690902709961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31340944383825575 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6110 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2143 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6995 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7512 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2414 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7146 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2246 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7616 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1821 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2654 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2507 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2464 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6528 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7895 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.1900 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2548 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2225 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2047 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7409 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1956 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2339 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6586 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2285 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2603 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2017 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7080 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.5956 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7047 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2064 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2161 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2446 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2600 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.2063 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.5922 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2478 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2482 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6044 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 1.2775 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.1865 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2206 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2256 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1954 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7327 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6700 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6370 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7403 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1700 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1607 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7214 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6970 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.1970 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2569 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6356 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6552 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2801 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6507 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.5716 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2101 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6943 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6617 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.9483 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.5344 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6655 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7096 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7230 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2018 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2219 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 1.2382 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1755 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2392 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6779 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7329 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1847 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2270 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1952 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2505 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7358 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.1753 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1847 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2234 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.7375 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.1961 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1962 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.5480 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2168 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7887 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2050 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1866 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2374 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8189 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2331 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6516 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1905 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.5682 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2170 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7232 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1916 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7215 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1600 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6699 | test accuracy: 0.805\n",
            "total time:  32.11965782200059\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6796 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2004 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1821 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7281 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7725 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5963 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6767 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1597 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.7195 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6518 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2064 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5413 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8710 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.8013 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2459 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2127 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6643 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6631 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5273 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2101 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2092 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6989 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6865 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2326 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2741 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2047 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6530 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2243 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2305 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1663 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6907 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.8808 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7211 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7894 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2245 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2788 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2417 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6198 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.2536 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6500 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6827 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.1831 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2260 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.2289 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1961 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2021 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1889 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.2023 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7450 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2423 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6116 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2180 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1534 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.4420 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2236 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.1960 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2422 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7338 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.8575 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1830 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2144 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.5607 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2545 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6211 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2675 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7750 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1895 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2363 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6181 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 1.1534 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2161 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2250 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.5924 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1950 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7027 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2258 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2098 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.7142 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7077 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1992 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6438 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2250 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5214 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.1379 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6536 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2599 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7486 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.1769 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2621 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2400 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6627 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2300 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.6552 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.7094 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2189 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.8164 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1819 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6230 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.5850 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1984 | test accuracy: 0.805\n",
            "total time:  35.33305402800033\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26920175552368164.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.460740327835083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6813291038785662 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24900174140930176.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4366612434387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5520148519958769 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26198816299438477.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4478034973144531\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46799824833869935 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24342012405395508.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4291849136352539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4209592010293688 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24431657791137695.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.4226877689361572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3900514283350536 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26198840141296387.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.43633484840393066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37581137248447966 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2508664131164551.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4240546226501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36268073575837273 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504563331604004.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.41897082328796387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35095951173986706 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2404320240020752.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4090759754180908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34597094995634897 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2385692596435547.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.41191720962524414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34212905892304013 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27060532569885254.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45032787322998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3380565115383693 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2474215030670166.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.429828405380249\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33506426087447577 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2642216682434082.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4400022029876709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3305207529238292 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25954747200012207.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44322991371154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32932965755462645 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25070834159851074.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43607401847839355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32748850669179647 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654075622558594.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43609619140625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3257908386843545 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2433168888092041.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4167978763580322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32476195309843336 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2573275566101074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4424564838409424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32414009400776456 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26026105880737305.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44537782669067383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3229811119181769 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2624847888946533.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44020628929138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3213878597532 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24840927124023438.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42470860481262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3215238737208503 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25040721893310547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4342844486236572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3203756762402398 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2566399574279785.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4364664554595947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3198260205132621 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2522420883178711.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4219484329223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31932239958218167 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25974583625793457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4416217803955078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31860316480909073 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2525055408477783.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42227697372436523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31861920399325233 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25377440452575684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4381136894226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3180895073073251 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24670672416687012.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4273345470428467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31783364500318256 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24975943565368652.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4420614242553711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31732096331460136 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26114511489868164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43825817108154297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3170696347951889 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24177813529968262.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4152953624725342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3166468875748771 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2799692153930664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4559001922607422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31660523329462326 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24364233016967773.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4173164367675781\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31637814939022063 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618739604949951.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.441744327545166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31629743490900314 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25598573684692383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43294668197631836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31611911611897603 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586803436279297.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4479997158050537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31607202802385603 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678675651550293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4490697383880615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31577551875795634 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24824166297912598.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4384148120880127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31568271219730376 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2755894660949707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45424723625183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3154075439487185 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25003767013549805.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42450380325317383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31532781294413975 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2734227180480957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4667656421661377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3152288611446108 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688271999359131.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4464750289916992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3152044300522123 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605123519897461.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.455005407333374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3150700105088098 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524557113647461.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43483638763427734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31493056799684255 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.257596492767334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45435166358947754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3149779281445912 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28135037422180176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4799537658691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.314845324414117 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27895116806030273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47716426849365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3148328678948539 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2834465503692627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46799540519714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31479684412479403 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24744248390197754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4187767505645752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.314702896135194 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26732611656188965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4509270191192627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31449331981795176 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516968250274658.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42145609855651855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3144434779882431 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24700188636779785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45223569869995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3145194981779371 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260983943939209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43563127517700195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31429930542196544 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2487475872039795.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4404482841491699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144171906369073 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25142955780029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42708635330200195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.314345674429621 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2404491901397705.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4175887107849121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143291405269078 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2729787826538086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45751309394836426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31417708269187383 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25232744216918945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4300410747528076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31415774609361374 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25981879234313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4772005081176758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141974210739136 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2588472366333008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44307756423950195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141498978648867 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2807738780975342.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46871113777160645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31409545966557095 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2530694007873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.428210973739624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3140308141708374 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24919795989990234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4302990436553955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140681530748095 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2784886360168457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4604682922363281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31396624147892 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24960756301879883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4265749454498291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31406640240124295 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762291431427002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4636249542236328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31399160964148387 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2500782012939453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4277825355529785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31390387756483895 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24400782585144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43082284927368164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3139261965240751 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510712146759033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4331049919128418\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139093437365123 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.271500825881958.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45062708854675293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.313945688520159 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25719618797302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4358675479888916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138704478740692 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25326108932495117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43071794509887695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31379696769373755 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25815677642822266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44148898124694824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31377683877944945 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24117517471313477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41019654273986816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138845043522971 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25212574005126953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43993186950683594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138281422001975 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26261210441589355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44073033332824707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3137848905154637 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26000261306762695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44126224517822266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31377983519009184 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647581100463867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4421513080596924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137598203761237 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2606954574584961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4451713562011719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136840650013515 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25672078132629395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45520806312561035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.313797099675451 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2668306827545166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46427369117736816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31368258850915093 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26927661895751953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46204042434692383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31370872472013744 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25777435302734375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45136356353759766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3136543661355972 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2686934471130371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4452345371246338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31369256377220156 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598912715911865.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4350118637084961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31368262427193777 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2513706684112549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4270060062408447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31369574793747496 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26868438720703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44286227226257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136314064264297 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2529275417327881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43529677391052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31371051243373327 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524595260620117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4286935329437256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31365887437547957 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23959112167358398.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4088468551635742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31365522827420916 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24276113510131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4160616397857666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136238775082997 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649388313293457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43369555473327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136124346937452 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24946856498718262.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317619800567627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31361047753265925 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521994113922119.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4272599220275879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31358817730631144 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.245436429977417.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4283316135406494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31361455534185684 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24683332443237305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41980886459350586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31356882112366813 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2482917308807373.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42667412757873535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136018076113292 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25968265533447266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43932437896728516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135908420596804 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605443000793457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4409921169281006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357382748808177 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2648015022277832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43667030334472656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31357382919107163 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2485976219177246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43298912048339844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135297349521092 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24107646942138672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41631579399108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135475435427257 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250354528427124.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4278395175933838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135645823819297 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26521944999694824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4520988464355469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31357966874326976 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25528669357299805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4327113628387451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31352312990597314 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25345897674560547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43888068199157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31354032627173833 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261399507522583.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4428894519805908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31352343388966153 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26070380210876465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45632266998291016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.313503451858248 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27077531814575195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4579288959503174\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135268462555749 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597942352294922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45392537117004395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31355085500649044 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26877284049987793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4517083168029785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135131755045482 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535536289215088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44028210639953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135051126991 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2635371685028076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4498558044433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134995528629848 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27004146575927734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45621180534362793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31347729861736295 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.275390625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4534718990325928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31349910625389643 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27852749824523926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4707620143890381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31348303258419036 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25162434577941895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384117126464844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3135005980730057 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667984962463379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4500603675842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31345837669713156 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24982190132141113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4210038185119629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134740105697087 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2636899948120117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44937777519226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134667932987213 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26282525062561035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44066333770751953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134562292269298 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2638094425201416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4530675411224365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31346857632909503 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2599177360534668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.434706449508667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31348477900028227 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568855285644531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44492340087890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134499017681394 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25234484672546387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43313097953796387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134484163352421 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582519054412842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4508988857269287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343778584684645 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622199058532715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4434225559234619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31345904895237514 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26157093048095703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4390828609466553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134485585348947 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.2441 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2245 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 1.2797 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 1.1518 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2134 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6718 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7124 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1956 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1808 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1831 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7009 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1642 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7107 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7304 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6959 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1981 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2241 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2523 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2198 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1755 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0567 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6744 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2043 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2274 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7109 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1902 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1693 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1552 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2015 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7333 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7512 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6962 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.0681 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2023 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2298 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6468 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6907 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2062 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.1830 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2296 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7147 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6555 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7147 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6489 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2232 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1988 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 1.2258 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6376 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1664 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2220 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6291 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 1.0417 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2149 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.1065 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.6134 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.5961 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7153 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.5908 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1985 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7153 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.5768 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6061 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2042 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2262 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.8396 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5570 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7198 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6924 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2172 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1847 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2381 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7048 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1845 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6168 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7484 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1999 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1650 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2091 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2114 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6370 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6925 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5547 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5434 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1703 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 1.0385 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2466 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1647 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.5598 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.8257 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6680 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2271 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 1.0725 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1667 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.0470 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2467 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6449 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 1.1056 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.7360 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6509 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6518 | test accuracy: 0.805\n",
            "total time:  32.4818167630001\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.2109 | test accuracy: 0.805\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3649 | test accuracy: 0.805\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5999 | test accuracy: 0.805\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2477 | test accuracy: 0.805\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.2222 | test accuracy: 0.805\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6879 | test accuracy: 0.805\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8012 | test accuracy: 0.805\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.9702 | test accuracy: 0.805\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5898 | test accuracy: 0.805\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6903 | test accuracy: 0.805\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1734 | test accuracy: 0.805\n",
            "Epoch:  11 Iteration:  840 | train loss: 1.4433 | test accuracy: 0.805\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2093 | test accuracy: 0.805\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.8113 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2129 | test accuracy: 0.805\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2339 | test accuracy: 0.805\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2811 | test accuracy: 0.805\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6337 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2199 | test accuracy: 0.805\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9627 | test accuracy: 0.805\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5603 | test accuracy: 0.805\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2417 | test accuracy: 0.805\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1553 | test accuracy: 0.805\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2300 | test accuracy: 0.805\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.8206 | test accuracy: 0.805\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2289 | test accuracy: 0.805\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.8014 | test accuracy: 0.805\n",
            "Epoch:  27 Iteration:  1960 | train loss: 1.1973 | test accuracy: 0.805\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2150 | test accuracy: 0.805\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2205 | test accuracy: 0.805\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6083 | test accuracy: 0.805\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.1911 | test accuracy: 0.805\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.0673 | test accuracy: 0.805\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2502 | test accuracy: 0.805\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.7606 | test accuracy: 0.805\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2242 | test accuracy: 0.805\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6372 | test accuracy: 0.805\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2004 | test accuracy: 0.805\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.2366 | test accuracy: 0.805\n",
            "Epoch:  39 Iteration:  2800 | train loss: 1.3213 | test accuracy: 0.805\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7928 | test accuracy: 0.805\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.2041 | test accuracy: 0.805\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2100 | test accuracy: 0.805\n",
            "Epoch:  43 Iteration:  3080 | train loss: 1.3726 | test accuracy: 0.805\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2290 | test accuracy: 0.805\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.6275 | test accuracy: 0.805\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.1964 | test accuracy: 0.805\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1944 | test accuracy: 0.805\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1882 | test accuracy: 0.805\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6899 | test accuracy: 0.805\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2230 | test accuracy: 0.805\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.2028 | test accuracy: 0.805\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.2281 | test accuracy: 0.805\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1917 | test accuracy: 0.805\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.2067 | test accuracy: 0.805\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2244 | test accuracy: 0.805\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.8814 | test accuracy: 0.805\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7390 | test accuracy: 0.805\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7061 | test accuracy: 0.805\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2032 | test accuracy: 0.805\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2181 | test accuracy: 0.805\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1938 | test accuracy: 0.805\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2216 | test accuracy: 0.805\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2097 | test accuracy: 0.805\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.6822 | test accuracy: 0.805\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5875 | test accuracy: 0.805\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2028 | test accuracy: 0.805\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2442 | test accuracy: 0.805\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2028 | test accuracy: 0.805\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.1963 | test accuracy: 0.805\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2113 | test accuracy: 0.805\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2128 | test accuracy: 0.805\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2323 | test accuracy: 0.805\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2015 | test accuracy: 0.805\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.2183 | test accuracy: 0.805\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1800 | test accuracy: 0.805\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2366 | test accuracy: 0.805\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1661 | test accuracy: 0.805\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.2137 | test accuracy: 0.805\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3152 | test accuracy: 0.805\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6155 | test accuracy: 0.805\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.7681 | test accuracy: 0.805\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2267 | test accuracy: 0.805\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6638 | test accuracy: 0.805\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1950 | test accuracy: 0.805\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6451 | test accuracy: 0.805\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6211 | test accuracy: 0.805\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2113 | test accuracy: 0.805\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2388 | test accuracy: 0.805\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2065 | test accuracy: 0.805\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.1996 | test accuracy: 0.805\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.2319 | test accuracy: 0.805\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2085 | test accuracy: 0.805\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.1008 | test accuracy: 0.805\n",
            "Epoch:  94 Iteration:  6650 | train loss: 1.0775 | test accuracy: 0.805\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.1713 | test accuracy: 0.805\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.7142 | test accuracy: 0.805\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6657 | test accuracy: 0.805\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.1812 | test accuracy: 0.805\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.2032 | test accuracy: 0.805\n",
            "total time:  35.52561457499996\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27307963371276855.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.4817802906036377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7033328754561288 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259476900100708.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4529881477355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5549061558076314 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2764770984649658.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.465740442276001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4700127031121935 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2793903350830078.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.48309755325317383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4243338188954762 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647824287414551.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.44605565071105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39488148859569006 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26735520362854004.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.45135974884033203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3750018894672394 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26720523834228516.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4543752670288086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3633156316620963 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27044034004211426.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.45697689056396484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35425824310098375 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2636430263519287.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4442458152770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34615071032728467 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26801204681396484.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45781826972961426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3421503237315587 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2758352756500244.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4646799564361572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33808925705296655 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.270113468170166.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4489452838897705\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33511495675359454 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2574574947357178.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.435579776763916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33131272281919205 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2562265396118164.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4290792942047119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32980714695794244 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26284098625183105.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.451885461807251\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3276467834200178 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629821300506592.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44975757598876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32609438257558004 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2612600326538086.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4589240550994873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3252058838094984 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651658058166504.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45360612869262695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3242140203714371 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547178268432617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4492630958557129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3223760404757091 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26441526412963867.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4451138973236084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3221401563712529 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251986026763916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.428617000579834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3208781114646367 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700960636138916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4576759338378906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3206259386880057 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2656590938568115.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45585131645202637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31991332939692907 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2830817699432373.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4729487895965576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31994988322257994 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25837087631225586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44157934188842773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31896868646144866 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25942373275756836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46247410774230957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31848626605101993 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2753465175628662.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4609527587890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3183215980018888 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628352642059326.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45871806144714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3177862754889897 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26520371437072754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44077348709106445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31737980587141856 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26068854331970215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4376399517059326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.317431378364563 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27155303955078125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45081233978271484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31689417106764656 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2519214153289795.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43376708030700684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31709514260292054 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27270078659057617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4603719711303711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3165458917617798 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26369142532348633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4586641788482666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31650107886110035 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26407575607299805.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46157240867614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31595604036535535 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27339816093444824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45550990104675293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31608264233384814 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25194263458251953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43746495246887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3157524125916617 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629203796386719.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43391847610473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3157210426671164 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24320220947265625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4245035648345947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3155864030122757 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27339768409729004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4465217590332031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31559260061808997 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534189224243164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42873311042785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31526943232331955 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2529897689819336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43612170219421387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3153114195380892 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23611116409301758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.40749239921569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3153502587761198 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24815082550048828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4251439571380615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31505273793424876 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631876468658447.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4419238567352295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3149823763540813 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259458065032959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4483771324157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3148696005344391 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27721381187438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46521830558776855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3148634510380881 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2640037536621094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4503285884857178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147550161395754 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28038620948791504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4810526371002197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31449274037565506 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615983486175537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4462432861328125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3145552469151361 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27030324935913086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48444271087646484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3144655691725867 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25998854637145996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4488644599914551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.314584858076913 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26334333419799805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.448575496673584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31451528540679385 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565042972564697.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43857598304748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144753898893084 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24527335166931152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42363691329956055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3144129672220775 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26901698112487793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44773054122924805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143667480775288 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25888752937316895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4417238235473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3143948461328234 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2671322822570801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4663717746734619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3142836162022182 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597544193267822.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44252848625183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141663466181074 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26700758934020996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44668126106262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141185705150877 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2755439281463623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4525136947631836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141872435808182 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24919438362121582.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43413305282592773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.314032804114478 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2674713134765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4544949531555176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31417629761355265 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2464292049407959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4303269386291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3141987008707864 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24901175498962402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43605732917785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140146753617695 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25660085678100586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42873692512512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140807760613305 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26233983039855957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43473005294799805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.314004870397704 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2757863998413086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4615144729614258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31396626574652536 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2830164432525635.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4875192642211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139167134250913 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27988171577453613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4626424312591553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138846333537783 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615020275115967.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4544212818145752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138520951781954 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.256619930267334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4315645694732666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31388397642544336 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24806666374206543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4305703639984131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.313861078449658 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2567448616027832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45133018493652344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138167155640466 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24822735786437988.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4287748336791992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138064980506897 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25421857833862305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4280569553375244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31373473618711745 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510221004486084.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4238903522491455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137733791555677 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24910640716552734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4310126304626465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31376746126583643 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24958515167236328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43621301651000977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137728150401797 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24640560150146484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42815089225769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31379289967673163 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25138425827026367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42999839782714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137378185987473 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250042200088501.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.427288293838501\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137544048683984 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25677943229675293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43601036071777344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31370894270283833 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26174116134643555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323873519897461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31372005428586686 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535693645477295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4284958839416504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137037179299763 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2470996379852295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42999267578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136833978550775 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24064993858337402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4268019199371338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31370673775672914 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2491757869720459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4210324287414551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.313638625400407 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2722206115722656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.463824987411499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31367370060511995 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25110864639282227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43297600746154785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31366717857973914 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2670457363128662.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45141005516052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136482055698122 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27660632133483887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4666330814361572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31362878339631217 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26693177223205566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48232388496398926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136624561888831 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28148937225341797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47400569915771484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136070081165859 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25733160972595215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4472005367279053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31368920803070066 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26585888862609863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.452117919921875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135945073195866 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596137523651123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43335485458374023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31359010679381233 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26744794845581055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4499948024749756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135767238480704 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565016746520996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42783236503601074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135853328875133 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260833740234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44402647018432617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135352143219539 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619171142578125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.440293550491333\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31355566297258647 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2551882266998291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4453761577606201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135392082589013 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25881028175354004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4416670799255371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135568359068462 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24438834190368652.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4127843379974365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135198793240956 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622072696685791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4479520320892334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135353535413742 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25184035301208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42688941955566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135786499295916 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2528078556060791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43096923828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31348852344921657 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2470548152923584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42215871810913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135082734482629 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2464761734008789.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4271352291107178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31353246016161784 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2491295337677002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4205925464630127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31351803924356186 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527484893798828.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42792797088623047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31353783564908166 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571425437927246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44664573669433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31353477920804707 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534976005554199.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42482638359069824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31352845004626684 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512810230255127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44471001625061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135080541883196 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.251220703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42318177223205566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134902375085013 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2515113353729248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4573819637298584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31348180856023516 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662651538848877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4601142406463623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31347181158406395 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25200963020324707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4286806583404541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134915556226458 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679140567779541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4504847526550293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134552185024534 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503640651702881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.429790735244751\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134751396519797 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2655770778656006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4512016773223877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31350053676537104 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24932169914245605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42510414123535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.313469369496618 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521333694458008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44577813148498535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31348863669804167 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24742650985717773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42670440673828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134729393890926 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26002979278564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4333806037902832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31346784702369146 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25865817070007324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389975070953369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134368658065796 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24116206169128418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.416107177734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134589054754802 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26081037521362305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44761109352111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134594393627984 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6284 | test accuracy: 0.626\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6973 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4793 | test accuracy: 0.670\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2915 | test accuracy: 0.657\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5171 | test accuracy: 0.670\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4850 | test accuracy: 0.680\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.9667 | test accuracy: 0.684\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8511 | test accuracy: 0.690\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6681 | test accuracy: 0.707\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2838 | test accuracy: 0.694\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2631 | test accuracy: 0.731\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4973 | test accuracy: 0.734\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.4297 | test accuracy: 0.714\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.4729 | test accuracy: 0.704\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.3528 | test accuracy: 0.737\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8553 | test accuracy: 0.714\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5187 | test accuracy: 0.727\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7802 | test accuracy: 0.737\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7731 | test accuracy: 0.724\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2128 | test accuracy: 0.724\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3411 | test accuracy: 0.731\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5844 | test accuracy: 0.737\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2405 | test accuracy: 0.737\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5296 | test accuracy: 0.734\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3944 | test accuracy: 0.731\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.7004 | test accuracy: 0.727\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6131 | test accuracy: 0.734\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.7787 | test accuracy: 0.714\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.4882 | test accuracy: 0.734\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6260 | test accuracy: 0.741\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.1556 | test accuracy: 0.741\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.3099 | test accuracy: 0.741\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.2379 | test accuracy: 0.741\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1940 | test accuracy: 0.744\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1958 | test accuracy: 0.744\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3326 | test accuracy: 0.747\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2052 | test accuracy: 0.747\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2516 | test accuracy: 0.744\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6363 | test accuracy: 0.747\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6963 | test accuracy: 0.747\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.9336 | test accuracy: 0.747\n",
            "Epoch:  41 Iteration:  2940 | train loss: 1.2009 | test accuracy: 0.747\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.3133 | test accuracy: 0.751\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.9286 | test accuracy: 0.747\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2984 | test accuracy: 0.747\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1609 | test accuracy: 0.747\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4012 | test accuracy: 0.751\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1751 | test accuracy: 0.751\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7658 | test accuracy: 0.747\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2345 | test accuracy: 0.751\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.4437 | test accuracy: 0.751\n",
            "Epoch:  51 Iteration:  3640 | train loss: 1.1294 | test accuracy: 0.751\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1558 | test accuracy: 0.751\n",
            "Epoch:  53 Iteration:  3780 | train loss: 1.2047 | test accuracy: 0.751\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7383 | test accuracy: 0.751\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2068 | test accuracy: 0.751\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.7654 | test accuracy: 0.758\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7610 | test accuracy: 0.751\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7451 | test accuracy: 0.758\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2531 | test accuracy: 0.751\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7484 | test accuracy: 0.751\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3110 | test accuracy: 0.751\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.5600 | test accuracy: 0.758\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.9508 | test accuracy: 0.751\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.1273 | test accuracy: 0.751\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6512 | test accuracy: 0.751\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.1939 | test accuracy: 0.754\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7890 | test accuracy: 0.751\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2137 | test accuracy: 0.761\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.8525 | test accuracy: 0.761\n",
            "Epoch:  70 Iteration:  4970 | train loss: 1.0417 | test accuracy: 0.761\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1445 | test accuracy: 0.751\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.7408 | test accuracy: 0.751\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.3441 | test accuracy: 0.751\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.8607 | test accuracy: 0.751\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.2883 | test accuracy: 0.751\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.2218 | test accuracy: 0.754\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.2548 | test accuracy: 0.751\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1663 | test accuracy: 0.751\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1796 | test accuracy: 0.761\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.3332 | test accuracy: 0.761\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.1869 | test accuracy: 0.758\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2065 | test accuracy: 0.751\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6075 | test accuracy: 0.754\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.2080 | test accuracy: 0.754\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2845 | test accuracy: 0.751\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6009 | test accuracy: 0.754\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2725 | test accuracy: 0.751\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.4272 | test accuracy: 0.751\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.9617 | test accuracy: 0.761\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6237 | test accuracy: 0.754\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1608 | test accuracy: 0.761\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7736 | test accuracy: 0.761\n",
            "Epoch:  93 Iteration:  6580 | train loss: 1.3630 | test accuracy: 0.761\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1457 | test accuracy: 0.761\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.9560 | test accuracy: 0.761\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2027 | test accuracy: 0.751\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1605 | test accuracy: 0.751\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7119 | test accuracy: 0.758\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6205 | test accuracy: 0.758\n",
            "total time:  31.159249861999342\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8605 | test accuracy: 0.623\n",
            "Epoch:  1 Iteration:  140 | train loss: 1.0967 | test accuracy: 0.630\n",
            "Epoch:  2 Iteration:  210 | train loss: 1.5074 | test accuracy: 0.633\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.9074 | test accuracy: 0.657\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.4385 | test accuracy: 0.660\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7006 | test accuracy: 0.670\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7300 | test accuracy: 0.640\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.3239 | test accuracy: 0.633\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6541 | test accuracy: 0.680\n",
            "Epoch:  9 Iteration:  700 | train loss: 1.1004 | test accuracy: 0.684\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5946 | test accuracy: 0.670\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5438 | test accuracy: 0.684\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2947 | test accuracy: 0.667\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.1044 | test accuracy: 0.684\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2203 | test accuracy: 0.677\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3698 | test accuracy: 0.687\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.8596 | test accuracy: 0.687\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4206 | test accuracy: 0.731\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7478 | test accuracy: 0.694\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4819 | test accuracy: 0.690\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0178 | test accuracy: 0.721\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5713 | test accuracy: 0.727\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6373 | test accuracy: 0.697\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7912 | test accuracy: 0.727\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.7837 | test accuracy: 0.694\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.3400 | test accuracy: 0.717\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2847 | test accuracy: 0.717\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8061 | test accuracy: 0.721\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.2203 | test accuracy: 0.714\n",
            "Epoch:  29 Iteration:  2100 | train loss: 1.1172 | test accuracy: 0.714\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.4813 | test accuracy: 0.710\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2827 | test accuracy: 0.710\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.4865 | test accuracy: 0.714\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2161 | test accuracy: 0.714\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2273 | test accuracy: 0.714\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2472 | test accuracy: 0.714\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6617 | test accuracy: 0.714\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.4782 | test accuracy: 0.714\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.1529 | test accuracy: 0.714\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1956 | test accuracy: 0.714\n",
            "Epoch:  40 Iteration:  2870 | train loss: 1.0226 | test accuracy: 0.710\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4658 | test accuracy: 0.717\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.4313 | test accuracy: 0.714\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7522 | test accuracy: 0.721\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1545 | test accuracy: 0.710\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2096 | test accuracy: 0.724\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7916 | test accuracy: 0.721\n",
            "Epoch:  47 Iteration:  3360 | train loss: 1.0215 | test accuracy: 0.714\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.3966 | test accuracy: 0.724\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.3990 | test accuracy: 0.721\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.8504 | test accuracy: 0.721\n",
            "Epoch:  51 Iteration:  3640 | train loss: 1.1789 | test accuracy: 0.717\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.3114 | test accuracy: 0.721\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1937 | test accuracy: 0.721\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4314 | test accuracy: 0.721\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2193 | test accuracy: 0.717\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6033 | test accuracy: 0.717\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7746 | test accuracy: 0.724\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.5999 | test accuracy: 0.724\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5870 | test accuracy: 0.721\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2904 | test accuracy: 0.721\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.8843 | test accuracy: 0.724\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.3203 | test accuracy: 0.721\n",
            "Epoch:  63 Iteration:  4480 | train loss: 1.0810 | test accuracy: 0.724\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2090 | test accuracy: 0.727\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7863 | test accuracy: 0.721\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.8632 | test accuracy: 0.731\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1828 | test accuracy: 0.731\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3991 | test accuracy: 0.731\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3952 | test accuracy: 0.727\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.2181 | test accuracy: 0.724\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.3520 | test accuracy: 0.727\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6617 | test accuracy: 0.727\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2574 | test accuracy: 0.727\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4623 | test accuracy: 0.724\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.3355 | test accuracy: 0.727\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1996 | test accuracy: 0.727\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1843 | test accuracy: 0.727\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5578 | test accuracy: 0.721\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2644 | test accuracy: 0.727\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.9450 | test accuracy: 0.724\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2952 | test accuracy: 0.724\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2395 | test accuracy: 0.727\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.7378 | test accuracy: 0.727\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1592 | test accuracy: 0.727\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.8171 | test accuracy: 0.727\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.8558 | test accuracy: 0.727\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.3768 | test accuracy: 0.724\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2880 | test accuracy: 0.727\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.3573 | test accuracy: 0.727\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6804 | test accuracy: 0.724\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.5527 | test accuracy: 0.727\n",
            "Epoch:  92 Iteration:  6510 | train loss: 1.0205 | test accuracy: 0.727\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.4455 | test accuracy: 0.727\n",
            "Epoch:  94 Iteration:  6650 | train loss: 1.1019 | test accuracy: 0.727\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.1861 | test accuracy: 0.727\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.3876 | test accuracy: 0.727\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.8085 | test accuracy: 0.727\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.3642 | test accuracy: 0.727\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.4608 | test accuracy: 0.727\n",
            "total time:  35.166037684\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24591517448425293.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 0.4183924198150635\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5257461701120649 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24521374702453613.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.4169297218322754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.44243048216615405 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24709153175354004.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.42505502700805664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.40044975195612226 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25185346603393555.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.42952823638916016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3750454579080854 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2497272491455078.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.420459508895874\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3622238687106541 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24582123756408691.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.42351293563842773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.351209106189864 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24032902717590332.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.41875648498535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3438543030193874 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604057788848877.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4357314109802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33740766431604113 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510662078857422.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42319273948669434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3335305784429823 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.267716646194458.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4485297203063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33083154261112213 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527904510498047.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.42130613327026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32851058372429437 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2465207576751709.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43056368827819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3266030286039625 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26784443855285645.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44361281394958496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32525905924183984 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24486589431762695.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4177262783050537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32311672142573766 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2542083263397217.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4357612133026123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3230594856398446 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25252318382263184.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4235060214996338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3219032176903316 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26239657402038574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44371604919433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32076868670327324 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25770115852355957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4334747791290283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3205305461372648 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2552633285522461.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4524497985839844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31958547702857426 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2493889331817627.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4252011775970459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3186064439160483 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523772716522217.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4322514533996582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.31858454857553753 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2727522850036621.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4548611640930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31778660331453595 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25702834129333496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43392252922058105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31748013964721133 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2840442657470703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46611857414245605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31746199769633154 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2495098114013672.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42620062828063965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.316979917883873 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26000094413757324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4536259174346924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3170011064835957 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539186477661133.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44295740127563477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3166145035198757 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25557518005371094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4275166988372803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31621640154293607 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2731940746307373.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44916343688964844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3160005565200533 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24060606956481934.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41465258598327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31580771250384193 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26930689811706543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4421114921569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31564644617693766 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2413337230682373.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.40610527992248535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31559060726846966 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2552330493927002.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44719505310058594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3152148549045835 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2437288761138916.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4194364547729492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3154750602585929 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24721407890319824.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4213554859161377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31517383967127116 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25869202613830566.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4318418502807617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3150851692472185 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24748611450195312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4228072166442871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31487841776439124 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512688636779785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43335509300231934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3148561916181019 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24581694602966309.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4248843193054199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31473774313926695 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26398658752441406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4442288875579834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3145927573953356 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25821685791015625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4313828945159912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3146893620491028 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26316094398498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.453571081161499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31465298107692175 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590501308441162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4320394992828369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31441168061324526 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26631975173950195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44866013526916504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31444127985409326 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26488256454467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4532649517059326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3143757445471627 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24309492111206055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4233436584472656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3143156281539372 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2608006000518799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4537770748138428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31434791088104247 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561302185058594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4335594177246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3143250350441251 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559385299682617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44177770614624023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31416463894503455 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721233367919922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4562687873840332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31416942732674735 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601664066314697.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4515361785888672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31421920614583154 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2763652801513672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45830368995666504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3140762899603162 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523350715637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4310932159423828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141364412648337 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568693161010742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.444446325302124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141190482037408 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24772095680236816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4257817268371582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3139416532857077 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2541346549987793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4238259792327881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140199886901038 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662160396575928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4486072063446045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140823896442141 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504420280456543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4304640293121338\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31392546253544945 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25864744186401367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4282050132751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139287212065288 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531473636627197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4351222515106201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31386949590274266 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24854207038879395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43265366554260254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3138735119785581 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26464390754699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4415855407714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31383463953222546 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595539093017578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44240546226501465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138636848756245 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24979043006896973.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.430574893951416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3137739969151361 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26950955390930176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4517099857330322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31381968472685134 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25831079483032227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43677830696105957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137377304690225 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2511467933654785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43320727348327637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3137152250323977 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2655196189880371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.452439546585083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3137144467660359 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524847984313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323770999908447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137283091034208 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2574727535247803.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359579086303711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137830180781228 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651848793029785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44411635398864746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137088396719524 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25553345680236816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43525123596191406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137156252350126 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586047649383545.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43569469451904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31366969091551644 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25653505325317383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44576263427734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3136629096099309 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25270676612854004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43947839736938477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3136198763336454 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25466418266296387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401564598083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136550737278802 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25577259063720703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4385554790496826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136504190308707 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26573944091796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4452495574951172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31365979186126164 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610156536102295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4441554546356201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31361963748931887 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26146864891052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4424135684967041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136113281760897 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2633187770843506.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44814252853393555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3135836341551372 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25863146781921387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44684720039367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.313634266597884 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26203489303588867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4511702060699463\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31359354555606844 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26019787788391113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4429962635040283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31358683833054135 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26885104179382324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47612547874450684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135847623859133 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719123363494873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44942545890808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135659830910819 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713158130645752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4620335102081299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31355902552604675 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26306700706481934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44100356101989746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31355891610894887 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25603604316711426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372897148132324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135414344923837 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26666712760925293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487907886505127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3135579560484205 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555389404296875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4414811134338379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31357191247599464 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630164623260498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43920159339904785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135284104517528 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25180554389953613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4343996047973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.313543581536838 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27321314811706543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4719572067260742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135147286312921 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27216577529907227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4549241065979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31355272020612446 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25622105598449707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44160890579223633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31354520746639797 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601590156555176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4410097599029541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135224997997284 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557947635650635.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4636194705963135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31350020851407734 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26672911643981934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4543771743774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31350632565362113 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25757622718811035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4433133602142334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3134908412184034 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25042009353637695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43381714820861816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135098206145423 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24893689155578613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4206259250640869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134989278657096 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632420063018799.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4439876079559326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31347979094300954 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25241708755493164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43857884407043457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134811473744256 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25396060943603516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4522247314453125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134922959974834 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25734639167785645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4459717273712158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31346370620386943 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2680501937866211.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.469130277633667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31348398838724406 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28244805335998535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47208523750305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134757399559021 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26050806045532227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44988536834716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134729743003845 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2932319641113281.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4780247211456299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134954401424953 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575235366821289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4367077350616455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31347094220774513 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.293332576751709.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.477475643157959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134484657219478 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26322507858276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4488835334777832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31342747764928003 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28556084632873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4727599620819092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134567673717226 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26012587547302246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4414479732513428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31345064512320925 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2508065700531006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4520847797393799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134811099086489 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27962493896484375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.470733642578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134329216820853 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582273483276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45408058166503906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31344645959990364 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25284671783447266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43810391426086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134554275444576 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25681638717651367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43977999687194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31342360377311707 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27436208724975586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4637935161590576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134442831788744 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532167434692383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4348421096801758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31344499715736934 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2729477882385254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4570157527923584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134205584015165 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26051807403564453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44021010398864746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343122167246684 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623887062072754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46599602699279785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134251556226185 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27294373512268066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45754432678222656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134403467178345 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26348090171813965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45519399642944336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134240882737296 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27700161933898926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4799070358276367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134377198559897 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Iteration:  70 | train loss: 0.4564 | test accuracy: 0.623\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6611 | test accuracy: 0.650\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4001 | test accuracy: 0.646\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6126 | test accuracy: 0.663\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.4143 | test accuracy: 0.667\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.2348 | test accuracy: 0.667\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6102 | test accuracy: 0.684\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6868 | test accuracy: 0.717\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6032 | test accuracy: 0.727\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2745 | test accuracy: 0.721\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4151 | test accuracy: 0.670\n",
            "Epoch:  11 Iteration:  840 | train loss: 1.1065 | test accuracy: 0.697\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.6636 | test accuracy: 0.710\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2753 | test accuracy: 0.724\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.3221 | test accuracy: 0.727\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4699 | test accuracy: 0.734\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6922 | test accuracy: 0.734\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2021 | test accuracy: 0.700\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7457 | test accuracy: 0.731\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.3358 | test accuracy: 0.731\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3610 | test accuracy: 0.731\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.2454 | test accuracy: 0.710\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.9764 | test accuracy: 0.721\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5789 | test accuracy: 0.721\n",
            "Epoch:  24 Iteration:  1750 | train loss: 1.0704 | test accuracy: 0.727\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5171 | test accuracy: 0.731\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.8729 | test accuracy: 0.737\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2899 | test accuracy: 0.731\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2254 | test accuracy: 0.731\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.7281 | test accuracy: 0.724\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6727 | test accuracy: 0.727\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2531 | test accuracy: 0.737\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.3235 | test accuracy: 0.727\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.4193 | test accuracy: 0.727\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.1068 | test accuracy: 0.727\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.3797 | test accuracy: 0.731\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6893 | test accuracy: 0.731\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5120 | test accuracy: 0.727\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.3020 | test accuracy: 0.731\n",
            "Epoch:  39 Iteration:  2800 | train loss: 1.0371 | test accuracy: 0.727\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7426 | test accuracy: 0.727\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3547 | test accuracy: 0.724\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.4563 | test accuracy: 0.727\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.3375 | test accuracy: 0.727\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8020 | test accuracy: 0.727\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2878 | test accuracy: 0.731\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7049 | test accuracy: 0.727\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.1708 | test accuracy: 0.724\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1828 | test accuracy: 0.727\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5410 | test accuracy: 0.731\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.1487 | test accuracy: 0.727\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1891 | test accuracy: 0.724\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.3388 | test accuracy: 0.731\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.3475 | test accuracy: 0.727\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1838 | test accuracy: 0.734\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.2451 | test accuracy: 0.731\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.1944 | test accuracy: 0.737\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.2486 | test accuracy: 0.734\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.3490 | test accuracy: 0.731\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.5257 | test accuracy: 0.731\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.9046 | test accuracy: 0.734\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1449 | test accuracy: 0.731\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2454 | test accuracy: 0.731\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.7111 | test accuracy: 0.731\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2821 | test accuracy: 0.731\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2018 | test accuracy: 0.731\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7212 | test accuracy: 0.731\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.3556 | test accuracy: 0.731\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.4262 | test accuracy: 0.734\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3959 | test accuracy: 0.731\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.5930 | test accuracy: 0.734\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1109 | test accuracy: 0.734\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2581 | test accuracy: 0.734\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1448 | test accuracy: 0.734\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1358 | test accuracy: 0.737\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1259 | test accuracy: 0.734\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6060 | test accuracy: 0.734\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3186 | test accuracy: 0.734\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3036 | test accuracy: 0.734\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3884 | test accuracy: 0.734\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1951 | test accuracy: 0.734\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2865 | test accuracy: 0.734\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.3504 | test accuracy: 0.737\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.8546 | test accuracy: 0.734\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.9396 | test accuracy: 0.731\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2407 | test accuracy: 0.734\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1073 | test accuracy: 0.737\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.7580 | test accuracy: 0.737\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.7455 | test accuracy: 0.737\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.2013 | test accuracy: 0.741\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.7210 | test accuracy: 0.741\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.9977 | test accuracy: 0.741\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2950 | test accuracy: 0.741\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1504 | test accuracy: 0.741\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6627 | test accuracy: 0.741\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.3353 | test accuracy: 0.744\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6816 | test accuracy: 0.741\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.2795 | test accuracy: 0.741\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.8657 | test accuracy: 0.741\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6146 | test accuracy: 0.741\n",
            "total time:  32.53698480000003\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.3368 | test accuracy: 0.613\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5681 | test accuracy: 0.657\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4547 | test accuracy: 0.640\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8413 | test accuracy: 0.643\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5673 | test accuracy: 0.657\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.9029 | test accuracy: 0.653\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.9017 | test accuracy: 0.673\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6379 | test accuracy: 0.660\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4904 | test accuracy: 0.670\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6579 | test accuracy: 0.690\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6415 | test accuracy: 0.690\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2222 | test accuracy: 0.684\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8778 | test accuracy: 0.690\n",
            "Epoch:  13 Iteration:  980 | train loss: 1.3916 | test accuracy: 0.643\n",
            "Epoch:  14 Iteration:  1050 | train loss: 1.2070 | test accuracy: 0.704\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3245 | test accuracy: 0.707\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5794 | test accuracy: 0.714\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4114 | test accuracy: 0.690\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2413 | test accuracy: 0.694\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5713 | test accuracy: 0.700\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5099 | test accuracy: 0.700\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.3605 | test accuracy: 0.721\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5257 | test accuracy: 0.694\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.8482 | test accuracy: 0.704\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2385 | test accuracy: 0.721\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.4165 | test accuracy: 0.731\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.2990 | test accuracy: 0.700\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5521 | test accuracy: 0.744\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.6452 | test accuracy: 0.724\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3209 | test accuracy: 0.731\n",
            "Epoch:  30 Iteration:  2170 | train loss: 1.0782 | test accuracy: 0.721\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.1759 | test accuracy: 0.731\n",
            "Epoch:  32 Iteration:  2310 | train loss: 1.5224 | test accuracy: 0.737\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.8401 | test accuracy: 0.737\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2144 | test accuracy: 0.734\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1682 | test accuracy: 0.731\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6647 | test accuracy: 0.731\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1716 | test accuracy: 0.731\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.5425 | test accuracy: 0.727\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1219 | test accuracy: 0.731\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2523 | test accuracy: 0.727\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4515 | test accuracy: 0.727\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.8330 | test accuracy: 0.737\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6305 | test accuracy: 0.734\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6574 | test accuracy: 0.727\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.3238 | test accuracy: 0.727\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4085 | test accuracy: 0.727\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6044 | test accuracy: 0.731\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6117 | test accuracy: 0.731\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2811 | test accuracy: 0.731\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.2220 | test accuracy: 0.734\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.3466 | test accuracy: 0.734\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.3557 | test accuracy: 0.734\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.2633 | test accuracy: 0.734\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4811 | test accuracy: 0.734\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.4090 | test accuracy: 0.727\n",
            "Epoch:  56 Iteration:  3990 | train loss: 1.1794 | test accuracy: 0.737\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1867 | test accuracy: 0.731\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1769 | test accuracy: 0.737\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1275 | test accuracy: 0.734\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7457 | test accuracy: 0.727\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.5800 | test accuracy: 0.737\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.4941 | test accuracy: 0.734\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2249 | test accuracy: 0.727\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2945 | test accuracy: 0.734\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.1964 | test accuracy: 0.744\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.4773 | test accuracy: 0.737\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5335 | test accuracy: 0.734\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.3206 | test accuracy: 0.741\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.7738 | test accuracy: 0.741\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1300 | test accuracy: 0.744\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.2958 | test accuracy: 0.744\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.2251 | test accuracy: 0.741\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.1634 | test accuracy: 0.741\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1762 | test accuracy: 0.741\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.1659 | test accuracy: 0.741\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.7003 | test accuracy: 0.741\n",
            "Epoch:  77 Iteration:  5460 | train loss: 1.5785 | test accuracy: 0.741\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.5935 | test accuracy: 0.741\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.3772 | test accuracy: 0.741\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.5774 | test accuracy: 0.737\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.1021 | test accuracy: 0.741\n",
            "Epoch:  82 Iteration:  5810 | train loss: 1.3248 | test accuracy: 0.741\n",
            "Epoch:  83 Iteration:  5880 | train loss: 1.0287 | test accuracy: 0.741\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.6888 | test accuracy: 0.741\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1912 | test accuracy: 0.741\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2498 | test accuracy: 0.741\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6467 | test accuracy: 0.741\n",
            "Epoch:  88 Iteration:  6230 | train loss: 1.2551 | test accuracy: 0.737\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.3858 | test accuracy: 0.751\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.4893 | test accuracy: 0.737\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.3551 | test accuracy: 0.744\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.9401 | test accuracy: 0.741\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6798 | test accuracy: 0.747\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.3155 | test accuracy: 0.744\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.5086 | test accuracy: 0.741\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.8404 | test accuracy: 0.737\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3346 | test accuracy: 0.744\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.5488 | test accuracy: 0.744\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7057 | test accuracy: 0.741\n",
            "total time:  35.2630358309998\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2506587505340576.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.4453442096710205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6819517689091819 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2685995101928711.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4445664882659912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5455363150153841 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24961209297180176.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.42777323722839355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4624330695186343 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28017520904541016.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4596986770629883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41863547095230647 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619364261627197.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.449537992477417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38911855135645185 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27642226219177246.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.4540700912475586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3753788747957775 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646830081939697.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4444091320037842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36323725410870145 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28550052642822266.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4828648567199707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35358073370797294 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25900864601135254.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44160985946655273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3472217146839414 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2638068199157715.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.498157262802124\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.342363760301045 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26245832443237305.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4531841278076172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3372415206262043 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2640547752380371.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.45587778091430664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3356548820223127 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26741528511047363.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44247007369995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3312049193041665 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24842357635498047.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4221956729888916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3302552819252014 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2648489475250244.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43689560890197754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32868785176958354 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615940570831299.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4354729652404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3257721879652568 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666020393371582.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44817018508911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3253065986292703 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25931835174560547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4318118095397949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32411703637668066 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25829505920410156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44316744804382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32227096089294976 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25727081298828125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42945051193237305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.321855052454131 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25098276138305664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4217510223388672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3213802614382335 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25589919090270996.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43344998359680176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32065414232867107 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24776959419250488.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.421720027923584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31950960755348207 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536332607269287.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4367244243621826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3197257263319833 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24593710899353027.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4216165542602539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.318866919193949 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2530639171600342.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4414680004119873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31827422635895863 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24776124954223633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4239976406097412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3182381876877376 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25705718994140625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4400632381439209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3176573974745614 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2731459140777588.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44516944885253906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31765423033918655 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2497706413269043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43704795837402344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31726273042815073 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2731668949127197.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4659843444824219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31701247819832395 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2549159526824951.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.437711238861084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3168023109436035 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27811741828918457.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5120511054992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3166760436126164 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26999592781066895.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45680665969848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31634129668985095 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592275142669678.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.436431884765625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31617421550410135 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628774642944336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4331471920013428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3159590810537338 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24825644493103027.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43010377883911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31581358100686757 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2569444179534912.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43247103691101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31571922983442036 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24619412422180176.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42346715927124023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31539631017616815 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26044321060180664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4469449520111084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3154031493834087 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25130701065063477.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.433290958404541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31538308347974503 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2780911922454834.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4696524143218994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31511900552681515 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2525765895843506.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4310753345489502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31513564160891944 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521021366119385.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4292886257171631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3150594643184117 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667059898376465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4428367614746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3150941878557205 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25169897079467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4374399185180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149760697569166 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25186944007873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43666815757751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3148307779005596 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26526737213134766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44323229789733887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3146947209324156 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25235700607299805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42289280891418457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3146702093737466 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27508068084716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4573333263397217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3146590492555073 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25998616218566895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45194077491760254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31467385079179494 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25382280349731445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4239926338195801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31458131670951844 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26612067222595215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45998382568359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31447638698986596 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572469711303711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4431791305541992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31443854740687777 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613985538482666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384589195251465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31428825642381397 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26780247688293457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46143198013305664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31435163446835107 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2575101852416992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45601439476013184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142074980906078 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629728317260742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4430704116821289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3141271552869252 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507474422454834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42313241958618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141856380871364 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595634460449219.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4348874092102051\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141420947653907 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2588965892791748.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43610382080078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141262595142637 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535214424133301.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4369344711303711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141641165528979 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25389552116394043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4351162910461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140642702579498 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24743437767028809.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42402148246765137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140120029449463 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603759765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4431915283203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31399769825594764 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26389455795288086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44132328033447266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3139114167009081 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2522919178009033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4294397830963135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31394107043743136 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24766898155212402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4327213764190674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31398470103740694 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27201318740844727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46298766136169434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31393646257264274 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512493133544922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4285001754760742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31396118445055826 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24711179733276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43074822425842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138885698148182 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555809020996094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4349496364593506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3139410129615239 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27240705490112305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45426487922668457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3139075130224228 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26669883728027344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4448277950286865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31383096149989537 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2692084312438965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46111178398132324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31379879713058473 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27686643600463867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4698946475982666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31377072419439045 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26218485832214355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47158098220825195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31372997377599987 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27782773971557617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46715855598449707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31373459228447503 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.256882905960083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44854140281677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137625971010753 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25745415687561035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47587060928344727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31378082164696286 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25759005546569824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43663907051086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313748305610248 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26705455780029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4545416831970215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136629159961428 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25728464126586914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44329023361206055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31368700180734904 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595815658569336.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43431615829467773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31369336886065347 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26278114318847656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43837857246398926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136783770152501 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2538936138153076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43687915802001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.313645322407995 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25882506370544434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43384885787963867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136375814676285 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24149560928344727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42541074752807617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31369327902793886 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682507038116455.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4570729732513428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136190073830741 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.255173921585083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359920024871826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31367591406617845 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2727818489074707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4807591438293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136221919740949 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2852954864501953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47043800354003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31360523360116144 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2676539421081543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45192623138427734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31362334064074926 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24930763244628906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42665743827819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136203127247947 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563972473144531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44405674934387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136054481778826 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.273836612701416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4538998603820801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135928341320583 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24947547912597656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42798352241516113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136035757405417 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28159642219543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46451377868652344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135687883411135 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2589097023010254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4362645149230957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135737717151642 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28513073921203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4809389114379883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135675034352711 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713189125061035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4532907009124756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135530101401465 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26296520233154297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47157931327819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135457826512201 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27096009254455566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45056605339050293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135348217827933 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2505652904510498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4360992908477783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31352252066135405 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713792324066162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45371031761169434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31353121314729965 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509291172027588.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.426300048828125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31351197234221867 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618694305419922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4376542568206787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31353003084659575 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24852657318115234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4267148971557617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135072452681405 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2704124450683594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4526674747467041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31354097042764933 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24557209014892578.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4169731140136719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135093808174133 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25459933280944824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4476206302642822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135486006736755 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26817846298217773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4469757080078125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31347769711698803 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25639986991882324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44136881828308105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31349451329026906 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28525567054748535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46814727783203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135307575975146 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2594766616821289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4563579559326172\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31349477895668576 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2864830493927002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.477708101272583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134829759597778 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2783162593841553.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47862863540649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3135077323232378 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3077046871185303.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.519477367401123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31348710443292344 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27460718154907227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48545074462890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31345464502062115 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30227112770080566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5010552406311035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31349243777138847 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26889729499816895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46277642250061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31345396552767074 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719438076019287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49563074111938477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134578389780862 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27864527702331543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4704122543334961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31344998819487435 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2690892219543457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45232200622558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344602831772395 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2616302967071533.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44823360443115234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345375776290896 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2624173164367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4492604732513428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134525226695197 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2717888355255127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4534153938293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134378501347133 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24913740158081055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42774128913879395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134479113987514 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.5391 | test accuracy: 0.640\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8099 | test accuracy: 0.643\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.3292 | test accuracy: 0.670\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3052 | test accuracy: 0.663\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6599 | test accuracy: 0.684\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8142 | test accuracy: 0.684\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2731 | test accuracy: 0.680\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.3724 | test accuracy: 0.684\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3977 | test accuracy: 0.680\n",
            "Epoch:  9 Iteration:  700 | train loss: 1.4719 | test accuracy: 0.707\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7145 | test accuracy: 0.707\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6431 | test accuracy: 0.714\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8386 | test accuracy: 0.694\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.7133 | test accuracy: 0.731\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2718 | test accuracy: 0.731\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6236 | test accuracy: 0.717\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2245 | test accuracy: 0.731\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.3154 | test accuracy: 0.690\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.8246 | test accuracy: 0.721\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4410 | test accuracy: 0.717\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.4918 | test accuracy: 0.724\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.6000 | test accuracy: 0.727\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4472 | test accuracy: 0.714\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2160 | test accuracy: 0.717\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.4821 | test accuracy: 0.727\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6276 | test accuracy: 0.734\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.5230 | test accuracy: 0.721\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5560 | test accuracy: 0.724\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.8309 | test accuracy: 0.727\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.8477 | test accuracy: 0.727\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7472 | test accuracy: 0.727\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2376 | test accuracy: 0.731\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7456 | test accuracy: 0.724\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7818 | test accuracy: 0.727\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6516 | test accuracy: 0.727\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.2465 | test accuracy: 0.727\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6695 | test accuracy: 0.731\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.1392 | test accuracy: 0.731\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.5512 | test accuracy: 0.731\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.1686 | test accuracy: 0.731\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2927 | test accuracy: 0.731\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4319 | test accuracy: 0.727\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7216 | test accuracy: 0.731\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4758 | test accuracy: 0.731\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3019 | test accuracy: 0.731\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7760 | test accuracy: 0.731\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.8741 | test accuracy: 0.731\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3146 | test accuracy: 0.731\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6297 | test accuracy: 0.727\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.1993 | test accuracy: 0.731\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.8089 | test accuracy: 0.731\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.1354 | test accuracy: 0.731\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.3169 | test accuracy: 0.727\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7003 | test accuracy: 0.734\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7105 | test accuracy: 0.731\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0404 | test accuracy: 0.734\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4339 | test accuracy: 0.731\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.4532 | test accuracy: 0.731\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.2099 | test accuracy: 0.731\n",
            "Epoch:  59 Iteration:  4200 | train loss: 1.0911 | test accuracy: 0.731\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3581 | test accuracy: 0.731\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2775 | test accuracy: 0.734\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1692 | test accuracy: 0.734\n",
            "Epoch:  63 Iteration:  4480 | train loss: 1.2376 | test accuracy: 0.731\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2332 | test accuracy: 0.731\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.2879 | test accuracy: 0.734\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.2917 | test accuracy: 0.731\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.4866 | test accuracy: 0.731\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1732 | test accuracy: 0.734\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.2575 | test accuracy: 0.737\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6180 | test accuracy: 0.737\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.8348 | test accuracy: 0.734\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1856 | test accuracy: 0.734\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.2911 | test accuracy: 0.734\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.8721 | test accuracy: 0.734\n",
            "Epoch:  75 Iteration:  5320 | train loss: 1.1117 | test accuracy: 0.737\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8253 | test accuracy: 0.737\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3510 | test accuracy: 0.744\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3775 | test accuracy: 0.741\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2735 | test accuracy: 0.737\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4324 | test accuracy: 0.737\n",
            "Epoch:  81 Iteration:  5740 | train loss: 1.1212 | test accuracy: 0.741\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.2902 | test accuracy: 0.744\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1739 | test accuracy: 0.744\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1591 | test accuracy: 0.737\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.2043 | test accuracy: 0.741\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.2459 | test accuracy: 0.744\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.2439 | test accuracy: 0.741\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2776 | test accuracy: 0.737\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.8654 | test accuracy: 0.741\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6228 | test accuracy: 0.747\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7250 | test accuracy: 0.737\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7617 | test accuracy: 0.741\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6757 | test accuracy: 0.741\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.3314 | test accuracy: 0.737\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4024 | test accuracy: 0.744\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2578 | test accuracy: 0.747\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.4967 | test accuracy: 0.744\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2496 | test accuracy: 0.747\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.8016 | test accuracy: 0.747\n",
            "total time:  32.15740987400022\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4298 | test accuracy: 0.620\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8140 | test accuracy: 0.630\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.4192 | test accuracy: 0.657\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.7401 | test accuracy: 0.653\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6944 | test accuracy: 0.653\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6748 | test accuracy: 0.636\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7217 | test accuracy: 0.646\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4408 | test accuracy: 0.680\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4511 | test accuracy: 0.643\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3607 | test accuracy: 0.687\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.6696 | test accuracy: 0.680\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.3157 | test accuracy: 0.673\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.9349 | test accuracy: 0.663\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.4263 | test accuracy: 0.690\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2641 | test accuracy: 0.690\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7679 | test accuracy: 0.710\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.6044 | test accuracy: 0.700\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.3043 | test accuracy: 0.724\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6626 | test accuracy: 0.704\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.4042 | test accuracy: 0.704\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.1458 | test accuracy: 0.697\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.9856 | test accuracy: 0.724\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.3468 | test accuracy: 0.690\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.9420 | test accuracy: 0.731\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1931 | test accuracy: 0.700\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8428 | test accuracy: 0.687\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1852 | test accuracy: 0.721\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2566 | test accuracy: 0.741\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.3652 | test accuracy: 0.741\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.5984 | test accuracy: 0.721\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.5774 | test accuracy: 0.714\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.4054 | test accuracy: 0.721\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.1731 | test accuracy: 0.727\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.2801 | test accuracy: 0.727\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.2911 | test accuracy: 0.727\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.9457 | test accuracy: 0.727\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.2002 | test accuracy: 0.727\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.2587 | test accuracy: 0.727\n",
            "Epoch:  38 Iteration:  2730 | train loss: 1.4646 | test accuracy: 0.727\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.2516 | test accuracy: 0.727\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.4147 | test accuracy: 0.727\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6680 | test accuracy: 0.727\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2620 | test accuracy: 0.727\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4410 | test accuracy: 0.727\n",
            "Epoch:  44 Iteration:  3150 | train loss: 1.1369 | test accuracy: 0.727\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2514 | test accuracy: 0.727\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6619 | test accuracy: 0.731\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3183 | test accuracy: 0.731\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1409 | test accuracy: 0.727\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.2334 | test accuracy: 0.724\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6114 | test accuracy: 0.731\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.3377 | test accuracy: 0.727\n",
            "Epoch:  52 Iteration:  3710 | train loss: 1.1318 | test accuracy: 0.724\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7387 | test accuracy: 0.734\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7459 | test accuracy: 0.727\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.3099 | test accuracy: 0.727\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.8039 | test accuracy: 0.731\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.6991 | test accuracy: 0.727\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.1453 | test accuracy: 0.727\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.4320 | test accuracy: 0.731\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.3421 | test accuracy: 0.727\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.1859 | test accuracy: 0.727\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2894 | test accuracy: 0.727\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3432 | test accuracy: 0.727\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2986 | test accuracy: 0.727\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.8587 | test accuracy: 0.727\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.5962 | test accuracy: 0.731\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1861 | test accuracy: 0.731\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1700 | test accuracy: 0.731\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.3912 | test accuracy: 0.731\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.8804 | test accuracy: 0.727\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.4913 | test accuracy: 0.731\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.3995 | test accuracy: 0.731\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.4896 | test accuracy: 0.727\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7140 | test accuracy: 0.731\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.7976 | test accuracy: 0.731\n",
            "Epoch:  76 Iteration:  5390 | train loss: 1.1021 | test accuracy: 0.727\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.3036 | test accuracy: 0.731\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3401 | test accuracy: 0.731\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.2215 | test accuracy: 0.731\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6146 | test accuracy: 0.734\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6699 | test accuracy: 0.731\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5265 | test accuracy: 0.731\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1951 | test accuracy: 0.731\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1958 | test accuracy: 0.731\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4140 | test accuracy: 0.731\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.4457 | test accuracy: 0.731\n",
            "Epoch:  87 Iteration:  6160 | train loss: 1.0171 | test accuracy: 0.727\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2322 | test accuracy: 0.737\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6709 | test accuracy: 0.727\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.2758 | test accuracy: 0.731\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.4221 | test accuracy: 0.734\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2763 | test accuracy: 0.727\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.3098 | test accuracy: 0.731\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.7011 | test accuracy: 0.734\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6076 | test accuracy: 0.734\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.1786 | test accuracy: 0.737\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.3082 | test accuracy: 0.731\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.4504 | test accuracy: 0.741\n",
            "Epoch:  99 Iteration:  7000 | train loss: 1.9327 | test accuracy: 0.741\n",
            "total time:  35.49102637499982\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2674746513366699.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.4463980197906494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7017316716057914 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23889636993408203.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.41302013397216797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5571315603596824 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26345062255859375.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4516167640686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.473813116976193 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25055575370788574.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4342162609100342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4210697493382863 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512655258178711.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.4494032859802246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3945235716445105 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698185443878174.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.44086790084838867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37574755123683384 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23857975006103516.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4143843650817871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36378915309906007 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649867534637451.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4371809959411621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3566627200160708 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2412576675415039.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.41376447677612305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3479905605316162 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2553822994232178.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43799567222595215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34244962206908636 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544684410095215.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42589545249938965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33950230734688897 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2524731159210205.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.425753116607666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33492364372525896 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2569296360015869.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4290471076965332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3325216740369797 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25195741653442383.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4250047206878662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.33061531697000773 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2676725387573242.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4428133964538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32801239362784795 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2594492435455322.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4351639747619629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32635093331336973 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2638278007507324.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.450927734375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3258794214044298 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2518730163574219.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42379307746887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.324498724937439 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2710874080657959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4608628749847412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3235978888613837 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27025508880615234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4554450511932373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.322124657034874 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578110694885254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44118165969848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3215373456478119 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2775554656982422.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4575638771057129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32097536410604205 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26424074172973633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.450427770614624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.32064036897250586 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25297021865844727.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44272899627685547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31970031005995614 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521665096282959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4274632930755615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3195370218583516 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2553420066833496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4492623805999756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3187594392469951 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2657473087310791.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4439873695373535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3180487756218229 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27325868606567383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4644801616668701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31805625557899475 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27098727226257324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45450878143310547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31754250058106015 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25519704818725586.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4417743682861328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3178459290947233 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25042104721069336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43192148208618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31701266169548037 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24649381637573242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4202613830566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31701580711773464 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2537980079650879.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44023799896240234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3168210353170122 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632322311401367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44418931007385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31651104858943396 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266826868057251.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4462921619415283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31636603261743274 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2566211223602295.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4299614429473877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31603334290640694 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26779818534851074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4575624465942383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31594808655125756 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28075098991394043.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4682774543762207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3160503472600664 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26627421379089355.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4600636959075928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31550055827413287 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26978254318237305.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45368361473083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3156646498611995 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25755977630615234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4483623504638672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3156782422746931 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25725626945495605.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44261837005615234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31548835464886255 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26776790618896484.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4492771625518799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3151949793100357 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2732367515563965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4589078426361084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3150797758783613 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2664453983306885.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45138025283813477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31512543175901686 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26029539108276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43303728103637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149509361812047 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25888848304748535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44992518424987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31504608392715455 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25484776496887207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44057655334472656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3150538610560553 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2656567096710205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44771862030029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31468150615692136 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24010562896728516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4127931594848633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31478298817362105 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2576639652252197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43604397773742676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146348169871739 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598757743835449.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44469356536865234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31460027737276897 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26233816146850586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4442603588104248\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31442431211471555 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507803440093994.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4452700614929199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143866228205817 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25759100914001465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44819188117980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143929285662515 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26221323013305664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4478156566619873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143761043037687 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509799003601074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44256043434143066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31432886293956214 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259702205657959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4369370937347412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31422199351446967 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26839303970336914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4502089023590088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3143198949950082 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2599365711212158.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44794201850891113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142718208687646 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27414727210998535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46260547637939453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31419099484171187 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26303911209106445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45256519317626953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31411570651190623 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24450278282165527.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4413893222808838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3142468188490186 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2893662452697754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47402286529541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140098797423499 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572615146636963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4396176338195801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140625021287373 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2769498825073242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46130895614624023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140737959316799 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605910301208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44271230697631836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139415506805692 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26767611503601074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.456937313079834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31404160601752146 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27156543731689453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45072054862976074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31399186977318355 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26952672004699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4625060558319092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31394819830145154 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2742302417755127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46807265281677246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31391565799713134 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.262009859085083.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46747446060180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.313836864062718 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.283829927444458.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46501874923706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138200649193355 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24599385261535645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.428072452545166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138800033501216 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582519054412842.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43372273445129395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.3138352198260171 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548794746398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4407792091369629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138437969344003 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26732397079467773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45225954055786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31381479927471706 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26432156562805176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44153380393981934\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3138263353279659 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2554144859313965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4509577751159668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31379327263150897 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619180679321289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44090747833251953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31376761538641795 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26177024841308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4599883556365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313778920684542 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.262681245803833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4492623805999756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.313720954316003 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25843024253845215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44018006324768066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31367264900888714 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2757434844970703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4607105255126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137118914297649 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2496201992034912.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42479634284973145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136935400111335 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2895381450653076.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4745142459869385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31371591431753976 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2599492073059082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4410057067871094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136921708072935 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25678372383117676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4557158946990967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136448110852923 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596275806427002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4396383762359619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31363748482295445 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2680525779724121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4635133743286133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136687606573105 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2573583126068115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43955516815185547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3137255081108638 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2548484802246094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4268224239349365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31364170355456217 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2712225914001465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45559024810791016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31365615384919304 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26077938079833984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44303464889526367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135991666998182 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2768886089324951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4594109058380127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136385802711759 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2574191093444824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4313013553619385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135775199958256 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571406364440918.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44860386848449707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136340515954154 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25013136863708496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.423992395401001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135713313307081 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.257244348526001.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43880701065063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135539459330695 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.266782283782959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44907617568969727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135829848902566 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24467706680297852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42418575286865234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135218824659075 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29306888580322266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47504496574401855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135523238352367 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24606895446777344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42296361923217773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31356152721813746 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26483654975891113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.451732873916626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31352687903812954 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679274082183838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4546468257904053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31354349851608276 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2659025192260742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4551198482513428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31357420086860655 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27311277389526367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44989633560180664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31352391157831466 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762181758880615.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4745185375213623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31352487206459045 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615487575531006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43850255012512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31356497492109026 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25992321968078613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4402756690979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31351960003376006 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679867744445801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4608941078186035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31352682667119164 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26605749130249023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.447568416595459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351025785718645 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25963711738586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44922900199890137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134873581784112 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24443554878234863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4200930595397949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134956006492887 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2680683135986328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44516634941101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134828954935074 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26794004440307617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44564366340637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134916961193085 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24410533905029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42046618461608887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134940666811807 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25540757179260254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4297044277191162\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3135077634028026 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24499130249023438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42345333099365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134592665093286 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2502110004425049.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44254183769226074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134773624794824 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2510111331939697.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4309806823730469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31346446956907 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694873809814453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4434952735900879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134690957409995 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2581064701080322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4325401782989502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134706331150872 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24997735023498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4262681007385254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31348400158541545 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25580501556396484.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4452097415924072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345574132033754 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25753307342529297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44043946266174316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31344562726361413 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597236633300781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4525022506713867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134573497942516 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609565258026123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46507954597473145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134611406496593 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVyVZf7/8dd9FkA5oKAcHHdjVBS3rCyjrFQSdcacpcJcWh9m6Wg5Zkj5w3JELZ0W2x1nMscazMixMaNvlpVFksuQkY2iae6CCoqyHOD+/YEcOYpKxWHxvJ+Phw/Ofe77uu/PYabz5rquezFM0zQRERGfZanrAkREpG4pCEREfJyCQETExykIRER8nIJARMTHKQhERHycra4LkEtX586d+fTTT2nRosU569544w3efvttXC4XLpeLK6+8kunTp3PgwAH+9Kc/AZCfn09+fr67/e9+9zuGDh3KgAEDuOeee3j00Uc99nnXXXfx448/8vHHH5+3pnXr1vGXv/wFgGPHjlFaWkrz5s0BGDduHMOHD6/WZzt06BD33nsv//nPfy643dSpU4mNjaV///7V2u/FFBcX8+KLL5KamkrFmd+xsbGMHz8ePz+/GjmG+B5D1xGIt5wvCD777DNmz57N0qVLCQ0Npbi4mEceeYQmTZrw5JNPurdLSUlh5cqVvP766+739u7dy2233UZgYCCpqalYLOWd2pycHG677TaACwZBZQsWLODgwYPMmjXrF37S2vPQQw9RUFDA008/TXBwMLm5uTz66KM4HA7mz59f1+VJA6WhIal127Zto127doSGhgLg5+fHrFmzmDp1arXaBwQE0LZtWzZs2OB+b/Xq1fTt2/cX19a/f39eeOEFBg0axP79+9m5cycjRoxg8ODBxMTEuHsAe/fupWvXrkB5YE2cOJGEhAQGDRrEkCFD2L59OwCjR4/m3//+N1AejCtWrGD48OFcd9117oArKytj5syZREdHM2LECF577TVGjx59Tm3bt2/n008/Ze7cuQQHBwPQtGlTkpKS+OMf/3jO8ao6/quvvsqgQYOYO3cuM2fOdG939OhRevXqxYkTJ8jKymLUqFEMGjSI3/72t2zZsgWAkydPMn78eAYPHsyAAQN4/PHHcblcv/h3LnVPQSC17tprr2XdunU8+uijfPrpp+Tn5+NwOHA4HNXeR2xsrMewzKpVq4iNja2R+g4dOkRqaiotW7bkqaee4qabbmL16tUkJSXx2GOPVfnl99lnn3HHHXeQmprK1VdfzeLFi6vcd1ZWFitWrOCll17ir3/9K6WlpXz66ad89tlnfPjhh7z88su8++67VbZNT0+nV69eNG3a1OP9Zs2aVTsETdMkNTWVwYMH88knn7jf/+STT7jmmmsIDAxk/Pjx3HLLLaSmpjJjxgwefPBBSkpKWLFiBcHBwaxevZrU1FSsVitZWVnVOq7UbwoCqXVdu3blrbfeoqysjPj4eK655hrGjx/P/v37q72Pm2++mY8//hiXy8W+ffsoLCykQ4cONVLfjTfe6H790ksvce+99wJwxRVXUFRURHZ29jltIiIi6NatG1D++Q4cOFDlvm+55RYAoqKiKCoq4siRI2zYsIEbb7yRwMBAmjZtytChQ6tsm5eXR7NmzX7JR3N/th49emCaJt9//z0A//d//8fgwYPZuXMnR44ccfcwrrjiCkJDQ9m8ebP757p16ygrK+OJJ56gS5cuv6geqR80WSx1onv37jz99NOYpklmZibPPfccDz/8MMnJydVq36RJE7p168a6devIyspi8ODBNVZbkyZN3K8///xzXn75ZY4dO4ZhGJimSVlZ2TltgoKC3K+tViulpaVV7rtiO6vVCpQPCx0/fpzw8HD3NpVfVxYSEsKhQ4d++geqpHJv4uabb2bNmjW0bduWTZs2MW/ePLZt20ZhYaHH7zM/P5/c3FwGDx5MXl4ezz33HDt37mTYsGFMmzZNk9SXAPUIpNZt2LDB/YVmGAbdunVjypQpbNu27SftZ+jQoaSmpvLBBx8wZMiQGq/T5XLx0EMP8cADD5CamsrKlSsxDKPGj+NwODh16pR7uaoeB0CfPn3IyMg4JwyOHz/Oc889h2maWCwWj6DKy8s773EHDRrExx9/zLp167jqqqtwOBw4nU4CAwP54IMP3P/WrVtHTEwMAHFxcbz99tu8//77ZGZmsmLFil/y0aWeUBBIrXvvvfdITEwkPz8fgJKSElatWsVVV131k/YzYMAA0tPTsVqttGnTpsbrLCgo4NSpU+4hn8WLF2O32z2+tGtC9+7dWbt2LYWFhRw/fpzVq1dXuV1ERARDhgxh8uTJ5OTkAJCbm8vkyZPdPZawsDD3cM/mzZvZtWvXeY97+eWXc+TIEVJSUtw9gFatWtGiRQs++OADoHwSefLkyZw6dYoXX3yR5cuXA+W9ltatW3slGKX2aWhIvGr06NHuYRCAv/zlLzz22GM888wz/OEPfwDKg+Dqq69m9uzZP2nfjRs3pmfPnnTv3r1Ga64QHBzMfffdx/Dhw2nWrBkPPPAAAwcOZNy4cbz66qs1dpyYmBjWrl1LbGws7dq1Y/DgwaSlpVW57cyZM3n55ZcZOXIkhmFgt9sZNmyYex7j7rvvZvLkyXz22Wf06dOH6Ojo8x7XMAwGDhzI22+/7T711DAM/vrXvzJjxgyeffZZLBYLd999N40bN+aWW25h2rRpLFy4EMMw6Nmzp3vOQxo2XUcgUg+Ypun+63rp0qV8+eWXvPjii3VclfgKDQ2J1LGtW7cyYMAA8vLyKCkp4cMPP6RXr151XZb4EA0NidSxLl26MHz4cH7/+99jtVrp1asXo0aNquuyxIdoaEhExMdpaEhExMc1qKGhwsJCvv32W8LCwjzORBERkfMrLS0lOzubbt26ERAQcM76BhUE3377LSNHjqzrMkREGqSlS5dy5ZVXnvN+gwqCsLAwoPzDVHWPexEROdfBgwcZOXKk+zv0bA0qCCqGg1q0aEHr1q3ruBoRkYblfEPqmiwWEfFxCgIRER+nIBAR8XEKAhERH+fVyeKkpCQyMjIwDIOEhAR69OgBlD8KcMqUKe7t9uzZw5///GdiY2OJj49n//79WK1WZs+e7ZXbC4uIyBleC4L09HR2795NcnIyO3bsICEhwf30qfDwcJYsWQKU34J49OjR9O/fn//85z8EBwczf/581q1bx/z583n22We9VaKIiODFoaG0tDQGDhwIlD9QIy8vz/0gksreffddBg0aRGBgIGlpae4nIV177bVs2rSpxurJPlFE9JyPyTp8bg0iIrVhzpw5jB49mtjYWG644QZGjx7NhAkTLtru4YcfprCw0Gt1ea1HkJOTQ1RUlHs5NDSU7OxsHA6Hx3Zvv/02f//7391tQkNDAbBYLBiGQXFxcY08E/XwiUL25RaQdTifXzsdF28gIlLD4uPjAUhJSWH79u08+uij1Wr3zDPPeLOs2rugrKqbnG7evJnLLrvsnHC4UJufK8BefiFFUUnVDxUXEakL8fHx2O12cnNzmT17Nn/+8585deoUhYWFTJ8+nR49etC/f3/ee+89Zs6cidPpJDMzk/379zNv3jyPP7h/Lq8FgdPpdD9XFeDw4cPnXN68du1a+vbt69EmOzubyMhIXC4XpmnWSG8AwN9WPgpW5Cq7yJYi4gve2biXZRv21Og+b7uyDX+44qff9aBJkybMnDmTH374gVtvvZWBAweSlpbGwoULWbBggce2xcXFLFq0iLfeeosVK1bUSBB4bY4gOjqa1NRUADIzM3E6nef85b9lyxYiIyM92lQ8NPuTTz7h6quvrrF6/G3qEYhI/VRxRmXz5s1JTU1lxIgRzJs3j9zc3HO2rbhpXIsWLaqcd/05vNYj6N27N1FRUcTFxWEYBomJiaSkpBAUFOSeEM7OzqZZs2buNkOGDOHLL79kxIgR+Pn5MWfOnBqrJ8B+ukdQoh6BiMAfrmj9s/569wa73Q7A4sWLCQ8P5+mnn2bLli089dRT52xb+X5BNTV87tU5gsrXCgAef/0DvPfeex7LFdcOeENFj6DQpR6BiNRPx44do3PnzgB89NFHuFyuWjmuz1xZbLcaGIZ6BCJSf91yyy384x//4J577qFHjx5kZ2fzzjvveP24DeqZxXv37mXAgAGsWbPmZ92Gusv0Dxjdtx0JQ7p4oToRkfrpYt+dPtMjAPC3WzQ0JCJyFt8KAptFp4+KiJzFp4IgwG7V6aMiImfxqSDwt1koVI9ARMSDTwWBegQiIufyqSDwt1l0+qiIyFl8LAisOmtIROQsPhUEAXb1CEREzuZTQeBvsyoIRETO4mNBoAvKRETO5ltBYFePQETkbL4VBDYLReoRiIh48K0gsFsoVI9ARMSDTwVBgM1KcUlZjT4LWUSkofPqg2mSkpLIyMjAMAwSEhLcj2MDOHDgAJMnT8blctG1a1eefPJJ1q9fz6RJk+jYsSMAnTp1Yvr06TVWj3+lp5RVPMxeRMTXeS0I0tPT2b17N8nJyezYsYOEhASSk5Pd6+fMmcM999xDTEwMTzzxBPv37wegT58+PP/8816pyf3cYpeCQESkgteGhtLS0hg4cCAAERER5OXluR+0XFZWxsaNG+nfvz8AiYmJtGzZ0luluJ15brEmjEVEKngtCHJycggJCXEvh4aGkp2dDcDRo0cJDAxk9uzZjBgxgvnz57u3y8rKYty4cYwYMYIvvviiRmty9wg0YSwi4ubVOYLKKk/QmqbJoUOHGDNmDK1atWLs2LGsXbuWLl26MGHCBAYPHsyePXsYM2YMH374IX5+fjVSg7+tPPd0UZmIyBle6xE4nU5ycnLcy4cPHyYsLAyAkJAQWrZsSdu2bbFarfTt25ft27cTHh7OkCFDMAyDtm3b0rx5cw4dOlRjNVXMC6hHICJyhteCIDo6mtTUVAAyMzNxOp04HA4AbDYbbdq0YdeuXe71HTp0YOXKlSxatAiA7Oxsjhw5Qnh4eI3VVNEj0ByBiMgZXhsa6t27N1FRUcTFxWEYBomJiaSkpBAUFERMTAwJCQnEx8djmiadOnWif//+nDp1iilTprBmzRpcLhczZsyosWEhqDw0pB6BiEgFr84RTJkyxWM5MjLS/bpdu3a89dZbHusdDgevvPKK1+o5MzSkHoGISAWfurLYfUGZegQiIm6+FQSnTx8tVI9ARMTNp4IgQD0CEZFz+FQQ6IIyEZFz+VgQ6IIyEZGz+WQQqEcgInKGTwWBzWrBZjF0+qiISCU+FQRQ8QB79QhERCr4XBAE2K3qEYiIVOJzQVD+AHv1CEREKvheENiteoC9iEglvhcENgtFOn1URMTN94LAbtXpoyIilfheENgsuqBMRKQSnwuCAPUIREQ8+FwQ+NssCgIRkUp8Mwg0NCQi4ubVJ5QlJSWRkZGBYRgkJCTQo0cP97oDBw4wefJkXC4XXbt25cknn7xom5qgoSEREU9e6xGkp6eze/dukpOTmTVrFrNmzfJYP2fOHO655x6WL1+O1Wpl//79F21TE8qHhtQjEBGp4LUgSEtLY+DAgQBERESQl5dHfn4+AGVlZWzcuJH+/fsDkJiYSMuWLS/Ypqb426y615CISCVeC4KcnBxCQkLcy6GhoWRnZwNw9OhRAgMDmT17NiNGjGD+/PkXbVNTAuzqEYiIVObVOYLKTNP0eH3o0CHGjBlDq1atGDt2LGvXrr1gm5rib7PiKjUpLTOxWowa37+ISEPjtR6B0+kkJyfHvXz48GHCwsIACAkJoWXLlrRt2xar1Urfvn3Zvn37BdvUFP+K5xarVyAiAngxCKKjo0lNTQUgMzMTp9OJw+EAwGaz0aZNG3bt2uVe36FDhwu2qSkBNj3AXkSkMq8NDfXu3ZuoqCji4uIwDIPExERSUlIICgoiJiaGhIQE4uPjMU2TTp060b9/fywWyzltapq/XQ+wFxGpzKtzBFOmTPFYjoyMdL9u164db7311kXb1DQ9wF5ExJPPXVkcoB6BiIgHnwuCih6BJotFRMr5YBCU9wh0UZmISDmfC4IAnT4qIuLB54Kgokeg00dFRMr5XBBU9AgK1SMQEQF8MAjUIxAR8eR7QeCeI1AQiIiADwZBgPusIQ0NiYiADwaBegQiIp58Lgj8rDp9VESkMp8LAovFwM9mUY9AROQ0nwsCKL/NhOYIRETK+WgQWNUjEBE5zSeDIMBu0XUEIiKn+WQQ+NssurJYROQ0rz6YJikpiYyMDAzDICEhgR49erjX9e/fnxYtWmC1lp/XP2/ePHbt2sWkSZPo2LEjAJ06dWL69Ok1XleA3UphsYJARAS8GATp6ens3r2b5ORkduzYQUJCAsnJyR7bLFy4kMDAQPfyrl276NOnD88//7y3ygLA4W/jRFGJV48hItJQeG1oKC0tjYEDBwIQERFBXl4e+fn53jrcTxIUYCe/UEEgIgJeDIKcnBxCQkLcy6GhoWRnZ3tsk5iYyIgRI5g3bx6maQKQlZXFuHHjGDFiBF988YVXagsKsHGiyOWVfYuINDRenSOorOKLvsLEiRO5/vrradKkCePHjyc1NZXLL7+cCRMmMHjwYPbs2cOYMWP48MMP8fPzq9FaggJs6hGIiJzmtR6B0+kkJyfHvXz48GHCwsLcy8OHD6dZs2bYbDb69evHtm3bCA8PZ8iQIRiGQdu2bWnevDmHDh2q8doc/jZOFJacE04iIr7Ia0EQHR1NamoqAJmZmTidThwOBwAnTpzg3nvvpbi4GICvv/6ajh07snLlShYtWgRAdnY2R44cITw8vMZrCwqwU1Jm6qIyERG8ODTUu3dvoqKiiIuLwzAMEhMTSUlJISgoiJiYGPr168ftt9+Ov78/Xbt2JTY2lpMnTzJlyhTWrFmDy+VixowZNT4sBOAIKP/YxwtdBNitNb5/EZGG5KJBkJ+fT3Z2Nh06dCA9PZ3vvvuOYcOGERoaetGdT5kyxWM5MjLS/frOO+/kzjvv9FjvcDh45ZVXqlv7zxZ8OgjyC0twBnn9cCIi9dpFh4YeeughDh8+zPbt25k7dy6hoaFMmzatNmrzGod/eRCc0ISxiMjFg6C4uJirr76a1atXc9dddzFs2DCKiopqozavCQqwA5Cvi8pERKoXBCtXrmTVqlXcdNNN7N27lxMnTtRGbV5zpkegawlERC4aBImJiXzzzTfMmDEDh8PBp59+ykMPPVQbtXlNUICGhkREKlx0srhNmzbccccdXHbZZaSnp+NyuYiKiqqN2rxGQSAicka1Jouzs7MvqcniwNNDQ5ojEBHx0cliu9VCI7tVcwQiIvjoZDGUX1SmHoGIyE+YLH7iiScumcliOH0HUs0RiIhcfLK4S5cuxMTEsHXrVrZt20a3bt3o3bt3bdTmVUH+CgIREahGjyApKYnXX38d0zQpLCzkpZde4plnnqmN2rwqKMCuoSEREarRI8jMzGTp0qXu5bFjxzJq1CivFlUbHP42Dp8orOsyRETq3EV7BCUlJRQWnvnCPHXqFKWlDf/B73o4jYhIuYv2CO68806GDRtG+/btKSsr48cff2Tq1Km1UZtXOTRZLCICVCMIhgwZwo033siuXbswDIP27dtjt9trozavCgqwk19cQlmZicVi1HU5IiJ1plpPKGvcuDFdu3alS5cuNGrUiHvuucfbdXldkL8N04STxeoViIhv+1mPqrwUnvVbcb8hnTkkIr7uZz2q0jCqN5SSlJRERkYGhmGQkJBAjx493Ov69+9PixYtsFrLHxU5b948wsPDL9imJjkq3XjuV028cggRkQbhvEEwd+7cKr/wTdNkz549F91xeno6u3fvJjk5mR07dpCQkEBycrLHNgsXLiQwMPAntakpekqZiEi58wZBp06dztvoQusqpKWlMXDgQAAiIiLIy8sjPz8fh8NRo21+roqnlOnGcyLi684bBL/73e9+0Y5zcnI8nlsQGhpKdna2x5d6YmIi+/bt44orruDPf/5ztdrUFM0RiIiU+1lzBD/H2RPMEydO5Prrr6dJkyaMHz+e1NTUi7apSXo4jYhIOa8FgdPpJCcnx718+PBhwsLC3MvDhw93v+7Xrx/btm27aJuaVDFHoKuLRcTXnff00fXr13ssFxcXu1+//fbbF91xdHS0+6/8zMxMnE6ne4jnxIkT3Hvvve59fv3113Ts2PGCbWpaoJ8Nw9AcgYjIeXsEL774IldffbV7+b777uONN94A4L333uPWW2+94I579+5NVFQUcXFxGIZBYmIiKSkpBAUFERMTQ79+/bj99tvx9/ena9euxMbGYhjGOW28xWIxcPjZOKE5AhHxcecNgrPH5ysvV3fsfsqUKR7LkZGR7td33nknd95550XbeJMeTiMicoGhobOvIai8XN0Lyuo7h+5AKiJy/h5BWVkZhYWF7r/+K5bLysooKyurtQK9KSjAzokizRGIiG87bxDs37+foUOHegwDDRkyBLiEegT+NnJPFV98QxGRS9h5g+Djjz+uzTrqRFCAjT1HT9V1GSIideq8cwQul4tnn30Wl+vM0Mn27dt5/vnna6Ww2hAUoLOGRETOGwRz584lPz/fY2ioXbt25Ofn88ILL9RKcd4WFGDXdQQi4vPOGwSbN2/m8ccfx8/Pz/2en58f8fHxfPHFF7VSnLc5/G0UuspwlV4ak98iIj/HeYOg4jkB5zSwWDyGixoy943ndAqpiPiw8wZBSEgIGzZsOOf9tWvX0rx5c68WVVtCGpf3do6c1JlDIuK7znvWUEJCAn/605+IiIigS5culJaWkpGRwYEDB1i0aFFt1ug1zR3+AOTkF/Frp3fuaSQiUt+dNwjatWvHihUr+OKLL9i5cyeGYTBq1Ciio6MvmesImged7hHkq0cgIr7rgrehtlgsXH/99Vx//fW1VU+tqtwjEBHxVeedI/AFIY39sBgKAhHxbT4dBFaLQWigv4JARHyaTwcBQHOHH9knNEcgIr5LQeBQj0BEfJtXH16flJRERkYGhmGQkJBAjx49ztlm/vz5/Pe//2XJkiWsX7+eSZMm0bFjRwA6derE9OnTvVkizR1+7D560qvHEBGpz7wWBOnp6ezevZvk5GR27NhBQkICycnJHttkZWXx9ddfY7fb3e/16dOnVm9s19zhT46GhkTEh3ltaCgtLY2BAwcCEBERQV5eHvn5+R7bzJkzh4cffthbJVRL8yB/ClylnNRdSEXER3ktCHJycggJCXEvh4aGkp2d7V5OSUmhT58+tGrVyqNdVlYW48aNY8SIEbVycztdSyAivs6rcwSVVb6ddW5uLikpKfzjH//g0KFD7vfbt2/PhAkTGDx4MHv27GHMmDF8+OGHHndArWnNHOX7zskvol2zQK8dR0SkvvJaj8DpdJKTk+NePnz4MGFhYQB89dVXHD16lJEjRzJhwgQyMzNJSkoiPDycIUOGYBgGbdu2pXnz5h5B4Q1hp3sEOoVURHyV14IgOjqa1NRUADIzM3E6nTgc5Td2i42N5f3332fZsmW88MILREVFkZCQwMqVK903tMvOzubIkSOEh4d7q0TgzNDQkZMaGhIR3+S1oaHevXsTFRVFXFwchmGQmJhISkoKQUFBxMTEVNmmf//+TJkyhTVr1uByuZgxY4ZXh4Wg0tCQegQi4qO8OkcwZcoUj+XIyMhztmndujVLliwBwOFw8Morr3izpHPYrRaaNrZrslhEfJbPX1kM0CzQT0EgIj5LQYBuMyEivk1BQPlFZXo4jYj4KgUB5aeQZqtHICI+SkFA+Y3nThSWUOgqretSRERqnYKAytcSaHhIRHyPggBoVnG/oRMaHhIR36MgoHxoCHTjORHxTQoCKg0N6cwhEfFBCgIgLKg8CA4dL6zjSkREap+CAAiwW2nVtBHbD+dffGMRkUuMguC0yBZB/O/gibouQ0Sk1ikITuvcIogd2fkUl5TVdSkiIrVKQXBa5xZBlJSZ7MjW8JCI+BYFwWldfhUMwPcHj9dxJSIitUtBcFqH5oHYrQbfa55ARHyMguA0u9XCr52aMBYR3+PVIEhKSuL2228nLi6Ob775pspt5s+fz+jRo39SG2+JbBHE9wcUBCLiW7wWBOnp6ezevZvk5GRmzZrFrFmzztkmKyuLr7/++ie18abOLYI4eLyQ3FO6wlhEfIfXgiAtLY2BAwcCEBERQV5eHvn5nmfkzJkzh4cffvgntfGmyBZBAJonEBGf4rUgyMnJISQkxL0cGhpKdna2ezklJYU+ffrQqlWrarfxtsgW5WcOaZ5ARHxJrU0Wm6bpfp2bm0tKSgp33313tdvUhvBgf5o2tqtHICI+xeatHTudTnJyctzLhw8fJiwsDICvvvqKo0ePMnLkSIqLi/nxxx9JSkq6YJvaYBgGncODdC2BiPgUr/UIoqOjSU1NBSAzMxOn04nD4QAgNjaW999/n2XLlvHCCy8QFRVFQkLCBdvUlm6tmvDd/uN6bKWI+Ayv9Qh69+5NVFQUcXFxGIZBYmIiKSkpBAUFERMTU+02te2ay5qxaN0PZOzJ5erLmtX68UVEapvXggBgypQpHsuRkZHnbNO6dWuWLFly3ja1rU/7UAwDvtp5VEEgIj5BVxafpUljO11/FcxXO4/UdSkiIrVCQVCFay5rxsYfj2meQER8goKgCtdc1ozikjL+uye3rksREfE6BUEVzswTaHhIRC59CoIqNGlsJ6ql5glExDcoCM7jmg7N2PRjruYJROSSpyA4j4p5gq93Ha3rUkREvEpBcB7XdWxOcICNtzfsretSRES8SkFwHgF2K7+7vBUffHuQYyf1fAIRuXQpCC4grk9bikvLeHfzvrouRUTEaxQEF9DlV8H0bNOUf339Y63fEltEpLYoCC5ixFVt2HYon826uExELlEKgov4Tc+WNPaz8vd1P9R1KSIiXqEguAiHv437rr+M/3xzgI+/P1TX5YiI1DgFQTVMuOnXdA4PYlrKFvIKXHVdjohIjVIQVIOfzcLTt/YgJ7+YWau+q+tyRERqlFcfTJOUlERGRgaGYZCQkECPHj3c65YtW8by5cuxWCxERkaSmJhIeno6kyZNomPHjgB06tSJ6dOnexhh6kwAABK6SURBVLPEauvRuilj+13Gy2t3cPtVbbmiXUhdlyQiUiO8FgTp6ens3r2b5ORkduzYQUJCAsnJyQAUFBSwatUqli5dit1uZ8yYMWzevBmAPn368Pzzz3urrF9kwk2/5u0Ne5mzeivL7u+LYRh1XZKIyC/mtaGhtLQ0Bg4cCEBERAR5eXnk5+cD0KhRIxYvXozdbqegoID8/HzCwsK8VUqNCfS38dDAjny96xgfbT1c1+WIiNQIrwVBTk4OISFnhk9CQ0PJzs722Oa1114jJiaG2NhY2rRpA0BWVhbjxo1jxIgRfPHFF94q72e7/ao2XNY8kLkffE9JaVldlyMi8ovV2mRxVVfmjh07lo8++ojPP/+cjRs30r59eyZMmMDLL7/M3Llzeeyxxygurl/3+bFbLUyN7UzW4Xxe/3JXXZcjIvKLeS0InE4nOTk57uXDhw+7h39yc3P5+uuvAQgICKBfv35s2rSJ8PBwhgwZgmEYtG3blubNm3PoUP07d39QVAsGdgkn6f2trP2fhohEpGHzWhBER0eTmpoKQGZmJk6nE4fDAUBJSQnx8fGcPHkSgC1bttChQwdWrlzJokWLAMjOzubIkSOEh4d7q8SfzTAMnovrRWSLYCa8uZnvDx6v65JERH42r5011Lt3b6KiooiLi8MwDBITE0lJSSEoKIiYmBjGjx/PmDFjsNlsdO7cmQEDBnDy5EmmTJnCmjVrcLlczJgxAz8/P2+V+IsE+ttYdNeV3PLCF4x47SseH9qV3/dupTOJRKTBMcwGdFvNvXv3MmDAANasWUPr1q3ruhwAdmTn88jbGWz6MZdrI5rxXNzlhAX513VZIiJuF/vu1JXFv1BEmIPl467lL8O7sfnHXOJeS+PQ8cK6LktEpNoUBDXAYjEYdU07Ft/Th4N5hdz2ahp7jp6q67JERKpFQVCD+nQIZcl9V3P0ZDExz3zKrFXfcSS/qK7LEhG5IAVBDevdNoT3J17P0O4tWbTuB6LnfsyUtzPY/OMxPeVMROolBYEXtAltzPzbevJ/k2/g971b8/6WA/zupS+5Y+F6vt2XV9fliYh4UBB4UUSYg6TfdWd9wgCm/6Yr3x88zm8WrOOBf27ko+8OUVyiW1SISN3z6m2opVxQgJ17r+vArVe25pW1O3gr/UdWf3uQIH8bbUIb4wz2J6plMP0jnfRqE4LVomsRRKT2KAhqUXCAnamxkTwc04nPt2ezZuthDuYVciCvkM+37+TFT3YQFGAjqmUw3Vo2Ibpjc66NaIa/zVrXpYvIJUxBUAfsVgv9I8PpH3nm9hl5p1x8tj2btJ1HyNx/nCVf7eZv637A4W/jinYhtAltRKumjWkV0ohWTRvhDPKnaWM7Dn+brmYWkV9EQVBPNGls57c9W/Lbni0BKHSVkrbjCKmZB9myL4+Mvbnknjr3ecmN7Fa6tgymW8tgQgP9CbBbCA30I8LpICLMQZNG9tr+KCLSwCgI6qkAu5WbIp3cFOl0v5dfVML+3AL2HjtFTn4xeadc7Mst4Nt9eSzfuJeTxaXn7CcsyJ+IsEB3INitFloEB+AM9sdVapJX4CLAbqVzeBC/djpoZLditxnYLBbsVgN/m5UAu0W9DpFLmIKgAXH42+gUHkSn8KAq15eWmRS6Ssk+UUTW4Xx2ZOe7f+4+Un6lc6GrlI+2HqLQVX7GUoDdQnFJGWUXuMTBajFw+Ntw+NsICij/5/C30djfhgFUbupvs9CySSNaNAnAYhiUlJXhKjUpKS0/ht1q4G+34m+14G+34Of+aXUvB9jLw8fPZqHMhLIykzLTpLTMxG61EBLoR6CfFcMwKDtduEUT7CI/m4LgEmK1GAT62wj0t9G+eSADqfoW3qZpcqKoBH+bBX+blUJXKVmH89mZc5IiVyklZeVf3K5Sk8KSUk4WlZBfWMKJip+FJeTkF3Oy0m00Kr6GC11lHDxeSOmFkqUG2CwGZabpDjCLUd7bKf9nYLOWh4rNamCzGNit5b0aAzCM0/8wTv8sf9O9jvJbjVsqbWMxDCyW0z9Pr7MYBoZhYK30vuF+//TP078ci8f+y/eFe9+etVTufVW8NCpve6H9Gp6fxWO9YZzzuSt39Cpf73i+//UqNj+7rovx+N1TuRbPeiv2efbnr3zsqtedf8PKi2f3bD3XVf36QjXVlOr0uA3gul83JySw5u/IrCDwQYZhEBxwZu4gwG6lW6smdGvVpEb2X1pmkpNfhGmCzWpgt5R/IVsMg+LSMopKSikuKaOopOysn6UUucqXC12lFJeWub9wrZYz7XNPFZN7yoXFMLBZy/8DKik1cZWV4SoxT/dCyoPMVVpGSalJcWnZ6S86E9Ms/6IzTfP0zzPLuJfLtyszy3+Wlpm4Sk1KT4ePaVb0Us68rtx7qdhvRXs48/rMvs/UU7kNlWorX/Cs98x+ytedaeu5D7n0TLjp10wZ1LnG96sgkBpntRiEBwdUua4RVkAT2N5mVgqLswPI4zUX+Kv4rD+tK6LJHVZU/Qhazzantz/rmJ4hXJFqnj0Rzx6K53HO13s5u54LlXe+/Z/d5kL7v5DqbvlTgrtD88Dqb/wTKAhELkEVQy8A1nMGVUQ86RYTIiI+zqs9gqSkJDIyMjAMg4SEBHr06OFet2zZMpYvX47FYiEyMpLExEQMw7hgGxERqXleC4L09HR2795NcnIyO3bsICEhgeTkZAAKCgpYtWoVS5cuxW63M2bMGDZv3kxJScl524iIiHd4bWgoLS2NgQMHAhAREUFeXh75+fkANGrUiMWLF2O32ykoKCA/P5+wsLALthEREe/wWhDk5OQQEhLiXg4NDSU7O9tjm9dee42YmBhiY2Np06ZNtdqIiEjNqrXJ4qpOuxo7diwfffQRn3/+ORs3bqxWGxERqVlemyNwOp3k5OS4lw8fPkxYWBgAubm5bN++nauuuoqAgAD69evHpk2bLtgGoLS0/F46Bw8e9FbZIiKXnIrvzIrv0LN5LQiio6NZsGABcXFxZGZm4nQ6cTgcAJSUlBAfH8/KlSsJDAxky5YtDBs2jNDQ0PO2AdzDRCNHjvRW2SIil6zs7GzatWt3zvuG6cXxl3nz5rFhwwYMwyAxMZHvvvuOoKAgYmJiSElJYenSpdhsNjp37swTTzyBYRjntImMjHTvr7CwkG+//ZawsDCsVj2sRUSkOkpLS8nOzqZbt24EBJx71b9Xg0BEROo/XVksIuLjfOZeQw3xiuWnnnqKjRs3UlJSwv3330/37t2ZOnUqpaWlhIWF8fTTT+PnV/O3pK1JhYWF/OY3v+HBBx+kb9++Dar+lStX8re//Q2bzcbEiRPp3Llzg6n/5MmTPProo+Tl5eFyuRg/fjxhYWHMmDEDwD0cW99s27aNBx98kLvuuotRo0Zx4MCBKn/nK1euZPHixVgsFm677TZuvfXWui4dqLr+adOmUVJSgs1m4+mnnyYsLKz+1W/6gPXr15tjx441TdM0s7KyzNtuu62OK7q4tLQ087777jNN0zSPHj1q3nDDDWZ8fLz5/vvvm6ZpmvPnzzeXLl1alyVWy1//+lfz97//vfnOO+80qPqPHj1q3nzzzeaJEyfMQ4cOmY8//niDqn/JkiXmvHnzTNM0zYMHD5qDBg0yR40aZWZkZJimaZqTJ082165dW5clnuPkyZPmqFGjzMcff9xcsmSJaZpmlb/zkydPmjfffLN5/Phxs6CgwBw6dKh57NixuizdNM2q6586daq5atUq0zRN85///Kc5d+7celm/TwwNNcQrlq+66iqee+45AIKDgykoKGD9+vUMGDAAgJtuuom0tLS6LPGiduzYQVZWFjfeeCNAg6o/LS2Nvn374nA4cDqdzJw5s0HVHxISQm5uLgDHjx+nadOm7Nu3z90Tro/1+/n5sXDhQpzOM49nrep3npGRQffu3QkKCiIgIIDevXuzadOmuirbrar6ExMTGTRoEHDmf5P6WL9PBEFDvGLZarXSuHFjAJYvX06/fv0oKChwD0U0a9as3n+GuXPnEh8f715uSPXv3buXwsJCxo0bxx133EFaWlqDqn/o0KHs37+fmJgYRo0axdSpUwkODnavr4/122y2c85oqep3npOTQ2hoqHub+vLfc1X1N27cGKvVSmlpKW+++Sa//e1v62X9PjNHUJnZgE6U+uijj1i+fDl///vfufnmm93v1/fPsGLFCnr16kWbNm2qXF/f64fyCx9feOEF9u/fz5gxYzxqru/1//vf/6Zly5YsWrSI77//nvHjxxMUdOZZ1/W9/qqcr+b6/llKS0uZOnUq11xzDX379uW9997zWF8f6veJILjYFcv11eeff84rr7zC3/72N4KCgmjcuDGFhYUEBARw6NAhjy5ofbN27Vr27NnD2rVrOXjwIH5+fg2q/mbNmnH55Zdjs9lo27YtgYGBWK3WBlP/pk2buO666wCIjIykqKiIkpIS9/r6Xn+Fqv4/U9V/z7169arDKi9s2rRptGvXjgkTJgBVfx/Vdf0+MTQUHR1NamoqQJVXLNdHJ06c4KmnnuLVV1+ladOmAFx77bXuz/Hhhx9y/fXX12WJF/Tss8/yzjvvsGzZMm699VYefPDBBlX/ddddx1dffUVZWRnHjh3j1KlTDar+du3akZGRAcC+ffsIDAwkIiKCDRs2APW//gpV/c579uzJli1bOH78OCdPnmTTpk1ceeWVdVxp1VauXIndbmfixInu9+pj/T5zQdmFrliuj5KTk1mwYAEdOnRwvzdnzhwef/xxioqKaNmyJbNnz8Zur//P/12wYAGtWrXiuuuu49FHH20w9f/rX/9i+fLlADzwwAN07969wdR/8uRJEhISOHLkCCUlJUyaNImwsDD+3//7f5SVldGzZ0+mTZtW12V6+Pbbb5k7dy779u3DZrMRHh7OvHnziI+PP+d3/sEHH7Bo0SIMw2DUqFEMGzasrsuvsv4jR47g7+/v/sMzIiKCGTNm1Lv6fSYIRESkaj4xNCQiIuenIBAR8XEKAhERH6cgEBHxcQoCEREfpyCQS8LevXu5/PLLGT16tMe/ivvt/BILFizgn//85wW36dy5Mx9//LF7ef369SxYsOBnH3P9+vUe556LeJNPXFksvqFDhw4sWbKkTo7dvn17XnjhBW644QY9PU8aHAWBXPLi4+Np3LgxO3fu5NixY8yePZuuXbuyePFi3n//fQAGDBjA2LFj2bdvH/Hx8ZSWltKyZUvmzp0LlN9n/v7772fXrl089thj9OvXz+MYTqeT7t278+677/LHP/7RY93VV1/N+vXrAZg4cSIjR44kPT2dY8eOsXv3bvbu3cukSZN455132LdvHwsXLgQgLy+P8ePHs2/fPmJiYhg/fjxZWVk8+eSTGIZBYGAgc+bM4fjx4zzyyCM0btyYUaNGcdNNN3n7VyqXGA0NiU8oKSnh9ddfZ9KkSbz44ovs2bOHd999l6VLl7J06VJWr17Njz/+yDPPPMNdd93Fm2++idPp5NtvvwXKb0D36quv8vjjj/Ovf/2rymPcf//9LF68mMLCwmrVlJeXx6JFi4iNjWXFihXu12vWrAHgf//7H0899RTLli3jnXfeITc3l5kzZ/Lkk0+yePFioqOjWbp0KQBbt25l3rx5CgH5WdQjkEvGDz/8wOjRo93LHTp04MknnwTK71kD0KtXL+bNm8fWrVvp2bMnNlv5fwK9e/fm+++/57vvvuOxxx4DYOrUqQB89tln9O7dG4Dw8HBOnDhR5fGbNGnCLbfcwhtvvEHPnj0vWm/37t0BPG6A2Lx5c/e8Rrdu3QgMDATKb02wZ88evvnmG6ZPnw5AcXGxex9t2rTxuNW6yE+hIJBLxoXmCMrKytyvDcPAMAyP2/+6XC4sFgtWq7XK2wJXBMbFjB49mj/+8Y+0b9++yvUul6vKfVZ+XXF8wzA82hqGQaNGjXjjjTc81u3du7fe3vNIGgYNDYlP2LhxIwCbN28mIiKCLl268N///peSkhJKSkrIyMigS5cudOvWja+++gqA5557ji+//PInHcff35+7776bV155xf2eYRgUFBRQUFDA1q1bq72v7777joKCAoqKitixYwdt27YlMjKSzz77DIBVq1bVu6eMScOkHoFcMs4eGgJ45JFHACgqKuL+++/nwIEDPP3007Ru3Zrbb7+dUaNGYZomt956K61atWLixIlMmzaNN998k1/96ldMmDDBHSLVNXz4cP7xj3+4l0eMGMFtt91GREQEUVFR1d5P165dSUhIYNeuXcTFxREcHMxjjz3G9OnTWbhwIf7+/syfP7/eP3ZV6j/dfVQuefHx8QwaNEgTqSLnoaEhEREfpx6BiIiPU49ARMTHKQhERHycgkBExMcpCEREfJyCQETExykIRER83P8HVlXCU1jVD+AAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"time\"\n",
        "rnn_arr4 = []\n",
        "nrnn_arr4 = []\n",
        "lstm_arr4 = []\n",
        "seed_arr4 = []\n",
        "method_arr4 = []\n",
        "specify_arr4 = []\n",
        "\n",
        "startprob = random_startprob(2)\n",
        "transmat1 = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])\n",
        "\n",
        "transmat2 = np.array([[0.05, 0.95],\n",
        "                     [0.05, 0.95]])\n",
        "\n",
        "transmat3 = np.array([[0.95, 0.05],\n",
        "                     [0.05, 0.95]])\n",
        "\n",
        "transmats = [transmat1, transmat2, transmat3]\n",
        "names = [\"transmat1\", \"transmat2\", \"transmat3\"]\n",
        "\n",
        "for idx, transmat in enumerate(transmats):\n",
        "  x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features,length, window_length, train_ratio, method, \n",
        "                      flip_probability=None, flip_probability_0=None, flip_probability_1=None,\n",
        "                      startprob=startprob, transmat=transmat1)\n",
        "\n",
        "  time_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "  time_train_flipped_loader = data_utils.DataLoader(time_train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "  time_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "  time_test_flipped_loader = data_utils.DataLoader(time_test_flipped, batch_size=25, shuffle=True)\n",
        "\n",
        "  for seed in seeds:\n",
        "      acc = driver(seed, time_train_flipped_loader, time_test_flipped_loader)\n",
        "      acc2 = driver(seed, time_train_flipped_loader, time_test_flipped_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "      acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
        "      rnn_arr4.append(acc)\n",
        "      nrnn_arr4.append(acc2)\n",
        "      lstm_arr4.append(acc3)\n",
        "      seed_arr4.append(seed)\n",
        "      method_arr4.append(method)\n",
        "      specify_arr4.append(names[idx])"
      ],
      "metadata": {
        "id": "IFriPAp0lm7Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d5114849-3443-4e8a-8799-bab7ab150021"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8140 | test accuracy: 0.603\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4895 | test accuracy: 0.603\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8853 | test accuracy: 0.603\n",
            "Epoch:  3 Iteration:  280 | train loss: 1.3189 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6531 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6524 | test accuracy: 0.603\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8103 | test accuracy: 0.603\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.4605 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5854 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6633 | test accuracy: 0.603\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8452 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6513 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.9620 | test accuracy: 0.603\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6515 | test accuracy: 0.603\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5364 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8580 | test accuracy: 0.603\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5550 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.8548 | test accuracy: 0.603\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.4756 | test accuracy: 0.603\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8367 | test accuracy: 0.603\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.4459 | test accuracy: 0.603\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.4331 | test accuracy: 0.603\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.8764 | test accuracy: 0.603\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6623 | test accuracy: 0.603\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.3838 | test accuracy: 0.603\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6494 | test accuracy: 0.603\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.8124 | test accuracy: 0.603\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.4267 | test accuracy: 0.603\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.4969 | test accuracy: 0.603\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6491 | test accuracy: 0.603\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6418 | test accuracy: 0.603\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.4252 | test accuracy: 0.603\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8085 | test accuracy: 0.603\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6260 | test accuracy: 0.603\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.4812 | test accuracy: 0.603\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6166 | test accuracy: 0.603\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6241 | test accuracy: 0.603\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6490 | test accuracy: 0.603\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6256 | test accuracy: 0.603\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.8066 | test accuracy: 0.603\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6634 | test accuracy: 0.603\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8103 | test accuracy: 0.603\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6505 | test accuracy: 0.603\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6298 | test accuracy: 0.603\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.8117 | test accuracy: 0.603\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.9708 | test accuracy: 0.603\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6255 | test accuracy: 0.603\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.4599 | test accuracy: 0.603\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.8277 | test accuracy: 0.603\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6286 | test accuracy: 0.603\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.8114 | test accuracy: 0.603\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.8520 | test accuracy: 0.603\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.8039 | test accuracy: 0.603\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7842 | test accuracy: 0.603\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4730 | test accuracy: 0.603\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.4475 | test accuracy: 0.603\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6605 | test accuracy: 0.603\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.4734 | test accuracy: 0.603\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.8097 | test accuracy: 0.603\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.8427 | test accuracy: 0.603\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.8328 | test accuracy: 0.603\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.8306 | test accuracy: 0.603\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6486 | test accuracy: 0.603\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6654 | test accuracy: 0.603\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7777 | test accuracy: 0.603\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6578 | test accuracy: 0.603\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7757 | test accuracy: 0.603\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6349 | test accuracy: 0.603\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.4841 | test accuracy: 0.603\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6560 | test accuracy: 0.603\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.8065 | test accuracy: 0.603\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6859 | test accuracy: 0.603\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6738 | test accuracy: 0.603\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6520 | test accuracy: 0.603\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.6265 | test accuracy: 0.603\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6427 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8027 | test accuracy: 0.603\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6357 | test accuracy: 0.603\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.4594 | test accuracy: 0.603\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.7625 | test accuracy: 0.603\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4486 | test accuracy: 0.603\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.4841 | test accuracy: 0.603\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4971 | test accuracy: 0.603\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6449 | test accuracy: 0.603\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.8286 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6219 | test accuracy: 0.603\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6065 | test accuracy: 0.603\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6389 | test accuracy: 0.603\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.4488 | test accuracy: 0.603\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4556 | test accuracy: 0.603\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9775 | test accuracy: 0.603\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6082 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.4939 | test accuracy: 0.603\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6310 | test accuracy: 0.603\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.8046 | test accuracy: 0.603\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4610 | test accuracy: 0.603\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4489 | test accuracy: 0.603\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.4736 | test accuracy: 0.603\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.4630 | test accuracy: 0.603\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.4654 | test accuracy: 0.603\n",
            "total time:  33.399790323999696\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4597 | test accuracy: 0.603\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6802 | test accuracy: 0.603\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.5110 | test accuracy: 0.603\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4982 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5363 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3286 | test accuracy: 0.603\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7611 | test accuracy: 0.603\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.8443 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3959 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5748 | test accuracy: 0.603\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4059 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5601 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.4775 | test accuracy: 0.603\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5055 | test accuracy: 0.603\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5127 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.8682 | test accuracy: 0.603\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.8663 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4726 | test accuracy: 0.603\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7485 | test accuracy: 0.603\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.9299 | test accuracy: 0.603\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.4631 | test accuracy: 0.603\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.8352 | test accuracy: 0.603\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6805 | test accuracy: 0.603\n",
            "Epoch:  23 Iteration:  1680 | train loss: 1.0712 | test accuracy: 0.603\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5548 | test accuracy: 0.603\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.4657 | test accuracy: 0.603\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4609 | test accuracy: 0.603\n",
            "Epoch:  27 Iteration:  1960 | train loss: 1.0442 | test accuracy: 0.603\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.4449 | test accuracy: 0.603\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6471 | test accuracy: 0.603\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7752 | test accuracy: 0.603\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.4718 | test accuracy: 0.603\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7365 | test accuracy: 0.603\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.7287 | test accuracy: 0.603\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.8995 | test accuracy: 0.603\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.5952 | test accuracy: 0.603\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5590 | test accuracy: 0.603\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6519 | test accuracy: 0.603\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6529 | test accuracy: 0.603\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5306 | test accuracy: 0.603\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7296 | test accuracy: 0.603\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.8049 | test accuracy: 0.603\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.4785 | test accuracy: 0.603\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6198 | test accuracy: 0.603\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6709 | test accuracy: 0.603\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.9652 | test accuracy: 0.603\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.9010 | test accuracy: 0.603\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.7153 | test accuracy: 0.603\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.6568 | test accuracy: 0.603\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.9370 | test accuracy: 0.603\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.6189 | test accuracy: 0.603\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.4602 | test accuracy: 0.603\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.7838 | test accuracy: 0.603\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4598 | test accuracy: 0.603\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5357 | test accuracy: 0.603\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.6591 | test accuracy: 0.603\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4323 | test accuracy: 0.603\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.8023 | test accuracy: 0.603\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.8579 | test accuracy: 0.603\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6074 | test accuracy: 0.603\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6316 | test accuracy: 0.603\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.4615 | test accuracy: 0.603\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6744 | test accuracy: 0.603\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.5949 | test accuracy: 0.603\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.8711 | test accuracy: 0.603\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.8633 | test accuracy: 0.603\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6524 | test accuracy: 0.603\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6616 | test accuracy: 0.603\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.5847 | test accuracy: 0.603\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6362 | test accuracy: 0.603\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6884 | test accuracy: 0.603\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.4610 | test accuracy: 0.603\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.5710 | test accuracy: 0.603\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8490 | test accuracy: 0.603\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.5850 | test accuracy: 0.603\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.6634 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.8823 | test accuracy: 0.603\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6958 | test accuracy: 0.603\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.4337 | test accuracy: 0.603\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.8692 | test accuracy: 0.603\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.6039 | test accuracy: 0.603\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5225 | test accuracy: 0.603\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.6224 | test accuracy: 0.603\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6189 | test accuracy: 0.603\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7068 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.6288 | test accuracy: 0.603\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6212 | test accuracy: 0.603\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6283 | test accuracy: 0.603\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6228 | test accuracy: 0.603\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7059 | test accuracy: 0.603\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.8200 | test accuracy: 0.603\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6291 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.4658 | test accuracy: 0.603\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.9003 | test accuracy: 0.603\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.8408 | test accuracy: 0.603\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4677 | test accuracy: 0.603\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6221 | test accuracy: 0.603\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.5560 | test accuracy: 0.603\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.5942 | test accuracy: 0.603\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.5942 | test accuracy: 0.603\n",
            "total time:  35.6809781740003\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2451932430267334.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0.43874621391296387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5248591201645988 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24770236015319824.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.42716550827026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.4428728231361934 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586236000061035.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4503512382507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.3996219928775515 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25585460662841797.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.43358397483825684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.37531672418117523 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26618385314941406.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4409327507019043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3609802897487368 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24960565567016602.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.42136502265930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3520716058356421 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2471306324005127.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.42734265327453613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.34607701216425213 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24468398094177246.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42758965492248535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.339179750425475 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25107908248901367.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43393850326538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3359155135495322 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25549912452697754.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4497959613800049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.33284790345600673 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2580986022949219.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4456765651702881\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3298798314162663 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2835721969604492.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4641611576080322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3278395648513521 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593526840209961.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43689775466918945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3256109982728958 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25928187370300293.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44605302810668945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32387755726064954 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25609898567199707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4425325393676758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3233456317867551 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2551286220550537.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45615339279174805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32187523373535704 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698230743408203.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45949459075927734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32109379598072596 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24660468101501465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4159712791442871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3210535785981587 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25043344497680664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42340779304504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.320120501943997 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536015510559082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42823290824890137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3193023209060941 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25832295417785645.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4336552619934082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3187259299414498 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2552483081817627.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44266819953918457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31840802558830805 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688615322113037.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45413875579833984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31817159567560466 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2511899471282959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4322240352630615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31761203621115003 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509489059448242.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4348940849304199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31718167960643767 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25291943550109863.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44004106521606445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31671699285507204 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26569247245788574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4496588706970215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3164889816726957 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25229334831237793.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4380970001220703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3165780812501907 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25293850898742676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4327859878540039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31604331518922535 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25459814071655273.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43624186515808105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31604261440890175 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26010990142822266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4560849666595459\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31583621885095325 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25330615043640137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4424600601196289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31561748172555654 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26706576347351074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4381999969482422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31563354858330317 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604048252105713.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45501065254211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3155193103211267 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25516223907470703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.429171085357666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31521165924412864 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527945041656494.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4284648895263672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31521925713334764 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24773502349853516.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4254782199859619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31524122357368467 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2434406280517578.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4281883239746094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31503005410943713 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24797725677490234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.425290584564209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31498376131057737 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24854707717895508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41294360160827637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31483508689062933 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24591898918151855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4230060577392578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3147988157612937 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24015593528747559.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.40962672233581543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3148051815373557 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23668742179870605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4157392978668213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3145760706492833 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24253058433532715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4287528991699219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3145848759583065 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25653767585754395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4240913391113281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31449457151549204 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2489762306213379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42682361602783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31447321474552153 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2502579689025879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42428135871887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3144256489617484 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25490665435791016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43614840507507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3143805772066116 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25116419792175293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.431593656539917\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31428047503743856 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24694442749023438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4261338710784912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31436884360654016 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24594855308532715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42914342880249023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3143764580999102 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2506120204925537.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4263145923614502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31427946601595197 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26703572273254395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4561755657196045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31424601418631415 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536780834197998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401266574859619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3141190937587193 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595233917236328.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45749783515930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3140797474554607 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25855231285095215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389028549194336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140778009380613 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24703598022460938.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44945812225341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31402582015310015 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2713618278503418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46234583854675293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3140135943889618 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24668216705322266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42031073570251465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139843685286386 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2592294216156006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389324188232422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31399415348257337 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2409510612487793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41484546661376953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3139319998877389 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2640972137451172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44399333000183105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3139202083860125 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24924254417419434.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4246864318847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31398434724126545 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24941205978393555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4428596496582031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31393630504608155 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583456039428711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43780517578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138980503593172 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25563907623291016.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43086957931518555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3138476363250187 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2631955146789551.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44524502754211426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139003383261817 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24794840812683105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4234583377838135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31380425436156134 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2653350830078125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45064616203308105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.31379382695470537 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2499701976776123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43151283264160156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3138210730893271 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24391698837280273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43506813049316406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137899079493114 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24740839004516602.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42151904106140137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3136755509035928 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2488722801208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43287158012390137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137153906481607 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2621302604675293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44344639778137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31374400215489523 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24603033065795898.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4233736991882324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31369258846555437 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591221332550049.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4639315605163574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31369182084287917 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25570106506347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4362311363220215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31366344136851176 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547452449798584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44563746452331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3136738198144095 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23795700073242188.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41138601303100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137588058199201 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527942657470703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44509458541870117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31366351587431773 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2712383270263672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46285128593444824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31364758695874895 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561149597167969.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4378628730773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31362938838345666 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2773244380950928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45662355422973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.313687863945961 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24425506591796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4275705814361572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31361161257539477 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25626063346862793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45285868644714355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136795358998435 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25579047203063965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4364020824432373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31359667224543436 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23785066604614258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41939568519592285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31358525454998015 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2714560031890869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4488811492919922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3135836354323796 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26293301582336426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4403820037841797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3135817949260984 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26561450958251953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4462709426879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31355662984507426 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25336456298828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317018985748291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135944093976702 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557547092437744.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4410097599029541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136039750916617 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2435915470123291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4141511917114258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31357095752443587 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2475588321685791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4237809181213379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31355100870132446 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25530242919921875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43498778343200684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31355018743446894 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2474052906036377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4285857677459717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31355215311050416 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.250077486038208.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4304800033569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135441818407604 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25243115425109863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4327969551086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135535682950701 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2520172595977783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4368762969970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135211770023618 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679929733276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4486517906188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31350183486938477 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607755661010742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44704747200012207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31352998784610203 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611260414123535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4365053176879883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135434980903353 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26329612731933594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4543273448944092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31351655636514936 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2648656368255615.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372224807739258\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135241934231349 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25777101516723633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4430861473083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3134898045233318 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25873756408691406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44745683670043945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31348724280084883 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24918580055236816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43019580841064453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31348430982657843 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2515072822570801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42899179458618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31349155902862547 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26119017601013184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43581390380859375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31347612696034566 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24503159523010254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42422032356262207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134984510285514 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601633071899414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4447951316833496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134759430374418 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2396376132965088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4259049892425537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134926642690386 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2444438934326172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4333155155181885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134499511548451 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545630931854248.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4381699562072754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31346855419022696 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25055408477783203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4301624298095703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31347129685538155 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559847831726074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4352602958679199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31343383150441306 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2495570182800293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4352116584777832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134463437965938 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2416226863861084.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4318273067474365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134527653455734 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2468271255493164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42922520637512207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134346182857241 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24615097045898438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42563843727111816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31343347557953427 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561495304107666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43635988235473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31346714624336786 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25040531158447266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42865872383117676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134416882480894 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2576262950897217.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44608235359191895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134160535676139 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666506767272949.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4492318630218506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31344578777040755 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25439000129699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4448721408843994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134145532335554 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2644996643066406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45502305030822754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134143271616527 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25785136222839355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44789838790893555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31342854159218925 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26918601989746094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46544623374938965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3133953256266458 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6337 | test accuracy: 0.603\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6114 | test accuracy: 0.603\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6393 | test accuracy: 0.603\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6215 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3732 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4116 | test accuracy: 0.603\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5207 | test accuracy: 0.603\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6750 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6574 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7701 | test accuracy: 0.603\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8097 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5416 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8474 | test accuracy: 0.603\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.9019 | test accuracy: 0.603\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4818 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.7917 | test accuracy: 0.603\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4906 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4876 | test accuracy: 0.603\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6494 | test accuracy: 0.603\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.8309 | test accuracy: 0.603\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.4258 | test accuracy: 0.603\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6655 | test accuracy: 0.603\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.8086 | test accuracy: 0.603\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.6393 | test accuracy: 0.603\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6472 | test accuracy: 0.603\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.6623 | test accuracy: 0.603\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6595 | test accuracy: 0.603\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8249 | test accuracy: 0.603\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.8484 | test accuracy: 0.603\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.4505 | test accuracy: 0.603\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.7530 | test accuracy: 0.603\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6345 | test accuracy: 0.603\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6474 | test accuracy: 0.603\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6442 | test accuracy: 0.603\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6159 | test accuracy: 0.603\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6236 | test accuracy: 0.603\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.4882 | test accuracy: 0.603\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.6301 | test accuracy: 0.603\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6455 | test accuracy: 0.603\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.8380 | test accuracy: 0.603\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6334 | test accuracy: 0.603\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6505 | test accuracy: 0.603\n",
            "Epoch:  42 Iteration:  3010 | train loss: 1.0015 | test accuracy: 0.603\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6552 | test accuracy: 0.603\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5987 | test accuracy: 0.603\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.8124 | test accuracy: 0.603\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.4552 | test accuracy: 0.603\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6575 | test accuracy: 0.603\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.4624 | test accuracy: 0.603\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8346 | test accuracy: 0.603\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.4523 | test accuracy: 0.603\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.4619 | test accuracy: 0.603\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6498 | test accuracy: 0.603\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.7628 | test accuracy: 0.603\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.5870 | test accuracy: 0.603\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.9721 | test accuracy: 0.603\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4914 | test accuracy: 0.603\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.4799 | test accuracy: 0.603\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7877 | test accuracy: 0.603\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.8096 | test accuracy: 0.603\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.8322 | test accuracy: 0.603\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6320 | test accuracy: 0.603\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6458 | test accuracy: 0.603\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.8113 | test accuracy: 0.603\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.8361 | test accuracy: 0.603\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.4597 | test accuracy: 0.603\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.7673 | test accuracy: 0.603\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.7510 | test accuracy: 0.603\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.7965 | test accuracy: 0.603\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5019 | test accuracy: 0.603\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.8136 | test accuracy: 0.603\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.4714 | test accuracy: 0.603\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6310 | test accuracy: 0.603\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.6017 | test accuracy: 0.603\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4796 | test accuracy: 0.603\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.8624 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6455 | test accuracy: 0.603\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.6135 | test accuracy: 0.603\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8000 | test accuracy: 0.603\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.8190 | test accuracy: 0.603\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4804 | test accuracy: 0.603\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.6626 | test accuracy: 0.603\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.8136 | test accuracy: 0.603\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.8153 | test accuracy: 0.603\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.9877 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4768 | test accuracy: 0.603\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.7977 | test accuracy: 0.603\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6505 | test accuracy: 0.603\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6731 | test accuracy: 0.603\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.7858 | test accuracy: 0.603\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.9793 | test accuracy: 0.603\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.6505 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.8198 | test accuracy: 0.603\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.8310 | test accuracy: 0.603\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.8654 | test accuracy: 0.603\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.6384 | test accuracy: 0.603\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6105 | test accuracy: 0.603\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.8022 | test accuracy: 0.603\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.6199 | test accuracy: 0.603\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7634 | test accuracy: 0.603\n",
            "total time:  31.357650167999964\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6637 | test accuracy: 0.603\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5791 | test accuracy: 0.603\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8141 | test accuracy: 0.603\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4556 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3149 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6545 | test accuracy: 0.603\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.8478 | test accuracy: 0.603\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5839 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6291 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7417 | test accuracy: 0.603\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.9195 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5805 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5733 | test accuracy: 0.603\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6285 | test accuracy: 0.603\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.8749 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6739 | test accuracy: 0.603\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5922 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.7451 | test accuracy: 0.603\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.7077 | test accuracy: 0.603\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6601 | test accuracy: 0.603\n",
            "Epoch:  20 Iteration:  1470 | train loss: 1.0264 | test accuracy: 0.603\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.8230 | test accuracy: 0.603\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4799 | test accuracy: 0.603\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.5126 | test accuracy: 0.603\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.5639 | test accuracy: 0.603\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.4727 | test accuracy: 0.603\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.8436 | test accuracy: 0.603\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.8989 | test accuracy: 0.603\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.5375 | test accuracy: 0.603\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6120 | test accuracy: 0.603\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.9278 | test accuracy: 0.603\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.5351 | test accuracy: 0.603\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.8038 | test accuracy: 0.603\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.4229 | test accuracy: 0.603\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.4462 | test accuracy: 0.603\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.7900 | test accuracy: 0.603\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.5606 | test accuracy: 0.603\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.9742 | test accuracy: 0.603\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6003 | test accuracy: 0.603\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6332 | test accuracy: 0.603\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6255 | test accuracy: 0.603\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6119 | test accuracy: 0.603\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.7923 | test accuracy: 0.603\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.8149 | test accuracy: 0.603\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.5864 | test accuracy: 0.603\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.7420 | test accuracy: 0.603\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6244 | test accuracy: 0.603\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.4521 | test accuracy: 0.603\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7555 | test accuracy: 0.603\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.5904 | test accuracy: 0.603\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.5641 | test accuracy: 0.603\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6052 | test accuracy: 0.603\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.5298 | test accuracy: 0.603\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6141 | test accuracy: 0.603\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4730 | test accuracy: 0.603\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.8211 | test accuracy: 0.603\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4520 | test accuracy: 0.603\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.9935 | test accuracy: 0.603\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.6875 | test accuracy: 0.603\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.3999 | test accuracy: 0.603\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6451 | test accuracy: 0.603\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.7659 | test accuracy: 0.603\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.5489 | test accuracy: 0.603\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.9128 | test accuracy: 0.603\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7838 | test accuracy: 0.603\n",
            "Epoch:  65 Iteration:  4620 | train loss: 1.0905 | test accuracy: 0.603\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.8707 | test accuracy: 0.603\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.8873 | test accuracy: 0.603\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.4405 | test accuracy: 0.603\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.5613 | test accuracy: 0.603\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6725 | test accuracy: 0.603\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6349 | test accuracy: 0.603\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6049 | test accuracy: 0.603\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8319 | test accuracy: 0.603\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.4759 | test accuracy: 0.603\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.5136 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.4621 | test accuracy: 0.603\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.8328 | test accuracy: 0.603\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6292 | test accuracy: 0.603\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.9441 | test accuracy: 0.603\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.8044 | test accuracy: 0.603\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.5041 | test accuracy: 0.603\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5137 | test accuracy: 0.603\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.5027 | test accuracy: 0.603\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.8481 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.7347 | test accuracy: 0.603\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.5000 | test accuracy: 0.603\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6157 | test accuracy: 0.603\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.8990 | test accuracy: 0.603\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.4350 | test accuracy: 0.603\n",
            "Epoch:  90 Iteration:  6370 | train loss: 1.0500 | test accuracy: 0.603\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7737 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7972 | test accuracy: 0.603\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.6589 | test accuracy: 0.603\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6751 | test accuracy: 0.603\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4936 | test accuracy: 0.603\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.6349 | test accuracy: 0.603\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.5014 | test accuracy: 0.603\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.7497 | test accuracy: 0.603\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.7866 | test accuracy: 0.603\n",
            "total time:  35.31445981199977\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2706723213195801.\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epoch took: 0.4522838592529297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6828500534806933 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24740839004516602.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.43366169929504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.54945430627891 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661404609680176.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.457242488861084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46476923184735436 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619974613189697.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4537630081176758\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4191282323428563 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27995753288269043.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.46657729148864746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.38966519023690904 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2644691467285156.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4476664066314697\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3745900882141931 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25787949562072754.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4422740936279297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3617106501545225 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26036548614501953.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4408752918243408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3525204530784062 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25539231300354004.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.43417954444885254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34743621477058956 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2517988681793213.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.44442033767700195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3414128452539444 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25412964820861816.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4372575283050537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.338486071569579 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2780303955078125.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.47836875915527344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33597678627286637 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2569236755371094.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4428558349609375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33210060937064034 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571525573730469.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4419388771057129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32958575700010573 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25994181632995605.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4365353584289551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3280917802027294 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25473618507385254.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4373321533203125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32679920324257444 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25029993057250977.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4346506595611572\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3253514766693115 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25633859634399414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4387073516845703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3247118021760668 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24884605407714844.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4318699836730957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32335254081657955 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2538325786590576.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42601752281188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32206031169210164 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254223108291626.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43410778045654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32154213190078734 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535736560821533.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4257636070251465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3209534040519169 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25342822074890137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4403557777404785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3204880978379931 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25525784492492676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4453246593475342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.31962533593177794 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25388193130493164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.436229944229126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3190044492483139 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2625582218170166.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4601442813873291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31866506593567984 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561490535736084.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4520435333251953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3185204301561628 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26204657554626465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45094895362854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3178823454039437 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547125816345215.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44208264350891113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31764337037290846 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25840234756469727.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4524695873260498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3174127587250301 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27544713020324707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4585075378417969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31703433053834096 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2683117389678955.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45221996307373047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31673438719340735 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2450416088104248.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4268934726715088\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3167084625789097 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2460165023803711.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4289226531982422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.3163947475808007 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25698065757751465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43457937240600586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3162385791540146 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26041364669799805.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4346010684967041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3161380584750857 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26209306716918945.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4407620429992676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3160682248217719 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25890660285949707.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44177865982055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31582741013595034 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2717270851135254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4474961757659912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3157921344041824 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24823999404907227.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44426417350769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3155356330530984 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2549564838409424.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4374687671661377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3154536438839776 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25751614570617676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4370589256286621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3152602323463985 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2656280994415283.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4549839496612549\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3153693050146103 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25641727447509766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4396238327026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31497567125729153 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25993990898132324.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43947768211364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3151122570037842 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583446502685547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43691182136535645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31506542818886896 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25447654724121094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4463231563568115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3148418447801045 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260451078414917.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4437441825866699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31483903314386097 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25585150718688965.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.447509765625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3147704588515418 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2802469730377197.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47263455390930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31456504421574727 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2539825439453125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4310131072998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31465795976775035 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25905442237854004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44319653511047363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31454005667141505 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2646663188934326.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4463803768157959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31457621114594597 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25328612327575684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4529566764831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3144602916070393 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546837329864502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4422299861907959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3145032048225403 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516200542449951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487600326538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31446833525385176 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2690088748931885.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4521453380584717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3143793429647173 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26333022117614746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4472923278808594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3141987234354019 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27683067321777344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47989535331726074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142166167497635 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601892948150635.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4499661922454834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.31424192019871305 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2709639072418213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4519999027252197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3141502435718264 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577197551727295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4349963665008545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3141957802431924 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24826478958129883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4382190704345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31404428013733454 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25826478004455566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4443552494049072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31404583624431065 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25879716873168945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4487953186035156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31408932975360326 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2693355083465576.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4608447551727295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31392563113144467 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504422664642334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4272325038909912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139061825616019 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27051591873168945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4527597427368164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31395275550229207 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2571232318878174.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4428732395172119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.313912987283298 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28295016288757324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4912862777709961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139584720134735 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719576358795166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4584825038909912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3139177773680006 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26713085174560547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4791374206542969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31391989503587997 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536804676055908.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4367690086364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138622603246144 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25653982162475586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46831655502319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3139485503946032 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597768306732178.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4450035095214844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31388992837497165 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25241518020629883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43303728103637695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31388718911579677 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2746603488922119.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4648005962371826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31382732050759454 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24390912055969238.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42490291595458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3137959028993334 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27405881881713867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4582340717315674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31379671479974475 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2503995895385742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42588329315185547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31373587122985297 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25771212577819824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4533195495605469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137375955070768 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24791789054870605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4222285747528076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.313732037799699 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2525041103363037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43715643882751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31372351263250625 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26290202140808105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389662742614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137211978435516 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24095535278320312.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41474485397338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136906717504774 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2557039260864258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4553682804107666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3137025088071823 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25481343269348145.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4296715259552002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3136920149837221 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2486705780029297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4571714401245117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136887937784195 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2643404006958008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4500389099121094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136770827429635 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2676393985748291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46127867698669434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31364480810506 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2653353214263916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4572463035583496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.313645487172263 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25110602378845215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43135857582092285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136136106082371 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27766847610473633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4621894359588623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136802558388029 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25420665740966797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45306396484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136360760246004 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26799798011779785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4538848400115967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31361022634165625 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2474839687347412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4263138771057129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135920460735049 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2614459991455078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4512786865234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31359329223632815 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2677314281463623.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4460916519165039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135843732527324 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2529153823852539.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44457173347473145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135768273047039 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2538142204284668.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43650078773498535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135831790310996 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24876761436462402.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4365506172180176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3136172665016992 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24657511711120605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4324803352355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135812767914363 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24657821655273438.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42205142974853516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135718848024096 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2566554546356201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4445819854736328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31362038254737856 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582104206085205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4321095943450928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135535900081907 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26360535621643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4455087184906006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31357206404209137 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682657241821289.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.447451114654541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135398460286004 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.254941463470459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43251800537109375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31353494014058797 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2551088333129883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4440474510192871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31355981571333746 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2515218257904053.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43045783042907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135244940008436 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25430989265441895.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4374878406524658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31351673943655833 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2655830383300781.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.444594144821167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135507264307567 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649075984954834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45470142364501953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31349257358482907 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26663637161254883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4464430809020996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.313516326887267 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26711320877075195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45092153549194336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134869865008763 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546563148498535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44588303565979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31350107746464867 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26856064796447754.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46199488639831543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.313498455286026 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261249303817749.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4619009494781494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134714641741344 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647056579589844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45078086853027344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31347891220024654 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26201963424682617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44602298736572266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31350465289184026 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27233028411865234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4626731872558594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134675030197416 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2784302234649658.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4699058532714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3135135212114879 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261735200881958.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44348645210266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134874756847109 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2614462375640869.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4359555244445801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134701192378998 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27041029930114746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4702298641204834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134369445698602 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2705223560333252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46881723403930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134483107498714 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595372200012207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45494675636291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31343683557850976 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25816965103149414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45157861709594727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31345716416835784 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9568 | test accuracy: 0.603\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6492 | test accuracy: 0.603\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6541 | test accuracy: 0.603\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8061 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5067 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6393 | test accuracy: 0.603\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4580 | test accuracy: 0.603\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6410 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.6448 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.8314 | test accuracy: 0.603\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.8633 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.5338 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.8630 | test accuracy: 0.603\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.8582 | test accuracy: 0.603\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.7844 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6471 | test accuracy: 0.603\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.4898 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4776 | test accuracy: 0.603\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6481 | test accuracy: 0.603\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.6705 | test accuracy: 0.603\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.4599 | test accuracy: 0.603\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.6448 | test accuracy: 0.603\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6536 | test accuracy: 0.603\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.7951 | test accuracy: 0.603\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.4179 | test accuracy: 0.603\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5157 | test accuracy: 0.603\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4043 | test accuracy: 0.603\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.6594 | test accuracy: 0.603\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.4972 | test accuracy: 0.603\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6519 | test accuracy: 0.603\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.8085 | test accuracy: 0.603\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.6386 | test accuracy: 0.603\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7934 | test accuracy: 0.603\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.6404 | test accuracy: 0.603\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6256 | test accuracy: 0.603\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6503 | test accuracy: 0.603\n",
            "Epoch:  36 Iteration:  2590 | train loss: 1.0160 | test accuracy: 0.603\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7925 | test accuracy: 0.603\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.6833 | test accuracy: 0.603\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.6221 | test accuracy: 0.603\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.6462 | test accuracy: 0.603\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6277 | test accuracy: 0.603\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6408 | test accuracy: 0.603\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.7807 | test accuracy: 0.603\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.6129 | test accuracy: 0.603\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4652 | test accuracy: 0.603\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.5861 | test accuracy: 0.603\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6711 | test accuracy: 0.603\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.8083 | test accuracy: 0.603\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.8366 | test accuracy: 0.603\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.4724 | test accuracy: 0.603\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.6409 | test accuracy: 0.603\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6387 | test accuracy: 0.603\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.4570 | test accuracy: 0.603\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.4731 | test accuracy: 0.603\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.8466 | test accuracy: 0.603\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.4598 | test accuracy: 0.603\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.8713 | test accuracy: 0.603\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.8009 | test accuracy: 0.603\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.6762 | test accuracy: 0.603\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.7581 | test accuracy: 0.603\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6409 | test accuracy: 0.603\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.6750 | test accuracy: 0.603\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.6346 | test accuracy: 0.603\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.7663 | test accuracy: 0.603\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.5934 | test accuracy: 0.603\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6343 | test accuracy: 0.603\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.6271 | test accuracy: 0.603\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6252 | test accuracy: 0.603\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.9795 | test accuracy: 0.603\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.8088 | test accuracy: 0.603\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.6576 | test accuracy: 0.603\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.4489 | test accuracy: 0.603\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8496 | test accuracy: 0.603\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.8573 | test accuracy: 0.603\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.5495 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.6315 | test accuracy: 0.603\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.8191 | test accuracy: 0.603\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.6307 | test accuracy: 0.603\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.6258 | test accuracy: 0.603\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.9954 | test accuracy: 0.603\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.8025 | test accuracy: 0.603\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.7949 | test accuracy: 0.603\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.6303 | test accuracy: 0.603\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7982 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4942 | test accuracy: 0.603\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.8264 | test accuracy: 0.603\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.9800 | test accuracy: 0.603\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.6310 | test accuracy: 0.603\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6344 | test accuracy: 0.603\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.6454 | test accuracy: 0.603\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.7803 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.8028 | test accuracy: 0.603\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.8269 | test accuracy: 0.603\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6006 | test accuracy: 0.603\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.7607 | test accuracy: 0.603\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.8300 | test accuracy: 0.603\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6453 | test accuracy: 0.603\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.4536 | test accuracy: 0.603\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6977 | test accuracy: 0.603\n",
            "total time:  31.560631655999714\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.1381 | test accuracy: 0.603\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6131 | test accuracy: 0.603\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7899 | test accuracy: 0.603\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4404 | test accuracy: 0.603\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5306 | test accuracy: 0.603\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.6475 | test accuracy: 0.603\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5928 | test accuracy: 0.603\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7710 | test accuracy: 0.603\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4496 | test accuracy: 0.603\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5884 | test accuracy: 0.603\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.7717 | test accuracy: 0.603\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8909 | test accuracy: 0.603\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.7571 | test accuracy: 0.603\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.8214 | test accuracy: 0.603\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4467 | test accuracy: 0.603\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.6526 | test accuracy: 0.603\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.5051 | test accuracy: 0.603\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.4522 | test accuracy: 0.603\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.4838 | test accuracy: 0.603\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.7908 | test accuracy: 0.603\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.6129 | test accuracy: 0.603\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5140 | test accuracy: 0.603\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.6259 | test accuracy: 0.603\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.9146 | test accuracy: 0.603\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.8231 | test accuracy: 0.603\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.5950 | test accuracy: 0.603\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6099 | test accuracy: 0.603\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.4453 | test accuracy: 0.603\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.5106 | test accuracy: 0.603\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.6286 | test accuracy: 0.603\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.6385 | test accuracy: 0.603\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.8730 | test accuracy: 0.603\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.6057 | test accuracy: 0.603\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.4653 | test accuracy: 0.603\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.6404 | test accuracy: 0.603\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.6941 | test accuracy: 0.603\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.6133 | test accuracy: 0.603\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.5844 | test accuracy: 0.603\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.9142 | test accuracy: 0.603\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.4817 | test accuracy: 0.603\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.7979 | test accuracy: 0.603\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.6490 | test accuracy: 0.603\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.6184 | test accuracy: 0.603\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.6716 | test accuracy: 0.603\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.4669 | test accuracy: 0.603\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.4202 | test accuracy: 0.603\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.8551 | test accuracy: 0.603\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.6345 | test accuracy: 0.603\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.7689 | test accuracy: 0.603\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.6186 | test accuracy: 0.603\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.8553 | test accuracy: 0.603\n",
            "Epoch:  51 Iteration:  3640 | train loss: 1.0028 | test accuracy: 0.603\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.6145 | test accuracy: 0.603\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.6612 | test accuracy: 0.603\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.7759 | test accuracy: 0.603\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0842 | test accuracy: 0.603\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.6614 | test accuracy: 0.603\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.7392 | test accuracy: 0.603\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.7347 | test accuracy: 0.603\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.7783 | test accuracy: 0.603\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.6815 | test accuracy: 0.603\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.6376 | test accuracy: 0.603\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.4745 | test accuracy: 0.603\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.4923 | test accuracy: 0.603\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.4810 | test accuracy: 0.603\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.7043 | test accuracy: 0.603\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.6778 | test accuracy: 0.603\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5236 | test accuracy: 0.603\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.6999 | test accuracy: 0.603\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.6486 | test accuracy: 0.603\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.6315 | test accuracy: 0.603\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.7848 | test accuracy: 0.603\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.6584 | test accuracy: 0.603\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.8166 | test accuracy: 0.603\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.7859 | test accuracy: 0.603\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.5112 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.4516 | test accuracy: 0.603\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.8498 | test accuracy: 0.603\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.8988 | test accuracy: 0.603\n",
            "Epoch:  79 Iteration:  5600 | train loss: 1.0147 | test accuracy: 0.603\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4649 | test accuracy: 0.603\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.4743 | test accuracy: 0.603\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.5847 | test accuracy: 0.603\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.4720 | test accuracy: 0.603\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.7472 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.4581 | test accuracy: 0.603\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.6120 | test accuracy: 0.603\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.6421 | test accuracy: 0.603\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.8342 | test accuracy: 0.603\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.6246 | test accuracy: 0.603\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.8347 | test accuracy: 0.603\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.9644 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.4596 | test accuracy: 0.603\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.8553 | test accuracy: 0.603\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.6141 | test accuracy: 0.603\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.4508 | test accuracy: 0.603\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4758 | test accuracy: 0.603\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.6196 | test accuracy: 0.603\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.5606 | test accuracy: 0.603\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.6798 | test accuracy: 0.603\n",
            "total time:  35.646247380000204\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24855852127075195.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.435650110244751\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7003729207175119 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25864267349243164.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.46136999130249023\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5606513376746859 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2528247833251953.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.43207287788391113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.47340904133660455 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2519991397857666.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44950199127197266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.4249478885105678 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2501184940338135.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.4340965747833252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39367937913962775 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26427626609802246.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.44294142723083496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3793198334319251 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26878833770751953.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.44425535202026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.366345140337944 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2460179328918457.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4292881488800049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35654545639242446 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25665879249572754.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4499399662017822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34988312891551426 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2433924674987793.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.42382216453552246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3444520545857293 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26304030418395996.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4497537612915039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33911613268511637 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24679088592529297.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4267733097076416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33641521547521863 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25708866119384766.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43448472023010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.33475493831293923 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28011107444763184.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46718406677246094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3313990925039564 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632002830505371.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46002674102783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.3289647523845945 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27866697311401367.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46570920944213867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.327269874726023 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28417062759399414.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.47117137908935547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.325546857714653 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24994850158691406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43929600715637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3247963526419231 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2534902095794678.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4393577575683594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32342131989342826 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25876665115356445.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4542841911315918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32277499565056395 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24813032150268555.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4259521961212158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3215877145528793 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507336139678955.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4259004592895508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3211017161607742 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2516162395477295.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4253215789794922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.32104324783597676 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25205230712890625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42281651496887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.32022922592503683 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24054574966430664.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42980098724365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3196582359927041 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24777603149414062.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4197111129760742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31906389253480094 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2572460174560547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43879270553588867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3183873006275722 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2673499584197998.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44399189949035645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3186303470815931 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25605177879333496.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4414951801300049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31767523969922745 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613687515258789.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4444541931152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3178235586200442 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25507020950317383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4461345672607422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31738782439913066 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25910472869873047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4470498561859131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31707431631428856 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24974679946899414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.438417911529541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31678641651357925 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2449195384979248.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.41652512550354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31688701978751593 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25871825218200684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43956661224365234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31654731631278993 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527470588684082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43118739128112793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31611175281660897 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2663838863372803.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4518256187438965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3161465572459357 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26013779640197754.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44754981994628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3159865089825222 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2544271945953369.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46002197265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31578756400517055 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2800581455230713.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48122119903564453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31567927726677486 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629861831665039.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46056103706359863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3154196343251637 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2602219581604004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44217824935913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31558122762611934 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24796700477600098.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42151832580566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31551043135779244 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652397155761719.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4423344135284424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3155382709843772 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.252826452255249.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4210946559906006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3152462784733091 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2509338855743408.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4373810291290283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3150036198752267 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2663381099700928.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45450401306152344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3150440731218883 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26313328742980957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45378804206848145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31483599288122993 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24955320358276367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44735145568847656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31475146966321127 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24921679496765137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4209761619567871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31486755992685045 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26286864280700684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43795204162597656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146790564060211 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547447681427002.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42678356170654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3146643898316792 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24848604202270508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.41872382164001465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3147021915231432 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628669738769531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4423501491546631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31455992587975096 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24292564392089844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4274942874908447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3145367762872151 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2499392032623291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4297478199005127\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31442402388368335 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2508883476257324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42763566970825195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31434477865695953 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27480053901672363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.456087589263916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3143303717885699 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24393463134765625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4219686985015869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31431515301976887 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26108884811401367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.455873966217041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142931423016957 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2513246536254883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4389522075653076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31423874752862113 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25278282165527344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47239160537719727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31414363426821573 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2667818069458008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4553389549255371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.31412421337195806 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2604973316192627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44065284729003906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31406636025224416 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.270371675491333.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45366692543029785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3141041989837374 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559835910797119.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4317748546600342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31404746983732496 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2718372344970703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4464070796966553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3141346833535603 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25058984756469727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42325854301452637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31400461409773145 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25092196464538574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4455077648162842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3140805125236511 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523643970489502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43944787979125977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3140115205730711 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25039124488830566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43570923805236816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31393510230949945 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2648897171020508.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44962477684020996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138929124389376 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25380730628967285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.431164026260376\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3139781351600375 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.269451379776001.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44559264183044434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.313844690152577 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2504465579986572.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4300200939178467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31390276636396136 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25626349449157715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4560058116912842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3138172745704651 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2602212429046631.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45125460624694824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31383191730294907 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2602665424346924.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4571058750152588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3138478772980826 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2791268825531006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4723191261291504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31380359785897394 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2661116123199463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46106767654418945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3138104340859822 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26509785652160645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4685084819793701\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31381830487932477 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2872645854949951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4905741214752197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3137796827725002 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3004786968231201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4917442798614502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31373645407812933 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2551758289337158.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45026254653930664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31381078021866937 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2852778434753418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4698905944824219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137172907590866 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26566386222839355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45432329177856445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3137617800916944 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2669403553009033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46470046043395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3137036425726754 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2636539936065674.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44558072090148926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3137314409017563 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598843574523926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4411904811859131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136846329484667 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25756382942199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384620189666748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31364405368055615 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531430721282959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.439772367477417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31368480239595686 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25372958183288574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384958744049072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136897282941001 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23089838027954102.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.40434741973876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136587517602103 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2430419921875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4337794780731201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136315592697689 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25365471839904785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4337315559387207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31362525480134146 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25675225257873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43176770210266113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136749970061438 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2718815803527832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4523031711578369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136336871555873 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25563502311706543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43952107429504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135997670037406 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26125597953796387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43555712699890137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3136291627373014 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2475597858428955.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44640111923217773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3136016769068582 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2844393253326416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47943592071533203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31360802395003184 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25371646881103516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44359850883483887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31355406982558115 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26393747329711914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.454479455947876\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135828286409378 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25832104682922363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.457233190536499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135702750512532 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2828960418701172.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4689927101135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31360633500984736 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568531036376953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384005069732666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135580403464181 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2844212055206299.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4652423858642578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135357660906655 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26629066467285156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46057987213134766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135161267859595 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27788352966308594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4723024368286133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31355670137064795 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593076229095459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4524266719818115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135248209748949 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25730466842651367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4461781978607178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135305264166423 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2589375972747803.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.448697566986084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135140150785446 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26366162300109863.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4552907943725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135369815996715 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26764535903930664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4604811668395996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31350366600922175 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27477335929870605.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4581618309020996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31350547202995843 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26728034019470215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.462752103805542\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3135256826877594 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609894275665283.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4384918212890625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31351029830319543 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2528417110443115.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43257856369018555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3135272396462304 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27664995193481445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4886810779571533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31348014899662563 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2828216552734375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46509790420532227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31348721640450616 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25208330154418945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45218849182128906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.313480572615351 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632253170013428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45958924293518066\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134582941021238 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2545313835144043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4361732006072998\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31347345794950215 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26670122146606445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45244860649108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31348224197115215 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577035427093506.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44197511672973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31346027851104735 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28188085556030273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4868133068084717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134606561490468 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2761380672454834.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47646212577819824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134553922074182 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27556371688842773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4879872798919678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134766859667642 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6404 | test accuracy: 0.781\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5230 | test accuracy: 0.781\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6235 | test accuracy: 0.781\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3143 | test accuracy: 0.785\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.7217 | test accuracy: 0.791\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4752 | test accuracy: 0.795\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4085 | test accuracy: 0.852\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5402 | test accuracy: 0.835\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2865 | test accuracy: 0.892\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2397 | test accuracy: 0.882\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5278 | test accuracy: 0.939\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2367 | test accuracy: 0.943\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1130 | test accuracy: 0.956\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0936 | test accuracy: 0.973\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0723 | test accuracy: 0.976\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0705 | test accuracy: 0.980\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0118 | test accuracy: 0.980\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0258 | test accuracy: 0.980\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0050 | test accuracy: 0.980\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0318 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0030 | test accuracy: 0.980\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0249 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0163 | test accuracy: 0.987\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0049 | test accuracy: 0.987\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0014 | test accuracy: 0.983\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0295 | test accuracy: 0.987\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0123 | test accuracy: 0.983\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0066 | test accuracy: 0.987\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0114 | test accuracy: 0.980\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0254 | test accuracy: 0.987\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.2628 | test accuracy: 0.976\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0005 | test accuracy: 0.987\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0044 | test accuracy: 0.987\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0252 | test accuracy: 0.987\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0038 | test accuracy: 0.987\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0215 | test accuracy: 0.987\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.1279 | test accuracy: 0.987\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0013 | test accuracy: 0.987\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0037 | test accuracy: 0.987\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0206 | test accuracy: 0.987\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0016 | test accuracy: 0.987\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0050 | test accuracy: 0.987\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0065 | test accuracy: 0.987\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0337 | test accuracy: 0.987\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0018 | test accuracy: 0.987\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0003 | test accuracy: 0.987\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0600 | test accuracy: 0.987\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0044 | test accuracy: 0.987\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0007 | test accuracy: 0.987\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0040 | test accuracy: 0.987\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0185 | test accuracy: 0.987\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0045 | test accuracy: 0.987\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0045 | test accuracy: 0.987\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0087 | test accuracy: 0.987\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0088 | test accuracy: 0.987\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0035 | test accuracy: 0.987\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0025 | test accuracy: 0.987\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0031 | test accuracy: 0.987\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0149 | test accuracy: 0.987\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0013 | test accuracy: 0.987\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0141 | test accuracy: 0.987\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0023 | test accuracy: 0.987\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0018 | test accuracy: 0.987\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0008 | test accuracy: 0.987\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0073 | test accuracy: 0.987\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0016 | test accuracy: 0.987\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.4004 | test accuracy: 0.987\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0004 | test accuracy: 0.987\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0003 | test accuracy: 0.987\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0014 | test accuracy: 0.987\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0127 | test accuracy: 0.987\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0023 | test accuracy: 0.987\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0273 | test accuracy: 0.987\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0023 | test accuracy: 0.987\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0015 | test accuracy: 0.987\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0037 | test accuracy: 0.987\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0058 | test accuracy: 0.987\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0083 | test accuracy: 0.987\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0014 | test accuracy: 0.987\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0817 | test accuracy: 0.987\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0069 | test accuracy: 0.987\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0136 | test accuracy: 0.987\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0048 | test accuracy: 0.987\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0047 | test accuracy: 0.987\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0065 | test accuracy: 0.987\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0013 | test accuracy: 0.987\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0105 | test accuracy: 0.987\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0030 | test accuracy: 0.987\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0010 | test accuracy: 0.987\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0045 | test accuracy: 0.987\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0153 | test accuracy: 0.987\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0007 | test accuracy: 0.987\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.2458 | test accuracy: 0.987\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0003 | test accuracy: 0.987\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.2727 | test accuracy: 0.987\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0019 | test accuracy: 0.987\n",
            "total time:  32.256772324999474\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.4475 | test accuracy: 0.781\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3962 | test accuracy: 0.781\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.1832 | test accuracy: 0.781\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.3210 | test accuracy: 0.781\n",
            "Epoch:  4 Iteration:  350 | train loss: 1.0870 | test accuracy: 0.781\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3013 | test accuracy: 0.785\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2200 | test accuracy: 0.781\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2058 | test accuracy: 0.781\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5049 | test accuracy: 0.785\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3930 | test accuracy: 0.795\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2750 | test accuracy: 0.832\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.2626 | test accuracy: 0.801\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1070 | test accuracy: 0.801\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1221 | test accuracy: 0.852\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5918 | test accuracy: 0.852\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3292 | test accuracy: 0.872\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3226 | test accuracy: 0.875\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0638 | test accuracy: 0.865\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.6152 | test accuracy: 0.896\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.2009 | test accuracy: 0.936\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1443 | test accuracy: 0.960\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0232 | test accuracy: 0.980\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.3611 | test accuracy: 0.973\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1473 | test accuracy: 0.983\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1502 | test accuracy: 0.983\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0559 | test accuracy: 0.983\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0745 | test accuracy: 0.983\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2442 | test accuracy: 0.990\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0286 | test accuracy: 0.990\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0802 | test accuracy: 0.990\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0275 | test accuracy: 0.990\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0754 | test accuracy: 0.990\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0147 | test accuracy: 0.990\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.3404 | test accuracy: 0.990\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0020 | test accuracy: 0.990\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0105 | test accuracy: 0.990\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0102 | test accuracy: 0.990\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0904 | test accuracy: 0.990\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0309 | test accuracy: 0.990\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0117 | test accuracy: 0.990\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0046 | test accuracy: 0.990\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0300 | test accuracy: 0.990\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0132 | test accuracy: 0.990\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0086 | test accuracy: 0.990\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0169 | test accuracy: 0.990\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0380 | test accuracy: 0.990\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0467 | test accuracy: 0.990\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0184 | test accuracy: 0.990\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0085 | test accuracy: 0.990\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0452 | test accuracy: 0.990\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0156 | test accuracy: 0.990\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0198 | test accuracy: 0.990\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0220 | test accuracy: 0.990\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0133 | test accuracy: 0.990\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0085 | test accuracy: 0.990\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0049 | test accuracy: 0.990\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0069 | test accuracy: 0.990\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0150 | test accuracy: 0.990\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0715 | test accuracy: 0.990\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.1994 | test accuracy: 0.990\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.2157 | test accuracy: 0.990\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0177 | test accuracy: 0.990\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0137 | test accuracy: 0.990\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0117 | test accuracy: 0.990\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0173 | test accuracy: 0.990\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0367 | test accuracy: 0.990\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0050 | test accuracy: 0.990\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0043 | test accuracy: 0.990\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0130 | test accuracy: 0.990\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0543 | test accuracy: 0.990\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0097 | test accuracy: 0.990\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0544 | test accuracy: 0.990\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0088 | test accuracy: 0.987\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0965 | test accuracy: 0.990\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0473 | test accuracy: 0.990\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0094 | test accuracy: 0.987\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0286 | test accuracy: 0.987\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.5606 | test accuracy: 0.987\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0344 | test accuracy: 0.987\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0090 | test accuracy: 0.987\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0222 | test accuracy: 0.987\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0196 | test accuracy: 0.990\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0122 | test accuracy: 0.990\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0211 | test accuracy: 0.987\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0139 | test accuracy: 0.990\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0245 | test accuracy: 0.987\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0054 | test accuracy: 0.990\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0042 | test accuracy: 0.987\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0143 | test accuracy: 0.987\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0171 | test accuracy: 0.990\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0119 | test accuracy: 0.990\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0113 | test accuracy: 0.990\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2852 | test accuracy: 0.990\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0019 | test accuracy: 0.990\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0164 | test accuracy: 0.987\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0213 | test accuracy: 0.990\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0053 | test accuracy: 0.987\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0799 | test accuracy: 0.990\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0303 | test accuracy: 0.987\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0184 | test accuracy: 0.990\n",
            "total time:  35.84352769299949\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26029467582702637.\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epoch took: 0.44864511489868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5231896834714073 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628440856933594.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.4612236022949219\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.44380107266562324 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27067995071411133.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4609394073486328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.398727091721126 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2751801013946533.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4586303234100342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3746487374816622 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2701089382171631.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4505579471588135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.36156595179012846 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619771957397461.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4491446018218994\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3517757241215025 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27524328231811523.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4530949592590332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3430892301457269 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2634713649749756.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4424436092376709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.33846908296857564 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26071858406066895.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4554176330566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.3352351669754301 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2586941719055176.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4434230327606201\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3316329483475004 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2634246349334717.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.45984840393066406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.32900711936610083 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2634549140930176.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4354438781738281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.32690766794340953 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527923583984375.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.43489909172058105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32554075292178564 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24705982208251953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42000436782836914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32342525678021566 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2645704746246338.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4425487518310547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.322851403270449 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512688636779785.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4406862258911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3217568746634892 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25256872177124023.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43457531929016113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.3210346136774336 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25273871421813965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4244844913482666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3202016532421112 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25758814811706543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4283003807067871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31973561091082436 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2549278736114502.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4322190284729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.31911773596491133 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25230908393859863.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4365425109863281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3183306187391281 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25510406494140625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4361300468444824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3178646458046777 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2669062614440918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45879435539245605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31756832216467173 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25603485107421875.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44852662086486816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3174437344074249 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25853919982910156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4375121593475342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3169576210635049 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2761845588684082.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48106908798217773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31691772512027194 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.265331506729126.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4429962635040283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3164106356246131 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679874897003174.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45480775833129883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31648903914860316 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.262876033782959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4481375217437744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31626161805221015 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25886964797973633.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.456756591796875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3158895113638469 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26473546028137207.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4546070098876953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31579923672335486 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547271251678467.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4444124698638916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31548749421324046 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26892924308776855.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4567587375640869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31534880527428216 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682230472564697.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48685359954833984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.315486655490739 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2727842330932617.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.472092866897583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31535417991025105 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2712101936340332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45282673835754395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3149995301451002 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613396644592285.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4440476894378662\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3150698810815811 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24869132041931152.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4154183864593506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31475769238812584 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25010156631469727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43029308319091797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31485629081726074 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25597381591796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4383728504180908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3148305505514145 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24811482429504395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42952728271484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31469413552965436 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25870800018310547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4345886707305908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3145997077226639 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577168941497803.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4365503787994385\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31452438363007135 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25603437423706055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43961453437805176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31441423296928406 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2469773292541504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4254443645477295\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3144352857555662 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2448878288269043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4380621910095215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31433872665677753 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2550532817840576.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43358302116394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31435718749250685 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26232171058654785.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45677852630615234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3142375030687877 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27225589752197266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45636892318725586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31413040799754005 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25539278984069824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4361124038696289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31424985357693264 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777583599090576.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4655179977416992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3141357545341764 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25232982635498047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43309640884399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3141475966998509 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25646090507507324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4532654285430908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3140857730593 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2506446838378906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42372727394104004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31399239642279486 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2517843246459961.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45151638984680176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31408683231898715 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26282310485839844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4473710060119629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3140252321958542 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628173828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4761013984680176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140012204647064 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2653937339782715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45539426803588867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31390032683100016 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24658417701721191.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42951154708862305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31393834693091255 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27561163902282715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4517173767089844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3139275908470154 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2506747245788574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42397522926330566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31392097984041484 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.23869562149047852.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42146825790405273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31390210092067716 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24554157257080078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4247925281524658\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3138757292713438 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2564849853515625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43518805503845215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3138471590621131 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2582969665527344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4421408176422119\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31384424822671075 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24639177322387695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4211246967315674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31389416456222535 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27143073081970215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45609235763549805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3138048908540181 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26174378395080566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4489409923553467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31374506737504687 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25717806816101074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45093798637390137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137732897486005 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2521493434906006.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4284172058105469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3137676234756197 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2659571170806885.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46536827087402344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31375431290694644 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2757418155670166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4544239044189453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31376942225864957 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259509801864624.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44209909439086914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.31369093230792455 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2688260078430176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44835782051086426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31367564584527696 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2590322494506836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.436138391494751\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31368546187877655 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25467729568481445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4481062889099121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.3136838619198118 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2490403652191162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42166757583618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137457877397537 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25618958473205566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.434859037399292\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31368212188993183 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2835862636566162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46539783477783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3136413080351693 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24342656135559082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4213120937347412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3136080013854163 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2720367908477783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44467854499816895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31363851853779384 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.247969388961792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4265146255493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31364009763513295 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2559623718261719.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4417266845703125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31361146867275236 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532165050506592.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42891693115234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3136295216424125 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25220775604248047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4212639331817627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3135775246790477 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603492736816406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4413456916809082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135852941444942 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24660277366638184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43564391136169434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135786269392286 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2650437355041504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4476642608642578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31356628494603295 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25504302978515625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4328024387359619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31360401894365036 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2596144676208496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4528923034667969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31355363300868444 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2633650302886963.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4531881809234619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135468695844923 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2623751163482666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4433932304382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135308521134513 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26473283767700195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4534139633178711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31357923660959514 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27166318893432617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44756579399108887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31352055626256126 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597684860229492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44086503982543945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31352881108011516 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2598581314086914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44350719451904297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.31351399421691895 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2658815383911133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44878721237182617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31351994701794217 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25807952880859375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4514024257659912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135366814477103 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2511920928955078.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4390437602996826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135142739330019 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2527186870574951.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.431516170501709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135253246341433 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25898265838623047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43953704833984375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31353690368788584 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27796459197998047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4650754928588867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3134972810745239 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2727034091949463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4632136821746826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3134953737258911 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597231864929199.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4442119598388672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3134887682540076 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2691960334777832.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4470984935760498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31350542051451546 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2479097843170166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42791032791137695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31348240545817785 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2449345588684082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42184925079345703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31346162855625154 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651090621948242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.441957950592041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.313468542269298 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2597801685333252.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4304840564727783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134871567998614 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24675250053405762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.42049074172973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.313447134409632 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.255098819732666.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4293324947357178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31347705125808717 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26396989822387695.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44145750999450684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134742770876203 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24907183647155762.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401242733001709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134728708437511 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578918933868408.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4388570785522461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134599766560963 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696225643157959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44565439224243164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134481877088547 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2697336673736572.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45789361000061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31346126198768615 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27066683769226074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4505743980407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31345206073352266 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26984405517578125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46177029609680176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31342036936964307 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26612401008605957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44733142852783203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31342531953539166 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2507960796356201.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44662928581237793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31343975492886134 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26462626457214355.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4528946876525879\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134491034916469 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25682640075683594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44353628158569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134341686964035 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25676441192626953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44437217712402344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31342281997203825 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26641845703125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44708681106567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31341721543243956 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27350783348083496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4715099334716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134262101990836 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26264500617980957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4448847770690918\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31343083679676054 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2642486095428467.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4576075077056885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134304927928107 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568695545196533.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.427997350692749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134083632911955 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6973 | test accuracy: 0.781\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.8721 | test accuracy: 0.781\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6243 | test accuracy: 0.781\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4274 | test accuracy: 0.785\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1419 | test accuracy: 0.785\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.8235 | test accuracy: 0.785\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4819 | test accuracy: 0.838\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.1066 | test accuracy: 0.801\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3188 | test accuracy: 0.842\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3705 | test accuracy: 0.835\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3071 | test accuracy: 0.869\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.3074 | test accuracy: 0.909\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1582 | test accuracy: 0.929\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0697 | test accuracy: 0.923\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.1412 | test accuracy: 0.963\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0673 | test accuracy: 0.963\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0101 | test accuracy: 0.966\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0366 | test accuracy: 0.980\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0996 | test accuracy: 0.980\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0048 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0018 | test accuracy: 0.983\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1454 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1795 | test accuracy: 0.987\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0065 | test accuracy: 0.987\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0116 | test accuracy: 0.987\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1466 | test accuracy: 0.987\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0053 | test accuracy: 0.987\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0023 | test accuracy: 0.983\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0043 | test accuracy: 0.987\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0778 | test accuracy: 0.987\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0166 | test accuracy: 0.987\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0123 | test accuracy: 0.987\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0021 | test accuracy: 0.987\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0023 | test accuracy: 0.987\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0171 | test accuracy: 0.987\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0008 | test accuracy: 0.987\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0576 | test accuracy: 0.987\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0111 | test accuracy: 0.987\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0092 | test accuracy: 0.987\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0091 | test accuracy: 0.987\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0075 | test accuracy: 0.987\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.3555 | test accuracy: 0.987\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0699 | test accuracy: 0.987\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0054 | test accuracy: 0.987\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0061 | test accuracy: 0.987\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0011 | test accuracy: 0.987\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0391 | test accuracy: 0.987\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0134 | test accuracy: 0.987\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0151 | test accuracy: 0.987\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0025 | test accuracy: 0.987\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0249 | test accuracy: 0.987\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0051 | test accuracy: 0.987\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0059 | test accuracy: 0.987\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0038 | test accuracy: 0.987\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0052 | test accuracy: 0.987\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0105 | test accuracy: 0.987\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0095 | test accuracy: 0.987\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0274 | test accuracy: 0.987\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0007 | test accuracy: 0.987\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0013 | test accuracy: 0.987\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0061 | test accuracy: 0.987\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0115 | test accuracy: 0.987\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0087 | test accuracy: 0.987\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0027 | test accuracy: 0.987\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0008 | test accuracy: 0.987\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0030 | test accuracy: 0.987\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0078 | test accuracy: 0.987\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0068 | test accuracy: 0.987\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0088 | test accuracy: 0.987\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0063 | test accuracy: 0.987\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0040 | test accuracy: 0.987\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0005 | test accuracy: 0.987\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0805 | test accuracy: 0.987\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0052 | test accuracy: 0.987\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0572 | test accuracy: 0.987\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.1234 | test accuracy: 0.987\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0081 | test accuracy: 0.987\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.1920 | test accuracy: 0.987\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0021 | test accuracy: 0.987\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0117 | test accuracy: 0.987\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0009 | test accuracy: 0.987\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0022 | test accuracy: 0.987\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0757 | test accuracy: 0.987\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0246 | test accuracy: 0.987\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0030 | test accuracy: 0.987\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0410 | test accuracy: 0.987\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0038 | test accuracy: 0.987\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0019 | test accuracy: 0.987\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0004 | test accuracy: 0.987\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0003 | test accuracy: 0.987\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0121 | test accuracy: 0.987\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0007 | test accuracy: 0.987\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0019 | test accuracy: 0.983\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0005 | test accuracy: 0.987\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0043 | test accuracy: 0.987\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0272 | test accuracy: 0.987\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0010 | test accuracy: 0.987\n",
            "total time:  31.964960352999697\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8079 | test accuracy: 0.781\n",
            "Epoch:  1 Iteration:  140 | train loss: 1.0169 | test accuracy: 0.781\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2451 | test accuracy: 0.781\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.2721 | test accuracy: 0.781\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5420 | test accuracy: 0.781\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3131 | test accuracy: 0.785\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2543 | test accuracy: 0.795\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.3759 | test accuracy: 0.818\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.2724 | test accuracy: 0.835\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.1069 | test accuracy: 0.842\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1860 | test accuracy: 0.852\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8508 | test accuracy: 0.859\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0550 | test accuracy: 0.872\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3509 | test accuracy: 0.865\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.2378 | test accuracy: 0.912\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4991 | test accuracy: 0.929\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.0386 | test accuracy: 0.949\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2023 | test accuracy: 0.919\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0495 | test accuracy: 0.963\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0641 | test accuracy: 0.976\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2559 | test accuracy: 0.980\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1672 | test accuracy: 0.980\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0925 | test accuracy: 0.983\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0974 | test accuracy: 0.987\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0448 | test accuracy: 0.987\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0501 | test accuracy: 0.987\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1233 | test accuracy: 0.987\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0167 | test accuracy: 0.987\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0645 | test accuracy: 0.987\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0296 | test accuracy: 0.987\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0204 | test accuracy: 0.987\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0408 | test accuracy: 0.987\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0030 | test accuracy: 0.987\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0222 | test accuracy: 0.987\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0301 | test accuracy: 0.987\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0159 | test accuracy: 0.987\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0115 | test accuracy: 0.987\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0213 | test accuracy: 0.987\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0152 | test accuracy: 0.987\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0414 | test accuracy: 0.987\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0051 | test accuracy: 0.987\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0117 | test accuracy: 0.987\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0013 | test accuracy: 0.987\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0072 | test accuracy: 0.987\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0215 | test accuracy: 0.987\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0070 | test accuracy: 0.987\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0031 | test accuracy: 0.987\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0157 | test accuracy: 0.987\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0181 | test accuracy: 0.987\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0257 | test accuracy: 0.987\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0087 | test accuracy: 0.987\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0047 | test accuracy: 0.987\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0128 | test accuracy: 0.987\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0075 | test accuracy: 0.987\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0084 | test accuracy: 0.987\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0495 | test accuracy: 0.987\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0269 | test accuracy: 0.987\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.1747 | test accuracy: 0.987\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0047 | test accuracy: 0.987\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0016 | test accuracy: 0.987\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0184 | test accuracy: 0.987\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0073 | test accuracy: 0.987\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0042 | test accuracy: 0.987\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0293 | test accuracy: 0.987\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0129 | test accuracy: 0.987\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0141 | test accuracy: 0.987\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0069 | test accuracy: 0.987\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0054 | test accuracy: 0.987\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0025 | test accuracy: 0.987\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0035 | test accuracy: 0.987\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0174 | test accuracy: 0.987\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0335 | test accuracy: 0.987\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0175 | test accuracy: 0.987\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1938 | test accuracy: 0.987\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0184 | test accuracy: 0.987\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0106 | test accuracy: 0.987\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0076 | test accuracy: 0.987\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0199 | test accuracy: 0.987\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0097 | test accuracy: 0.987\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0072 | test accuracy: 0.987\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0008 | test accuracy: 0.987\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0202 | test accuracy: 0.987\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0088 | test accuracy: 0.987\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0064 | test accuracy: 0.990\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0040 | test accuracy: 0.990\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0130 | test accuracy: 0.987\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0146 | test accuracy: 0.987\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0068 | test accuracy: 0.987\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0123 | test accuracy: 0.987\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0009 | test accuracy: 0.987\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0724 | test accuracy: 0.987\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0100 | test accuracy: 0.987\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0048 | test accuracy: 0.987\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0124 | test accuracy: 0.987\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0091 | test accuracy: 0.987\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0054 | test accuracy: 0.987\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0031 | test accuracy: 0.987\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1922 | test accuracy: 0.990\n",
            "total time:  36.24927076200038\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25265073776245117.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.43874549865722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6912251489503043 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654597759246826.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.4403069019317627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.547656272990363 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26006484031677246.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.4454917907714844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46469037319932666 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2512025833129883.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4436984062194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41756738552025385 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2558472156524658.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.44365406036376953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3883104217903955 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25164222717285156.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.4328935146331787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3750184702021735 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25522899627685547.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4404125213623047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36050349644252233 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605400085449219.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4469327926635742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35271705516747065 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2609572410583496.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4421873092651367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34611920373780386 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.261920690536499.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43859052658081055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3418806382587978 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26331067085266113.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.459261417388916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33776164736066544 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.270585298538208.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4556148052215576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3335027354104178 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2615633010864258.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.45402956008911133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3316184124776295 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2704946994781494.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4709606170654297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32952661173684256 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26161956787109375.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44301772117614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32756825728075845 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27271461486816406.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4679069519042969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3262856547321592 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561004161834717.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4372286796569824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32479463304792133 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629828453063965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.443112850189209\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3242137951510293 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26138877868652344.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4444434642791748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3226032274109977 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2547891139984131.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44362688064575195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.32174146941729953 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26595449447631836.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4600083827972412\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3217536470719746 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26288509368896484.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45562291145324707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3203488209417888 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26542186737060547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46178245544433594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.319938628588404 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25820398330688477.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4357335567474365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3188473050083433 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.267902135848999.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.461989164352417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31935038694313594 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26293373107910156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4402580261230469\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31866445881979805 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27065110206604004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45829248428344727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3180825080190386 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2681894302368164.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45244669914245605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.317842658502715 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25719237327575684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4539759159088135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3171347630875451 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2690560817718506.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45411014556884766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3169421924012048 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2513139247894287.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42444825172424316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31681650919573645 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27720069885253906.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4564979076385498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3164630400282996 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2457268238067627.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.429781436920166\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3164671923433031 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26946401596069336.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.457348108291626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31642163012708935 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26589107513427734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4467606544494629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31594955665724617 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25722336769104004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46613025665283203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31603127334799086 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2651047706604004.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4594459533691406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31578168528420586 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26270532608032227.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4624903202056885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31548691945416585 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2865619659423828.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4798247814178467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.315414765051433 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2620809078216553.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44614315032958984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31533569480691637 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2650179862976074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44410133361816406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.3153838745185307 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25792479515075684.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4427471160888672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3151231382574354 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2849602699279785.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47853875160217285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.315162935427257 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26500701904296875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45388293266296387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31497374858175003 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2881019115447998.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48452043533325195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31498510880129676 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27296876907348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46190643310546875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3149276716368539 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2804076671600342.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47347497940063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3147275196654456 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25843238830566406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4425632953643799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.31480407076222555 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2523071765899658.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43534302711486816\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31464067101478577 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.273296594619751.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4467151165008545\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3145599901676178 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2446901798248291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4232640266418457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31456506678036283 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.267223596572876.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4448740482330322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31456424636500224 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25585007667541504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43551182746887207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31437624990940094 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26948118209838867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46122074127197266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143741318157741 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2710392475128174.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46569013595581055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143875905445644 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26795244216918945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5052201747894287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3144994463239397 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2797255516052246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46385788917541504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31427174423422133 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2793290615081787.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4774174690246582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31419258798871724 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27057909965515137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45719146728515625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3141968050173351 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26035547256469727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4596741199493408\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142026028462819 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28200721740722656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46883416175842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3140974129949297 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25255703926086426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43582606315612793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.3140682131052017 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27050137519836426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45078349113464355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141681458268847 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607901096343994.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4391179084777832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3139967007296426 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26015615463256836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44841718673706055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140382345233645 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649977207183838.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4427211284637451\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.31396077147551943 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565646171569824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4372227191925049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3139406940766743 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26955199241638184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4480726718902588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31390865274838037 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25455355644226074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43886852264404297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139222996575492 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26984333992004395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45256710052490234\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.31384905789579665 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2634849548339844.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4448246955871582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31390403296266284 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2708470821380615.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46468234062194824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138410393680845 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25482988357543945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43564748764038086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138164588383266 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2685966491699219.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4567525386810303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3139358695064272 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27168726921081543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45004987716674805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31380358806678227 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605173587799072.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4552769660949707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31382312732083456 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27022838592529297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45737481117248535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3137568312031882 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2731473445892334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46657657623291016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31384780406951907 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2687218189239502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.450878381729126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137685490506036 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27524542808532715.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48316264152526855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31376348776476726 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27162766456604004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47101640701293945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137370479958398 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26252031326293945.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45551609992980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.3136927753686905 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27597641944885254.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4755098819732666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31368512298379625 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25612473487854004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45314908027648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31374461225100925 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26305222511291504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44722890853881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.31369934124605997 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2695000171661377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45264673233032227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31366043942315236 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2778792381286621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4576406478881836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31363776624202727 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29409074783325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47718119621276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136745938232967 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2613048553466797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4742445945739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3136297255754471 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27071714401245117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4529590606689453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136425793170929 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2595975399017334.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44454383850097656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31363026584897724 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25536656379699707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44463467597961426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136443061488015 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26425886154174805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4789605140686035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31362865907805304 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27488040924072266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5038421154022217\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3136164009571075 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27050232887268066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46522068977355957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135832497051784 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679893970489502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4524056911468506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135569568191256 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2834446430206299.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4846630096435547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3135777665036065 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26552844047546387.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45095252990722656\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31359759867191317 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2796590328216553.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4713263511657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135851877076285 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27039670944213867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.457592248916626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135700626032693 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2687408924102783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4932870864868164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135458162852696 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2913634777069092.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4846799373626709\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135784489767892 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26197385787963867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4584484100341797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31350940976824077 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28075504302978516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4803352355957031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135479237352099 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2731618881225586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47047924995422363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135523991925376 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2678189277648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4516024589538574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.313545355626515 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25943446159362793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4340364933013916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31351544729300906 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2658379077911377.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4509921073913574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3135292696101325 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26428866386413574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4568142890930176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135357013770512 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27330899238586426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4775235652923584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3135007568768093 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29071474075317383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47644543647766113\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135158108813422 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26745152473449707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4621872901916504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351217244352614 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2694876194000244.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45534205436706543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134979741913932 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2637014389038086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45947909355163574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31351521696363177 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2593059539794922.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44054460525512695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31351279275757926 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2514030933380127.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44393038749694824\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3135005533695221 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27855992317199707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4646732807159424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31350024385111674 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2546062469482422.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4350013732910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31346106103488375 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27158498764038086.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46407008171081543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134638679879052 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27440476417541504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48376941680908203\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31346086263656614 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2879369258880615.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4936237335205078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134433201381138 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28360605239868164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47986650466918945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31346351802349093 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2714273929595947.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4872729778289795\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134640966142927 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26291871070861816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45627903938293457\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134555480309895 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2639658451080322.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4681525230407715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.313480401464871 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2774343490600586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46953535079956055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134391392980303 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25879979133605957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4602341651916504\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31346698573657444 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2479240894317627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4323406219482422\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31343557962349483 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6763 | test accuracy: 0.781\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.2859 | test accuracy: 0.781\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.2760 | test accuracy: 0.781\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6525 | test accuracy: 0.781\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1614 | test accuracy: 0.781\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4027 | test accuracy: 0.798\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3972 | test accuracy: 0.825\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7617 | test accuracy: 0.832\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.1375 | test accuracy: 0.815\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6586 | test accuracy: 0.859\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4805 | test accuracy: 0.865\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4664 | test accuracy: 0.902\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0654 | test accuracy: 0.912\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.0428 | test accuracy: 0.936\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.0508 | test accuracy: 0.970\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1553 | test accuracy: 0.970\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1945 | test accuracy: 0.976\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1042 | test accuracy: 0.976\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3156 | test accuracy: 0.983\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0053 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0137 | test accuracy: 0.987\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0484 | test accuracy: 0.987\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0383 | test accuracy: 0.980\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0280 | test accuracy: 0.980\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0185 | test accuracy: 0.980\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0175 | test accuracy: 0.987\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0032 | test accuracy: 0.980\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0098 | test accuracy: 0.983\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0106 | test accuracy: 0.987\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0017 | test accuracy: 0.980\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0078 | test accuracy: 0.983\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0137 | test accuracy: 0.987\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0029 | test accuracy: 0.987\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0110 | test accuracy: 0.987\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0028 | test accuracy: 0.987\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0170 | test accuracy: 0.987\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0051 | test accuracy: 0.987\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0078 | test accuracy: 0.987\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0041 | test accuracy: 0.987\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0045 | test accuracy: 0.987\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0176 | test accuracy: 0.987\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0144 | test accuracy: 0.987\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0095 | test accuracy: 0.987\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0116 | test accuracy: 0.987\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0081 | test accuracy: 0.987\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0101 | test accuracy: 0.987\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3296 | test accuracy: 0.987\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0050 | test accuracy: 0.987\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0869 | test accuracy: 0.987\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0213 | test accuracy: 0.987\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0060 | test accuracy: 0.987\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0022 | test accuracy: 0.987\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0206 | test accuracy: 0.987\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0166 | test accuracy: 0.987\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0067 | test accuracy: 0.987\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0016 | test accuracy: 0.987\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0017 | test accuracy: 0.987\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0130 | test accuracy: 0.987\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0171 | test accuracy: 0.987\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0033 | test accuracy: 0.987\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0357 | test accuracy: 0.987\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0184 | test accuracy: 0.987\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0066 | test accuracy: 0.987\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0983 | test accuracy: 0.987\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0028 | test accuracy: 0.987\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0016 | test accuracy: 0.987\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.4167 | test accuracy: 0.987\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0012 | test accuracy: 0.987\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0047 | test accuracy: 0.987\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0018 | test accuracy: 0.987\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0061 | test accuracy: 0.987\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0298 | test accuracy: 0.987\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0034 | test accuracy: 0.987\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0006 | test accuracy: 0.987\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0054 | test accuracy: 0.987\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0393 | test accuracy: 0.987\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0026 | test accuracy: 0.987\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0690 | test accuracy: 0.987\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0036 | test accuracy: 0.987\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0030 | test accuracy: 0.987\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0014 | test accuracy: 0.987\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0029 | test accuracy: 0.987\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0008 | test accuracy: 0.987\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0007 | test accuracy: 0.987\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0012 | test accuracy: 0.987\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0013 | test accuracy: 0.987\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0193 | test accuracy: 0.987\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0183 | test accuracy: 0.987\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0007 | test accuracy: 0.987\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0023 | test accuracy: 0.987\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0074 | test accuracy: 0.987\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0014 | test accuracy: 0.987\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0061 | test accuracy: 0.987\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0037 | test accuracy: 0.983\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0114 | test accuracy: 0.987\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0037 | test accuracy: 0.983\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0019 | test accuracy: 0.987\n",
            "total time:  32.32123613699969\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.0000 | test accuracy: 0.781\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.1561 | test accuracy: 0.781\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6517 | test accuracy: 0.781\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8745 | test accuracy: 0.781\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5555 | test accuracy: 0.781\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.7531 | test accuracy: 0.781\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7819 | test accuracy: 0.785\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6305 | test accuracy: 0.785\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3259 | test accuracy: 0.835\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.3640 | test accuracy: 0.838\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.5345 | test accuracy: 0.842\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1343 | test accuracy: 0.842\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.1415 | test accuracy: 0.852\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.6248 | test accuracy: 0.865\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.3061 | test accuracy: 0.882\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.1923 | test accuracy: 0.882\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.1470 | test accuracy: 0.909\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.0925 | test accuracy: 0.902\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0897 | test accuracy: 0.956\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1289 | test accuracy: 0.970\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1261 | test accuracy: 0.976\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1646 | test accuracy: 0.980\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.5893 | test accuracy: 0.976\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0522 | test accuracy: 0.980\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1935 | test accuracy: 0.980\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0417 | test accuracy: 0.987\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0162 | test accuracy: 0.987\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.1141 | test accuracy: 0.987\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.2148 | test accuracy: 0.987\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0072 | test accuracy: 0.987\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0084 | test accuracy: 0.990\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0437 | test accuracy: 0.987\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0490 | test accuracy: 0.987\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0463 | test accuracy: 0.987\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0489 | test accuracy: 0.987\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0786 | test accuracy: 0.987\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0118 | test accuracy: 0.987\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0385 | test accuracy: 0.987\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0218 | test accuracy: 0.987\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0493 | test accuracy: 0.987\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.2311 | test accuracy: 0.987\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0024 | test accuracy: 0.987\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0305 | test accuracy: 0.987\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0046 | test accuracy: 0.987\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0043 | test accuracy: 0.987\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0335 | test accuracy: 0.987\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0117 | test accuracy: 0.987\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0123 | test accuracy: 0.987\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0029 | test accuracy: 0.987\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0188 | test accuracy: 0.987\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0307 | test accuracy: 0.987\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0052 | test accuracy: 0.987\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0336 | test accuracy: 0.987\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0062 | test accuracy: 0.987\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0379 | test accuracy: 0.987\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0028 | test accuracy: 0.987\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0088 | test accuracy: 0.987\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0025 | test accuracy: 0.987\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0207 | test accuracy: 0.987\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0044 | test accuracy: 0.987\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0096 | test accuracy: 0.987\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0072 | test accuracy: 0.987\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0132 | test accuracy: 0.987\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0287 | test accuracy: 0.987\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.2102 | test accuracy: 0.987\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0035 | test accuracy: 0.987\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0130 | test accuracy: 0.987\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0066 | test accuracy: 0.987\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0063 | test accuracy: 0.987\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0104 | test accuracy: 0.987\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0515 | test accuracy: 0.987\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0073 | test accuracy: 0.987\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0027 | test accuracy: 0.987\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0933 | test accuracy: 0.987\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0202 | test accuracy: 0.987\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0098 | test accuracy: 0.987\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0118 | test accuracy: 0.983\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0038 | test accuracy: 0.983\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.3312 | test accuracy: 0.983\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0709 | test accuracy: 0.987\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4427 | test accuracy: 0.987\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0084 | test accuracy: 0.987\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0032 | test accuracy: 0.987\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0040 | test accuracy: 0.987\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0097 | test accuracy: 0.987\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0141 | test accuracy: 0.983\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0052 | test accuracy: 0.983\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.3848 | test accuracy: 0.987\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0127 | test accuracy: 0.983\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0111 | test accuracy: 0.983\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0117 | test accuracy: 0.983\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0064 | test accuracy: 0.983\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0049 | test accuracy: 0.983\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0514 | test accuracy: 0.983\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0167 | test accuracy: 0.983\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0108 | test accuracy: 0.983\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0025 | test accuracy: 0.987\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0029 | test accuracy: 0.987\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0078 | test accuracy: 0.987\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0461 | test accuracy: 0.987\n",
            "total time:  36.663976634999926\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27622509002685547.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.4830806255340576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7034648963383265 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2761352062225342.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.4642305374145508\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5576398989983967 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2725374698638916.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.47802209854125977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4714836869921003 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2803635597229004.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.48073625564575195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.42190405087811605 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2890636920928955.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.48017287254333496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3961945491177695 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2858307361602783.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.47452259063720703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3781943815095084 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2579782009124756.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4496340751647949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.36519001509462085 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719910144805908.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4574563503265381\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.3552851872784751 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27089595794677734.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.4607722759246826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34563023235116686 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2864267826080322.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.47827863693237305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.34417945529733385 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2648313045501709.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.44956088066101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3386394739151001 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2740519046783447.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.468839168548584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33537290947777887 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721128463745117.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4620821475982666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3327331887824195 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28115129470825195.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.47966551780700684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3306316703557968 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26618051528930664.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46335649490356445\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32802198358944484 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2751600742340088.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4721503257751465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3267585418054036 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2758924961090088.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4629642963409424\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32559441115174975 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.276170015335083.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4855184555053711\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3247824613537107 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2855715751647949.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48458099365234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3235428090606417 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2775421142578125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4983985424041748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3224705627986363 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2780795097351074.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4675459861755371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3215979239770344 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26445579528808594.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46378064155578613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32035158063684194 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27140307426452637.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45836329460144043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3199403771332332 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632884979248047.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44809865951538086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.319718222958701 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2841055393218994.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4770660400390625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3190307753426688 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2555422782897949.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44446396827697754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3186328087534223 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2781200408935547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4664037227630615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.3182429360491889 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25276994705200195.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4470069408416748\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.3178717042718615 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27038049697875977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4701263904571533\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3175792340721403 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26200437545776367.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45569920539855957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3174822492258889 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2619295120239258.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4553706645965576\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3169473384107862 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260143518447876.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44237613677978516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.3169929938656943 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2605164051055908.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4348764419555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3167034102337701 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28418517112731934.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46607279777526855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31669782187257495 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700979709625244.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46151280403137207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.316262469121388 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27349352836608887.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4580979347229004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3159628412553242 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.282977819442749.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4804561138153076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.3161299079656601 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2857239246368408.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49285054206848145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31559343252863203 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26918601989746094.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4678657054901123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3156173118523189 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2810044288635254.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4919931888580322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3153479733637401 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26372718811035156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45513081550598145\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31549233709062846 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2764890193939209.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49100184440612793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31540502522672925 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27319979667663574.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4649019241333008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3154640993901661 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27123165130615234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4533882141113281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3152781614235469 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2691011428833008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.448199987411499\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3150990733078548 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2659907341003418.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.459153413772583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.314993074962071 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2861785888671875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4744744300842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.3149391485112054 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25548243522644043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4449293613433838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3149757700307029 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26208925247192383.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45918941497802734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3146500089338848 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26125407218933105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44736242294311523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3145845204591751 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26387619972229004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45238566398620605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3145684702055795 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26091432571411133.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44643568992614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.31463833366121563 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762162685394287.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.471665620803833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3144905320235661 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2814512252807617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.469010591506958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3143687307834625 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27589845657348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4739854335784912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3144886084965297 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2772562503814697.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4659905433654785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.3143355152436665 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2797839641571045.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48055291175842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3142717659473419 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28183913230895996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4804227352142334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31428245859486714 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2785971164703369.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5048606395721436\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.31431082103933605 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29329872131347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4936063289642334\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141550706965583 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2926468849182129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48685383796691895\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31423209607601166 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2743675708770752.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4643669128417969\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.314165027652468 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26515674591064453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46232175827026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141141755240304 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.280515193939209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48535752296447754\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3140557646751404 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2770957946777344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4778866767883301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140771005834852 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28382325172424316.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.477703332901001\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140545721564974 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26535844802856445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45934343338012695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.314040436063494 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26743221282958984.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45976781845092773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31399541582380025 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.272235631942749.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4618501663208008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3139088473149708 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28301000595092773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46727895736694336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139735711472375 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25853824615478516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4431934356689453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31392134938921246 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26215600967407227.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45714378356933594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3139247532401766 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25801849365234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45313167572021484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3139071358101709 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2663240432739258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.456866979598999\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.3138875833579472 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26302361488342285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4552125930786133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31383648344448634 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2649867534637451.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45163989067077637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31381149632590155 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24928522109985352.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4257991313934326\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31379426419734957 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696516513824463.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45166945457458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31382127829960416 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662529945373535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44691944122314453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3138213276863098 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26732873916625977.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4567875862121582\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31371291066919055 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2714712619781494.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46086740493774414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.313721467767443 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26896142959594727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47353124618530273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31376754002911705 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26931023597717285.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4640069007873535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137734404632023 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24927520751953125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4440586566925049\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137167036533356 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2722642421722412.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46344780921936035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136996124471937 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26676440238952637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46033692359924316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31372119188308717 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28617262840270996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4820389747619629\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31369894870689935 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2565329074859619.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43636322021484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.3136816514389856 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26967406272888184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4699513912200928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31367946692875454 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27187108993530273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45766544342041016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.3136949577501842 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26532673835754395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45307469367980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136015636580331 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26469969749450684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4546184539794922\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3136225155421666 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2848234176635742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4928445816040039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136322511093957 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2806434631347656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4741835594177246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31358485775334494 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2695341110229492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4656550884246826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3135968961885997 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2822558879852295.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47187209129333496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136036787714277 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2629115581512451.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4471011161804199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136070715529578 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28734707832336426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4699897766113281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135881368603025 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24772095680236816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4331977367401123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31357688094888414 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28710341453552246.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4776895046234131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31358254424163273 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27667760848999023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4701833724975586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.31357285933835166 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2684507369995117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46471285820007324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135770491191319 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28228187561035156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4701249599456787\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135391627039228 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27791428565979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47919535636901855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31354832095759255 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26825690269470215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44797754287719727\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.3135624889816557 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2561013698577881.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4641563892364502\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31358606602464406 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2762131690979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4745504856109619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31355572044849395 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27167701721191406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4685802459716797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.3134999717984881 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2813222408294678.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47001075744628906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3135313894067492 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2511124610900879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43480658531188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31351222566195897 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733426094055176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4706411361694336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3135143769638879 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654225826263428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45259594917297363\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135102242231369 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2926442623138428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4866468906402588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31357080723558156 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2676198482513428.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45230841636657715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3134960630110332 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2790648937225342.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.490156888961792\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3135006721530642 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27553629875183105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47280383110046387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134962754590171 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2701723575592041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4620347023010254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134710362979344 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2740354537963867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4618535041809082\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134961064372744 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26228761672973633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44689440727233887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3135033134903227 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28972887992858887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47464537620544434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31344217019421716 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2662785053253174.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4604485034942627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31348799637385777 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2844390869140625.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47632813453674316\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31347793681280955 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2859184741973877.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48511767387390137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134803307907922 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2843751907348633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4758002758026123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.313461497426033 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698249816894531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4684267044067383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.3134461692401341 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3168787956237793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5174665451049805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31345310381480623 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25727200508117676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4392812252044678\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134492938007627 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698087692260742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48340773582458496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.3134546480008534 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6195 | test accuracy: 0.606\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.4423 | test accuracy: 0.609\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7478 | test accuracy: 0.636\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.5817 | test accuracy: 0.660\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.8241 | test accuracy: 0.657\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4695 | test accuracy: 0.700\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.5019 | test accuracy: 0.724\n",
            "Epoch:  7 Iteration:  560 | train loss: 1.2577 | test accuracy: 0.751\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.5310 | test accuracy: 0.758\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.2317 | test accuracy: 0.754\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2702 | test accuracy: 0.798\n",
            "Epoch:  11 Iteration:  840 | train loss: 1.0188 | test accuracy: 0.838\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2754 | test accuracy: 0.852\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.1762 | test accuracy: 0.892\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.1169 | test accuracy: 0.859\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0438 | test accuracy: 0.909\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3073 | test accuracy: 0.936\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2299 | test accuracy: 0.973\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.0801 | test accuracy: 0.963\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0383 | test accuracy: 0.949\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1665 | test accuracy: 0.943\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0668 | test accuracy: 0.960\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1287 | test accuracy: 0.973\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0122 | test accuracy: 0.976\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.6997 | test accuracy: 0.966\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0066 | test accuracy: 0.943\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1103 | test accuracy: 0.953\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0142 | test accuracy: 0.963\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0130 | test accuracy: 0.973\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0086 | test accuracy: 0.963\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0253 | test accuracy: 0.966\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.2292 | test accuracy: 0.966\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0123 | test accuracy: 0.966\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0667 | test accuracy: 0.966\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0860 | test accuracy: 0.963\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0106 | test accuracy: 0.966\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0100 | test accuracy: 0.966\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0347 | test accuracy: 0.966\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0474 | test accuracy: 0.966\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0051 | test accuracy: 0.960\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0764 | test accuracy: 0.966\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0667 | test accuracy: 0.960\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.2037 | test accuracy: 0.966\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.1087 | test accuracy: 0.960\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0401 | test accuracy: 0.966\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.1043 | test accuracy: 0.960\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0645 | test accuracy: 0.960\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.4795 | test accuracy: 0.966\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0584 | test accuracy: 0.970\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0463 | test accuracy: 0.963\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0276 | test accuracy: 0.970\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.9529 | test accuracy: 0.960\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0142 | test accuracy: 0.960\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0150 | test accuracy: 0.970\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0135 | test accuracy: 0.966\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.3166 | test accuracy: 0.960\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0063 | test accuracy: 0.970\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0107 | test accuracy: 0.970\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0054 | test accuracy: 0.960\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0089 | test accuracy: 0.970\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1352 | test accuracy: 0.960\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.2005 | test accuracy: 0.963\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0117 | test accuracy: 0.963\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0114 | test accuracy: 0.970\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0078 | test accuracy: 0.966\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0189 | test accuracy: 0.960\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0099 | test accuracy: 0.963\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0024 | test accuracy: 0.963\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0179 | test accuracy: 0.970\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0026 | test accuracy: 0.970\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0093 | test accuracy: 0.970\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0196 | test accuracy: 0.960\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0360 | test accuracy: 0.956\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0077 | test accuracy: 0.960\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0140 | test accuracy: 0.960\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0111 | test accuracy: 0.963\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0014 | test accuracy: 0.956\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0646 | test accuracy: 0.956\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0872 | test accuracy: 0.960\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0652 | test accuracy: 0.956\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0098 | test accuracy: 0.960\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.1831 | test accuracy: 0.960\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1312 | test accuracy: 0.970\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0803 | test accuracy: 0.970\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0120 | test accuracy: 0.960\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0199 | test accuracy: 0.970\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0048 | test accuracy: 0.970\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0165 | test accuracy: 0.960\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0168 | test accuracy: 0.970\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0035 | test accuracy: 0.960\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0721 | test accuracy: 0.970\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0314 | test accuracy: 0.960\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.1789 | test accuracy: 0.960\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0631 | test accuracy: 0.960\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0295 | test accuracy: 0.970\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0053 | test accuracy: 0.970\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0032 | test accuracy: 0.956\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0655 | test accuracy: 0.960\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0432 | test accuracy: 0.970\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0255 | test accuracy: 0.966\n",
            "total time:  33.08469913799945\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.8771 | test accuracy: 0.606\n",
            "Epoch:  1 Iteration:  140 | train loss: 1.2032 | test accuracy: 0.606\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.7356 | test accuracy: 0.616\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4523 | test accuracy: 0.633\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.1998 | test accuracy: 0.606\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3967 | test accuracy: 0.697\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.4436 | test accuracy: 0.640\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6323 | test accuracy: 0.677\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4271 | test accuracy: 0.717\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6843 | test accuracy: 0.700\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2653 | test accuracy: 0.734\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4167 | test accuracy: 0.734\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.5300 | test accuracy: 0.788\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2777 | test accuracy: 0.815\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5107 | test accuracy: 0.801\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3351 | test accuracy: 0.788\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3348 | test accuracy: 0.822\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1943 | test accuracy: 0.896\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.2878 | test accuracy: 0.882\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.3781 | test accuracy: 0.916\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.3067 | test accuracy: 0.966\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0436 | test accuracy: 0.946\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1415 | test accuracy: 0.960\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1324 | test accuracy: 0.963\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.2497 | test accuracy: 0.949\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0902 | test accuracy: 0.960\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0729 | test accuracy: 0.963\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.5391 | test accuracy: 0.966\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0651 | test accuracy: 0.963\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0404 | test accuracy: 0.949\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0501 | test accuracy: 0.960\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.4867 | test accuracy: 0.956\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0192 | test accuracy: 0.956\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0126 | test accuracy: 0.956\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0571 | test accuracy: 0.956\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0157 | test accuracy: 0.956\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0188 | test accuracy: 0.956\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0228 | test accuracy: 0.956\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.4732 | test accuracy: 0.956\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0381 | test accuracy: 0.956\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0082 | test accuracy: 0.956\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0678 | test accuracy: 0.956\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1264 | test accuracy: 0.956\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0770 | test accuracy: 0.956\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0383 | test accuracy: 0.960\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0434 | test accuracy: 0.956\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0238 | test accuracy: 0.956\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0239 | test accuracy: 0.960\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.1544 | test accuracy: 0.960\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0137 | test accuracy: 0.956\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0196 | test accuracy: 0.956\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.5921 | test accuracy: 0.960\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0594 | test accuracy: 0.956\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0879 | test accuracy: 0.956\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0169 | test accuracy: 0.956\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0159 | test accuracy: 0.956\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.1961 | test accuracy: 0.956\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0147 | test accuracy: 0.956\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0096 | test accuracy: 0.956\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.2919 | test accuracy: 0.956\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.1592 | test accuracy: 0.956\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0213 | test accuracy: 0.956\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.4082 | test accuracy: 0.956\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0266 | test accuracy: 0.956\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0167 | test accuracy: 0.956\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3327 | test accuracy: 0.956\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0884 | test accuracy: 0.956\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0160 | test accuracy: 0.960\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0185 | test accuracy: 0.960\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0342 | test accuracy: 0.960\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0475 | test accuracy: 0.960\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.1695 | test accuracy: 0.960\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0262 | test accuracy: 0.960\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0740 | test accuracy: 0.956\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0132 | test accuracy: 0.960\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0459 | test accuracy: 0.960\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0445 | test accuracy: 0.960\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.1650 | test accuracy: 0.960\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0339 | test accuracy: 0.960\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0166 | test accuracy: 0.960\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0091 | test accuracy: 0.960\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0304 | test accuracy: 0.953\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0638 | test accuracy: 0.960\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1066 | test accuracy: 0.960\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0297 | test accuracy: 0.960\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0237 | test accuracy: 0.953\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.1981 | test accuracy: 0.960\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0036 | test accuracy: 0.960\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1169 | test accuracy: 0.960\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0139 | test accuracy: 0.960\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0351 | test accuracy: 0.960\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0089 | test accuracy: 0.963\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0117 | test accuracy: 0.963\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0228 | test accuracy: 0.960\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1031 | test accuracy: 0.960\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.1346 | test accuracy: 0.960\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0111 | test accuracy: 0.960\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0183 | test accuracy: 0.960\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0448 | test accuracy: 0.960\n",
            "Epoch:  99 Iteration:  7000 | train loss: 1.2757 | test accuracy: 0.960\n",
            "total time:  36.28251958400051\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2601447105407715.\n",
            "\n",
            "  Average training loss: 0.53\n",
            "  Training epoch took: 0.4515798091888428\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.5293023092406136 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2733161449432373.\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epoch took: 0.49052977561950684\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.443261473945209 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2707662582397461.\n",
            "\n",
            "  Average training loss: 0.40\n",
            "  Training epoch took: 0.4670398235321045\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4007493283067431 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25218963623046875.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epoch took: 0.4412546157836914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.3731015282017844 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26648640632629395.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.44441819190979004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3627154746225902 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25847411155700684.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.44145941734313965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.35093400052615575 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25838661193847656.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.441666841506958\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3435822486877441 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2551538944244385.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4356420040130615\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.338655971629279 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2673792839050293.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4616570472717285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.334978534919875 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25750041007995605.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44440340995788574\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3320446193218231 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25801634788513184.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.44758033752441406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3290418722799846 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2577056884765625.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4484729766845703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3269327976873943 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2531886100769043.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4372444152832031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.32605181336402894 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27469635009765625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4608418941497803\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.32422048449516294 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2747819423675537.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47203779220581055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32257672590868813 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.287311315536499.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48304128646850586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32192354457718986 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2802877426147461.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4777846336364746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32099934773785727 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3005962371826172.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4996049404144287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.32052546697003503 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2868208885192871.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4845600128173828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.31984839269093107 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2775247097015381.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48666882514953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3189041086605617 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26399970054626465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4611515998840332\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.3188210683209555 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2641918659210205.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47437429428100586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.31850299324308123 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2764759063720703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.469407320022583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.31768702609198435 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2847864627838135.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48884105682373047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3177811247961862 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26442408561706543.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45273447036743164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31695559024810793 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2657902240753174.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44464874267578125\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31707327195576257 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2672600746154785.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45326948165893555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.316651063305991 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2517850399017334.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.42784762382507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31639665109770637 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26866769790649414.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44234132766723633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.31610696017742157 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2537362575531006.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4325857162475586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3161331628050123 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25089359283447266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44128990173339844\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3158317915030888 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26030445098876953.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44400453567504883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31554035714694434 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25301337242126465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4243133068084717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31568269218717304 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2626650333404541.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.43677186965942383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31546873578003476 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24996256828308105.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4354257583618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3155134843928473 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2679862976074219.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4451630115509033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3151815095118114 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2486724853515625.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4296751022338867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31513428177152364 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2697629928588867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4691338539123535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3149634599685669 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26804161071777344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4638652801513672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.3148214782987322 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26319003105163574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44530439376831055\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.31472142338752745 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654087543487549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4584813117980957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31477875198636734 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27130651473999023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4539914131164551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.3145577358348029 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2748866081237793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4649314880371094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3144614462341581 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27631115913391113.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4643588066101074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.31450388601848056 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2751307487487793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4575967788696289\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.31456997181688034 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2753584384918213.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46718430519104004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31450964297567097 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27124929428100586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4665048122406006\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31452676483563013 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2620534896850586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46421384811401367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3142784812620708 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27109646797180176.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47408461570739746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.3143924776996885 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.259507417678833.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44470930099487305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.3142939648457936 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28104496002197266.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47043442726135254\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.314225948708398 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26969361305236816.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44916820526123047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3142227509192058 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25833630561828613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4370429515838623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3141011587211064 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2641870975494385.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44449329376220703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31414671838283537 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27159881591796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4530203342437744\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31406989225319454 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26251673698425293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44950366020202637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31409542390278405 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25311899185180664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4338998794555664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3140950645719256 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2570221424102783.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4403865337371826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31401976517268587 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27658510208129883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48306751251220703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3139761916228703 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28008031845092773.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47145676612854004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3139338514634541 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2716560363769531.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4595601558685303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.3139161897557122 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26833558082580566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4652388095855713\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.313915953040123 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.278231143951416.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48885011672973633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3139534162623542 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27570223808288574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46849918365478516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31382484138011935 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2921457290649414.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49015021324157715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3138732135295868 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29075169563293457.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4895644187927246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3137874918324607 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2794759273529053.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4823768138885498\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31379134953022003 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2808256149291992.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4855375289916992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3138539446251733 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29760265350341797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5095770359039307\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3137973236186164 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2806425094604492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48007869720458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.313774413721902 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28133559226989746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48417043685913086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3137404407773699 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2880711555480957.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49651575088500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3137373443160738 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2817552089691162.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.492995023727417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3137082874774933 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29389166831970215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4818441867828369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31370886521680014 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25820207595825195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4421098232269287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31367814796311516 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2693150043487549.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4479231834411621\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31367293809141433 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26630377769470215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4521627426147461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.3136565957750593 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27584242820739746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46218037605285645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.313657426408359 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2536792755126953.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4383859634399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.31360989468438283 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25237488746643066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46457934379577637\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.31362394733088356 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25586462020874023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4461233615875244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31361018419265746 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27877092361450195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48183131217956543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31367128108228953 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28455138206481934.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4743053913116455\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.31363706631319865 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26053285598754883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4524669647216797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31364759291921346 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27279233932495117.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45490360260009766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3136523263795035 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26102733612060547.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44562554359436035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3135611035994121 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.283125638961792.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47638869285583496\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3135640182665416 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26691150665283203.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45633506774902344\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31360336201531547 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2707705497741699.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4602949619293213\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31357462321008955 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2636582851409912.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4510321617126465\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31357280356543404 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.276195764541626.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4777674674987793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3135620700461524 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630581855773926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45012974739074707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31360138739858356 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25189733505249023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43752074241638184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.31357695162296295 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26123571395874023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44407224655151367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135814091988972 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24969029426574707.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4400806427001953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31355475655623843 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25946831703186035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4401512145996094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3135091142995017 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26365137100219727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44158220291137695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.31352495976856776 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2554152011871338.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44536757469177246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.3135731535298484 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25649309158325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4353148937225342\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135183947426932 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568812370300293.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4644286632537842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31350877029555185 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26711034774780273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44660496711730957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135278063161033 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25525617599487305.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44153547286987305\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135244676045009 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27361178398132324.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46048593521118164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135080563170569 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27582526206970215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46823835372924805\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.31348216703959875 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2839689254760742.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46514129638671875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31345594525337217 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26002073287963867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4714486598968506\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.31347547216074806 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2960238456726074.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49027538299560547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3134824786867414 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583146095275879.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4444692134857178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31346818549292427 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2770810127258301.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46786022186279297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.3134846900190626 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2610781192779541.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4430124759674072\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.3134858646563121 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2759575843811035.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47326231002807617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.3134703563792365 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698030471801758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45114755630493164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3134797751903534 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26686978340148926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4658851623535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.31346493661403657 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2892916202545166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48514223098754883\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31345551652567727 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27003002166748047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.487778902053833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134510317019054 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28534555435180664.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4729762077331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.31347253705774036 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2653801441192627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4598534107208252\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31344772917883734 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25304412841796875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44789671897888184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134573970522199 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25973939895629883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4495861530303955\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31344746649265287 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25732421875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4551994800567627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.3134254310812269 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2603943347930908.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4467177391052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134269348212651 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26689863204956055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45302677154541016\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134355093751635 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260012149810791.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4352116584777832\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31342702763421193 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25606703758239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.439882755279541\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.31343178153038026 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26941609382629395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4561958312988281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345115559441705 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26748013496398926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4656510353088379\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134180703333446 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.265460729598999.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.456073522567749\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134099977357047 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28184008598327637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4914255142211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31343151458672114 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAESCAYAAADwnNLKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZd7/8feZkoQ0IJCAdMwqJTRRsYCoFAnwLLCuIgjYfzZYUBcpAR5QVjo2sC3LKrK4CwoiPohxlUVEI4hgVMSFgCCdBEiDlCnn90fIkJAEgmQyCefzuq5czJk55TsDzCf3fZ9zH8M0TRMREbEsW6ALEBGRwFIQiIhYnIJARMTiFAQiIhanIBARsTgFgYiIxTkCXYBculq0aMHnn39O/fr1S7z29ttv8+677+JyuXC5XFxzzTVMmjSJQ4cO8ac//QmA7OxssrOzfdv/4Q9/oG/fvnTv3p0HHniAsWPHFtvnfffdx6+//sratWvLrGnDhg385S9/AeDEiRN4PB7q1q0LwKOPPsqAAQPK9d6OHDnCgw8+yP/93/+dc70xY8YQHx9Pt27dyrXf88nPz+eVV14hMTGRwjO/4+PjGT58OEFBQRVyDLEeQ9cRiL+UFQTr169n+vTpLFmyhKioKPLz83n66aepWbMmzz77rG+9FStWsGrVKt566y3fc/v372fgwIGEhYWRmJiIzVbQqE1LS2PgwIEA5wyCoubNm8fhw4d57rnnLvKdVp4nnniCnJwcZs+eTWRkJOnp6YwdO5bw8HDmzp0b6PKkmlLXkFS6HTt20LRpU6KiogAICgriueeeY8yYMeXaPiQkhCZNmrB582bfc2vWrOGGG2646Nq6devG/Pnz6dWrFwcPHmT37t0MHjyY3r1707NnT18LYP/+/bRu3RooCKyRI0eSkJBAr1696NOnDzt37gRg2LBhfPDBB0BBMK5cuZIBAwbQpUsXX8B5vV6mTp1K586dGTx4MH/9618ZNmxYidp27tzJ559/zsyZM4mMjASgVq1aTJs2jTvuuKPE8Uo7/htvvEGvXr2YOXMmU6dO9a13/PhxOnToQFZWFikpKQwdOpRevXrx+9//nh9++AGAkydPMnz4cHr37k337t2ZOHEiLpfroj9zCTwFgVS6G2+8kQ0bNjB27Fg+//xzsrOzCQ8PJzw8vNz7iI+PL9Yts3r1auLj4yukviNHjpCYmEiDBg2YNWsWt956K2vWrGHatGlMmDCh1C+/9evXc/fdd5OYmMh1113HokWLSt13SkoKK1eu5NVXX+X555/H4/Hw+eefs379ej755BNee+013n///VK33bRpEx06dKBWrVrFnq9Tp065Q9A0TRITE+nduzf/+c9/fM//5z//4frrrycsLIzhw4fTv39/EhMTmTJlCo8//jhut5uVK1cSGRnJmjVrSExMxG63k5KSUq7jStWmIJBK17p1a/75z3/i9XoZN24c119/PcOHD+fgwYPl3sdtt93G2rVrcblcHDhwgNzcXJo3b14h9d1yyy2+x6+++ioPPvggAFdffTV5eXmkpqaW2CY2NpY2bdoABe/v0KFDpe67f//+AMTFxZGXl8exY8fYvHkzt9xyC2FhYdSqVYu+ffuWum1GRgZ16tS5mLfme2/t2rXDNE1+/vlnAP7973/Tu3dvdu/ezbFjx3wtjKuvvpqoqCi2bt3q+3PDhg14vV6eeeYZWrVqdVH1SNWgwWIJiLZt2zJ79mxM02Tbtm289NJLPPnkkyxdurRc29esWZM2bdqwYcMGUlJS6N27d4XVVrNmTd/jL774gtdee40TJ05gGAamaeL1ektsExER4Xtst9vxeDyl7rtwPbvdDhR0C2VmZlKvXj3fOkUfF1W7dm2OHDly4W+oiKKtidtuu43PPvuMJk2asGXLFubMmcOOHTvIzc0t9nlmZ2eTnp5O7969ycjI4KWXXmL37t3069eP8ePHa5D6EqAWgVS6zZs3+77QDMOgTZs2jB49mh07dlzQfvr27UtiYiIff/wxffr0qfA6XS4XTzzxBI899hiJiYmsWrUKwzAq/Djh4eGcOnXKt1xaiwOgU6dOJCcnlwiDzMxMXnrpJUzTxGazFQuqjIyMMo/bq1cv1q5dy4YNG7j22msJDw8nJiaGsLAwPv74Y9/Phg0b6NmzJwCDBg3i3Xff5aOPPmLbtm2sXLnyYt66VBEKAql0H374IZMnTyY7OxsAt9vN6tWrufbaay9oP927d2fTpk3Y7XYaN25c4XXm5ORw6tQpX5fPokWLcDqdxb60K0Lbtm1Zt24dubm5ZGZmsmbNmlLXi42NpU+fPjz11FOkpaUBkJ6ezlNPPeVrsURHR/u6e7Zu3cqePXvKPO5VV13FsWPHWLFiha8F0LBhQ+rXr8/HH38MFAwiP/XUU5w6dYpXXnmF9957DyhotTRq1MgvwSiVT11D4lfDhg3zdYMA/OUvf2HChAm88MIL/PGPfwQKguC6665j+vTpF7Tv0NBQ2rdvT9u2bSu05kKRkZE89NBDDBgwgDp16vDYY4/Ro0cPHn30Ud54440KO07Pnj1Zt24d8fHxNG3alN69e5OUlFTqulOnTuW1115jyJAhGIaB0+mkX79+vnGM+++/n6eeeor169fTqVMnOnfuXOZxDcOgR48evPvuu75TTw3D4Pnnn2fKlCm8+OKL2Gw27r//fkJDQ+nfvz/jx49nwYIFGIZB+/btfWMeUr3pOgKRKsA0Td9v10uWLOGrr77ilVdeCXBVYhXqGhIJsO3bt9O9e3cyMjJwu9188skndOjQIdBliYWoa0gkwFq1asWAAQO4/fbbsdvtdOjQgaFDhwa6LLEQdQ2JiFicuoZERCyuWnUN5ebm8uOPPxIdHV3sTBQRESmbx+MhNTWVNm3aEBISUuL1ahUEP/74I0OGDAl0GSIi1dKSJUu45pprSjxfrYIgOjoaKHgzpc1xLyIiJR0+fJghQ4b4vkPPVq2CoLA7qH79+jRq1CjA1YiIVC9ldalrsFhExOIUBCIiFqcgEBGxOAWBiIjFKQhERCxOQSAiYnGWCYLUrDw6z1jLrtTsQJciIhY1Y8YMhg0bRnx8PDfffDPDhg1jxIgR593uySefJDc31291VavrCC7G0axcDqTnsPNINrHR4YEuR0QsaNy4cQCsWLGCnTt3Mnbs2HJt98ILL/izLOsEQbCj4EKKfE/JG4+LiATKuHHjcDqdpKenM336dP785z9z6tQpcnNzmTRpEu3ataNbt258+OGHTJ06lZiYGLZt28bBgweZM2cOcXFxF12DhYKgoBcsz+UJcCUiUhUs/3Y/yzbvq9B9DrymMX+8+sJnPahZsyZTp07ll19+4c4776RHjx4kJSWxYMEC5s2bV2zd/Px8Fi5cyD//+U9WrlypILgQviBwq0UgIlVLu3btAKhbty6vvvoqCxcuJD8/n9DQ0BLrFk4aV79+fb7//vsKOb6FgqCga0hBICIAf7y60W/67d0fnE4nAIsWLaJevXrMnj2bH374gVmzZpVYt+h8QRV1XzHLnDUU7CxsEahrSESqphMnTtCkSRMAPv30U1wuV6Uc1zJBEGQveKv5ahGISBXVv39/3nzzTR544AHatWtHamoqy5cv9/txq9U9i/fv30/37t357LPPftM01FdOWMODNzVnbHxLP1QnIlI1ne+70zItAoAgh408l1oEIiJFWSoIgh02jRGIiJzFckGgMQIRkeKsFQROu04fFRE5i7WCQF1DIiIlWCoIghw2tQhERM5iqSAI1llDIiIlWCwI7Jp9VETkLBYLAo0RiIiczVpB4FTXkIjI2SwVBEF2DRaLiJzNUkEQ7LCra0hE5CzWCgKnriwWETmbtYJA1xGIiJRgsSDQFBMiImfz660qp02bRnJyMoZhkJCQ4LsvJ0C3bt2oX7++77Zrc+bMoV69eufc5mIFOWx4vCZujxeH3VIZKCJSJr8FwaZNm9i7dy9Lly5l165dJCQksHTp0mLrLFiwgLCwsAva5mIU3sA+X0EgIuLjt2/DpKQkevToAUBsbCwZGRlkZ2dX+DYXojAIdC2BiMgZfguCtLQ0ateu7VuOiooiNTW12DqTJ09m8ODBzJkzB9M0y7XNxQh2FnRDaZxAROQMv44RFHX2rZFHjhzJTTfdRM2aNRk+fDiJiYnn3eZi+VoEupZARMTHb0EQExNDWlqab/no0aNER0f7lgcMGOB73LVrV3bs2HHebS5WkC8I1CIQESnkt66hzp07+37L37ZtGzExMYSHhwOQlZXFgw8+SH5+PgDffPMNV1xxxTm3qQjBjoKuIV1UJiJyht9aBB07diQuLo5BgwZhGAaTJ09mxYoVRERE0LNnT7p27cpdd91FcHAwrVu3Jj4+HsMwSmxTkdQ1JCJSkl/HCEaPHl1suWXLlr7H9957L/fee+95t6lIOmtIRKQkS51MrzECEZGSLBUEhWMECgIRkTOsFQROjRGIiJzNWkGgriERkRIsFgTqGhIROZulgsA3WOxS15CISCFLBUHR2UdFRKSAJYNA1xGIiJxhqSAwDIMg3a5SRKQYSwUBFN63WGMEIiKFLBoEahGIiBSyYBDYNfuoiEgRFgwCtQhERIqyXBAEOWy6jkBEpAjLBUGw064WgYhIEdYLArtNYwQiIkVYLwicOn1URKQo6wWBBotFRIqxYBBojEBEpCgLBoG6hkREirJcEAQ5NFgsIlKU5YJAYwQiIsVZLwicdk1DLSJShPWCQGMEIiLFWDIIvCa4dZcyERHAgkHgu2+xxglERAALBkGwww4oCEREClkwCApbBBonEBEBKwaBUzewFxEpynJBEGQv6BrK12CxiAhgwSDwdQ2pRSAiAlgxCJwaIxARKcp6QaCzhkREirFgEKhFICJSlOWCoPCCMs1AKiJSwHJBEKwri0VEirFeEDhPjxHorCEREQAc/tz5tGnTSE5OxjAMEhISaNeuXYl15s6dy3fffcfixYvZuHEjo0aN4oorrgDgyiuvZNKkSRVak8YIRESK81sQbNq0ib1797J06VJ27dpFQkICS5cuLbZOSkoK33zzDU6n0/dcp06dePnll/1VlrqGRETO4reuoaSkJHr06AFAbGwsGRkZZGdnF1tnxowZPPnkk/4qoVSafVREpDi/BUFaWhq1a9f2LUdFRZGamupbXrFiBZ06daJhw4bFtktJSeHRRx9l8ODBfPnllxVeV5BdQSAiUpRfxwiKMk3T9zg9PZ0VK1bw5ptvcuTIEd/zzZo1Y8SIEfTu3Zt9+/Zxzz338MknnxAUFFRhdRiGobuUiYgU4bcWQUxMDGlpab7lo0ePEh0dDcDXX3/N8ePHGTJkCCNGjGDbtm1MmzaNevXq0adPHwzDoEmTJtStW7dYUFSUYIdNZw2JiJzmtyDo3LkziYmJAGzbto2YmBjCw8MBiI+P56OPPmLZsmXMnz+fuLg4EhISWLVqFQsXLgQgNTWVY8eOUa9evQqvLdhp1+yjIiKn+a1rqGPHjsTFxTFo0CAMw2Dy5MmsWLGCiIgIevbsWeo23bp1Y/To0Xz22We4XC6mTJlSod1ChYLsahGIiBTy6xjB6NGjiy23bNmyxDqNGjVi8eLFAISHh/P666/7sySgYAZSjRGIiBSw3JXFUDADaa5LQSAiAhYNgogQB1m57kCXISJSJVgyCCJDnAoCEZHTzhsE2dnZ/PLLL0DBtBFvvfUWx48f93th/hRZw0FmrivQZYiIVAnnDYInnniCo0ePsnPnTmbOnElUVBTjx4+vjNr8JjLESWaOgkBEBMoRBPn5+Vx33XWsWbOG++67j379+pGXl1cZtflNZIiDrDw3Xq95/pVFRC5x5QqCVatWsXr1am699Vb2799PVlZWZdTmN5E1nJgmnMzXOIGIyHmDYPLkyXz//fdMmTKF8PBwPv/8c5544onKqM1vIkMKpr3O1ICxiMj5Lyhr3Lgxd999N5dffjmbNm3C5XIRFxdXGbX5TWSNgredmeOiYa0aAa5GRCSwyjVYnJqaekkNFkcUtgg0YCwiYtXBYnUNiYgUsuhgcUHXUJauJRARKf9g8TPPPHPpDRara0hE5PyDxa1ataJnz55s376dHTt20KZNGzp27FgZtflNeMjpwWJ1DYmInL9FMG3aNN566y1M0yQ3N5dXX32VF154oTJq8xun3UZokF0tAhERytEi2LZtG0uWLPEtP/zwwwwdOtSvRVWGyBCn5hsSEaEcLQK3201ubq5v+dSpU3g81X8u/8gamopaRATK0SK499576devH82aNcPr9fLrr78yZsyYyqjNryLUIhARAcoRBH369OGWW25hz549GIZBs2bNcDqdlVGbX0WGOEjLzg90GSIiAVeuG9OEhobSunVrWrVqRY0aNXjggQf8XZffRdZQi0BEBH7jHcpMs/pP36y7lImIFPhNQWAYRkXXUekiazjIzHFdEqEmInIxyhwjmDlzZqlf+KZpsm/fPr8WVRkiQpy4vSY5Lg+hQecdKhERuWSV+Q145ZVXlrnRuV6rLs5MM+FWEIiIpZX5DfiHP/yhMuuodL57EuS6qF8zJMDViIgEzm8aI7gUFLYINAOpiFidZYMgonDiuRydOSQi1lZmEGzcuLHYcn7+mYuv3n33Xf9VVEkiaxTenEYtAhGxtjKD4JVXXim2/NBDD/kef/jhh/6rqJLongQiIgXKDIKzz68vunwpnHsfoXsSiIgA5wiCs68hKLp8KVxQFuK0E+SwqWtIRCyvzNNHvV4vubm5vt/+C5e9Xi9er7fSCvSnyBCnBotFxPLKDIKDBw/St2/fYt1Affr0AS6NFgGcnmZCLQIRsbgyg2Dt2rWVWUdAFLQIFAQiYm1ljhG4XC5efPFFXK4zX5Q7d+7k5ZdfrpTCKkNEiO5SJiJSZhDMnDmT7OzsYl1DTZs2JTs7m/nz51dKcf6mexKIiJwjCLZu3crEiRMJCgryPRcUFMS4ceP48ssvy7XzadOmcddddzFo0CC+//77UteZO3cuw4YNu6BtKooGi0VEzhEEdru99A1stmLdRWXZtGkTe/fuZenSpTz33HM899xzJdZJSUnhm2++uaBtKpIGi0VEzhEEtWvXZvPmzSWeX7duHXXr1j3vjpOSkujRowcAsbGxZGRkkJ2dXWydGTNm8OSTT17QNhUpMsRJvttLrsvjt2OIiFR1ZZ41lJCQwJ/+9CdiY2Np1aoVHo+H5ORkDh06xMKFC8+747S0NOLi4nzLUVFRpKamEh4eDsCKFSvo1KkTDRs2LPc2Fc0331COixBn6S0gEZFLXZlB0LRpU1auXMmXX37J7t27MQyDoUOH0rlz5990HUHRQef09HRWrFjBm2++yZEjR8q1jT9EhxeMf6Rm5xETqXsSiIg1nfPWXDabjZtuuombbrrpgnccExNDWlqab/no0aNER0cD8PXXX3P8+HGGDBlCfn4+v/76K9OmTTvnNv4QHVHw5X80K4+486wrInKp8tv9CDp37kxiYiIA27ZtIyYmxtfFEx8fz0cffcSyZcuYP38+cXFxJCQknHMbf4iJCAYgNSvPb8cQEanq/Haz3o4dOxIXF8egQYMwDIPJkyezYsUKIiIi6NmzZ7m38ae64QoCERG/3rV99OjRxZZbtmxZYp1GjRqxePHiMrfxpxpBdiKCHQoCEbE0y96qslB0RDCp2QoCEbEuBUFEMKmZCgIRsS4FgVoEImJxCoKIYI0RiIilKQgigsnOc3MqX5PPiYg1KQh0CqmIWJzlg6BwagkFgYhYleWDQC0CEbE6BUHhNBM6c0hELMryQRAVFoTNUItARKzL8kFgtxnUDQ/mqC4qExGLsnwQgC4qExFrUxCgi8pExNoUBBScOaQgEBGrUhBQ0CJIy87D6/XvrTFFRKoiBQEFdypze01OnMoPdCkiIpVOQcCZexdrwFhErEhBQJGLyjROICIWpCBAQSAi1qYg4EwQHMrIDXAlIiKVT0EAhAc7aFonlK2/pge6FBGRSqcgOO2Gy+uw6ZdjeHQKqYhYjILgtOsvr0NmrpvthzIDXYqISKVSEJx2/eV1APh697EAVyIiUrkUBKfVrxlC87phJO1SEIiItSgIirj+8jps+uU4bo830KWIiFQaBUER118eRVaem580TiAiFqIgKOKG0+ME6h4SEStREBQRExnC5dFhGjAWEUtREJzl+svrsHnPCU1JLSKWoSA4S8cmtcnKc7PzaHagSxERqRQKgrN0bFILgC2/nghwJSIilUNBcJbmdcOoHepky14FgYhYg4LgLIZhcFWT2mzdpwnoRMQaFASl6NikFilHs8k45Qp0KSIifqcgKMVVTWoDsHWfuodE5NKnIChF+8a1sBmwRfcnEBELcPhz59OmTSM5ORnDMEhISKBdu3a+15YtW8Z7772HzWajZcuWTJ48mU2bNjFq1CiuuOIKAK688komTZrkzxJLFR7s4Mp6EWzVmUMiYgF+C4JNmzaxd+9eli5dyq5du0hISGDp0qUA5OTksHr1apYsWYLT6eSee+5h69atAHTq1ImXX37ZX2WVW8emtfnwu4N4vSY2mxHockRE/MZvXUNJSUn06NEDgNjYWDIyMsjOLrhIq0aNGixatAin00lOTg7Z2dlER0f7q5TfpPDCsh1HswJdioiIX/ktCNLS0qhdu7ZvOSoqitTU1GLr/PWvf6Vnz57Ex8fTuHFjAFJSUnj00UcZPHgwX375pb/KO68bYgsmoPv0pyMBq0FEpDJU2mCxaZacu+fhhx/m008/5YsvvuDbb7+lWbNmjBgxgtdee42ZM2cyYcIE8vPzK6vEYhrWqsG1zWqz8ruDpdYuInKp8FsQxMTEkJaW5ls+evSor/snPT2db775BoCQkBC6du3Kli1bqFevHn369MEwDJo0aULdunU5ciRwv5H379CQlKPZbDuo+xOIyKXLb0HQuXNnEhMTAdi2bRsxMTGEh4cD4Ha7GTduHCdPngTghx9+oHnz5qxatYqFCxcCkJqayrFjx6hXr56/Sjyvvm0vw2Ez+OC7AwGrQUTE3/x21lDHjh2Ji4tj0KBBGIbB5MmTWbFiBREREfTs2ZPhw4dzzz334HA4aNGiBd27d+fkyZOMHj2azz77DJfLxZQpUwgKCvJXiedVOyyIW1pEsyr5ION6t8Kus4dE5BLk1+sIRo8eXWy5ZcuWvse33347t99+e7HXw8PDef311/1Z0gUbcFVDPt1+lI27j3Hj7+oGuhwRkQqnK4vPo0ereoQHO3j32/2BLkVExC8UBOcR4rQz6NrGfPDdAVJ0TYGIXIIUBOXw+K2/IzTIwZzEHYEuRUSkwikIyiEqLIiHu17Ox9sO853uUyAilxgFQTk92KU5dcKCmLnmZ11gJiKXFAVBOYUFOxjV4wqSdh/TwLGIXFIUBBdg6HVNueHyOkxZtY09aScDXY6ISIVQEFwAm83g+bva47TbGLX0O1web6BLEhG5aAqCC3RZzRrMuL0tyfvSeW71do0XiEi1pyD4DXq3vYyHujTnra/2sHDDL4EuR0Tkovh1iolLWUKfVhzKyOUvq7dTLzKE37dvEOiSRER+EwXBb2SzGcwd2J6jWbn8eVkyMRHBXHd5nUCXJSJywdQ1dBFCnHYW3HMNjaNq8P/e3szOI5qCQkSqHwXBRaoVGsRb93ci2Gnnvje/Ye8xnVYqItWLgqACNI4K5c37riU7z02/+V/yxc7U828kIlJFKAgqSJuGNflwRBfqR4Zw79838b8f/Mi3e0/g9er0UhGp2hQEFahJnVBWPH4jf+zYiH9t2scfX/uKbnPX8fXuY4EuTUSkTAqCChYW7GD2ne3ZPKkHc+9sD8DgBV8zfc128tyeAFcnIlKSgsBPIkOc/PHqRqweeRODrm3CG5/vps9LX7BRrQMRqWIUBH4WFuxg+u1teev+a8lze7nrr1/z8NubWbJxL7tTswNdnoiILiirLLe0iOGTJ7syb20Ky7/dzyc/HQGgZf0I+nVoQNcrormiXjjBDnuAKxURq1EQVKLQIAdj41syplcL9hw7xfodqXzw3QFmffxfZn38Xxw2gyvqRXD95VF0jq1L59/VpUaQgkFE/EtBEACGYdC8bhjN64Zx743N2Hf8FN/tS+fnw5kk78vgnY2/8uaXewgPdhDfpj6df1eHGk47IU47l9WsQcPaNQgP1l+diFQMfZtUAY2jQmkcFeqbuC7P7eGbX06wKvkAa344zHul3BHtdzHh9Gxdj07No/B6TVweL20b1aJhrRqVXb6IVHMKgioo2GGnyxV16XJFXZ7t34YD6Tnku72czHNzKCOXX4+f4qtdaSxYv5vX1u0qtm3bhjVpfVkkOS4Pbq+XxlGh/C46nCZRodSLDKF+zRBCnOpuEpEzFARVXIjTTmx0eInnh9/6OzJOudhxNIsge8HJX1/tOsYnPx3mP/89SmiQHZth8O+fjuDyFL+6uWYNJzERwdhtBqYJNYLsNKgVwmU1a3BZzRAa1KqBAWTlufF4TWqHBhEVFoRhgNtjEhpkp1HtGqefMyrjYxARP1IQVGM1Q51c2yzKt9y+cS0euyW22Dpuj5e9x09xMD2HI5l5HMnM5UhmLkcz8/CYJnbD4GS+m/8ezuI/P6eS4yr/RW/BDhtBDhtOu40aTjthwXbCgx2EBTsIP/0TFuzAaTdweUxM06RWaBB1w4MIC3ZQw2nHabfhNU1MwDRNTBO8JiWei6zhIDo8hLBgO6fyPeS5PThsNoKdNoIddl8tdsPAZjOwGWC3GdiMgh/76ecUXCIlKQgucQ67jdjo8FJbFWczTZOMHBeHMnIBCA92YLcZHD+ZT/opF1Dw5Zqd52bf8VMcycwl3+PF7THJcXnIznVzMt9NVq6bwxm5ZOe5yc514zFNHLaCL+DMXLf/3mw5GAYFYWEY2GzgsNmw2wwctoKwKPxxnA4Rw8AXJr7HNvB4wes1MYyCa0VCg+x4TROXx8QAHHYDu81WbD8lajldT+HrhmFgQLFjGQYYFIbYudel2PLpdQq3OX0srwmmCSYFAVu0lsIHxumlgmNT5PGZ54tvYxRbPvN68f34ni/yYRTut+g+i25f9Pz6DxQAAAuYSURBVHMrfM/F93lWrUVqObvmovsu/vdQxl/O+Z8q8xeLsn7dKPX4Zax8dl2GATddEU1UWFAZe//tFATiYxgGtUKDqBVa/B9agwocgHZ7vJw45eJknptctweX2zz9xXb6C89W9IvL8P0HzshxkZqVx8l8N6FBDkKcdjxeL3kuL3luL7kuD/keLx6vWdCi8Jp4TBOvaRY89ha0Mgp/Cpc93oIft7dgW7encLmgReI1C1oyXt/2BYFpGAVh4TFNTuW7yc5zYz/d8gDIc3lxez2+fZWmsLVjcma/BcuFxz3zp1l4bN+6p9c/a12KtKbObHvmOVuRL97Cz7zgFXzHLlig1OcL79F9ZrnC/mlIOfyp2+/4820tKny/CgKpVA67jeiIYKIjggNdiviBLyhOB4Sve8/3evGAKb6uec7tOGu9ouFknrUfTEocs7TQKi3HzFJWvJDAK2tds5Sjlb1u6ZpGhZa/kAugIBCRClO0q6bIswGpRcpPcw2JiFicgkBExOIUBCIiFqcgEBGxOAWBiIjFKQhERCyuWp0+6vEUTH9w+PDhAFciIlJ9FH5nFn6Hnq1aBUFqaioAQ4YMCXAlIiLVT2pqKk2bNi3xvGGWdhldFZWbm8uPP/5IdHQ0drumUhYRKQ+Px0Nqaipt2rQhJCSkxOvVKghERKTiabBYRMTiqtUYwcWYNm0aycnJGIZBQkIC7dq1C3RJ5zVr1iy+/fZb3G43jzzyCG3btmXMmDF4PB6io6OZPXs2QUEVPyVtRcrNzeV//ud/ePzxx7nhhhuqVf2rVq3ib3/7Gw6Hg5EjR9KiRYtqU//JkycZO3YsGRkZuFwuhg8fTnR0NFOmTAGgRYsWPPPMM4EtshQ7duzg8ccf57777mPo0KEcOnSo1M981apVLFq0CJvNxsCBA7nzzjsDXTpQev3jx4/H7XbjcDiYPXs20dHRVa9+0wI2btxoPvzww6ZpmmZKSoo5cODAAFd0fklJSeZDDz1kmqZpHj9+3Lz55pvNcePGmR999JFpmqY5d+5cc8mSJYEssVyef/558/bbbzeXL19ereo/fvy4edttt5lZWVnmkSNHzIkTJ1ar+hcvXmzOmTPHNE3TPHz4sNmrVy9z6NChZnJysmmapvnUU0+Z69atC2SJJZw8edIcOnSoOXHiRHPx4sWmaZqlfuYnT540b7vtNjMzM9PMyckx+/bta544cSKQpZumWXr9Y8aMMVevXm2apmn+4x//MGfOnFkl67dE11BSUhI9evQAIDY2loyMDLKzswNc1blde+21vPTSSwBERkaSk5PDxo0b6d69OwC33norSUlJgSzxvHbt2kVKSgq33HILQLWqPykpiRtuuIHw8HBiYmKYOnVqtaq/du3apKenA5CZmUmtWrU4cOCAryVcFesPCgpiwYIFxMTE+J4r7TNPTk6mbdu2REREEBISQseOHdmyZUugyvYprf7JkyfTq1cv4MzfSVWs3xJBkJaWRu3atX3LUVFRvlNRqyq73U5oaMHc4++99x5du3YlJyfH1xVRp06dKv8eZs6cybhx43zL1an+/fv3k5uby6OPPsrdd99NUlJStaq/b9++HDx4kJ49ezJ06FDGjBlDZGSk7/WqWL/D4ShxRktpn3laWhpRUWdu0VpV/j+XVn9oaCh2ux2Px8M777zD73//+ypZv2XGCIoyq9GJUp9++invvfcef//737ntttt8z1f197By5Uo6dOhA48aNS329qtcPkJ6ezvz58zl48CD33HNPsZqrev0ffPABDRo0YOHChfz8888MHz6ciIgI3+tVvf7SlFVzVX8vHo+HMWPGcP3113PDDTfw4YcfFnu9KtRviSCIiYkhLS3Nt3z06FGio6MDWFH5fPHFF7z++uv87W9/IyIigtDQUHJzcwkJCeHIkSPFmqBVzbp169i3bx/r1q3j8OHDBAUFVav669Spw1VXXYXD4aBJkyaEhYVht9urTf1btmyhS5cuALRs2ZK8vDzc7jP3i67q9Rcq7d9Maf+fO3ToEMAqz238+PE0bdqUESNGAKV/HwW6fkt0DXXu3JnExEQAtm3bRkxMDOHh57+ZeyBlZWUxa9Ys3njjDWrVqgXAjTfe6Hsfn3zyCTfddFMgSzynF198keXLl7Ns2TLuvPNOHn/88WpVf5cuXfj666/xer2cOHGCU6dOVav6mzZtSnJyMgAHDhwgLCyM2NhYNm/eDFT9+guV9pm3b9+eH374gczMTE6ePMmWLVu45pprAlxp6VatWoXT6WTkyJG+56pi/Za5oGzOnDls3rwZwzCYPHkyLVu2DHRJ57R06VLmzZtH8+bNfc/NmDGDiRMnkpeXR4MGDZg+fTpOpzOAVZbPvHnzaNiwIV26dGHs2LHVpv5//etfvPfeewA89thjtG3bttrUf/LkSRISEjh27Bhut5tRo0YRHR3N//7v/+L1emnfvj3jx48PdJnF/Pjjj8ycOZMDBw7gcDioV68ec+bMYdy4cSU+848//piFCxdiGAZDhw6lX79+gS6/1PqPHTtGcHCw7xfP2NhYpkyZUuXqt0wQiIhI6SzRNSQiImVTEIiIWJyCQETE4hQEIiIWpyAQEbE4BYFcEvbv389VV13FsGHDiv0UzrdzMebNm8c//vGPc67TokUL1q5d61veuHEj8+bN+83H3LhxY7Fzz0X8yRJXFos1NG/enMWLFwfk2M2aNWP+/PncfPPNunueVDsKArnkjRs3jtDQUHbv3s2JEyeYPn06rVu3ZtGiRXz00UcAdO/enYcffpgDBw4wbtw4PB4PDRo0YObMmUDBPPOPPPIIe/bsYcKECXTt2rXYMWJiYmjbti3vv/8+d9xxR7HXrrvuOjZu3AjAyJEjGTJkCJs2beLEiRPs3buX/fv3M2rUKJYvX86BAwdYsGABABkZGQwfPpwDBw7Qs2dPhg8fTkpKCs8++yyGYRAWFsaMGTPIzMzk6aefJjQ0lKFDh3Lrrbf6+yOVS4y6hsQS3G43b731FqNGjeKVV15h3759vP/++yxZsoQlS5awZs0afv31V1544QXuu+8+3nnnHWJiYvjxxx+Bggno3njjDSZOnMi//vWvUo/xyCOPsGjRInJzc8tVU0ZGBgsXLiQ+Pp6VK1f6Hn/22WcA/Pe//2XWrFksW7aM5cuXk56eztSpU3n22WdZtGgRnTt3ZsmSJQBs376dOXPmKATkN1GLQC4Zv/zyC8OGDfMtN2/enGeffRYomLMGoEOHDsyZM4ft27fTvn17HI6C/wIdO3bk559/5qeffmLChAkAjBkzBoD169fTsWNHAOrVq0dWVlapx69Zsyb9+/fn7bffpn379uett23btgDFJkCsW7eub1yjTZs2hIWFAQVTE+zbt4/vv/+eSZMmAZCfn+/bR+PGjYtNtS5yIRQEcsk41xiB1+v1PTYMA8Mwik3/63K5sNls2O32UqcFLgyM8xk2bBh33HEHzZo1K/V1l8tV6j6LPi48vmEYxbY1DIMaNWrw9ttvF3tt//79VXbOI6ke1DUklvDtt98CsHXrVmJjY2nVqhXfffcdbrcbt9tNcnIyrVq1ok2bNnz99dcAvPTSS3z11VcXdJzg4GDuv/9+Xn/9dd9zhmGQk5NDTk4O27dvL/e+fvrpJ3JycsjLy2PXrl00adKEli1bsn79egBWr15d5e4yJtWTWgRyyTi7awjg6aefBiAvL49HHnmEQ4cOMXv2bBo1asRdd93F0KFDMU2TO++8k4YNGzJy5EjGjx/PO++8w2WXXcaIESN8IVJeAwYM4M033/QtDx48mIEDBxIbG0tcXFy599O6dWsSEhLYs2cPgwYNIjIykgkTJjBp0iQWLFhAcHAwc+fOrfK3XZWqT7OPyiVv3Lhx9OrVSwOpImVQ15CIiMWpRSAiYnFqEYiIWJyCQETE4hQEIiIWpyAQEbE4BYGIiMUpCERELO7/A65vttJ6WMQRAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:  0 Iteration:  70 | train loss: 0.7740 | test accuracy: 0.606\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.3161 | test accuracy: 0.606\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8192 | test accuracy: 0.636\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6246 | test accuracy: 0.626\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5890 | test accuracy: 0.710\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.9202 | test accuracy: 0.663\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.6697 | test accuracy: 0.721\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.6729 | test accuracy: 0.717\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4478 | test accuracy: 0.727\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.4589 | test accuracy: 0.754\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3080 | test accuracy: 0.764\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.1189 | test accuracy: 0.771\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.3603 | test accuracy: 0.781\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.4268 | test accuracy: 0.828\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4613 | test accuracy: 0.822\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3625 | test accuracy: 0.892\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3466 | test accuracy: 0.939\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.3016 | test accuracy: 0.970\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.1549 | test accuracy: 0.976\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1059 | test accuracy: 0.980\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0687 | test accuracy: 0.956\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.1867 | test accuracy: 0.956\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.4952 | test accuracy: 0.970\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0331 | test accuracy: 0.966\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0061 | test accuracy: 0.956\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0074 | test accuracy: 0.956\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0164 | test accuracy: 0.966\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0667 | test accuracy: 0.946\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0201 | test accuracy: 0.960\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.0293 | test accuracy: 0.956\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0494 | test accuracy: 0.976\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0094 | test accuracy: 0.970\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0455 | test accuracy: 0.970\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0211 | test accuracy: 0.970\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.3736 | test accuracy: 0.963\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0339 | test accuracy: 0.970\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0069 | test accuracy: 0.970\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0124 | test accuracy: 0.966\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0361 | test accuracy: 0.963\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0952 | test accuracy: 0.966\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0487 | test accuracy: 0.960\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0153 | test accuracy: 0.970\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0092 | test accuracy: 0.963\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0054 | test accuracy: 0.970\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.1088 | test accuracy: 0.970\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0869 | test accuracy: 0.966\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.3805 | test accuracy: 0.963\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0158 | test accuracy: 0.970\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.2654 | test accuracy: 0.966\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0485 | test accuracy: 0.963\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0298 | test accuracy: 0.963\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0389 | test accuracy: 0.963\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.1673 | test accuracy: 0.963\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0090 | test accuracy: 0.963\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0094 | test accuracy: 0.966\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0079 | test accuracy: 0.970\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2326 | test accuracy: 0.970\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0145 | test accuracy: 0.966\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0109 | test accuracy: 0.966\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0988 | test accuracy: 0.970\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0413 | test accuracy: 0.966\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.3214 | test accuracy: 0.966\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0125 | test accuracy: 0.966\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0232 | test accuracy: 0.963\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0167 | test accuracy: 0.966\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0489 | test accuracy: 0.966\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0124 | test accuracy: 0.970\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1270 | test accuracy: 0.960\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0138 | test accuracy: 0.966\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0870 | test accuracy: 0.966\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1747 | test accuracy: 0.966\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0985 | test accuracy: 0.970\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0157 | test accuracy: 0.966\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0111 | test accuracy: 0.963\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0107 | test accuracy: 0.966\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0018 | test accuracy: 0.966\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0302 | test accuracy: 0.966\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0056 | test accuracy: 0.966\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0325 | test accuracy: 0.966\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0082 | test accuracy: 0.970\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.4529 | test accuracy: 0.966\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0120 | test accuracy: 0.963\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0389 | test accuracy: 0.970\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0231 | test accuracy: 0.966\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1908 | test accuracy: 0.966\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0080 | test accuracy: 0.970\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0020 | test accuracy: 0.966\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0537 | test accuracy: 0.966\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0020 | test accuracy: 0.966\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0156 | test accuracy: 0.966\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0028 | test accuracy: 0.970\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0215 | test accuracy: 0.966\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.7019 | test accuracy: 0.966\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0020 | test accuracy: 0.966\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0013 | test accuracy: 0.970\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.5353 | test accuracy: 0.966\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4501 | test accuracy: 0.966\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0116 | test accuracy: 0.966\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0141 | test accuracy: 0.970\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0013 | test accuracy: 0.966\n",
            "total time:  33.20139149900024\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.7255 | test accuracy: 0.606\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.9620 | test accuracy: 0.606\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.3706 | test accuracy: 0.606\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8851 | test accuracy: 0.606\n",
            "Epoch:  4 Iteration:  350 | train loss: 1.1178 | test accuracy: 0.606\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.4671 | test accuracy: 0.670\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.2932 | test accuracy: 0.667\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7312 | test accuracy: 0.684\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.8995 | test accuracy: 0.680\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.7116 | test accuracy: 0.694\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.4521 | test accuracy: 0.727\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.7132 | test accuracy: 0.727\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2824 | test accuracy: 0.717\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5703 | test accuracy: 0.778\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.4339 | test accuracy: 0.788\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.4562 | test accuracy: 0.788\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.2944 | test accuracy: 0.788\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.6347 | test accuracy: 0.805\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.5971 | test accuracy: 0.852\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1537 | test accuracy: 0.859\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.0737 | test accuracy: 0.892\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5690 | test accuracy: 0.892\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.0916 | test accuracy: 0.936\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2365 | test accuracy: 0.953\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0619 | test accuracy: 0.936\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.2094 | test accuracy: 0.970\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.1884 | test accuracy: 0.949\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0549 | test accuracy: 0.966\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0816 | test accuracy: 0.966\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.2013 | test accuracy: 0.963\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0523 | test accuracy: 0.963\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0400 | test accuracy: 0.963\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0238 | test accuracy: 0.963\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0687 | test accuracy: 0.963\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0262 | test accuracy: 0.960\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.1015 | test accuracy: 0.960\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0377 | test accuracy: 0.963\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0324 | test accuracy: 0.960\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0430 | test accuracy: 0.960\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0252 | test accuracy: 0.960\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0345 | test accuracy: 0.956\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0445 | test accuracy: 0.960\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0703 | test accuracy: 0.960\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0575 | test accuracy: 0.960\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.3999 | test accuracy: 0.960\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.2967 | test accuracy: 0.960\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.6818 | test accuracy: 0.960\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0467 | test accuracy: 0.960\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0621 | test accuracy: 0.960\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0272 | test accuracy: 0.960\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0430 | test accuracy: 0.953\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0755 | test accuracy: 0.953\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0230 | test accuracy: 0.960\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0427 | test accuracy: 0.953\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0373 | test accuracy: 0.953\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.1048 | test accuracy: 0.953\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0451 | test accuracy: 0.953\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.0293 | test accuracy: 0.953\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0182 | test accuracy: 0.953\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0255 | test accuracy: 0.953\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0341 | test accuracy: 0.956\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.5582 | test accuracy: 0.953\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.1213 | test accuracy: 0.953\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0224 | test accuracy: 0.953\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0110 | test accuracy: 0.953\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0069 | test accuracy: 0.953\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0329 | test accuracy: 0.960\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.5563 | test accuracy: 0.953\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0294 | test accuracy: 0.953\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0594 | test accuracy: 0.953\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0458 | test accuracy: 0.953\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0347 | test accuracy: 0.953\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0307 | test accuracy: 0.953\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0131 | test accuracy: 0.953\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0339 | test accuracy: 0.953\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0199 | test accuracy: 0.953\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0098 | test accuracy: 0.956\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0239 | test accuracy: 0.960\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1681 | test accuracy: 0.953\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0110 | test accuracy: 0.953\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0098 | test accuracy: 0.953\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0179 | test accuracy: 0.953\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0387 | test accuracy: 0.953\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0648 | test accuracy: 0.956\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0102 | test accuracy: 0.953\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0884 | test accuracy: 0.953\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0654 | test accuracy: 0.953\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0198 | test accuracy: 0.953\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.1200 | test accuracy: 0.953\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0758 | test accuracy: 0.953\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0245 | test accuracy: 0.953\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.1948 | test accuracy: 0.953\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.2310 | test accuracy: 0.953\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0124 | test accuracy: 0.953\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.0264 | test accuracy: 0.953\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0320 | test accuracy: 0.953\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0223 | test accuracy: 0.953\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.1036 | test accuracy: 0.953\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0061 | test accuracy: 0.956\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0268 | test accuracy: 0.953\n",
            "total time:  37.53346223300014\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27129030227661133.\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epoch took: 0.4766077995300293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.6860229117529733 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2721211910247803.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epoch took: 0.46502041816711426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.5468523310763496 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2804579734802246.\n",
            "\n",
            "  Average training loss: 0.46\n",
            "  Training epoch took: 0.4903836250305176\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.46373097854001183 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696044445037842.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.4745297431945801\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.41883057483604974 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2845456600189209.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.48828554153442383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.3906100975615638 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.284696102142334.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.4769570827484131\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.3751331239938736 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2611124515533447.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.45090222358703613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3621623247861862 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26989221572875977.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.46575403213500977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.352107304760388 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27806663513183594.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.47475361824035645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.34676992084298813 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2791597843170166.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.46944427490234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3416052907705307 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.271808385848999.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4659416675567627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.3389045157602855 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2635321617126465.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4626963138580322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.33470401636191777 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28734707832336426.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4847393035888672\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3316985023873193 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.260969877243042.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4544389247894287\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3298158752066748 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2776970863342285.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4738485813140869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32847678193024227 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2776169776916504.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4971597194671631\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.32696144325392584 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.275773286819458.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47650146484375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32467519981520515 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2665400505065918.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4566006660461426\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3242434058870588 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2837190628051758.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5050957202911377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.3229787183659417 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2625439167022705.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44903993606567383\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3221247153622763 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2820000648498535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4879035949707031\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.321806777375085 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2663421630859375.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45743298530578613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.32105260406221664 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2705981731414795.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45726752281188965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.320236531325749 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.264707088470459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4509389400482178\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3197264939546585 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.24993324279785156.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.455125093460083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.3193609416484833 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27495479583740234.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.461864709854126\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.3188095488718578 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28409290313720703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4840834140777588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31834841668605807 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27915287017822266.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4773120880126953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31811426707676477 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2638833522796631.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4588313102722168\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.317739645923887 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2813413143157959.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47573304176330566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.31721761865275244 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27791500091552734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46561408042907715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.3169029576437814 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2815523147583008.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4721496105194092\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31733143883092063 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2698953151702881.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45394420623779297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.31653450940336497 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2634720802307129.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.451627254486084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31659061184951237 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2735445499420166.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46062207221984863\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.3164138410772596 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2583301067352295.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4609811305999756\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.31597629742962974 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632725238800049.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4503939151763916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31586424197469437 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26485562324523926.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4655008316040039\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.31578175723552704 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27247118949890137.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4555525779724121\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31553968233721597 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2581806182861328.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4511082172393799\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3154975082193102 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29851746559143066.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48024535179138184\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31539755974497113 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2682838439941406.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4555990695953369\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31530470252037046 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.298642635345459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49366140365600586\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.3151937948805945 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26685142517089844.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44582581520080566\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3152156910726002 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26888418197631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46068572998046875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3149628175156457 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28122377395629883.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47205209732055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.31500918439456393 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2666740417480469.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4876365661621094\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.314852123601096 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27070140838623047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46148061752319336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.3147564589977264 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618434429168701.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46698856353759766\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31474319398403167 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27811121940612793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47405123710632324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31457097615514484 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26986074447631836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46282267570495605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.3146007090806961 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28525209426879883.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47512173652648926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.314478588955743 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2913951873779297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4832339286804199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.31453332432678766 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27455973625183105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4610288143157959\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.31441403116498673 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2706944942474365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45574522018432617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.3143171365771975 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28035879135131836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4773595333099365\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31441081251416886 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27145862579345703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4658994674682617\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.3143256528036935 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2970752716064453.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5206317901611328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.31427921950817106 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28731322288513184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4939911365509033\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.314250767656735 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29789137840270996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5307142734527588\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3142167236123766 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28043055534362793.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.502985954284668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31407288823808943 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.31902313232421875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5218813419342041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31417604173932756 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2692580223083496.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46422648429870605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3140517217772348 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2763664722442627.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48218226432800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.31408651173114777 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2711348533630371.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4628775119781494\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.3140134309019361 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28019213676452637.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46943140029907227\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.313952420439039 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26119542121887207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4486861228942871\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.3140524370329721 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2675039768218994.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45374560356140137\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.3140208729675838 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26808691024780273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44901561737060547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.313913408773286 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2588472366333008.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45014166831970215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.313845505458968 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28777575492858887.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4737396240234375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.3138397480760302 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2563197612762451.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4569580554962158\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.3138924824340003 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2759990692138672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46660327911376953\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3138768438782011 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26821184158325195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4570431709289551\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31388731726578306 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27256345748901367.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4686911106109619\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31382881573268345 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2648284435272217.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4503014087677002\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31382409589631216 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2705256938934326.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46617984771728516\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31382223069667814 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2558896541595459.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4407496452331543\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.3138156750372478 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2726099491119385.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4622519016265869\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137311722551073 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29120779037475586.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47984838485717773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137034288474492 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2769200801849365.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4654397964477539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.31375506605420794 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27901387214660645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4651758670806885\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31377286953585487 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.287614107131958.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.505359411239624\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137696185282299 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28524088859558105.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4816720485687256\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.31369144150188993 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27349185943603516.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4706108570098877\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.313704006586756 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26674437522888184.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47387003898620605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.3136896175997598 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652137279510498.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46095943450927734\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.3137165725231171 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25385284423828125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4478423595428467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31369730532169343 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2654755115509033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4604506492614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.3137147086007254 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2802562713623047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47347593307495117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31369474572794775 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.276155948638916.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4571065902709961\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.3136015943118504 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27472448348999023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47263121604919434\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.3135751038789749 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2726457118988037.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45291972160339355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136383435555867 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28011631965637207.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4711928367614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.3135859991822924 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27872490882873535.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4647083282470703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.3136307397059032 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652308940887451.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4543001651763916\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.3136210407529558 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27147626876831055.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4601163864135742\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.3136253561292376 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26479148864746094.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46962499618530273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31358183239187515 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27068543434143066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4691810607910156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.31359812489577704 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2766537666320801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4670557975769043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.31359583863190243 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26877641677856445.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4615042209625244\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135250010660717 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28522777557373047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.478074312210083\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.3135522335767746 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2738051414489746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4598708152770996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.31354456756796156 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2834174633026123.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49627065658569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135638671261924 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2812948226928711.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47756052017211914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31354944918836863 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2826244831085205.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4881305694580078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.3135417580604553 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2939302921295166.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4989922046661377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.31353664781366075 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27832889556884766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4757049083709717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31354536030973706 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2680356502532959.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45972132682800293\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.313525282059397 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2633981704711914.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4667181968688965\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31350112940583913 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27928757667541504.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4836232662200928\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31351672070367 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2816028594970703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49709081649780273\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.3135117143392563 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2853226661682129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4958515167236328\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3135152054684503 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2748110294342041.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4850947856903076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.3135436700923102 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27873897552490234.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4673776626586914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.31349820622376035 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2790555953979492.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47228121757507324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134882905653545 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2749671936035156.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45830726623535156\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.31353873567921775 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26848292350769043.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46365976333618164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.3134608417749405 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2798933982849121.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4693784713745117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.31348484456539155 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2618141174316406.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4521138668060303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31347636835915704 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27707862854003906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.471294641494751\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.31346015547003064 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27663564682006836.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4890477657318115\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.31347826080662866 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2952399253845215.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5063540935516357\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.3134359759943826 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28749752044677734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47581911087036133\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3134621786219733 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27531981468200684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.497816801071167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31345972163336616 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28725194931030273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48667240142822266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.3134762989623206 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28090357780456543.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49959421157836914\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.3134330140692847 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2921142578125.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49752330780029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31343480220862796 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6572 | test accuracy: 0.650\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.5390 | test accuracy: 0.633\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.8106 | test accuracy: 0.643\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.4599 | test accuracy: 0.697\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.3985 | test accuracy: 0.717\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.3177 | test accuracy: 0.724\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3686 | test accuracy: 0.727\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.2630 | test accuracy: 0.734\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.3208 | test accuracy: 0.764\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.8926 | test accuracy: 0.774\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.2034 | test accuracy: 0.822\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.8258 | test accuracy: 0.838\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.0707 | test accuracy: 0.872\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.2134 | test accuracy: 0.862\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.6378 | test accuracy: 0.916\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.0719 | test accuracy: 0.943\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.3377 | test accuracy: 0.963\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.1087 | test accuracy: 0.943\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.1439 | test accuracy: 0.943\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.0100 | test accuracy: 0.960\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.1127 | test accuracy: 0.956\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.0171 | test accuracy: 0.956\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.3552 | test accuracy: 0.973\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.0816 | test accuracy: 0.949\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0140 | test accuracy: 0.960\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.1320 | test accuracy: 0.970\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.0189 | test accuracy: 0.970\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.0440 | test accuracy: 0.960\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.0601 | test accuracy: 0.953\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.4882 | test accuracy: 0.966\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0235 | test accuracy: 0.960\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.0709 | test accuracy: 0.963\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0201 | test accuracy: 0.963\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0056 | test accuracy: 0.963\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0110 | test accuracy: 0.956\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0112 | test accuracy: 0.960\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0123 | test accuracy: 0.963\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.0622 | test accuracy: 0.963\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0156 | test accuracy: 0.963\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0194 | test accuracy: 0.956\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0097 | test accuracy: 0.963\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0098 | test accuracy: 0.960\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0392 | test accuracy: 0.963\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0025 | test accuracy: 0.963\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.2905 | test accuracy: 0.966\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0261 | test accuracy: 0.966\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.0040 | test accuracy: 0.956\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0062 | test accuracy: 0.956\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0139 | test accuracy: 0.956\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0310 | test accuracy: 0.960\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.1186 | test accuracy: 0.956\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0083 | test accuracy: 0.966\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0193 | test accuracy: 0.963\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0017 | test accuracy: 0.963\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.1333 | test accuracy: 0.963\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0089 | test accuracy: 0.956\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0039 | test accuracy: 0.960\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3301 | test accuracy: 0.963\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0156 | test accuracy: 0.960\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0602 | test accuracy: 0.963\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0146 | test accuracy: 0.960\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.5849 | test accuracy: 0.970\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.0157 | test accuracy: 0.966\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.0378 | test accuracy: 0.963\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0091 | test accuracy: 0.966\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.6261 | test accuracy: 0.960\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.0639 | test accuracy: 0.960\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.0129 | test accuracy: 0.960\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.0058 | test accuracy: 0.966\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0113 | test accuracy: 0.963\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0734 | test accuracy: 0.966\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0075 | test accuracy: 0.970\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0185 | test accuracy: 0.963\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0789 | test accuracy: 0.966\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0116 | test accuracy: 0.963\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0285 | test accuracy: 0.966\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0060 | test accuracy: 0.963\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0403 | test accuracy: 0.970\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.1238 | test accuracy: 0.966\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0151 | test accuracy: 0.963\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0081 | test accuracy: 0.966\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0168 | test accuracy: 0.963\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.0047 | test accuracy: 0.963\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.1321 | test accuracy: 0.960\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0518 | test accuracy: 0.966\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1155 | test accuracy: 0.966\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0098 | test accuracy: 0.966\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0128 | test accuracy: 0.966\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2496 | test accuracy: 0.963\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0043 | test accuracy: 0.960\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0028 | test accuracy: 0.963\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0066 | test accuracy: 0.960\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.4778 | test accuracy: 0.966\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0149 | test accuracy: 0.966\n",
            "Epoch:  94 Iteration:  6650 | train loss: 1.1323 | test accuracy: 0.963\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0028 | test accuracy: 0.970\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0167 | test accuracy: 0.966\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0089 | test accuracy: 0.966\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0022 | test accuracy: 0.966\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.0312 | test accuracy: 0.970\n",
            "total time:  33.72134869999991\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.9093 | test accuracy: 0.606\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.6809 | test accuracy: 0.606\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6593 | test accuracy: 0.606\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.8578 | test accuracy: 0.606\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5637 | test accuracy: 0.633\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5097 | test accuracy: 0.643\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.3782 | test accuracy: 0.626\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.7374 | test accuracy: 0.667\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.8537 | test accuracy: 0.673\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.5301 | test accuracy: 0.643\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.1846 | test accuracy: 0.677\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.6757 | test accuracy: 0.707\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2330 | test accuracy: 0.667\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.3314 | test accuracy: 0.677\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.8264 | test accuracy: 0.741\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.2659 | test accuracy: 0.747\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7263 | test accuracy: 0.768\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.2700 | test accuracy: 0.825\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.3536 | test accuracy: 0.825\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.1703 | test accuracy: 0.869\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.5788 | test accuracy: 0.892\n",
            "Epoch:  21 Iteration:  1540 | train loss: 0.5296 | test accuracy: 0.923\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.2370 | test accuracy: 0.933\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.1893 | test accuracy: 0.919\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.0387 | test accuracy: 0.892\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.8282 | test accuracy: 0.936\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.6869 | test accuracy: 0.963\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2256 | test accuracy: 0.966\n",
            "Epoch:  28 Iteration:  2030 | train loss: 0.1555 | test accuracy: 0.946\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.3616 | test accuracy: 0.953\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0250 | test accuracy: 0.966\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.1195 | test accuracy: 0.963\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.0410 | test accuracy: 0.970\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.0817 | test accuracy: 0.970\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.0724 | test accuracy: 0.960\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0341 | test accuracy: 0.963\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.0566 | test accuracy: 0.966\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.4640 | test accuracy: 0.960\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.0260 | test accuracy: 0.966\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.0497 | test accuracy: 0.960\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0749 | test accuracy: 0.960\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.0462 | test accuracy: 0.960\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.0352 | test accuracy: 0.960\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.0575 | test accuracy: 0.960\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0228 | test accuracy: 0.960\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.0351 | test accuracy: 0.960\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.5964 | test accuracy: 0.960\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.0167 | test accuracy: 0.960\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0699 | test accuracy: 0.960\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0094 | test accuracy: 0.960\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.0143 | test accuracy: 0.960\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0757 | test accuracy: 0.960\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0896 | test accuracy: 0.960\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.0522 | test accuracy: 0.960\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0289 | test accuracy: 0.956\n",
            "Epoch:  55 Iteration:  3920 | train loss: 1.0391 | test accuracy: 0.960\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.0335 | test accuracy: 0.960\n",
            "Epoch:  57 Iteration:  4060 | train loss: 0.3175 | test accuracy: 0.960\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0514 | test accuracy: 0.963\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0884 | test accuracy: 0.960\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0615 | test accuracy: 0.960\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0266 | test accuracy: 0.960\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2156 | test accuracy: 0.963\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.2331 | test accuracy: 0.960\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.4834 | test accuracy: 0.960\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.0202 | test accuracy: 0.960\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3283 | test accuracy: 0.960\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.2654 | test accuracy: 0.960\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.2963 | test accuracy: 0.966\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0627 | test accuracy: 0.960\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.0376 | test accuracy: 0.960\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0388 | test accuracy: 0.960\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.0369 | test accuracy: 0.960\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0198 | test accuracy: 0.960\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.0094 | test accuracy: 0.960\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.4614 | test accuracy: 0.960\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.5283 | test accuracy: 0.960\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0079 | test accuracy: 0.960\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.0144 | test accuracy: 0.960\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.0210 | test accuracy: 0.960\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.0226 | test accuracy: 0.960\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.2386 | test accuracy: 0.966\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.1065 | test accuracy: 0.960\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0137 | test accuracy: 0.960\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.0634 | test accuracy: 0.960\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.0207 | test accuracy: 0.960\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.0179 | test accuracy: 0.953\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.3576 | test accuracy: 0.956\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.2559 | test accuracy: 0.963\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0867 | test accuracy: 0.963\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0208 | test accuracy: 0.956\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.0095 | test accuracy: 0.963\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.0097 | test accuracy: 0.960\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.1325 | test accuracy: 0.956\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.2031 | test accuracy: 0.956\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0329 | test accuracy: 0.956\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.0940 | test accuracy: 0.960\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.0767 | test accuracy: 0.963\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.8799 | test accuracy: 0.956\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.3018 | test accuracy: 0.956\n",
            "total time:  37.411801010999625\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29300785064697266.\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epoch took: 0.48668956756591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 0.7040643393993378 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2919118404388428.\n",
            "\n",
            "  Average training loss: 0.56\n",
            "  Training epoch took: 0.496079683303833\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 0.561928818481309 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2925088405609131.\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epoch took: 0.4961128234863281\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 0.4727778238909585 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25884509086608887.\n",
            "\n",
            "  Average training loss: 0.42\n",
            "  Training epoch took: 0.44262242317199707\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 0.42323290024484905 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2670001983642578.\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epoch took: 0.44661951065063477\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 0.39168219438620977 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25052976608276367.\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epoch took: 0.43456125259399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 0.37710749336651395 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2854270935058594.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.4853644371032715\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 0.3638309282915933 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.284045934677124.\n",
            "\n",
            "  Average training loss: 0.36\n",
            "  Training epoch took: 0.47849154472351074\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 0.35515695554869514 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2702803611755371.\n",
            "\n",
            "  Average training loss: 0.35\n",
            "  Training epoch took: 0.488649845123291\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 0.347785946726799 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2535696029663086.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.43588781356811523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 0.3430905852999006 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26204609870910645.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.461651086807251\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 0.33963439421994346 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27120304107666016.\n",
            "\n",
            "  Average training loss: 0.34\n",
            "  Training epoch took: 0.4804961681365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 0.3358667105436325 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27335548400878906.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.49291038513183594\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 0.3333883366414479 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700343132019043.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.46247220039367676\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 0.3309564381837845 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26561856269836426.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4727776050567627\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 0.32854497006961275 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29400157928466797.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4841139316558838\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 0.3267837894814355 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26732349395751953.\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epoch took: 0.4616842269897461\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 0.32664714200156075 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29708003997802734.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4859428405761719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 0.3244697485651289 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2632720470428467.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4486124515533447\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 0.32381433631692613 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27710723876953125.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4647834300994873\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 0.3223291622740882 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25647401809692383.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4334900379180908\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 0.32205462030002047 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27187442779541016.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47152256965637207\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 0.3210057841879981 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26676201820373535.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45691967010498047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 0.3204624112163271 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27115345001220703.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4863090515136719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 0.3204580851963588 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26657986640930176.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4593315124511719\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 0.31943382493087225 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27466535568237305.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46907520294189453\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 0.31885818881647926 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2782022953033447.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4743819236755371\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 0.31869289875030515 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26417088508605957.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4673006534576416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 0.31836344046252113 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2909505367279053.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4842681884765625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 0.3175068859543119 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26116418838500977.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4557070732116699\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 0.3172566971608571 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2993466854095459.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.48418664932250977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 0.31723765305110385 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2532620429992676.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4412815570831299\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 0.31711936933653695 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2607896327972412.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4580042362213135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 0.3166042425802776 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28171753883361816.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47739148139953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 0.31695878888879503 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2786552906036377.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.5003814697265625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 0.31614442212241034 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2669689655303955.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46639585494995117\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 0.3165032322917666 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28289008140563965.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.49352335929870605\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 0.31603629418781826 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26120829582214355.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45329809188842773\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 0.3160232790878841 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27303290367126465.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.47181129455566406\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 0.31573796357427325 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2726423740386963.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46066904067993164\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 0.3157266429492405 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26135945320129395.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.44763970375061035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 0.31565393081733156 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2825431823730469.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.4711771011352539\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 0.31552084939820424 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26305317878723145.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45621585845947266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 0.31546465797083717 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2722814083099365.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45607590675354004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 0.3151822315795081 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2628471851348877.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.45250868797302246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 0.3151375596012388 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2716503143310547.\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epoch took: 0.46532130241394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 0.3151338360139302 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26033616065979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4523661136627197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 0.31482735872268675 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27991819381713867.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4620516300201416\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 0.314948142852102 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2696042060852051.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45006418228149414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 0.31472795818533217 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2620511054992676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4607665538787842\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 0.31483582981995173 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29561281204223633.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4887230396270752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 0.31456338252340044 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622039318084717.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44567394256591797\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 0.3145564764738083 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29791784286499023.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.485853910446167\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 0.3145378657749721 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.272083044052124.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47261881828308105\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 0.3145170326743807 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28627991676330566.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49765682220458984\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 0.31437913136822837 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.283069372177124.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4962644577026367\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 0.31457304060459135 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2718343734741211.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4730067253112793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 0.31435671831880296 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.303699254989624.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5197803974151611\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 0.3143892786332539 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2776825428009033.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4766194820404053\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 0.3142552959067481 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27361583709716797.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.471146821975708\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 0.3141388888869967 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26267051696777344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46367669105529785\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 0.31423336608069286 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26375341415405273.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46085524559020996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 0.31410514073712487 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578575611114502.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43985438346862793\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 0.3141747602394649 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27916526794433594.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47896838188171387\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 0.3141454496553966 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29268836975097656.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49089956283569336\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 0.31416380660874504 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2858867645263672.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47240614891052246\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 0.3140688623700823 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2776198387145996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4832763671875\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 0.31403434787477763 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2857034206390381.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4821741580963135\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 0.31400113701820376 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26970887184143066.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4577512741088867\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 0.3140320952449526 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2700331211090088.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46956562995910645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 0.3139596343040466 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27826762199401855.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4776577949523926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 0.31402417591639925 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2752816677093506.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4753735065460205\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 0.31387174342359814 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2578873634338379.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.43930554389953613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 0.3139497399330139 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2664191722869873.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47898125648498535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 0.31380096205643243 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28522372245788574.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4810471534729004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 0.31380001178809575 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2710094451904297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4645218849182129\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 0.31380684546061927 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2904014587402344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49183177947998047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 0.31375391951629095 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2887845039367676.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48211145401000977\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 0.31388716782842363 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2957444190979004.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5049850940704346\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 0.3137886030333383 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2777376174926758.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4759552478790283\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 0.3137445441314152 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29485106468200684.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5054550170898438\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 0.3137522488832474 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2829289436340332.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4865729808807373\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 0.31373817622661593 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27671289443969727.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4768035411834717\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 0.3137526776109423 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2658498287200928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.443021297454834\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 0.3137250942843301 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2568025588989258.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.464282751083374\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 0.3137355067900249 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26700758934020996.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4608025550842285\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 0.31368536395686014 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2647838592529297.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45607757568359375\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 0.31371639583792005 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26673388481140137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4522664546966553\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 0.31365994555609567 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2624473571777344.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4478189945220947\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 0.31371123152119773 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2862124443054199.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47595834732055664\n",
            "\n",
            "Running Validation...\n",
            "Epoch 90: train loss 0.31372306517192294 \n",
            "\n",
            "======== Epoch 91 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.288327693939209.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4792194366455078\n",
            "\n",
            "Running Validation...\n",
            "Epoch 91: train loss 0.31367376872471403 \n",
            "\n",
            "======== Epoch 92 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28536224365234375.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4721693992614746\n",
            "\n",
            "Running Validation...\n",
            "Epoch 92: train loss 0.31364373011248453 \n",
            "\n",
            "======== Epoch 93 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26082658767700195.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4463765621185303\n",
            "\n",
            "Running Validation...\n",
            "Epoch 93: train loss 0.3136487254074642 \n",
            "\n",
            "======== Epoch 94 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27211666107177734.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47300243377685547\n",
            "\n",
            "Running Validation...\n",
            "Epoch 94: train loss 0.31362918189593725 \n",
            "\n",
            "======== Epoch 95 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27425122261047363.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4654557704925537\n",
            "\n",
            "Running Validation...\n",
            "Epoch 95: train loss 0.31362417553152355 \n",
            "\n",
            "======== Epoch 96 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.291640043258667.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48906755447387695\n",
            "\n",
            "Running Validation...\n",
            "Epoch 96: train loss 0.313639902642795 \n",
            "\n",
            "======== Epoch 97 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2719912528991699.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47097134590148926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 97: train loss 0.313649605853217 \n",
            "\n",
            "======== Epoch 98 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26880908012390137.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47694993019104004\n",
            "\n",
            "Running Validation...\n",
            "Epoch 98: train loss 0.31361442932060785 \n",
            "\n",
            "======== Epoch 99 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2856903076171875.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4951319694519043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 99: train loss 0.3135901676756995 \n",
            "\n",
            "======== Epoch 100 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.3010678291320801.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.5189030170440674\n",
            "\n",
            "Running Validation...\n",
            "Epoch 100: train loss 0.3135652324983052 \n",
            "\n",
            "======== Epoch 101 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27520322799682617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4646625518798828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 101: train loss 0.3135900889124189 \n",
            "\n",
            "======== Epoch 102 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652311325073242.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4752509593963623\n",
            "\n",
            "Running Validation...\n",
            "Epoch 102: train loss 0.31355343546186176 \n",
            "\n",
            "======== Epoch 103 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29755496978759766.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49576759338378906\n",
            "\n",
            "Running Validation...\n",
            "Epoch 103: train loss 0.3135860762425831 \n",
            "\n",
            "======== Epoch 104 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27292370796203613.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46221041679382324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 104: train loss 0.3135649110589709 \n",
            "\n",
            "======== Epoch 105 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2630422115325928.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44657230377197266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 105: train loss 0.31354126759937834 \n",
            "\n",
            "======== Epoch 106 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.25418567657470703.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4337010383605957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 106: train loss 0.313586671437536 \n",
            "\n",
            "======== Epoch 107 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2795753479003906.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4747805595397949\n",
            "\n",
            "Running Validation...\n",
            "Epoch 107: train loss 0.3135299840143749 \n",
            "\n",
            "======== Epoch 108 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26734089851379395.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44916439056396484\n",
            "\n",
            "Running Validation...\n",
            "Epoch 108: train loss 0.31350829601287844 \n",
            "\n",
            "======== Epoch 109 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29328441619873047.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.49082207679748535\n",
            "\n",
            "Running Validation...\n",
            "Epoch 109: train loss 0.31353313284260886 \n",
            "\n",
            "======== Epoch 110 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2622683048248291.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.44443225860595703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 110: train loss 0.31354966844831195 \n",
            "\n",
            "======== Epoch 111 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2769503593444824.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46859097480773926\n",
            "\n",
            "Running Validation...\n",
            "Epoch 111: train loss 0.31352735885551997 \n",
            "\n",
            "======== Epoch 112 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2591996192932129.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4490070343017578\n",
            "\n",
            "Running Validation...\n",
            "Epoch 112: train loss 0.31351526635033744 \n",
            "\n",
            "======== Epoch 113 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2691044807434082.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4621317386627197\n",
            "\n",
            "Running Validation...\n",
            "Epoch 113: train loss 0.3134990142924445 \n",
            "\n",
            "======== Epoch 114 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.30440688133239746.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4863309860229492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 114: train loss 0.31351222481046404 \n",
            "\n",
            "======== Epoch 115 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2600064277648926.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45168352127075195\n",
            "\n",
            "Running Validation...\n",
            "Epoch 115: train loss 0.3134817842926298 \n",
            "\n",
            "======== Epoch 116 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27742743492126465.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45511341094970703\n",
            "\n",
            "Running Validation...\n",
            "Epoch 116: train loss 0.3134923155818667 \n",
            "\n",
            "======== Epoch 117 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2652432918548584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4520902633666992\n",
            "\n",
            "Running Validation...\n",
            "Epoch 117: train loss 0.3134992275919233 \n",
            "\n",
            "======== Epoch 118 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.29482221603393555.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4807627201080322\n",
            "\n",
            "Running Validation...\n",
            "Epoch 118: train loss 0.31351642608642577 \n",
            "\n",
            "======== Epoch 119 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27157068252563477.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46482229232788086\n",
            "\n",
            "Running Validation...\n",
            "Epoch 119: train loss 0.3134847053459712 \n",
            "\n",
            "======== Epoch 120 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.26602888107299805.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47057032585144043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 120: train loss 0.31348969425473894 \n",
            "\n",
            "======== Epoch 121 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28344297409057617.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.482985258102417\n",
            "\n",
            "Running Validation...\n",
            "Epoch 121: train loss 0.3134665931974139 \n",
            "\n",
            "======== Epoch 122 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.28787851333618164.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.48767828941345215\n",
            "\n",
            "Running Validation...\n",
            "Epoch 122: train loss 0.3134820410183498 \n",
            "\n",
            "======== Epoch 123 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27071309089660645.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4621572494506836\n",
            "\n",
            "Running Validation...\n",
            "Epoch 123: train loss 0.31346838303974695 \n",
            "\n",
            "======== Epoch 124 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2728462219238281.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46988749504089355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 124: train loss 0.3135044745036534 \n",
            "\n",
            "======== Epoch 125 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2771875858306885.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.46734619140625\n",
            "\n",
            "Running Validation...\n",
            "Epoch 125: train loss 0.31344636934144154 \n",
            "\n",
            "======== Epoch 126 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2724454402923584.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.4627046585083008\n",
            "\n",
            "Running Validation...\n",
            "Epoch 126: train loss 0.31345430740288327 \n",
            "\n",
            "======== Epoch 127 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.27153897285461426.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.45667338371276855\n",
            "\n",
            "Running Validation...\n",
            "Epoch 127: train loss 0.31345539093017577 \n",
            "\n",
            "======== Epoch 128 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 0.2766585350036621.\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epoch took: 0.47696757316589355\n",
            "\n",
            "Running Validation...\n",
            "Epoch 128: train loss 0.31346220288957866 \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "rnn_arr = rnn_arr1 + rnn_arr2 + rnn_arr3 + rnn_arr4\n",
        "nrnn_arr = nrnn_arr1 + nrnn_arr2 + nrnn_arr3 + nrnn_arr4\n",
        "lstm_arr = lstm_arr1 + lstm_arr2 + lstm_arr3 + lstm_arr4\n",
        "seed_arr = seed_arr1 + seed_arr2 + seed_arr3 + seed_arr4\n",
        "method_arr = method_arr1 + method_arr2 + method_arr3 + method_arr4\n",
        "specify_arr = specify_arr1 + specify_arr2 + specify_arr3 + specify_arr4\n",
        "\n",
        "df1 = pd.DataFrame(list(zip(rnn_arr, nrnn_arr, lstm_arr, seed_arr, method_arr, specify_arr)), columns = [\"RNN Accuracy\", \"NRNN Accuracy\", \"LSTM Accuracy\", \"Seed\", \"Method\", \"Specify\"])\n",
        "df1\n",
        "df1.to_csv('out.csv')  "
      ],
      "metadata": {
        "id": "fr3APVezQQ70"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}