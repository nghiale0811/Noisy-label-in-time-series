{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hmm-fit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px3RmzMhFa-A",
        "outputId": "a4bfa7de-737a-4097-b375-7e2ace3ab137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting hmmlearn\n",
            "  Downloading hmmlearn-0.2.7-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (129 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 129 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
            "Installing collected packages: hmmlearn\n",
            "Successfully installed hmmlearn-0.2.7\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install hmmlearn\n",
        "from hmmlearn import hmm\n",
        "from torch.distributions import uniform\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Datasets using HMM\n",
        "\n",
        "def random_transmat(n_states):\n",
        "    matrix = np.random.rand(n_states, n_states)\n",
        "    return matrix/matrix.sum(axis=1)[:,None]\n",
        "\n",
        "def random_startprob(n_states):\n",
        "    startprob = np.random.rand(n_states)\n",
        "    return startprob/startprob.sum()\n",
        "\n",
        "def random_means(n_features):\n",
        "    return np.random.randint(5, size=(n_features,n_features))\n",
        "\n",
        "def generate_hmm(n_states, n_features , length):\n",
        "    #GENERATING A MODEL\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = random_startprob(n_states)\n",
        "    model.transmat_ = random_transmat(n_states)\n",
        "\n",
        "    model.means_ = random_means(n_features)\n",
        "    model.covars_ = np.tile(np.identity(n_features), (n_features, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "    dataset=[]\n",
        "    states = []\n",
        "\n",
        "    \n",
        "    X, Z = model.sample(length)\n",
        "    dataset.append(np.array(X))\n",
        "    states.append(Z)\n",
        "\n",
        "    dataset = np.stack(dataset)\n",
        "    \n",
        "    return np.array(X), np.array(Z)\n",
        "\n",
        "def sliding_windows(dataset, Z, window_length = 10):\n",
        "    lis = []\n",
        "    targets=[]\n",
        "\n",
        "    window_length = 10\n",
        "    if len(dataset) >= window_length+1:\n",
        "        for i in range(0, len(dataset)-window_length, 1):\n",
        "\n",
        "            x_window = dataset[i:i+window_length, :]\n",
        "            z_window = Z[i:i+window_length]\n",
        "\n",
        "            lis.append(x_window)\n",
        "            targets.append(z_window[-1])\n",
        "    return np.array(lis), np.array(targets)\n",
        "\n",
        "def generate_time_dependent_flip(length, startprob, transmat):\n",
        "    #GENERATING A MODEL\n",
        "\n",
        "\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = startprob\n",
        "    model.transmat_ = transmat\n",
        "\n",
        "    #this doesn't actually matter for us\n",
        "    model.means_ = np.array([[0.0, 0.0], \n",
        "                             [5.0, 10.0]])\n",
        "    model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "\n",
        "\n",
        "    X, Z = model.sample(length)\n",
        "\n",
        "    \n",
        "    return Z"
      ],
      "metadata": {
        "id": "xpW126KEFfM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Injecting Noise into Labels\n",
        "\n",
        "#Given a flip_mask, flip an input\n",
        "def flip(array, flip_mask):\n",
        "    flipped_array = np.logical_xor(array, flip_mask, out=array)\n",
        "    return flipped_array\n",
        "\n",
        "#Class Independent / Time Independent\n",
        "def flip_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Dependent / Time Independent\n",
        "def flip_labels_class(array, flip_probability_0, flip_probability_1):\n",
        "    flip_mask = []\n",
        "    for elem in array:\n",
        "        if elem == 0:\n",
        "            to_flip = np.random.binomial(1, flip_probability_0, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "        else:\n",
        "            to_flip = np.random.binomial(1, flip_probability_1, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "            \n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Independent / Time Dependent\n",
        "def flip_labels_time(array, startprob, transmat):\n",
        "    flip_mask = generate_time_dependent_flip(len(array), startprob, transmat)[0]\n",
        "\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "\n",
        "#Class Dependent / Time Dependent\n",
        "#This can be achieved by careful design of the transition matrix (transmat)"
      ],
      "metadata": {
        "id": "OgyRemBtNY8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset,Z = generate_hmm(2,3,100)"
      ],
      "metadata": {
        "id": "oW2zggR4NdQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmIkYpDuNgTw",
        "outputId": "d6bd3043-cb8e-4b46-bf52-755d9c2f88bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startprob = random_startprob(2)\n",
        "transmat = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])"
      ],
      "metadata": {
        "id": "PXjUWBM5NjW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                     flip_probability= None, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=None, transmat=None):\n",
        "    \n",
        "    #Generate Data\n",
        "    dataset, states_true = generate_hmm(n_states, n_features , length)\n",
        "    \n",
        "    if method == \"basic\":\n",
        "        states_flipped = (flip_labels_basic(states_true, flip_probability))\n",
        "        \n",
        "    elif method == \"class\":\n",
        "        states_flipped = (flip_labels_class(states_true, flip_probability_0, flip_probability_1))\n",
        "        \n",
        "    elif method == \"time\":\n",
        "        states_flipped = (flip_labels_time(states_true, startprob, transmat))\n",
        "    \n",
        "    #RESHAPE INTO WINDOWS\n",
        "    dataset_windows, states_true = sliding_windows(dataset, states_true, window_length= window_length)\n",
        "    \n",
        "    #RESHAPE INTO WINDOWS\n",
        "    _, states_flipped = sliding_windows(dataset, states_flipped, window_length= window_length)\n",
        "    \n",
        "    x_train = dataset_windows[:int(train_ratio*len(dataset_windows)),:,:]\n",
        "    x_test = dataset_windows[int(train_ratio*len(dataset_windows)):,:,:]\n",
        "    \n",
        "   \n",
        "    #Flip The Labels according to method\n",
        "    \n",
        "    \n",
        "    y_train_true = states_true[:int(train_ratio*len(dataset_windows))]\n",
        "    y_test_true = states_true[int(train_ratio*len(dataset_windows)):]\n",
        "    \n",
        "    y_train_flipped = np.array(states_flipped[:int(train_ratio*len(dataset_windows))])\n",
        "    y_test_flipped = np.array(states_flipped[int(train_ratio*len(dataset_windows)):])\n",
        "    \n",
        "    \n",
        "    return x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped"
      ],
      "metadata": {
        "id": "1q2hceLfNoDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 10000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"basic\"\n",
        "flip_probability = 0.1\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped= generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                     flip_probability, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=None, transmat=None)"
      ],
      "metadata": {
        "id": "-sCnZqB2Nt6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_flipped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCE3DxcaNx7G",
        "outputId": "469b666b-cc5d-414c-843d-d0584fce88d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6993,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"class\"\n",
        "flip_probability_0 = 0.1\n",
        "flip_probability_1 = 0.2\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
        "                     flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1,\n",
        "                    startprob=None, transmat=None)"
      ],
      "metadata": {
        "id": "T0BHlmSTN1Ti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcjN-zbCN5Yc",
        "outputId": "afeb507f-525c-4dac-e84a-a03651d11d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(693, 10, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "length = 1000\n",
        "window_length = 10\n",
        "train_ratio = 0.7\n",
        "method = \"time\"\n",
        "\n",
        "startprob = random_startprob(2)\n",
        "transmat = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features,length, window_length, train_ratio, method, \n",
        "                     flip_probability=None, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=startprob, transmat=transmat)"
      ],
      "metadata": {
        "id": "H1umMDueOA4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_flipped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OPUhz7oOCBf",
        "outputId": "00261830-18f7-4fdf-9f59-bd16af7b7028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(693,)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_true))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_true))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=25, shuffle=True)"
      ],
      "metadata": {
        "id": "Mydxw0ZvON9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=25, shuffle=True)"
      ],
      "metadata": {
        "id": "2eQmYq68OizY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "OphpChUuPAo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f6HsUdvpPGMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='mnist', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.0005, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# get dataset\n",
        "#==============================================================================\n",
        "  \n",
        "model = NoisyRNN(input_dim=int(3), output_classes=2, n_units=args.n_units, \n",
        "              eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "              init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "              add_noise=args.add_noise, mult_noise=args.mult_noise).to(device)\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)        \n",
        "noise = torch.randn(1,693,10,3).float()\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# Model summary\n",
        "#==============================================================================\n",
        "print(model)    \n",
        "print('**** Setup ****')\n",
        "print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "print('************')    \n",
        "   \n",
        "\n",
        "if args.optimizer == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "elif  args.optimizer == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "else:\n",
        "    print(\"Unexpected optimizer!\")\n",
        "    raise \n",
        "\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# training and testing\n",
        "count = 0\n",
        "loss_hist = []\n",
        "test_acc = []\n",
        "\n",
        "t0 = timeit.default_timer()\n",
        "for epoch in range(args.epochs):\n",
        "    model.train()\n",
        "    lossaccum = 0\n",
        "    \n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        count += 1\n",
        "        \n",
        "        # Reshape data for recurrent unit\n",
        "        inputs = Variable(x.view(-1, 10, int(3))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "        targets = Variable(y).to(device)\n",
        "\n",
        "                 \n",
        "        # send data to recurrent unit    \n",
        "        output = model(inputs, mode='train')\n",
        "        loss = loss_func(output, targets.long())\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()          \n",
        "        \n",
        "        if args.gclip != 0.0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "            \n",
        "        optimizer.step() # update weights\n",
        "        lossaccum += loss.item()\n",
        "\n",
        "        if args.model == 'test':\n",
        "            D = model.W.weight.data.cpu().numpy()  \n",
        "            u, s, v = np.linalg.svd(D, 0)\n",
        "            model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "    loss_hist.append(lossaccum)    \n",
        "     \n",
        "    if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total_num = 0\n",
        "        for data, target in test_loader: \n",
        "            data, target = data.to(device), target.to(device)               \n",
        "            output = model(data.view(-1, 10, int(3)))                  \n",
        "            \n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            #print(output.shape)\n",
        "            #print(pred.shape)\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "            total_num += len(data)\n",
        "        \n",
        "        accuracy = correct / total_num\n",
        "        test_acc.append(accuracy)\n",
        "        print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "#        if args.model == 'NoisyRNN':\n",
        "#            B = model.B.data.cpu().numpy()            \n",
        "#            A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "#            A = 0.5 * (A + A.T)\n",
        "#            e, _ = np.linalg.eig(A)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "#            \n",
        "#            C = model.C.data.cpu().numpy()            \n",
        "#            W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "#            e, _ = np.linalg.eig(W)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "            \n",
        "             \n",
        "\n",
        "    # schedule learning rate decay    \n",
        "    optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "\n",
        "torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "pickle.dump(data,f)\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbqUW26PJZz",
        "outputId": "94b549ed-7cf4-4682-c7cb-e5f74e1906a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6378 | test accuracy: 0.586\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7074 | test accuracy: 0.630\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6137 | test accuracy: 0.620\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6175 | test accuracy: 0.643\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5949 | test accuracy: 0.653\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5565 | test accuracy: 0.667\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7352 | test accuracy: 0.697\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5439 | test accuracy: 0.630\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4992 | test accuracy: 0.670\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6942 | test accuracy: 0.643\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3449 | test accuracy: 0.761\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4159 | test accuracy: 0.778\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2595 | test accuracy: 0.754\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5163 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5367 | test accuracy: 0.811\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3726 | test accuracy: 0.825\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7933 | test accuracy: 0.818\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.3430 | test accuracy: 0.848\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.1681 | test accuracy: 0.855\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5095 | test accuracy: 0.862\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2123 | test accuracy: 0.845\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.0090 | test accuracy: 0.855\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1006 | test accuracy: 0.892\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2542 | test accuracy: 0.892\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1003 | test accuracy: 0.896\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0561 | test accuracy: 0.872\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4942 | test accuracy: 0.886\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2968 | test accuracy: 0.882\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.3210 | test accuracy: 0.892\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1538 | test accuracy: 0.892\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0331 | test accuracy: 0.875\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.9083 | test accuracy: 0.879\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7293 | test accuracy: 0.875\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1304 | test accuracy: 0.875\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5256 | test accuracy: 0.875\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0284 | test accuracy: 0.879\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.4870 | test accuracy: 0.879\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7249 | test accuracy: 0.879\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.8619 | test accuracy: 0.879\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5280 | test accuracy: 0.875\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0526 | test accuracy: 0.879\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4335 | test accuracy: 0.879\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1230 | test accuracy: 0.879\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4489 | test accuracy: 0.882\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0286 | test accuracy: 0.875\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.3114 | test accuracy: 0.879\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7721 | test accuracy: 0.875\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3326 | test accuracy: 0.879\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0599 | test accuracy: 0.875\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0631 | test accuracy: 0.879\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.3467 | test accuracy: 0.879\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0378 | test accuracy: 0.879\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0207 | test accuracy: 0.879\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1123 | test accuracy: 0.875\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0895 | test accuracy: 0.875\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0715 | test accuracy: 0.882\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2231 | test accuracy: 0.879\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.5001 | test accuracy: 0.882\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0943 | test accuracy: 0.882\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0332 | test accuracy: 0.882\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0813 | test accuracy: 0.882\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0945 | test accuracy: 0.875\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2650 | test accuracy: 0.879\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3068 | test accuracy: 0.875\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0703 | test accuracy: 0.882\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3539 | test accuracy: 0.882\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3402 | test accuracy: 0.882\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1544 | test accuracy: 0.889\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1286 | test accuracy: 0.882\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0310 | test accuracy: 0.875\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1016 | test accuracy: 0.882\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0111 | test accuracy: 0.882\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1406 | test accuracy: 0.882\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0780 | test accuracy: 0.886\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1660 | test accuracy: 0.889\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0254 | test accuracy: 0.889\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0472 | test accuracy: 0.882\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0563 | test accuracy: 0.886\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7077 | test accuracy: 0.889\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.8521 | test accuracy: 0.889\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1223 | test accuracy: 0.882\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0189 | test accuracy: 0.886\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4706 | test accuracy: 0.886\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0163 | test accuracy: 0.886\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1509 | test accuracy: 0.889\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1766 | test accuracy: 0.886\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.5410 | test accuracy: 0.886\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0820 | test accuracy: 0.886\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0862 | test accuracy: 0.889\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0162 | test accuracy: 0.886\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0837 | test accuracy: 0.886\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.4907 | test accuracy: 0.889\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.8332 | test accuracy: 0.889\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0136 | test accuracy: 0.886\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1828 | test accuracy: 0.886\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0533 | test accuracy: 0.892\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4254 | test accuracy: 0.892\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.4894 | test accuracy: 0.892\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0667 | test accuracy: 0.886\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1009 | test accuracy: 0.896\n",
            "total time:  43.161694825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# get dataset\n",
        "#==============================================================================\n",
        "  \n",
        "model = NoisyRNN(input_dim=int(3), output_classes=2, n_units=args.n_units, \n",
        "              eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "              init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "              add_noise=args.add_noise, mult_noise=args.mult_noise).to(device)\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)        \n",
        "noise = torch.randn(1,693,10,3).float()\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# Model summary\n",
        "#==============================================================================\n",
        "print(model)    \n",
        "print('**** Setup ****')\n",
        "print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "print('************')    \n",
        "   \n",
        "\n",
        "if args.optimizer == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "elif  args.optimizer == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "else:\n",
        "    print(\"Unexpected optimizer!\")\n",
        "    raise \n",
        "\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# training and testing\n",
        "count = 0\n",
        "loss_hist = []\n",
        "test_acc = []\n",
        "\n",
        "t0 = timeit.default_timer()\n",
        "for epoch in range(args.epochs):\n",
        "    model.train()\n",
        "    lossaccum = 0\n",
        "    \n",
        "    for step, (x, y) in enumerate(train_flipped_loader):\n",
        "        count += 1\n",
        "        \n",
        "        # Reshape data for recurrent unit\n",
        "        inputs = Variable(x.view(-1, 10, int(3))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "        targets = Variable(y).to(device)\n",
        "\n",
        "                 \n",
        "        # send data to recurrent unit    \n",
        "        output = model(inputs, mode='train')\n",
        "        loss = loss_func(output, targets.long())\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()          \n",
        "        \n",
        "        if args.gclip != 0.0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "            \n",
        "        optimizer.step() # update weights\n",
        "        lossaccum += loss.item()\n",
        "\n",
        "        if args.model == 'test':\n",
        "            D = model.W.weight.data.cpu().numpy()  \n",
        "            u, s, v = np.linalg.svd(D, 0)\n",
        "            model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "    loss_hist.append(lossaccum)    \n",
        "     \n",
        "    if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total_num = 0\n",
        "        for data, target in test_flipped_loader: \n",
        "            data, target = data.to(device), target.to(device)               \n",
        "            output = model(data.view(-1, 10, int(3)))                  \n",
        "            \n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            #print(output.shape)\n",
        "            #print(pred.shape)\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "            total_num += len(data)\n",
        "        \n",
        "        accuracy = correct / total_num\n",
        "        test_acc.append(accuracy)\n",
        "        print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "#        if args.model == 'NoisyRNN':\n",
        "#            B = model.B.data.cpu().numpy()            \n",
        "#            A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "#            A = 0.5 * (A + A.T)\n",
        "#            e, _ = np.linalg.eig(A)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "#            \n",
        "#            C = model.C.data.cpu().numpy()            \n",
        "#            W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "#            e, _ = np.linalg.eig(W)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "            \n",
        "             \n",
        "\n",
        "    # schedule learning rate decay    \n",
        "    optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "\n",
        "torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "pickle.dump(data,f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "Of8YS2vvcNL2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0b430a0-84d7-4420-a4ae-5ef8cf1bb834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 8.64k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 0.6378 | test accuracy: 0.586\n",
            "Epoch:  1 Iteration:  140 | train loss: 0.7074 | test accuracy: 0.630\n",
            "Epoch:  2 Iteration:  210 | train loss: 0.6137 | test accuracy: 0.620\n",
            "Epoch:  3 Iteration:  280 | train loss: 0.6175 | test accuracy: 0.643\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.5949 | test accuracy: 0.653\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5565 | test accuracy: 0.667\n",
            "Epoch:  6 Iteration:  490 | train loss: 0.7352 | test accuracy: 0.697\n",
            "Epoch:  7 Iteration:  560 | train loss: 0.5439 | test accuracy: 0.630\n",
            "Epoch:  8 Iteration:  630 | train loss: 0.4992 | test accuracy: 0.670\n",
            "Epoch:  9 Iteration:  700 | train loss: 0.6942 | test accuracy: 0.643\n",
            "Epoch:  10 Iteration:  770 | train loss: 0.3449 | test accuracy: 0.761\n",
            "Epoch:  11 Iteration:  840 | train loss: 0.4159 | test accuracy: 0.778\n",
            "Epoch:  12 Iteration:  910 | train loss: 0.2595 | test accuracy: 0.754\n",
            "Epoch:  13 Iteration:  980 | train loss: 0.5163 | test accuracy: 0.805\n",
            "Epoch:  14 Iteration:  1050 | train loss: 0.5367 | test accuracy: 0.811\n",
            "Epoch:  15 Iteration:  1120 | train loss: 0.3726 | test accuracy: 0.825\n",
            "Epoch:  16 Iteration:  1190 | train loss: 0.7933 | test accuracy: 0.818\n",
            "Epoch:  17 Iteration:  1260 | train loss: 0.3430 | test accuracy: 0.848\n",
            "Epoch:  18 Iteration:  1330 | train loss: 0.1681 | test accuracy: 0.855\n",
            "Epoch:  19 Iteration:  1400 | train loss: 0.5095 | test accuracy: 0.862\n",
            "Epoch:  20 Iteration:  1470 | train loss: 0.2123 | test accuracy: 0.845\n",
            "Epoch:  21 Iteration:  1540 | train loss: 1.0090 | test accuracy: 0.855\n",
            "Epoch:  22 Iteration:  1610 | train loss: 0.1006 | test accuracy: 0.892\n",
            "Epoch:  23 Iteration:  1680 | train loss: 0.2542 | test accuracy: 0.892\n",
            "Epoch:  24 Iteration:  1750 | train loss: 0.1003 | test accuracy: 0.896\n",
            "Epoch:  25 Iteration:  1820 | train loss: 0.0561 | test accuracy: 0.872\n",
            "Epoch:  26 Iteration:  1890 | train loss: 0.4942 | test accuracy: 0.886\n",
            "Epoch:  27 Iteration:  1960 | train loss: 0.2968 | test accuracy: 0.882\n",
            "Epoch:  28 Iteration:  2030 | train loss: 1.3210 | test accuracy: 0.892\n",
            "Epoch:  29 Iteration:  2100 | train loss: 0.1538 | test accuracy: 0.892\n",
            "Epoch:  30 Iteration:  2170 | train loss: 0.0331 | test accuracy: 0.875\n",
            "New learning rate is:  5e-05\n",
            "Epoch:  31 Iteration:  2240 | train loss: 0.9083 | test accuracy: 0.879\n",
            "Epoch:  32 Iteration:  2310 | train loss: 0.7293 | test accuracy: 0.875\n",
            "Epoch:  33 Iteration:  2380 | train loss: 0.1304 | test accuracy: 0.875\n",
            "Epoch:  34 Iteration:  2450 | train loss: 0.5256 | test accuracy: 0.875\n",
            "Epoch:  35 Iteration:  2520 | train loss: 0.0284 | test accuracy: 0.879\n",
            "Epoch:  36 Iteration:  2590 | train loss: 0.4870 | test accuracy: 0.879\n",
            "Epoch:  37 Iteration:  2660 | train loss: 0.7249 | test accuracy: 0.879\n",
            "Epoch:  38 Iteration:  2730 | train loss: 0.8619 | test accuracy: 0.879\n",
            "Epoch:  39 Iteration:  2800 | train loss: 0.5280 | test accuracy: 0.875\n",
            "Epoch:  40 Iteration:  2870 | train loss: 0.0526 | test accuracy: 0.879\n",
            "Epoch:  41 Iteration:  2940 | train loss: 0.4335 | test accuracy: 0.879\n",
            "Epoch:  42 Iteration:  3010 | train loss: 0.1230 | test accuracy: 0.879\n",
            "Epoch:  43 Iteration:  3080 | train loss: 0.4489 | test accuracy: 0.882\n",
            "Epoch:  44 Iteration:  3150 | train loss: 0.0286 | test accuracy: 0.875\n",
            "Epoch:  45 Iteration:  3220 | train loss: 0.3114 | test accuracy: 0.879\n",
            "Epoch:  46 Iteration:  3290 | train loss: 0.7721 | test accuracy: 0.875\n",
            "Epoch:  47 Iteration:  3360 | train loss: 0.3326 | test accuracy: 0.879\n",
            "Epoch:  48 Iteration:  3430 | train loss: 0.0599 | test accuracy: 0.875\n",
            "Epoch:  49 Iteration:  3500 | train loss: 0.0631 | test accuracy: 0.879\n",
            "Epoch:  50 Iteration:  3570 | train loss: 0.3467 | test accuracy: 0.879\n",
            "Epoch:  51 Iteration:  3640 | train loss: 0.0378 | test accuracy: 0.879\n",
            "Epoch:  52 Iteration:  3710 | train loss: 0.0207 | test accuracy: 0.879\n",
            "Epoch:  53 Iteration:  3780 | train loss: 0.1123 | test accuracy: 0.875\n",
            "Epoch:  54 Iteration:  3850 | train loss: 0.0895 | test accuracy: 0.875\n",
            "Epoch:  55 Iteration:  3920 | train loss: 0.0715 | test accuracy: 0.882\n",
            "Epoch:  56 Iteration:  3990 | train loss: 0.2231 | test accuracy: 0.879\n",
            "Epoch:  57 Iteration:  4060 | train loss: 1.5001 | test accuracy: 0.882\n",
            "Epoch:  58 Iteration:  4130 | train loss: 0.0943 | test accuracy: 0.882\n",
            "Epoch:  59 Iteration:  4200 | train loss: 0.0332 | test accuracy: 0.882\n",
            "Epoch:  60 Iteration:  4270 | train loss: 0.0813 | test accuracy: 0.882\n",
            "Epoch:  61 Iteration:  4340 | train loss: 0.0945 | test accuracy: 0.875\n",
            "Epoch:  62 Iteration:  4410 | train loss: 0.2650 | test accuracy: 0.879\n",
            "Epoch:  63 Iteration:  4480 | train loss: 0.3068 | test accuracy: 0.875\n",
            "Epoch:  64 Iteration:  4550 | train loss: 0.0703 | test accuracy: 0.882\n",
            "Epoch:  65 Iteration:  4620 | train loss: 0.3539 | test accuracy: 0.882\n",
            "Epoch:  66 Iteration:  4690 | train loss: 0.3402 | test accuracy: 0.882\n",
            "Epoch:  67 Iteration:  4760 | train loss: 0.1544 | test accuracy: 0.889\n",
            "Epoch:  68 Iteration:  4830 | train loss: 0.1286 | test accuracy: 0.882\n",
            "Epoch:  69 Iteration:  4900 | train loss: 0.0310 | test accuracy: 0.875\n",
            "Epoch:  70 Iteration:  4970 | train loss: 0.1016 | test accuracy: 0.882\n",
            "Epoch:  71 Iteration:  5040 | train loss: 0.0111 | test accuracy: 0.882\n",
            "Epoch:  72 Iteration:  5110 | train loss: 0.1406 | test accuracy: 0.882\n",
            "Epoch:  73 Iteration:  5180 | train loss: 0.0780 | test accuracy: 0.886\n",
            "Epoch:  74 Iteration:  5250 | train loss: 0.1660 | test accuracy: 0.889\n",
            "Epoch:  75 Iteration:  5320 | train loss: 0.0254 | test accuracy: 0.889\n",
            "Epoch:  76 Iteration:  5390 | train loss: 0.0472 | test accuracy: 0.882\n",
            "Epoch:  77 Iteration:  5460 | train loss: 0.0563 | test accuracy: 0.886\n",
            "Epoch:  78 Iteration:  5530 | train loss: 0.7077 | test accuracy: 0.889\n",
            "Epoch:  79 Iteration:  5600 | train loss: 0.8521 | test accuracy: 0.889\n",
            "Epoch:  80 Iteration:  5670 | train loss: 0.1223 | test accuracy: 0.882\n",
            "Epoch:  81 Iteration:  5740 | train loss: 0.0189 | test accuracy: 0.886\n",
            "Epoch:  82 Iteration:  5810 | train loss: 0.4706 | test accuracy: 0.886\n",
            "Epoch:  83 Iteration:  5880 | train loss: 0.0163 | test accuracy: 0.886\n",
            "Epoch:  84 Iteration:  5950 | train loss: 0.1509 | test accuracy: 0.889\n",
            "Epoch:  85 Iteration:  6020 | train loss: 0.1766 | test accuracy: 0.886\n",
            "Epoch:  86 Iteration:  6090 | train loss: 0.5410 | test accuracy: 0.886\n",
            "Epoch:  87 Iteration:  6160 | train loss: 0.0820 | test accuracy: 0.886\n",
            "Epoch:  88 Iteration:  6230 | train loss: 0.0862 | test accuracy: 0.889\n",
            "Epoch:  89 Iteration:  6300 | train loss: 0.0162 | test accuracy: 0.886\n",
            "Epoch:  90 Iteration:  6370 | train loss: 0.0837 | test accuracy: 0.886\n",
            "Epoch:  91 Iteration:  6440 | train loss: 0.4907 | test accuracy: 0.889\n",
            "Epoch:  92 Iteration:  6510 | train loss: 0.8332 | test accuracy: 0.889\n",
            "Epoch:  93 Iteration:  6580 | train loss: 0.0136 | test accuracy: 0.886\n",
            "Epoch:  94 Iteration:  6650 | train loss: 0.1828 | test accuracy: 0.886\n",
            "Epoch:  95 Iteration:  6720 | train loss: 0.0533 | test accuracy: 0.892\n",
            "Epoch:  96 Iteration:  6790 | train loss: 0.4254 | test accuracy: 0.892\n",
            "Epoch:  97 Iteration:  6860 | train loss: 0.4894 | test accuracy: 0.892\n",
            "Epoch:  98 Iteration:  6930 | train loss: 0.0667 | test accuracy: 0.886\n",
            "Epoch:  99 Iteration:  7000 | train loss: 0.1009 | test accuracy: 0.896\n",
            "total time:  37.25772218999998\n"
          ]
        }
      ]
    }
  ]
}