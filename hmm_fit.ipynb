{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hmm-fit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Px3RmzMhFa-A",
        "outputId": "e0201b47-1e8b-4630-837a-0020dbdd617b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.7/dist-packages (0.2.7)\n",
            "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.21.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.16->hmmlearn) (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install hmmlearn\n",
        "from hmmlearn import hmm\n",
        "from torch.distributions import uniform\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate Datasets using HMM\n",
        "\n",
        "def random_transmat(n_states):\n",
        "    matrix = np.random.rand(n_states, n_states)\n",
        "    return matrix/matrix.sum(axis=1)[:,None]\n",
        "\n",
        "def random_startprob(n_states):\n",
        "    startprob = np.random.rand(n_states)\n",
        "    return startprob/startprob.sum()\n",
        "\n",
        "def random_means(n_features):\n",
        "    return np.random.randint(5, size=(n_features,n_features))\n",
        "\n",
        "def generate_hmm(n_states, n_features , n_samples, length):\n",
        "    #GENERATING A MODEL\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = random_startprob(n_states)\n",
        "    model.transmat_ = random_transmat(n_states)\n",
        "\n",
        "    model.means_ = random_means(n_features)\n",
        "    model.covars_ = np.tile(np.identity(n_features), (n_features, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "\n",
        "    #Number of Samples in Dataset\n",
        "    dataset=[]\n",
        "    states = []\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        X, Z = model.sample(length)\n",
        "        dataset.append(np.array(X))\n",
        "        states.append(Z)\n",
        "\n",
        "    dataset = np.stack(dataset)\n",
        "    \n",
        "    return dataset, np.array(states)\n",
        "\n",
        "def generate_time_dependent_flip(n_samples, length, startprob, transmat):\n",
        "    #GENERATING A MODEL\n",
        "\n",
        "\n",
        "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
        "    model.startprob_ = startprob\n",
        "    model.transmat_ = transmat\n",
        "\n",
        "    #this doesn't actually matter for us\n",
        "    model.means_ = np.array([[0.0, 0.0], \n",
        "                             [5.0, 10.0]])\n",
        "    model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n",
        "\n",
        "\n",
        "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
        "    \n",
        "    #Number of Samples in Dataset\n",
        "    dataset=[]\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        X, Z = model.sample(length)\n",
        "        dataset.append(np.array(Z))\n",
        "\n",
        "    dataset = np.stack(dataset)\n",
        "    \n",
        "    return dataset"
      ],
      "metadata": {
        "id": "xpW126KEFfM3"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Injecting Noise into Labels\n",
        "\n",
        "#Given a flip_mask, flip an input\n",
        "def flip(array, flip_mask):\n",
        "    flipped_array = np.logical_xor(array, flip_mask, out=array)\n",
        "    return flipped_array\n",
        "\n",
        "#Class Independent / Time Independent\n",
        "def flip_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Dependent / Time Independent\n",
        "def flip_labels_class(array, flip_probability_0, flip_probability_1):\n",
        "    flip_mask = []\n",
        "    for elem in array:\n",
        "        if elem == 0:\n",
        "            to_flip = np.random.binomial(1, flip_probability_0, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "        else:\n",
        "            to_flip = np.random.binomial(1, flip_probability_1, 1)[0]\n",
        "            flip_mask.append(to_flip)\n",
        "            \n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "#Class Independent / Time Dependent\n",
        "def flip_labels_time(array, startprob, transmat):\n",
        "    flip_mask = generate_time_dependent_flip(1, len(array), startprob, transmat)[0]\n",
        "\n",
        "    return flip(array, flip_mask)\n",
        "\n",
        "\n",
        "#Class Dependent / Time Dependent\n",
        "#This can be achieved by careful design of the transition matrix (transmat)"
      ],
      "metadata": {
        "id": "OgyRemBtNY8o"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset,Z = generate_hmm(2,3,10, 100)"
      ],
      "metadata": {
        "id": "oW2zggR4NdQS"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmIkYpDuNgTw",
        "outputId": "a1262ac0-6d09-4a4c-8352-42898ec7d2c9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "startprob = random_startprob(2)\n",
        "transmat = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])"
      ],
      "metadata": {
        "id": "PXjUWBM5NjW5"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(n_states, n_features,n_samples, length, train_ratio, method, \n",
        "                     flip_probability= None, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=None, transmat=None):\n",
        "    \n",
        "    #Generate Data\n",
        "    dataset, states_true = generate_hmm(n_states, n_features , n_samples, length)\n",
        "    \n",
        "    x_train = dataset[:int(train_ratio*n_samples)]\n",
        "    x_test = dataset[int(train_ratio*n_samples):]\n",
        "    \n",
        "   \n",
        "    #Flip The Labels according to method\n",
        "    \n",
        "    states_flipped = []\n",
        "    \n",
        "    if method == \"basic\":\n",
        "        for item in states_true:\n",
        "            states_flipped.append(flip_labels_basic(item, flip_probability))\n",
        "        \n",
        "    elif method == \"class\":\n",
        "        for item in states_true:\n",
        "            states_flipped.append(flip_labels_class(item, flip_probability_0, flip_probability_1))\n",
        "        \n",
        "    elif method == \"time\":\n",
        "        for item in states_true:\n",
        "            states_flipped.append(flip_labels_time(item, startprob, transmat))\n",
        "        \n",
        "    y_train_true = states_true[:int(train_ratio*n_samples)]\n",
        "    y_test_true = states_true[int(train_ratio*n_samples):]\n",
        "    \n",
        "    y_train_flipped = np.array(states_flipped[:int(train_ratio*n_samples)])\n",
        "    y_test_flipped = np.array(states_flipped[int(train_ratio*n_samples):])\n",
        "    \n",
        "    \n",
        "    return x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped"
      ],
      "metadata": {
        "id": "1q2hceLfNoDS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "n_samples = 100\n",
        "length = 1000\n",
        "train_ratio = 0.7\n",
        "method = \"basic\"\n",
        "flip_probability = 0.1\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped= generate_dataset(n_states, n_features,n_samples, length, train_ratio, method, \n",
        "                     flip_probability, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=None, transmat=None)"
      ],
      "metadata": {
        "id": "-sCnZqB2Nt6D"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCE3DxcaNx7G",
        "outputId": "61189a73-f406-4c74-b377-1f1216675a74"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 1000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "n_samples = 100\n",
        "length = 1000\n",
        "train_ratio = 0.7\n",
        "method = \"class\"\n",
        "flip_probability_0 = 0.1\n",
        "flip_probability_1 = 0.2\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features,n_samples, length, train_ratio, method, \n",
        "                     flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1,\n",
        "                    startprob=None, transmat=None)"
      ],
      "metadata": {
        "id": "T0BHlmSTN1Ti"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcjN-zbCN5Yc",
        "outputId": "cbc4ea4d-6164-4002-b20d-5a466df7f3b7"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 1000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_states = 2\n",
        "n_features = 3\n",
        "n_samples = 100\n",
        "length = 1000\n",
        "train_ratio = 0.7\n",
        "method = \"time\"\n",
        "\n",
        "startprob = random_startprob(2)\n",
        "transmat = np.array([[0.95, 0.05],\n",
        "                    [0.95, 0.05]])\n",
        "\n",
        "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features,n_samples, length, train_ratio, method, \n",
        "                     flip_probability=None, flip_probability_0=None, flip_probability_1=None,\n",
        "                    startprob=startprob, transmat=transmat)"
      ],
      "metadata": {
        "id": "H1umMDueOA4M"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_flipped.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OPUhz7oOCBf",
        "outputId": "420c9d09-b8a8-451f-dde0-10ccf2430111"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_true))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_true))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=25, shuffle=True)"
      ],
      "metadata": {
        "id": "Mydxw0ZvON9Z"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
        "train_fipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
        "test_fipped_loader = data_utils.DataLoader(test_flipped, batch_size=25, shuffle=True)"
      ],
      "metadata": {
        "id": "2eQmYq68OizY"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "OphpChUuPAo9"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ],
      "metadata": {
        "id": "f6HsUdvpPGMi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='mnist', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.02, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# get dataset\n",
        "#==============================================================================\n",
        "  \n",
        "model = NoisyRNN(input_dim=int(3), output_classes=1000, n_units=args.n_units, \n",
        "              eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "              init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "              add_noise=args.add_noise, mult_noise=args.mult_noise).to(device)\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)        \n",
        "noise = torch.randn(1,70,1000,3).float()\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# Model summary\n",
        "#==============================================================================\n",
        "print(model)    \n",
        "print('**** Setup ****')\n",
        "print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "print('************')    \n",
        "   \n",
        "\n",
        "if args.optimizer == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "elif  args.optimizer == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "else:\n",
        "    print(\"Unexpected optimizer!\")\n",
        "    raise \n",
        "\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# training and testing\n",
        "count = 0\n",
        "loss_hist = []\n",
        "test_acc = []\n",
        "\n",
        "t0 = timeit.default_timer()\n",
        "for epoch in range(args.epochs):\n",
        "    model.train()\n",
        "    lossaccum = 0\n",
        "    \n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        count += 1\n",
        "        \n",
        "        # Reshape data for recurrent unit\n",
        "        inputs = Variable(x.view(-1, 1000, int(3))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "        targets = Variable(y).to(device)\n",
        "\n",
        "                 \n",
        "        # send data to recurrent unit    \n",
        "        output = model(inputs, mode='train')\n",
        "        loss = loss_func(output, targets.float())\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()          \n",
        "        \n",
        "        if args.gclip != 0.0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "            \n",
        "        optimizer.step() # update weights\n",
        "        lossaccum += loss.item()\n",
        "\n",
        "        if args.model == 'test':\n",
        "            D = model.W.weight.data.cpu().numpy()  \n",
        "            u, s, v = np.linalg.svd(D, 0)\n",
        "            model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "    loss_hist.append(lossaccum)    \n",
        "     \n",
        "    if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total_num = 0\n",
        "        for data, target in test_loader: \n",
        "            data, target = data.to(device), target.to(device)               \n",
        "            output = model(data.view(-1, 1, int(3)))                  \n",
        "            \n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            #print(output.shape)\n",
        "            #print(pred.shape)\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "            total_num += len(data)\n",
        "        \n",
        "        accuracy = correct / total_num\n",
        "        test_acc.append(accuracy)\n",
        "        print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "#        if args.model == 'NoisyRNN':\n",
        "#            B = model.B.data.cpu().numpy()            \n",
        "#            A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "#            A = 0.5 * (A + A.T)\n",
        "#            e, _ = np.linalg.eig(A)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "#            \n",
        "#            C = model.C.data.cpu().numpy()            \n",
        "#            W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "#            e, _ = np.linalg.eig(W)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "            \n",
        "             \n",
        "\n",
        "    # schedule learning rate decay    \n",
        "    optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "\n",
        "torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "pickle.dump(data,f)\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzbqUW26PJZz",
        "outputId": "6ff90e19-8458-41f2-d4ab-6eb3df3ed806"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=1000, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 73.51k\n",
            "************\n",
            "Epoch:  0 Iteration:  7 | train loss: 16187.3438 | test accuracy: 0.000\n",
            "Epoch:  1 Iteration:  14 | train loss: 32849.6680 | test accuracy: 0.000\n",
            "Epoch:  2 Iteration:  21 | train loss: 10942.8926 | test accuracy: 0.000\n",
            "Epoch:  3 Iteration:  28 | train loss: 6418.6597 | test accuracy: 0.000\n",
            "Epoch:  4 Iteration:  35 | train loss: 6152.6553 | test accuracy: 0.000\n",
            "Epoch:  5 Iteration:  42 | train loss: 40529.3984 | test accuracy: 0.000\n",
            "Epoch:  6 Iteration:  49 | train loss: 8468.4795 | test accuracy: 0.000\n",
            "Epoch:  7 Iteration:  56 | train loss: 6916.9517 | test accuracy: 0.000\n",
            "Epoch:  8 Iteration:  63 | train loss: 6433.4702 | test accuracy: 0.000\n",
            "Epoch:  9 Iteration:  70 | train loss: 6125.6685 | test accuracy: 0.000\n",
            "Epoch:  10 Iteration:  77 | train loss: 5920.1216 | test accuracy: 0.000\n",
            "Epoch:  11 Iteration:  84 | train loss: 5914.5049 | test accuracy: 0.000\n",
            "Epoch:  12 Iteration:  91 | train loss: 5844.7202 | test accuracy: 0.000\n",
            "Epoch:  13 Iteration:  98 | train loss: 5801.9414 | test accuracy: 0.000\n",
            "Epoch:  14 Iteration:  105 | train loss: 5747.9507 | test accuracy: 0.000\n",
            "Epoch:  15 Iteration:  112 | train loss: 5726.9561 | test accuracy: 0.000\n",
            "Epoch:  16 Iteration:  119 | train loss: 5753.5581 | test accuracy: 0.000\n",
            "Epoch:  17 Iteration:  126 | train loss: 5745.5728 | test accuracy: 0.000\n",
            "Epoch:  18 Iteration:  133 | train loss: 5739.5190 | test accuracy: 0.000\n",
            "Epoch:  19 Iteration:  140 | train loss: 5739.0942 | test accuracy: 0.000\n",
            "Epoch:  20 Iteration:  147 | train loss: 5727.0557 | test accuracy: 0.000\n",
            "Epoch:  21 Iteration:  154 | train loss: 5731.0039 | test accuracy: 0.000\n",
            "Epoch:  22 Iteration:  161 | train loss: 5686.9541 | test accuracy: 0.000\n",
            "Epoch:  23 Iteration:  168 | train loss: 5664.8174 | test accuracy: 0.000\n",
            "Epoch:  24 Iteration:  175 | train loss: 5754.7568 | test accuracy: 0.000\n",
            "Epoch:  25 Iteration:  182 | train loss: 5721.5103 | test accuracy: 0.000\n",
            "Epoch:  26 Iteration:  189 | train loss: 5760.7710 | test accuracy: 0.000\n",
            "Epoch:  27 Iteration:  196 | train loss: 5693.2017 | test accuracy: 0.000\n",
            "Epoch:  28 Iteration:  203 | train loss: 5715.6123 | test accuracy: 0.000\n",
            "Epoch:  29 Iteration:  210 | train loss: 5699.9756 | test accuracy: 0.000\n",
            "Epoch:  30 Iteration:  217 | train loss: 5662.9365 | test accuracy: 0.000\n",
            "New learning rate is:  0.002\n",
            "Epoch:  31 Iteration:  224 | train loss: 5663.6611 | test accuracy: 0.000\n",
            "Epoch:  32 Iteration:  231 | train loss: 5667.2090 | test accuracy: 0.000\n",
            "Epoch:  33 Iteration:  238 | train loss: 5699.4673 | test accuracy: 0.000\n",
            "Epoch:  34 Iteration:  245 | train loss: 5681.9932 | test accuracy: 0.000\n",
            "Epoch:  35 Iteration:  252 | train loss: 5670.8101 | test accuracy: 0.000\n",
            "Epoch:  36 Iteration:  259 | train loss: 5627.9116 | test accuracy: 0.000\n",
            "Epoch:  37 Iteration:  266 | train loss: 5646.1919 | test accuracy: 0.000\n",
            "Epoch:  38 Iteration:  273 | train loss: 5737.3027 | test accuracy: 0.000\n",
            "Epoch:  39 Iteration:  280 | train loss: 5667.1768 | test accuracy: 0.000\n",
            "Epoch:  40 Iteration:  287 | train loss: 5686.1514 | test accuracy: 0.000\n",
            "Epoch:  41 Iteration:  294 | train loss: 5664.4058 | test accuracy: 0.000\n",
            "Epoch:  42 Iteration:  301 | train loss: 5671.5981 | test accuracy: 0.000\n",
            "Epoch:  43 Iteration:  308 | train loss: 5709.5303 | test accuracy: 0.000\n",
            "Epoch:  44 Iteration:  315 | train loss: 5663.0156 | test accuracy: 0.000\n",
            "Epoch:  45 Iteration:  322 | train loss: 5738.2339 | test accuracy: 0.000\n",
            "Epoch:  46 Iteration:  329 | train loss: 5726.8623 | test accuracy: 0.000\n",
            "Epoch:  47 Iteration:  336 | train loss: 5702.6040 | test accuracy: 0.000\n",
            "Epoch:  48 Iteration:  343 | train loss: 5611.6572 | test accuracy: 0.000\n",
            "Epoch:  49 Iteration:  350 | train loss: 5663.6382 | test accuracy: 0.000\n",
            "Epoch:  50 Iteration:  357 | train loss: 5672.1494 | test accuracy: 0.000\n",
            "Epoch:  51 Iteration:  364 | train loss: 5666.3823 | test accuracy: 0.000\n",
            "Epoch:  52 Iteration:  371 | train loss: 5658.0703 | test accuracy: 0.000\n",
            "Epoch:  53 Iteration:  378 | train loss: 5697.2446 | test accuracy: 0.000\n",
            "Epoch:  54 Iteration:  385 | train loss: 5707.5244 | test accuracy: 0.000\n",
            "Epoch:  55 Iteration:  392 | train loss: 5731.0088 | test accuracy: 0.000\n",
            "Epoch:  56 Iteration:  399 | train loss: 5682.7002 | test accuracy: 0.000\n",
            "Epoch:  57 Iteration:  406 | train loss: 5717.8345 | test accuracy: 0.000\n",
            "Epoch:  58 Iteration:  413 | train loss: 5681.2905 | test accuracy: 0.000\n",
            "Epoch:  59 Iteration:  420 | train loss: 5684.7002 | test accuracy: 0.000\n",
            "Epoch:  60 Iteration:  427 | train loss: 5723.2671 | test accuracy: 0.000\n",
            "Epoch:  61 Iteration:  434 | train loss: 5642.1313 | test accuracy: 0.000\n",
            "Epoch:  62 Iteration:  441 | train loss: 5640.1748 | test accuracy: 0.000\n",
            "Epoch:  63 Iteration:  448 | train loss: 5685.9033 | test accuracy: 0.000\n",
            "Epoch:  64 Iteration:  455 | train loss: 5681.0679 | test accuracy: 0.000\n",
            "Epoch:  65 Iteration:  462 | train loss: 5665.7612 | test accuracy: 0.000\n",
            "Epoch:  66 Iteration:  469 | train loss: 5670.4644 | test accuracy: 0.000\n",
            "Epoch:  67 Iteration:  476 | train loss: 5666.6064 | test accuracy: 0.000\n",
            "Epoch:  68 Iteration:  483 | train loss: 5685.6948 | test accuracy: 0.000\n",
            "Epoch:  69 Iteration:  490 | train loss: 5664.8530 | test accuracy: 0.000\n",
            "Epoch:  70 Iteration:  497 | train loss: 5719.4297 | test accuracy: 0.000\n",
            "Epoch:  71 Iteration:  504 | train loss: 5696.2603 | test accuracy: 0.000\n",
            "Epoch:  72 Iteration:  511 | train loss: 5655.1650 | test accuracy: 0.000\n",
            "Epoch:  73 Iteration:  518 | train loss: 5683.1182 | test accuracy: 0.000\n",
            "Epoch:  74 Iteration:  525 | train loss: 5726.2266 | test accuracy: 0.000\n",
            "Epoch:  75 Iteration:  532 | train loss: 5703.9258 | test accuracy: 0.000\n",
            "Epoch:  76 Iteration:  539 | train loss: 5678.8931 | test accuracy: 0.000\n",
            "Epoch:  77 Iteration:  546 | train loss: 5641.4443 | test accuracy: 0.000\n",
            "Epoch:  78 Iteration:  553 | train loss: 5666.8774 | test accuracy: 0.000\n",
            "Epoch:  79 Iteration:  560 | train loss: 5682.3623 | test accuracy: 0.000\n",
            "Epoch:  80 Iteration:  567 | train loss: 5686.0781 | test accuracy: 0.000\n",
            "Epoch:  81 Iteration:  574 | train loss: 5616.9307 | test accuracy: 0.000\n",
            "Epoch:  82 Iteration:  581 | train loss: 5729.1489 | test accuracy: 0.000\n",
            "Epoch:  83 Iteration:  588 | train loss: 5631.0615 | test accuracy: 0.000\n",
            "Epoch:  84 Iteration:  595 | train loss: 5662.3975 | test accuracy: 0.000\n",
            "Epoch:  85 Iteration:  602 | train loss: 5689.1943 | test accuracy: 0.000\n",
            "Epoch:  86 Iteration:  609 | train loss: 5639.5884 | test accuracy: 0.000\n",
            "Epoch:  87 Iteration:  616 | train loss: 5753.4131 | test accuracy: 0.000\n",
            "Epoch:  88 Iteration:  623 | train loss: 5669.8794 | test accuracy: 0.000\n",
            "Epoch:  89 Iteration:  630 | train loss: 5668.6304 | test accuracy: 0.000\n",
            "Epoch:  90 Iteration:  637 | train loss: 5714.6729 | test accuracy: 0.000\n",
            "Epoch:  91 Iteration:  644 | train loss: 5702.1763 | test accuracy: 0.000\n",
            "Epoch:  92 Iteration:  651 | train loss: 5708.2598 | test accuracy: 0.000\n",
            "Epoch:  93 Iteration:  658 | train loss: 5667.3662 | test accuracy: 0.000\n",
            "Epoch:  94 Iteration:  665 | train loss: 5637.7656 | test accuracy: 0.000\n",
            "Epoch:  95 Iteration:  672 | train loss: 5687.4834 | test accuracy: 0.000\n",
            "Epoch:  96 Iteration:  679 | train loss: 5683.2295 | test accuracy: 0.000\n",
            "Epoch:  97 Iteration:  686 | train loss: 5723.7021 | test accuracy: 0.000\n",
            "Epoch:  98 Iteration:  693 | train loss: 5725.3623 | test accuracy: 0.000\n",
            "Epoch:  99 Iteration:  700 | train loss: 5699.4282 | test accuracy: 0.000\n",
            "total time:  206.06472514899997\n"
          ]
        }
      ]
    }
  ]
}