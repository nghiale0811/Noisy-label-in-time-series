{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Px3RmzMhFa-A",
    "outputId": "3210657b-96ee-4ad4-f8af-59eea30417fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hmmlearn in /cpu008/snagaraj/.local/lib/python3.7/site-packages (0.2.7)\n",
      "Requirement already satisfied: scipy>=0.19 in /cpu008/snagaraj/.local/lib/python3.7/site-packages (from hmmlearn) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /cpu008/snagaraj/.local/lib/python3.7/site-packages (from hmmlearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.10 in /cpu008/snagaraj/.local/lib/python3.7/site-packages (from hmmlearn) (1.19.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /cpu008/snagaraj/.local/lib/python3.7/site-packages (from scikit-learn>=0.16->hmmlearn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /cpu008/snagaraj/.local/lib/python3.7/site-packages (from scikit-learn>=0.16->hmmlearn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "!pip install hmmlearn\n",
    "from hmmlearn import hmm\n",
    "from torch.distributions import uniform\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from torch._utils import _accumulate\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.utils.clip_grad as clip_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "import datetime as dt\n",
    "import time\n",
    "import random\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xpW126KEFfM3"
   },
   "outputs": [],
   "source": [
    "#Generate Datasets using HMM\n",
    "\n",
    "def random_transmat(n_states):\n",
    "    matrix = np.random.rand(n_states, n_states)\n",
    "    return matrix/matrix.sum(axis=1)[:,None]\n",
    "\n",
    "def random_startprob(n_states):\n",
    "    startprob = np.random.rand(n_states)\n",
    "    return startprob/startprob.sum()\n",
    "\n",
    "def random_means(n_features):\n",
    "    return np.random.randint(5, size=(n_features,n_features))\n",
    "\n",
    "def generate_hmm(n_states, n_features , length):\n",
    "    #GENERATING A MODEL\n",
    "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
    "    model.startprob_ = random_startprob(n_states)\n",
    "    model.transmat_ = random_transmat(n_states)\n",
    "\n",
    "    model.means_ = random_means(n_features)\n",
    "    model.covars_ = np.tile(np.identity(n_features), (n_features, 1, 1))\n",
    "\n",
    "\n",
    "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
    "\n",
    "    #Number of Samples in Dataset\n",
    "    dataset=[]\n",
    "    states = []\n",
    "\n",
    "    \n",
    "    X, Z = model.sample(length)\n",
    "    dataset.append(np.array(X))\n",
    "    states.append(Z)\n",
    "\n",
    "    dataset = np.stack(dataset)\n",
    "    \n",
    "    return np.array(X), np.array(Z)\n",
    "\n",
    "def sliding_windows(dataset, Z, window_length = 10):\n",
    "    lis = []\n",
    "    targets=[]\n",
    "\n",
    "    window_length = 10\n",
    "    if len(dataset) >= window_length+1:\n",
    "        for i in range(0, len(dataset)-window_length, 1):\n",
    "\n",
    "            x_window = dataset[i:i+window_length, :]\n",
    "            z_window = Z[i:i+window_length]\n",
    "\n",
    "            lis.append(x_window)\n",
    "            targets.append(z_window[-1])\n",
    "    return np.array(lis), np.array(targets)\n",
    "\n",
    "def generate_time_dependent_flip(length, startprob, transmat):\n",
    "    #GENERATING A MODEL\n",
    "\n",
    "\n",
    "    model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"full\")\n",
    "    model.startprob_ = startprob\n",
    "    model.transmat_ = transmat\n",
    "\n",
    "    #this doesn't actually matter for us\n",
    "    model.means_ = np.array([[0.0, 0.0], \n",
    "                             [5.0, 10.0]])\n",
    "    model.covars_ = np.tile(np.identity(2), (3, 1, 1))\n",
    "\n",
    "\n",
    "    #SAMPLING FROM MODEL and STORING IN TENSOR\n",
    "\n",
    "    #Number of Samples in Dataset\n",
    "\n",
    "\n",
    "    X, Z = model.sample(length)\n",
    "\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "OgyRemBtNY8o"
   },
   "outputs": [],
   "source": [
    "#Injecting Noise into Labels\n",
    "\n",
    "#Given a flip_mask, flip an input\n",
    "def flip(array, flip_mask):\n",
    "    flipped_array = np.logical_xor(array, flip_mask ,out=array.copy())\n",
    "    #flipped_array = np.logical_not(array, where=flip_mask, out=array.copy())\n",
    "    #flipped_array = np.where(flip_mask, ~array, array)\n",
    "    return flipped_array\n",
    "\n",
    "#Class Independent / Time Independent\n",
    "def flip_labels_basic(array, flip_probability):\n",
    "    flip_mask = np.random.binomial(1, flip_probability, len(array))\n",
    "    return flip(array, flip_mask)\n",
    "\n",
    "#Class Dependent / Time Independent\n",
    "def flip_labels_class(array, flip_probability_0, flip_probability_1):\n",
    "    flip_mask = []\n",
    "    for elem in array:\n",
    "        if elem == 0:\n",
    "            to_flip = np.random.binomial(1, flip_probability_0, 1)[0]\n",
    "            flip_mask.append(to_flip)\n",
    "        else:\n",
    "            to_flip = np.random.binomial(1, flip_probability_1, 1)[0]\n",
    "            flip_mask.append(to_flip)\n",
    "            \n",
    "    return flip(array, flip_mask)\n",
    "\n",
    "#Class Independent / Time Dependent\n",
    "def flip_labels_time(array, startprob, transmat):\n",
    "    flip_mask = generate_time_dependent_flip(len(array), startprob, transmat)\n",
    "    #print(flip_mask)\n",
    "    return flip(array, flip_mask)\n",
    "\n",
    "\n",
    "#Class Dependent / Time Dependent\n",
    "#This can be achieved by careful design of the transition matrix (transmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oW2zggR4NdQS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zmIkYpDuNgTw",
    "outputId": "8cdd0741-3063-4aa5-df5b-20072a35b335"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXjUWBM5NjW5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "1q2hceLfNoDS"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
    "                     flip_probability= None, flip_probability_0=None, flip_probability_1=None,\n",
    "                    startprob=None, transmat=None):\n",
    "    \n",
    "    #Generate Data\n",
    "    dataset, states_true = generate_hmm(n_states, n_features , length)\n",
    "    \n",
    "    if method == \"basic\":\n",
    "        states_flipped = (flip_labels_basic(states_true, flip_probability))\n",
    "        \n",
    "    elif method == \"class\":\n",
    "        states_flipped = (flip_labels_class(states_true, flip_probability_0, flip_probability_1))\n",
    "        \n",
    "    elif method == \"time\":\n",
    "        states_flipped = (flip_labels_time(states_true, startprob, transmat))\n",
    "    \n",
    "    #RESHAPE INTO WINDOWS\n",
    "    dataset_windows, states_true = sliding_windows(dataset, states_true, window_length= window_length)\n",
    "    \n",
    "    #RESHAPE INTO WINDOWS\n",
    "    _, states_flipped = sliding_windows(dataset, states_flipped, window_length= window_length)\n",
    "    \n",
    "    x_train = dataset_windows[:int(train_ratio*len(dataset_windows)),:,:]\n",
    "    x_test = dataset_windows[int(train_ratio*len(dataset_windows)):,:,:]\n",
    "    \n",
    "   \n",
    "    #Flip The Labels according to method\n",
    "    \n",
    "    \n",
    "    y_train_true = states_true[:int(train_ratio*len(dataset_windows))]\n",
    "    y_test_true = states_true[int(train_ratio*len(dataset_windows)):]\n",
    "    \n",
    "    y_train_flipped = np.array(states_flipped[:int(train_ratio*len(dataset_windows))])\n",
    "    y_test_flipped = np.array(states_flipped[int(train_ratio*len(dataset_windows)):])\n",
    "    \n",
    "    \n",
    "    return x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OphpChUuPAo9"
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get a gpu if available.\"\"\"\n",
    "    if torch.cuda.device_count()>0:\n",
    "        device = torch.device('cuda')\n",
    "        print(\"Connected to a GPU\")\n",
    "    else:\n",
    "        print(\"Using the CPU\")\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "def which_device(model):\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "\n",
    "def add_channels(X):\n",
    "    if len(X.shape) == 2:\n",
    "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
    "\n",
    "    elif len(X.shape) == 3:\n",
    "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
    "\n",
    "    else:\n",
    "        return \"dimenional error\"\n",
    "    \n",
    "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
    "\n",
    "    if strategy=='normal':\n",
    "        if epoch in decayEpoch:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= decay_eff\n",
    "            print('New learning rate is: ', param_group['lr'])\n",
    "    else:\n",
    "        print('wrong strategy')\n",
    "        raise ValueError('A very specific bad thing happened.')\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "    \n",
    "    \n",
    "def gaussian_init_(n_units, std=1):    \n",
    "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
    "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
    "    return A_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "g_bwMeNQSvCD"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, feature_size, n_state, hidden_size=16, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
    "                 seed=random.seed('2021')):\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_state = n_state\n",
    "        self.seed = seed\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.rnn_type = rnn\n",
    "        self.regres = regres\n",
    "        self.return_all = return_all\n",
    "        \n",
    "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
    "\n",
    "        self.regressor = nn.Sequential(#nn.BatchNorm1d(num_features=self.hidden_size),\n",
    "                                       #nn.ReLU(),\n",
    "                                       #nn.Dropout(0.1),\n",
    "                                       nn.Linear(self.hidden_size, self.n_state))\n",
    "                                       #nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input, past_state=None, **kwargs):\n",
    "        input = input.to(self.device)\n",
    "        self.rnn.to(self.device)\n",
    "        self.regressor.to(self.device)\n",
    "        if not past_state:\n",
    "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
    "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            all_encodings, encoding = self.rnn(input, past_state)\n",
    "        else:\n",
    "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
    "        \n",
    "        if self.regres:\n",
    "            if not self.return_all:\n",
    "                #print(encoding.shape)\n",
    "                #print(encoding.view(encoding.shape[1], -1).shape)\n",
    "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
    "            else:\n",
    "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
    "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
    "        else:\n",
    "            return encoding.view(encoding.shape[1], -1)\n",
    "        \n",
    "        \n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
    "            \n",
    "def save_ckpt(generator_model, output_dir, data):\n",
    "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
    "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
    "    os.makedirs(check_pt_dir, exist_ok=True)\n",
    "    torch.save(generator_model.state_dict(), fname)\n",
    "    \n",
    "def get_accuracy(model, loader):\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    for xs, ts in loader:\n",
    "        xs.to(device)\n",
    "        ts.to(device)\n",
    "        zs = model(xs)\n",
    "        \n",
    "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        ts = torch.argmax(ts, 1)\n",
    "\n",
    "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
    "        total += int(ts.shape[0])\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ZfHKnAlSTDiC"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, n_epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
    "    \n",
    "    best_loss = 10000.0\n",
    "    \n",
    "    n=0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t0 = time.time()\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        total_train_loss = 0\n",
    "        train_losses=[]\n",
    "        model = model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = (time.time() - t0)\n",
    "                #print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            b_input= batch[0].to(device)\n",
    "            b_target =  batch[1].to(device)\n",
    "            iters.append(n)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(b_input)\n",
    "            #print(out)\n",
    "            #target = torch.argmax(b_target, 1)\n",
    "            target = b_target.squeeze()\n",
    "            #print(b_target)\n",
    "            #print(target)\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if n % 10 == 0:\n",
    "                iters_sub.append(n)\n",
    "                \n",
    "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
    "                #print(get_accuracy(model, train_dataloader))\n",
    "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
    "            # increment the iteration number\n",
    "            n += 1\n",
    "\n",
    "        training_time = (time.time() - t0)\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        #print(\"\")\n",
    "        #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        #print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "    \n",
    "        history['train'].append(train_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train loss {train_loss} ')\n",
    "    #plt.style.use('seaborn-white')\n",
    "    #plt.plot(history['train'])\n",
    "\n",
    "    #plt.title('GRU  Training Curves')\n",
    "    #plt.ylabel('CE Loss')\n",
    "    #plt.xlabel('Epoch Number')\n",
    "    #plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    #plt.show()\n",
    "    \n",
    "    return model.eval(), history\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    #INFERENCE ON TEST SET\n",
    "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
    "    #print(model(x_test).max(1, keepdim=True))\n",
    "    #print(predictions)\n",
    "    y_test = y_test.numpy()\n",
    "    #print(y_test)\n",
    "    y_pred = predictions.squeeze().cpu().numpy()\n",
    "    #print(y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #f1 = f1_score(y_test, y_pred)\n",
    "    #precision = precision_score(y_test, y_pred)\n",
    "    #print(accuracy)\n",
    "    #print(f1)\n",
    "    #print(precision)\n",
    "    return accuracy\n",
    "\n",
    "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
    "    train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train.reshape(-1, 1)))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
    "    \n",
    "    #print(x_test)\n",
    "    #x_test = torch.flip(torch.tensor(x_test, dtype=torch.float), [1,2])\n",
    "    #print(x_test)\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "    \n",
    "    y_test = torch.tensor(y_test, dtype=torch.int)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LSTMClassifier(3, #num features \n",
    "                  2, #num classes,\n",
    "                  seed = seed,\n",
    "                  rnn=\"GRU\" #rnn type\n",
    "                  #return_all = True      \n",
    "            )\n",
    "\n",
    "    model, history = train_model(model, train_loader, 50, 1e-3)\n",
    "    acc = evaluate_model(model, x_test, y_test)\n",
    "    print(\"Test Accuracy: \", acc)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "f6HsUdvpPGMi"
   },
   "outputs": [],
   "source": [
    "class NoisyRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_classes, n_units=16, eps=0.01, \n",
    "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
    "                 solver='base', add_noise=0, mult_noise=0):\n",
    "        super(NoisyRNN, self).__init__()\n",
    "\n",
    "        self.device = get_device()\n",
    "\n",
    "\n",
    "        self.n_units = n_units\n",
    "        self.eps = eps\n",
    "        self.solver = solver\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.gamma_A = gamma_A\n",
    "        self.gamma_W = gamma_W\n",
    "        self.add_noise = add_noise\n",
    "        self.mult_noise = mult_noise\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.E = nn.Linear(input_dim, n_units)\n",
    "        self.D = nn.Linear(n_units, output_classes)     \n",
    "                                            \n",
    "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
    "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
    "        self.I = torch.eye(n_units).to(self.device)   \n",
    "\n",
    "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
    "\n",
    "\n",
    "    def forward(self, x, mode='test'):\n",
    "        T = x.shape[1]\n",
    "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
    "\n",
    "        for i in range(T):\n",
    "            z = self.E(x[:,i,:])\n",
    "\n",
    "            if i == 0:\n",
    "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
    "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
    "                \n",
    "                        \n",
    "            add_noise = 0.0\n",
    "            mult_noise = 1.0\n",
    "            if mode == 'train':\n",
    "                if self.add_noise > 0:\n",
    "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
    "                            \n",
    "                if self.mult_noise > 0:\n",
    "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
    "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
    "                        \n",
    "\n",
    "            if self.solver == 'base': \n",
    "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
    "                h = h + self.eps * h_update\n",
    "            elif self.solver == 'noisy':\n",
    "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
    "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
    "                 \n",
    "                \n",
    "        # Decoder \n",
    "        #----------\n",
    "        out = self.D(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SzbqUW26PJZz",
    "outputId": "7b2333df-8451-4c1d-fe46-55fbdbfc9d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n"
     ]
    }
   ],
   "source": [
    "#code for the driver\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
    "#\n",
    "parser.add_argument('-f')\n",
    "#\n",
    "parser.add_argument('--name', type=str, default='mnist', metavar='N', help='dataset')\n",
    "#\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
    "#\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
    "#\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N', help='number of epochs to train (default: 90)')\n",
    "#\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR', help='learning rate (default: 0.1)')\n",
    "#\n",
    "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
    "#\n",
    "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
    "#\n",
    "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
    "#\n",
    "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
    "#\n",
    "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
    "#\n",
    "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
    "#\n",
    "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
    "#\n",
    "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
    "#\n",
    "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
    "#\n",
    "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
    "#\n",
    "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
    "#\n",
    "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
    "#\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
    "#\n",
    "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
    "#\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
    "#\n",
    "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
    "#\n",
    "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
    "#\n",
    "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
    "#\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not os.path.isdir(args.name + '_results'):\n",
    "    os.mkdir(args.name + '_results')\n",
    "\n",
    "#==============================================================================\n",
    "# set random seed to reproduce the work\n",
    "#==============================================================================\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "#==============================================================================\n",
    "# get device\n",
    "#==============================================================================\n",
    "device = get_device()\n",
    "\n",
    "#==============================================================================\n",
    "# get dataset\n",
    "#==============================================================================\n",
    "\n",
    "def driver(seed, train, test, add_noise=0, mult_noise=0, nrnn = False):\n",
    "  \n",
    "  model = NoisyRNN(input_dim=int(3), output_classes=2, n_units=args.n_units, \n",
    "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
    "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
    "                add_noise=add_noise, mult_noise=mult_noise).to(device)\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)        \n",
    "  noise = torch.randn(1,693,10,3).float()\n",
    "\n",
    "\n",
    "  #==============================================================================\n",
    "  # set random seed to reproduce the work\n",
    "  #==============================================================================\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "  #==============================================================================\n",
    "  # Model summary\n",
    "  #==============================================================================\n",
    "  print(model)    \n",
    "  print('**** Setup ****')\n",
    "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
    "  print('************')    \n",
    "    \n",
    "\n",
    "  if args.optimizer == 'SGD':\n",
    "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
    "  elif  args.optimizer == 'Adam':\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "  else:\n",
    "      print(\"Unexpected optimizer!\")\n",
    "      raise \n",
    "\n",
    "\n",
    "  loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "  # training and testing\n",
    "  count = 0\n",
    "  loss_hist = []\n",
    "  test_acc = []\n",
    "\n",
    "  t0 = timeit.default_timer()\n",
    "  for epoch in range(args.epochs):\n",
    "      model.train()\n",
    "      lossaccum = 0\n",
    "      \n",
    "      for step, (x, y) in enumerate(train):\n",
    "          count += 1\n",
    "          \n",
    "          # Reshape data for recurrent unit\n",
    "          inputs = Variable(x.view(-1, 10, int(3))).to(device) # reshape x to (batch, time_step, input_size)         \n",
    "          targets = Variable(y).to(device)\n",
    "\n",
    "                  \n",
    "          # send data to recurrent unit    \n",
    "          output = model(inputs, mode='train')\n",
    "          loss = loss_func(output, targets.long())\n",
    "          \n",
    "          \n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()          \n",
    "          \n",
    "          if args.gclip != 0.0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
    "              \n",
    "          optimizer.step() # update weights\n",
    "          lossaccum += loss.item()\n",
    "\n",
    "          if args.model == 'test':\n",
    "              D = model.W.weight.data.cpu().numpy()  \n",
    "              u, s, v = np.linalg.svd(D, 0)\n",
    "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
    "\n",
    "      loss_hist.append(lossaccum)    \n",
    "      \n",
    "      if epoch % 1 == 0:\n",
    "          model.eval()\n",
    "          correct = 0\n",
    "          total_num = 0\n",
    "          for data, target in test: \n",
    "              data, target = data.to(device), target.to(device)               \n",
    "              output = model(data.view(-1, 10, int(3)))                  \n",
    "              \n",
    "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "              #print(output.shape)\n",
    "              #print(pred.shape)\n",
    "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "              total_num += len(data)\n",
    "          \n",
    "          accuracy = correct / total_num\n",
    "          test_acc.append(accuracy)\n",
    "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
    "\n",
    "\n",
    "      if nrnn == True:\n",
    "          B = model.B.data.cpu().numpy()            \n",
    "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
    "          A = 0.5 * (A + A.T)\n",
    "          e, _ = np.linalg.eig(A)\n",
    "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
    "          \n",
    "          C = model.C.data.cpu().numpy()            \n",
    "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
    "          e, _ = np.linalg.eig(W)\n",
    "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
    "              \n",
    "              \n",
    "\n",
    "      # schedule learning rate decay    \n",
    "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
    "\n",
    "  print('total time: ', timeit.default_timer()  - t0 )\n",
    "\n",
    "\n",
    "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
    "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
    "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
    "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
    "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
    "              + '_seed_' + str(args.seed) + '.pkl')  \n",
    "\n",
    "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
    "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
    "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
    "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
    "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
    "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
    "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
    "\n",
    "  pickle.dump(data,f)\n",
    "  f.close()\n",
    "\n",
    "  return max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Of8YS2vvcNL2"
   },
   "outputs": [],
   "source": [
    "seeds = [2019, 2020, 2021]\n",
    "seeds = [2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_features = 3\n",
    "length = 1000\n",
    "window_length = 10\n",
    "train_ratio = 0.7\n",
    "\n",
    "x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method = \"class\", \n",
    "                     flip_probability= 0.0, flip_probability_0=0, flip_probability_1 = 0, startprob=None, transmat=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0KIIDEL9fsrd",
    "outputId": "3c4560ab-cd09-40c4-c7ca-81a99616414d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 0.5259356771196638 \n",
      "Epoch 2: train loss 0.31364330436502186 \n",
      "Epoch 3: train loss 0.179926057692085 \n",
      "Epoch 4: train loss 0.13113783108336585 \n",
      "Epoch 5: train loss 0.11266598685511521 \n",
      "Epoch 6: train loss 0.10361167341470719 \n",
      "Epoch 7: train loss 0.10356674263519901 \n",
      "Epoch 8: train loss 0.09835024364292622 \n",
      "Epoch 9: train loss 0.09764320839728628 \n",
      "Epoch 10: train loss 0.09814601173358305 \n",
      "Epoch 11: train loss 0.09677909053862095 \n",
      "Epoch 12: train loss 0.09731968779649053 \n",
      "Epoch 13: train loss 0.09459405706397124 \n",
      "Epoch 14: train loss 0.09550013818911143 \n",
      "Epoch 15: train loss 0.09593549248363291 \n",
      "Epoch 16: train loss 0.09551046708864826 \n",
      "Epoch 17: train loss 0.09594572125268834 \n",
      "Epoch 18: train loss 0.09412331118115357 \n",
      "Epoch 19: train loss 0.0977759849280119 \n",
      "Epoch 20: train loss 0.0946435481842075 \n",
      "Epoch 21: train loss 0.09456845921065127 \n",
      "Epoch 22: train loss 0.09427558577486447 \n",
      "Epoch 23: train loss 0.09601039865187236 \n",
      "Epoch 24: train loss 0.09560157991945743 \n",
      "Epoch 25: train loss 0.09895976132580213 \n",
      "Epoch 26: train loss 0.09454574989421027 \n",
      "Epoch 27: train loss 0.09365022448556763 \n",
      "Epoch 28: train loss 0.09489808566868305 \n",
      "Epoch 29: train loss 0.09461780323513917 \n",
      "Epoch 30: train loss 0.09596979144428458 \n",
      "Epoch 31: train loss 0.09480550698935986 \n",
      "Epoch 32: train loss 0.09600438278700624 \n",
      "Epoch 33: train loss 0.09412069922047002 \n",
      "Epoch 34: train loss 0.09589151654924666 \n",
      "Epoch 35: train loss 0.09791696130165033 \n",
      "Epoch 36: train loss 0.09366690947541169 \n",
      "Epoch 37: train loss 0.09459897545831544 \n",
      "Epoch 38: train loss 0.09814189071101802 \n",
      "Epoch 39: train loss 0.0964959696999618 \n",
      "Epoch 40: train loss 0.09469068433557237 \n",
      "Epoch 41: train loss 0.09580724649131298 \n",
      "Epoch 42: train loss 0.09365749654493162 \n",
      "Epoch 43: train loss 0.0953383275440761 \n",
      "Epoch 44: train loss 0.0940923529544047 \n",
      "Epoch 45: train loss 0.09482518588857991 \n",
      "Epoch 46: train loss 0.09878998941608838 \n",
      "Epoch 47: train loss 0.09419556850833552 \n",
      "Epoch 48: train loss 0.09355094033692564 \n",
      "Epoch 49: train loss 0.09930661373904773 \n",
      "Epoch 50: train loss 0.09487183328185762 \n",
      "Test Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "n_states = 2\n",
    "n_features = 3\n",
    "length = 1000\n",
    "window_length = 10\n",
    "train_ratio = 0.7\n",
    "method = \"basic\"\n",
    "flip_probability = 0.0\n",
    "flip_probability_0 = 0.1\n",
    "flip_probability_1 = 0.2\n",
    "rnn_arr1 = []\n",
    "nrnn_arr1 = []\n",
    "lstm_arr1 = []\n",
    "seed_arr1 = []\n",
    "method_arr1 = []\n",
    "specify_arr1 = []\n",
    "\n",
    "#x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method = \"class\", \n",
    "#                     flip_probability= 0.0, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1, startprob=None, transmat=None)\n",
    "\n",
    "train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_true))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_true))\n",
    "test_loader = data_utils.DataLoader(test, batch_size=10, shuffle=True)\n",
    "\n",
    "for seed in seeds:\n",
    "    acc = driver(seed, train_loader, test_loader)\n",
    "    acc2 = driver(seed, train_loader, test_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "    acc3 = lstm_driver(seed, x_test, y_test_true, x_train, y_train_true)\n",
    "    rnn_arr1.append(acc)\n",
    "    nrnn_arr1.append(acc2)\n",
    "    lstm_arr1.append(acc3)\n",
    "    seed_arr1.append(seed)\n",
    "    method_arr1.append(method)\n",
    "    specify_arr1.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8Wy7bjBuWxp5",
    "outputId": "00f87ee4-f036-4210-dc3d-5956ca63527f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n",
      "NoisyRNN(\n",
      "  (tanh): Tanh()\n",
      "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "**** Setup ****\n",
      "Total params: 8.64k\n",
      "************\n",
      "Epoch:  0 Iteration:  70 | train loss: 0.7731 | test accuracy: 0.609\n",
      "Epoch:  1 Iteration:  140 | train loss: 0.7709 | test accuracy: 0.650\n",
      "Epoch:  2 Iteration:  210 | train loss: 1.0124 | test accuracy: 0.663\n",
      "Epoch:  3 Iteration:  280 | train loss: 0.7366 | test accuracy: 0.700\n",
      "Epoch:  4 Iteration:  350 | train loss: 0.4815 | test accuracy: 0.747\n",
      "Epoch:  5 Iteration:  420 | train loss: 0.4213 | test accuracy: 0.771\n",
      "Epoch:  6 Iteration:  490 | train loss: 0.4395 | test accuracy: 0.798\n",
      "Epoch:  7 Iteration:  560 | train loss: 0.0924 | test accuracy: 0.855\n",
      "Epoch:  8 Iteration:  630 | train loss: 0.2000 | test accuracy: 0.859\n",
      "Epoch:  9 Iteration:  700 | train loss: 0.1088 | test accuracy: 0.865\n",
      "Epoch:  10 Iteration:  770 | train loss: 0.7274 | test accuracy: 0.859\n",
      "Epoch:  11 Iteration:  840 | train loss: 0.0606 | test accuracy: 0.862\n",
      "Epoch:  12 Iteration:  910 | train loss: 0.4503 | test accuracy: 0.862\n",
      "Epoch:  13 Iteration:  980 | train loss: 0.2474 | test accuracy: 0.862\n",
      "Epoch:  14 Iteration:  1050 | train loss: 0.0422 | test accuracy: 0.879\n",
      "Epoch:  15 Iteration:  1120 | train loss: 0.0623 | test accuracy: 0.855\n",
      "Epoch:  16 Iteration:  1190 | train loss: 0.1651 | test accuracy: 0.859\n",
      "Epoch:  17 Iteration:  1260 | train loss: 0.2720 | test accuracy: 0.865\n",
      "Epoch:  18 Iteration:  1330 | train loss: 0.7113 | test accuracy: 0.855\n",
      "Epoch:  19 Iteration:  1400 | train loss: 0.1530 | test accuracy: 0.855\n",
      "Epoch:  20 Iteration:  1470 | train loss: 0.4624 | test accuracy: 0.859\n",
      "Epoch:  21 Iteration:  1540 | train loss: 0.3685 | test accuracy: 0.859\n",
      "Epoch:  22 Iteration:  1610 | train loss: 0.4436 | test accuracy: 0.862\n",
      "Epoch:  23 Iteration:  1680 | train loss: 0.0417 | test accuracy: 0.862\n",
      "Epoch:  24 Iteration:  1750 | train loss: 0.0528 | test accuracy: 0.862\n",
      "Epoch:  25 Iteration:  1820 | train loss: 0.3423 | test accuracy: 0.859\n",
      "Epoch:  26 Iteration:  1890 | train loss: 0.2717 | test accuracy: 0.855\n",
      "Epoch:  27 Iteration:  1960 | train loss: 0.0415 | test accuracy: 0.862\n",
      "Epoch:  28 Iteration:  2030 | train loss: 0.3226 | test accuracy: 0.865\n",
      "Epoch:  29 Iteration:  2100 | train loss: 0.9377 | test accuracy: 0.859\n",
      "Epoch:  30 Iteration:  2170 | train loss: 0.1161 | test accuracy: 0.859\n",
      "New learning rate is:  0.0001\n",
      "Epoch:  31 Iteration:  2240 | train loss: 1.1469 | test accuracy: 0.869\n",
      "Epoch:  32 Iteration:  2310 | train loss: 0.3628 | test accuracy: 0.869\n",
      "Epoch:  33 Iteration:  2380 | train loss: 0.1801 | test accuracy: 0.862\n",
      "Epoch:  34 Iteration:  2450 | train loss: 1.2163 | test accuracy: 0.865\n",
      "Epoch:  35 Iteration:  2520 | train loss: 0.6538 | test accuracy: 0.865\n",
      "Epoch:  36 Iteration:  2590 | train loss: 0.1084 | test accuracy: 0.855\n",
      "Epoch:  37 Iteration:  2660 | train loss: 0.0385 | test accuracy: 0.855\n",
      "Epoch:  38 Iteration:  2730 | train loss: 0.0577 | test accuracy: 0.852\n",
      "Epoch:  39 Iteration:  2800 | train loss: 0.1346 | test accuracy: 0.852\n",
      "Epoch:  40 Iteration:  2870 | train loss: 0.1148 | test accuracy: 0.855\n",
      "Epoch:  41 Iteration:  2940 | train loss: 0.1378 | test accuracy: 0.855\n",
      "Epoch:  42 Iteration:  3010 | train loss: 0.5092 | test accuracy: 0.862\n",
      "Epoch:  43 Iteration:  3080 | train loss: 0.0708 | test accuracy: 0.859\n",
      "Epoch:  44 Iteration:  3150 | train loss: 0.0507 | test accuracy: 0.855\n",
      "Epoch:  45 Iteration:  3220 | train loss: 0.7336 | test accuracy: 0.855\n",
      "Epoch:  46 Iteration:  3290 | train loss: 0.2293 | test accuracy: 0.859\n",
      "Epoch:  47 Iteration:  3360 | train loss: 0.5710 | test accuracy: 0.859\n",
      "Epoch:  48 Iteration:  3430 | train loss: 0.0743 | test accuracy: 0.859\n",
      "Epoch:  49 Iteration:  3500 | train loss: 0.1024 | test accuracy: 0.865\n",
      "total time:  34.625579494051635\n",
      "Connected to a GPU\n",
      "NoisyRNN(\n",
      "  (tanh): Tanh()\n",
      "  (E): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (D): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "**** Setup ****\n",
      "Total params: 8.64k\n",
      "************\n",
      "Epoch:  0 Iteration:  70 | train loss: 0.7365 | test accuracy: 0.613\n",
      "Epoch:  1 Iteration:  140 | train loss: 0.6940 | test accuracy: 0.717\n"
     ]
    }
   ],
   "source": [
    "n_states = 2\n",
    "n_features = 3\n",
    "length = 1000\n",
    "window_length = 10\n",
    "train_ratio = 0.7\n",
    "method = \"basic\"\n",
    "flip_probabilities = [0.025, 0.05, 0.1, 0.2, 0.4]\n",
    "rnn_arr2 = []\n",
    "nrnn_arr2 = []\n",
    "lstm_arr2 = []\n",
    "seed_arr2 = []\n",
    "method_arr2 = []\n",
    "specify_arr2 = []\n",
    "\n",
    "\n",
    "for flip_probability in flip_probabilities:\n",
    "\n",
    "    #x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped= generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
    "    #                  flip_probability, flip_probability_0=None, flip_probability_1=None,\n",
    "    #                  startprob=None, transmat=None)\n",
    "    \n",
    "    y_train_flipped = flip_labels_basic(y_train_true, flip_probability)\n",
    "\n",
    "    basic_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
    "    basic_train_flipped_loader = data_utils.DataLoader(basic_train_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    basic_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
    "    basic_test_flipped_loader = data_utils.DataLoader(basic_test_flipped, batch_size=25, shuffle=True)\n",
    "\n",
    "    for seed in seeds:\n",
    "        acc = driver(seed, basic_train_flipped_loader, basic_test_flipped_loader)\n",
    "        acc2 = driver(seed, basic_train_flipped_loader, basic_test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "        acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
    "        rnn_arr2.append(acc)\n",
    "        nrnn_arr2.append(acc2)\n",
    "        lstm_arr2.append(acc3)\n",
    "        seed_arr2.append(seed)\n",
    "        method_arr2.append(method)\n",
    "        specify_arr2.append(flip_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KIEQl0-6i6ia",
    "outputId": "7674730e-8588-4974-fd7c-2445e941a9a0"
   },
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_features = 3\n",
    "length = 1000\n",
    "window_length = 10\n",
    "train_ratio = 0.7\n",
    "method = \"class\"\n",
    "rnn_arr3 = []\n",
    "nrnn_arr3 = []\n",
    "lstm_arr3 = []\n",
    "seed_arr3 = []\n",
    "method_arr3 = []\n",
    "specify_arr3 = []\n",
    "\n",
    "flip_probabilities_0 = [0.2, 0.8]\n",
    "flip_probabilities_1 = [0.2, 0.8]\n",
    "\n",
    "for flip_probability_0 in flip_probabilities_0:\n",
    "    for flip_probability_1 in flip_probabilities_1:\n",
    "\n",
    "    #x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features, length,window_length, train_ratio, method, \n",
    "    #                    flip_probability= None, flip_probability_0=flip_probability_0, flip_probability_1 = flip_probability_1,\n",
    "    #                    startprob=None, transmat=None)\n",
    "    \n",
    "    y_train_flipped = flip_labels_class(y_train_true, flip_probability_0, flip_probability_1)\n",
    "    \n",
    "    class_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
    "    class_train_flipped_loader = data_utils.DataLoader(class_train_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    class_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
    "    class_test_flipped_loader = data_utils.DataLoader(class_test_flipped, batch_size=25, shuffle=True)\n",
    "\n",
    "    for seed in seeds:\n",
    "        acc = driver(seed, class_train_flipped_loader, class_test_flipped_loader)\n",
    "        acc2 = driver(seed, class_train_flipped_loader, class_test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "        acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
    "        rnn_arr3.append(acc)\n",
    "        nrnn_arr3.append(acc2)\n",
    "        lstm_arr3.append(acc3)\n",
    "        seed_arr3.append(seed)\n",
    "        method_arr3.append(method)\n",
    "        specify_arr3.append((flip_probability_0, flip_probability_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IFriPAp0lm7Y",
    "outputId": "d5114849-3443-4e8a-8799-bab7ab150021"
   },
   "outputs": [],
   "source": [
    "n_states = 2\n",
    "n_features = 3\n",
    "length = 1000\n",
    "window_length = 10\n",
    "train_ratio = 0.7\n",
    "method = \"time\"\n",
    "rnn_arr4 = []\n",
    "nrnn_arr4 = []\n",
    "lstm_arr4 = []\n",
    "seed_arr4 = []\n",
    "method_arr4 = []\n",
    "specify_arr4 = []\n",
    "\n",
    "startprob = random_startprob(2)\n",
    "transmat1 = np.array([[0.80, 0.2],\n",
    "                    [0.80, 0.2]])\n",
    "\n",
    "transmat2 = np.array([[0.6, 0.4],\n",
    "                     [0.6, 0.4]])\n",
    "\n",
    "transmat3 = np.array([[0.4, 0.6],\n",
    "                     [0.4, 0.6]])\n",
    "\n",
    "transmats = [transmat1, transmat2, transmat3]\n",
    "names = [\"transmat1\", \"transmat2\", \"transmat3\"]\n",
    "\n",
    "for idx, transmat in enumerate(transmats):\n",
    "    #x_train, y_train_true, y_train_flipped, x_test, y_test_true, y_test_flipped = generate_dataset(n_states, n_features,length, window_length, train_ratio, method, \n",
    "    #                  flip_probability=None, flip_probability_0=None, flip_probability_1=None,\n",
    "    #                  startprob=startprob, transmat=transmat1)\n",
    "\n",
    "    y_train_flipped = flip_labels_time(y_train_true, startprob, transmat)\n",
    "\n",
    "    time_train_flipped = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train_flipped))\n",
    "    time_train_flipped_loader = data_utils.DataLoader(time_train_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    time_test_flipped = data_utils.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test_flipped))\n",
    "    time_test_flipped_loader = data_utils.DataLoader(time_test_flipped, batch_size=25, shuffle=True)\n",
    "\n",
    "    for seed in seeds:\n",
    "        acc = driver(seed, time_train_flipped_loader, time_test_flipped_loader)\n",
    "        acc2 = driver(seed, time_train_flipped_loader, time_test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "        acc3 = lstm_driver(seed, x_test, y_test_flipped, x_train, y_train_flipped)\n",
    "        rnn_arr4.append(acc)\n",
    "        nrnn_arr4.append(acc2)\n",
    "        lstm_arr4.append(acc3)\n",
    "        seed_arr4.append(seed)\n",
    "        method_arr4.append(method)\n",
    "        specify_arr4.append(names[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr3APVezQQ70"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rnn_arr = rnn_arr1 + rnn_arr2 + rnn_arr3 + rnn_arr4\n",
    "nrnn_arr = nrnn_arr1 + nrnn_arr2 + nrnn_arr3 + nrnn_arr4\n",
    "lstm_arr = lstm_arr1 + lstm_arr2 + lstm_arr3 + lstm_arr4\n",
    "seed_arr = seed_arr1 + seed_arr2 + seed_arr3 + seed_arr4\n",
    "method_arr = method_arr1 + method_arr2 + method_arr3 + method_arr4\n",
    "specify_arr = specify_arr1 + specify_arr2 + specify_arr3 + specify_arr4\n",
    "\n",
    "all_acc = rnn_arr1 + rnn_arr2 + rnn_arr3 + rnn_arr4 + nrnn_arr1 + nrnn_arr2 + nrnn_arr3 + nrnn_arr4 + lstm_arr1 + lstm_arr2 + lstm_arr3 + lstm_arr4\n",
    "seed_arr_all = seed_arr1 + seed_arr2 + seed_arr3 + seed_arr4 + seed_arr1 + seed_arr2 + seed_arr3 + seed_arr4 + seed_arr1 + seed_arr2 + seed_arr3 + seed_arr4\n",
    "method_arr_all = method_arr1 + method_arr2 + method_arr3 + method_arr4 + method_arr1 + method_arr2 + method_arr3 + method_arr4 + method_arr1 + method_arr2 + method_arr3 + method_arr4\n",
    "specify_arr_all = specify_arr1 + specify_arr2 + specify_arr3 + specify_arr4 + specify_arr1 + specify_arr2 + specify_arr3 + specify_arr4 + specify_arr1 + specify_arr2 + specify_arr3 + specify_arr4\n",
    "models = [\"RNN\" for i in range (0,33)] + [\"NRNN\" for i in range (0,33)] + [\"LSTM\" for i in range (0,33)]\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(all_acc, seed_arr_all, method_arr_all, specify_arr_all, models)), columns = [\"Accuracy\", \"Seed\", \"Method\", \"Specify\", \"Model\"])\n",
    "df1\n",
    "df1.to_csv('out.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[\"NRNN\" for i in range (0,33)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hmm-fit.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Stress Recov(3.6)",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
