{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzBraGTpl7gT"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r28CXXZps-g1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy\n",
        "from subprocess import call\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
        "import datetime as dt\n",
        "import time\n",
        "import random\n",
        "from scipy import signal\n",
        "from sklearn.utils import shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwn7kurmByV"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gdown --id 16MIleqoIr1vYxlGk4GKnGmrsCPuWkkpT\n",
        "#!unzip -qq ECG5000.zip\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "!pip install arff2pandas\n",
        "from arff2pandas import a2p\n",
        "\n",
        "with open('ECG5000_TRAIN.arff') as f:\n",
        "  train = a2p.load(f)\n",
        "with open('ECG5000_TEST.arff') as f:\n",
        "  test = a2p.load(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6i0nvYebzPU9",
        "outputId": "59ef62d4-2e33-42a8-bdfc-ca3fbe20ef16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arff2pandas in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: liac-arff in /usr/local/lib/python3.7/dist-packages (from arff2pandas) (2.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from arff2pandas) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->arff2pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->arff2pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->arff2pandas) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->arff2pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = train.append(test)\n",
        "df = df.sample(frac=1.0)\n",
        "print(df.shape)\n",
        "\n",
        "CLASS_NORMAL = 1\n",
        "class_names = ['Normal','R on T','PVC','SP','UB']\n",
        "new_columns = list(df.columns)\n",
        "new_columns[-1] = 'target'\n",
        "df.columns = new_columns\n",
        "\n",
        "X = df.drop(\"target\", axis = 1)\n",
        "#print(X.shape)\n",
        "y = df[\"target\"]\n",
        "#print(y.shape)\n",
        "\n",
        "X_train = X[:700].to_numpy(dtype=float).reshape(700, -1, 1)\n",
        "y_train = y[:700].to_numpy(dtype=int).reshape(-1, 1)\n",
        "\n",
        "X_test = X[700:1000].to_numpy(dtype=float).reshape(300, -1, 1)\n",
        "y_test = y[700:1000].to_numpy(dtype=int).reshape(-1, 1)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=16, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=16, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cG8knSsvz_b-",
        "outputId": "63dcde2c-dcc0-482b-dfd4-6abf0360af27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 141)\n",
            "(300, 140, 1)\n",
            "(300, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYa-ggaVkzkI"
      },
      "outputs": [],
      "source": [
        "# print(\"\")\n",
        "\n",
        "# print(\"Downloading...\")\n",
        "# if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "#     call(\n",
        "#         'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
        "#         shell=True\n",
        "#     )\n",
        "#     print(\"Downloading done.\\n\")\n",
        "# else:\n",
        "#     print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "\n",
        "# print(\"Extracting...\")\n",
        "# extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
        "# if not os.path.exists(extract_directory):\n",
        "#     call(\n",
        "#         'unzip -nq \"UCI HAR Dataset.zip\"',\n",
        "#         shell=True\n",
        "#     )\n",
        "#     print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
        "# else:\n",
        "#     print(\"Dataset already extracted. Did not extract twice.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwLgDKQytkRN"
      },
      "outputs": [],
      "source": [
        "# def load_X(X_signals_paths):\n",
        "#     X_signals = []\n",
        "\n",
        "#     for signal_type_path in X_signals_paths:\n",
        "#         file = open(signal_type_path, 'r')\n",
        "#         # Read dataset from disk, dealing with text files' syntax\n",
        "#         X_signals.append(\n",
        "#             [np.array(serie, dtype=np.float32) for serie in [\n",
        "#                 row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "#             ]]\n",
        "#         )\n",
        "#         file.close()\n",
        "\n",
        "#     return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "# # Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "# def load_y(y_path):\n",
        "#     file = open(y_path, 'r')\n",
        "#     # Read dataset from disk, dealing with text file's syntax\n",
        "#     y_ = np.array(\n",
        "#         [elem for elem in [\n",
        "#             row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "#         ]],\n",
        "#         dtype=np.int32\n",
        "#     )\n",
        "#     file.close()\n",
        "\n",
        "#     # Substract 1 to each output class for friendly 0-based indexing\n",
        "#     return y_ - 1\n",
        "\n",
        "# INPUT_SIGNAL_TYPES = [\n",
        "#     \"body_acc_x_\",\n",
        "#     \"body_acc_y_\",\n",
        "#     \"body_acc_z_\",\n",
        "#     \"body_gyro_x_\",\n",
        "#     \"body_gyro_y_\",\n",
        "#     \"body_gyro_z_\",\n",
        "#     \"total_acc_x_\",\n",
        "#     \"total_acc_y_\",\n",
        "#     \"total_acc_z_\"\n",
        "# ]\n",
        "\n",
        "# TRAIN = \"train/\"\n",
        "# TEST = \"test/\"\n",
        "# DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
        "\n",
        "# X_train_signals_paths = [\n",
        "#     DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "# ]\n",
        "# X_test_signals_paths = [\n",
        "#     DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "# ]\n",
        "\n",
        "# y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "# y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "\n",
        "# X_train = load_X(X_train_signals_paths)\n",
        "# X_test = load_X(X_test_signals_paths)\n",
        "# y_train = load_y(y_train_path)\n",
        "# y_test = load_y(y_test_path)\n",
        "\n",
        "# #Take 700 training datapoints\n",
        "\n",
        "# X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
        "# X_train = X_train[:700]\n",
        "# y_train = y_train[:700]\n",
        "\n",
        "# #Take 300 testing datapoints\n",
        "# X_test, y_test = shuffle(X_test, y_test, random_state = 0)\n",
        "# X_test = X_test[:300]\n",
        "# y_test = y_test[:300]\n",
        "\n",
        "# # Input Data\n",
        "\n",
        "# training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "# print(X_train.shape)\n",
        "# test_data_count = len(X_test)  # 2947 testing series\n",
        "# print(test_data_count)\n",
        "# n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "# n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "# print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "# print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "# print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "\n",
        "# train = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "# train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "# test = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "# test_loader = data_utils.DataLoader(test, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13RRzP01mLkI"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQz7VXgDFLfN"
      },
      "outputs": [],
      "source": [
        "#ADD NOISE TO HAR\n",
        "\n",
        "noise_pair_45= np.array([[.55,0.45,0.0,0.0,0.0,0.0],\n",
        "                        [0.0,.55,0.45,0.0,0.0,0.0],\n",
        "                        [0.0,0.0,.55,0.45,0.0,0.0],\n",
        "                        [0.0,0.0,0.0,.55,0.45,0.0],\n",
        "                        [0.0,0.0,0.0,0.0,.55,0.45],\n",
        "                        [0.45,0.0,0.0,0.0,0.0,.55]])\n",
        "noise_sym_50 = np.array([[.50,.10,.10,.10,.10,.10],\n",
        "                        [.10,.50,.10,.10,.10,.10],\n",
        "                        [.10,.10,.50,.10,.10,.10],\n",
        "                        [.10,.10,.10,.50,.10,.10],\n",
        "                        [.10,.10,.10,.10,.50,.10],\n",
        "                        [.10,.10,.10,.10,.10,.50]])\n",
        "\n",
        "noise_sym_25 = np.array([[.25,.15,.15,.15,.15,.15],\n",
        "                        [.15,.25,.15,.15,.15,.15],\n",
        "                        [.15,.15,.25,.15,.15,.15],\n",
        "                        [.15,.15,.15,.25,.15,.15],\n",
        "                        [.15,.15,.15,.15,.25,.15],\n",
        "                        [.15,.15,.15,.15,.15,.25]])\n",
        "\n",
        "def flip_HAR_labels(array, noise_matrix):\n",
        "    flipped = []\n",
        "    for elem in array.flatten():\n",
        "      flipped.append(np.random.choice([0,1,2,3,4,5], p=noise_matrix[int(elem)]))\n",
        "    \n",
        "    flipped = np.array(flipped)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped\n",
        "\n",
        "\n",
        "def flip_HAR_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, flip_probability, len(array))\n",
        "    flipped = []\n",
        "    for i in range(len(array)):\n",
        "        if flip_mask[i]==1:\n",
        "            options = [0.0,1.0,2.0,3.0,4.0,5.0]\n",
        "            new_options = [x for x in options if x != array[i]]\n",
        "            flipped.append(np.random.choice(new_options, p=[0.2,0.2,0.2,0.2, 0.2]))\n",
        "        else:\n",
        "            flipped.append(array[i])\n",
        "    \n",
        "    flipped = np.array(flipped, dtype=np.int)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8xzQyNFPpW",
        "outputId": "6e432f99-de21-4f32-9804-c38199012f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ],
      "source": [
        "array = np.array([0.0,1.0,2.0,3.0,4.0,5.0])\n",
        "noise_matrix = noise_sym_25\n",
        "\n",
        "flip_HAR_labels_basic(array, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkrafFRyGKhG",
        "outputId": "7b3b091d-143d-45bf-f18c-fde2d4fdf45c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 2, 1, 0, 2, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ],
      "source": [
        "flip_HAR_labels(array, noise_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ7gc-5cFT7S",
        "outputId": "3116685a-f741-4f3c-8217-d2558db6bed7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 1, 1, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 212
        }
      ],
      "source": [
        "flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "flip_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4RXMRyAtFvC"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PouKNWlImO5K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5KjAjajQA9p"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, feature_size, n_state, hidden_size=16, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
        "                 seed=random.seed('2021')):\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_state = n_state\n",
        "        self.seed = seed\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.rnn_type = rnn\n",
        "        self.regres = regres\n",
        "        self.return_all = return_all\n",
        "        \n",
        "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "\n",
        "        self.regressor = nn.Sequential(nn.Linear(self.hidden_size, self.n_state))\n",
        "\n",
        "    def forward(self, input, past_state=None, **kwargs):\n",
        "        input = input.to(self.device)\n",
        "        self.rnn.to(self.device)\n",
        "        self.regressor.to(self.device)\n",
        "        if not past_state:\n",
        "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
        "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            all_encodings, encoding = self.rnn(input, past_state)\n",
        "        else:\n",
        "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
        "        \n",
        "        if self.regres:\n",
        "            if not self.return_all:\n",
        "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
        "            else:\n",
        "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
        "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
        "        else:\n",
        "            return encoding.view(encoding.shape[1], -1)\n",
        "        \n",
        "        \n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
        "            \n",
        "def save_ckpt(generator_model, output_dir, data):\n",
        "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
        "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
        "    os.makedirs(check_pt_dir, exist_ok=True)\n",
        "    torch.save(generator_model.state_dict(), fname)\n",
        "    \n",
        "def get_accuracy(model, loader):\n",
        "    \n",
        "    correct, total = 0, 0\n",
        "    for xs, ts in loader:\n",
        "        xs.to(device)\n",
        "        ts.to(device)\n",
        "        zs = model(xs)\n",
        "        \n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        ts = torch.argmax(ts, 1)\n",
        "\n",
        "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
        "        total += int(ts.shape[0])\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe1d7EkrQC8s"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, n_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0.1)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    history = dict(train=[], val=[])\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
        "    \n",
        "    best_loss = 10000.0\n",
        "    \n",
        "    n=0\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t0 = time.time()\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "        train_losses=[]\n",
        "        model = model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = (time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input= batch[0].to(device)\n",
        "            b_target =  batch[1].to(device)\n",
        "            iters.append(n)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(b_input)\n",
        "\n",
        "            target = torch.argmax(b_target, 1)\n",
        "\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if n % 10 == 0:\n",
        "                iters_sub.append(n)\n",
        "                \n",
        "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
        "                #print(get_accuracy(model, train_dataloader))\n",
        "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "        training_time = (time.time() - t0)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        #print(\"\")\n",
        "        #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        #print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "        #print(\"\")\n",
        "        #print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        val_losses = []\n",
        "        model = model.eval()\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "    \n",
        "        history['train'].append(train_loss)\n",
        "        \n",
        "        #print(f'Epoch {epoch}: train loss {train_loss} ')\n",
        "    # plt.style.use('seaborn-white')\n",
        "    # plt.plot(history['train'])\n",
        "\n",
        "    # plt.title('LSTM  Training Curves')\n",
        "    # plt.ylabel('CE Loss')\n",
        "    # plt.xlabel('Epoch Number')\n",
        "    # plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    # plt.show()\n",
        "    \n",
        "    return model.eval(), history\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    #INFERENCE ON TEST SET\n",
        "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
        "    \n",
        "    y_test = y_test.numpy()\n",
        "\n",
        "    y_pred = predictions.cpu().numpy()\n",
        "\n",
        "    print(y_pred)\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    #f1 = f1_score(y_test, y_pred, average='micro')\n",
        "    #precision = precision_score(y_test, y_pred, average='micro')\n",
        "    #print(accuracy)\n",
        "    #print(f1)\n",
        "    #print(precision)\n",
        "    return accuracy\n",
        "\n",
        "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
        "  train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train))\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "  x_test = torch.tensor(x_test, dtype=torch.float)\n",
        "    \n",
        "  y_test = torch.tensor(y_test, dtype=torch.int)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = LSTMClassifier(1, #num features \n",
        "                  5, #num classes,\n",
        "                  seed = seed,\n",
        "                  rnn=\"GRU\" #rnn type    \n",
        "            )\n",
        "\n",
        "  model, history = train_model(model, train_loader, 50, 2e-3)\n",
        "  acc = evaluate_model(model, x_test, y_test)\n",
        "  print(\"Test Accuracy: \", acc)\n",
        "\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MELlsVkos0ZP"
      },
      "outputs": [],
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=16, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK_RFvzqvl65",
        "outputId": "1c95e564-d36c-4fd0-d70b-e38f6ad579f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n"
          ]
        }
      ],
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='HAR', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=50, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.002, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=16, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# initialize model\n",
        "#==============================================================================\n",
        "seeds = [1, 2, 3]\n",
        "\n",
        "\n",
        "def driver(seed, train, test, add_noise = 0, mult_noise = 0, nrnn = False):\n",
        "\n",
        "  final_acc = []\n",
        "  model = NoisyRNN(input_dim=int(140), output_classes=6, n_units=args.n_units, \n",
        "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "                add_noise=add_noise, mult_noise=mult_noise).to(device)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  noise = torch.randn(1,700,140,1).float()\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # set random seed to reproduce the work\n",
        "  #==============================================================================\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # Model summary\n",
        "  #==============================================================================\n",
        "  print(model)    \n",
        "  print('**** Setup ****')\n",
        "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "  print('************')    \n",
        "    \n",
        "\n",
        "  if args.optimizer == 'SGD':\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "  elif  args.optimizer == 'Adam':\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  else:\n",
        "      print(\"Unexpected optimizer!\")\n",
        "      raise \n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # training and testing\n",
        "  count = 0\n",
        "  loss_hist = []\n",
        "  test_acc = []\n",
        "\n",
        "  t0 = timeit.default_timer()\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      lossaccum = 0\n",
        "      \n",
        "      for step, (x, y) in enumerate(train):\n",
        "          count += 1\n",
        "          \n",
        "          # Reshape data for recurrent unit\n",
        "          inputs = Variable(x.view(-1, 1, int(140))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "          targets = Variable(y).to(device) \n",
        "\n",
        "                  \n",
        "          # send data to recurrent unit    \n",
        "          output = model(inputs, mode='train') \n",
        "          loss = loss_func(output, targets.squeeze(1).long())\n",
        "          \n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()          \n",
        "          \n",
        "          if args.gclip != 0.0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "              \n",
        "          optimizer.step() # update weights\n",
        "          lossaccum += loss.item()\n",
        "\n",
        "          if args.model == 'test':\n",
        "              D = model.W.weight.data.cpu().numpy()  \n",
        "              u, s, v = np.linalg.svd(D, 0)\n",
        "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "      loss_hist.append(lossaccum)    \n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          total_num = 0\n",
        "          for data, target in test: \n",
        "              data, target = data.to(device), target.to(device)                \n",
        "              output = model(data.view(-1, 1, int(140)))\n",
        "              \n",
        "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                   \n",
        "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "              total_num += len(data)\n",
        "          \n",
        "          accuracy = correct / total_num\n",
        "          test_acc.append(accuracy)\n",
        "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "      if nrnn == True:\n",
        "          B = model.B.data.cpu().numpy()            \n",
        "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "          A = 0.5 * (A + A.T)\n",
        "          e, _ = np.linalg.eig(A)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "          \n",
        "          C = model.C.data.cpu().numpy()            \n",
        "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "          e, _ = np.linalg.eig(W)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "              \n",
        "              \n",
        "\n",
        "      # schedule learning rate decay    \n",
        "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "  print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "  final_acc.append(max(test_acc))\n",
        "\n",
        "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "  pickle.dump(data,f)\n",
        "  f.close()\n",
        "  return max(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 1\n",
        "lstm_acc = lstm_driver(seed, X_test, y_test, X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HE54moWQ-gTu",
        "outputId": "bc60af0b-9462-4054-c708-4ca5b98dca49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2593626976013184.\n",
            "\n",
            "======== Epoch 2 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2094058990478516.\n",
            "\n",
            "======== Epoch 3 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2130630016326904.\n",
            "\n",
            "======== Epoch 4 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2079119682312012.\n",
            "\n",
            "======== Epoch 5 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2010016441345215.\n",
            "\n",
            "======== Epoch 6 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1887497901916504.\n",
            "\n",
            "======== Epoch 7 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.394110918045044.\n",
            "\n",
            "======== Epoch 8 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.192244291305542.\n",
            "\n",
            "======== Epoch 9 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1926050186157227.\n",
            "\n",
            "======== Epoch 10 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2077052593231201.\n",
            "\n",
            "======== Epoch 11 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.203674554824829.\n",
            "\n",
            "======== Epoch 12 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2239174842834473.\n",
            "\n",
            "======== Epoch 13 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2119176387786865.\n",
            "\n",
            "======== Epoch 14 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2170257568359375.\n",
            "\n",
            "======== Epoch 15 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2651824951171875.\n",
            "\n",
            "======== Epoch 16 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2304191589355469.\n",
            "\n",
            "======== Epoch 17 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2158775329589844.\n",
            "\n",
            "======== Epoch 18 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1999363899230957.\n",
            "\n",
            "======== Epoch 19 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2281057834625244.\n",
            "\n",
            "======== Epoch 20 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2189500331878662.\n",
            "\n",
            "======== Epoch 21 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1882691383361816.\n",
            "\n",
            "======== Epoch 22 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1985700130462646.\n",
            "\n",
            "======== Epoch 23 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2129981517791748.\n",
            "\n",
            "======== Epoch 24 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.207432746887207.\n",
            "\n",
            "======== Epoch 25 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1984999179840088.\n",
            "\n",
            "======== Epoch 26 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1902611255645752.\n",
            "\n",
            "======== Epoch 27 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2050879001617432.\n",
            "\n",
            "======== Epoch 28 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2001395225524902.\n",
            "\n",
            "======== Epoch 29 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2153651714324951.\n",
            "\n",
            "======== Epoch 30 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.210381031036377.\n",
            "\n",
            "======== Epoch 31 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2191405296325684.\n",
            "\n",
            "======== Epoch 32 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.195749044418335.\n",
            "\n",
            "======== Epoch 33 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1928045749664307.\n",
            "\n",
            "======== Epoch 34 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.198575496673584.\n",
            "\n",
            "======== Epoch 35 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1951525211334229.\n",
            "\n",
            "======== Epoch 36 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2016503810882568.\n",
            "\n",
            "======== Epoch 37 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1908292770385742.\n",
            "\n",
            "======== Epoch 38 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.212353229522705.\n",
            "\n",
            "======== Epoch 39 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.189455509185791.\n",
            "\n",
            "======== Epoch 40 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1902413368225098.\n",
            "\n",
            "======== Epoch 41 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2009994983673096.\n",
            "\n",
            "======== Epoch 42 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2387585639953613.\n",
            "\n",
            "======== Epoch 43 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2372336387634277.\n",
            "\n",
            "======== Epoch 44 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.5953478813171387.\n",
            "\n",
            "======== Epoch 45 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2071173191070557.\n",
            "\n",
            "======== Epoch 46 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1997630596160889.\n",
            "\n",
            "======== Epoch 47 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2024648189544678.\n",
            "\n",
            "======== Epoch 48 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.201995849609375.\n",
            "\n",
            "======== Epoch 49 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1988484859466553.\n",
            "\n",
            "======== Epoch 50 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.1907944679260254.\n",
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-230-7133b53171d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-229-4e0dba8d8239>\u001b[0m in \u001b[0;36mlstm_driver\u001b[0;34m(seed, x_test, y_test, x_train, y_train)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m   \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-229-4e0dba8d8239>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(model, x_test, y_test)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;31m#f1 = f1_score(y_test, y_pred, average='micro')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m#precision = precision_score(y_test, y_pred, average='micro')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of unknown and binary targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O7EmSIf4vSwB",
        "outputId": "c0ecd52e-1dc0-4120-fda6-c9e1774f095d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=140, out_features=16, bias=True)\n",
            "  (D): Linear(in_features=16, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 2.89k\n",
            "************\n",
            "Epoch:  0 Iteration:  44 | train loss: 1.0877 | test accuracy: 0.933\n",
            "Epoch:  1 Iteration:  88 | train loss: 0.4790 | test accuracy: 0.933\n",
            "Epoch:  2 Iteration:  132 | train loss: 0.4224 | test accuracy: 0.930\n",
            "Epoch:  3 Iteration:  176 | train loss: 0.1371 | test accuracy: 0.930\n",
            "Epoch:  4 Iteration:  220 | train loss: 0.2686 | test accuracy: 0.933\n",
            "Epoch:  5 Iteration:  264 | train loss: 0.5300 | test accuracy: 0.930\n",
            "Epoch:  6 Iteration:  308 | train loss: 0.2787 | test accuracy: 0.930\n",
            "Epoch:  7 Iteration:  352 | train loss: 0.0617 | test accuracy: 0.933\n",
            "Epoch:  8 Iteration:  396 | train loss: 0.0439 | test accuracy: 0.937\n",
            "Epoch:  9 Iteration:  440 | train loss: 0.0347 | test accuracy: 0.930\n",
            "Epoch:  10 Iteration:  484 | train loss: 0.0295 | test accuracy: 0.937\n",
            "Epoch:  11 Iteration:  528 | train loss: 0.7214 | test accuracy: 0.947\n",
            "Epoch:  12 Iteration:  572 | train loss: 0.0364 | test accuracy: 0.940\n",
            "Epoch:  13 Iteration:  616 | train loss: 0.0704 | test accuracy: 0.953\n",
            "Epoch:  14 Iteration:  660 | train loss: 0.0223 | test accuracy: 0.953\n",
            "Epoch:  15 Iteration:  704 | train loss: 0.0651 | test accuracy: 0.953\n",
            "Epoch:  16 Iteration:  748 | train loss: 0.2859 | test accuracy: 0.953\n",
            "Epoch:  17 Iteration:  792 | train loss: 0.2200 | test accuracy: 0.957\n",
            "Epoch:  18 Iteration:  836 | train loss: 0.0169 | test accuracy: 0.953\n",
            "Epoch:  19 Iteration:  880 | train loss: 0.0378 | test accuracy: 0.950\n",
            "Epoch:  20 Iteration:  924 | train loss: 0.1197 | test accuracy: 0.957\n",
            "Epoch:  21 Iteration:  968 | train loss: 0.0300 | test accuracy: 0.957\n",
            "Epoch:  22 Iteration:  1012 | train loss: 0.2221 | test accuracy: 0.957\n",
            "Epoch:  23 Iteration:  1056 | train loss: 0.0521 | test accuracy: 0.947\n",
            "Epoch:  24 Iteration:  1100 | train loss: 0.0100 | test accuracy: 0.950\n",
            "Epoch:  25 Iteration:  1144 | train loss: 0.0940 | test accuracy: 0.950\n",
            "Epoch:  26 Iteration:  1188 | train loss: 0.2302 | test accuracy: 0.950\n",
            "Epoch:  27 Iteration:  1232 | train loss: 0.0132 | test accuracy: 0.943\n",
            "Epoch:  28 Iteration:  1276 | train loss: 0.0120 | test accuracy: 0.950\n",
            "Epoch:  29 Iteration:  1320 | train loss: 0.0058 | test accuracy: 0.947\n",
            "Epoch:  30 Iteration:  1364 | train loss: 0.0230 | test accuracy: 0.950\n",
            "New learning rate is:  0.0002\n",
            "Epoch:  31 Iteration:  1408 | train loss: 0.0249 | test accuracy: 0.950\n",
            "Epoch:  32 Iteration:  1452 | train loss: 0.0262 | test accuracy: 0.950\n",
            "Epoch:  33 Iteration:  1496 | train loss: 0.2793 | test accuracy: 0.950\n",
            "Epoch:  34 Iteration:  1540 | train loss: 0.1438 | test accuracy: 0.950\n",
            "Epoch:  35 Iteration:  1584 | train loss: 0.0741 | test accuracy: 0.950\n",
            "Epoch:  36 Iteration:  1628 | train loss: 0.2781 | test accuracy: 0.950\n",
            "Epoch:  37 Iteration:  1672 | train loss: 0.0221 | test accuracy: 0.950\n",
            "Epoch:  38 Iteration:  1716 | train loss: 0.0492 | test accuracy: 0.950\n",
            "Epoch:  39 Iteration:  1760 | train loss: 0.0283 | test accuracy: 0.950\n",
            "Epoch:  40 Iteration:  1804 | train loss: 0.0472 | test accuracy: 0.950\n",
            "Epoch:  41 Iteration:  1848 | train loss: 0.0204 | test accuracy: 0.950\n",
            "Epoch:  42 Iteration:  1892 | train loss: 0.3937 | test accuracy: 0.950\n",
            "Epoch:  43 Iteration:  1936 | train loss: 0.0204 | test accuracy: 0.950\n",
            "Epoch:  44 Iteration:  1980 | train loss: 0.0055 | test accuracy: 0.950\n",
            "Epoch:  45 Iteration:  2024 | train loss: 0.0306 | test accuracy: 0.950\n",
            "Epoch:  46 Iteration:  2068 | train loss: 0.0241 | test accuracy: 0.950\n",
            "Epoch:  47 Iteration:  2112 | train loss: 0.0379 | test accuracy: 0.950\n",
            "Epoch:  48 Iteration:  2156 | train loss: 0.0469 | test accuracy: 0.950\n",
            "Epoch:  49 Iteration:  2200 | train loss: 0.0092 | test accuracy: 0.950\n",
            "total time:  6.99013604799984\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=140, out_features=16, bias=True)\n",
            "  (D): Linear(in_features=16, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 2.89k\n",
            "************\n",
            "Epoch:  0 Iteration:  44 | train loss: 1.1829 | test accuracy: 0.927\n",
            "Epoch:  1 Iteration:  88 | train loss: 0.7766 | test accuracy: 0.937\n",
            "Epoch:  2 Iteration:  132 | train loss: 0.4927 | test accuracy: 0.937\n",
            "Epoch:  3 Iteration:  176 | train loss: 0.1525 | test accuracy: 0.937\n",
            "Epoch:  4 Iteration:  220 | train loss: 0.0880 | test accuracy: 0.933\n",
            "Epoch:  5 Iteration:  264 | train loss: 0.3379 | test accuracy: 0.937\n",
            "Epoch:  6 Iteration:  308 | train loss: 0.0619 | test accuracy: 0.933\n",
            "Epoch:  7 Iteration:  352 | train loss: 0.5003 | test accuracy: 0.933\n",
            "Epoch:  8 Iteration:  396 | train loss: 0.2441 | test accuracy: 0.933\n",
            "Epoch:  9 Iteration:  440 | train loss: 0.0664 | test accuracy: 0.940\n",
            "Epoch:  10 Iteration:  484 | train loss: 0.1274 | test accuracy: 0.940\n",
            "Epoch:  11 Iteration:  528 | train loss: 0.2176 | test accuracy: 0.937\n",
            "Epoch:  12 Iteration:  572 | train loss: 0.0307 | test accuracy: 0.937\n",
            "Epoch:  13 Iteration:  616 | train loss: 0.2486 | test accuracy: 0.940\n",
            "Epoch:  14 Iteration:  660 | train loss: 0.2784 | test accuracy: 0.937\n",
            "Epoch:  15 Iteration:  704 | train loss: 0.0447 | test accuracy: 0.947\n",
            "Epoch:  16 Iteration:  748 | train loss: 0.1977 | test accuracy: 0.947\n",
            "Epoch:  17 Iteration:  792 | train loss: 0.0242 | test accuracy: 0.947\n",
            "Epoch:  18 Iteration:  836 | train loss: 0.3100 | test accuracy: 0.950\n",
            "Epoch:  19 Iteration:  880 | train loss: 0.0626 | test accuracy: 0.947\n",
            "Epoch:  20 Iteration:  924 | train loss: 0.1967 | test accuracy: 0.950\n",
            "Epoch:  21 Iteration:  968 | train loss: 0.0733 | test accuracy: 0.953\n",
            "Epoch:  22 Iteration:  1012 | train loss: 0.4277 | test accuracy: 0.957\n",
            "Epoch:  23 Iteration:  1056 | train loss: 0.0197 | test accuracy: 0.950\n",
            "Epoch:  24 Iteration:  1100 | train loss: 0.0224 | test accuracy: 0.950\n",
            "Epoch:  25 Iteration:  1144 | train loss: 0.0930 | test accuracy: 0.950\n",
            "Epoch:  26 Iteration:  1188 | train loss: 0.2122 | test accuracy: 0.950\n",
            "Epoch:  27 Iteration:  1232 | train loss: 0.0697 | test accuracy: 0.950\n",
            "Epoch:  28 Iteration:  1276 | train loss: 0.5095 | test accuracy: 0.947\n",
            "Epoch:  29 Iteration:  1320 | train loss: 0.0929 | test accuracy: 0.957\n",
            "Epoch:  30 Iteration:  1364 | train loss: 0.0408 | test accuracy: 0.947\n",
            "New learning rate is:  0.0002\n",
            "Epoch:  31 Iteration:  1408 | train loss: 0.0121 | test accuracy: 0.947\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-221-f112c1807a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mmodel_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mnrnn_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmult_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0macc_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrnn_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mseed_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-220-f5319c1a2e61>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(seed, train, test, add_noise, mult_noise, nrnn)\u001b[0m\n\u001b[1;32m    158\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m               \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m               \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m               \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-216-a1840ac593a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_A\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma_W\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "method = \"No noise\"\n",
        "acc_arr1 = []\n",
        "seed_arr1 = []\n",
        "method_arr1 = []\n",
        "specify_arr1 = []\n",
        "model_arr1 = []\n",
        "\n",
        "for seed in seeds:\n",
        "  rnn_acc = driver(seed, train_loader, test_loader)\n",
        "  acc_arr1.append(rnn_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(0.0)\n",
        "  model_arr1.append(\"RNN\")\n",
        "\n",
        "  nrnn_acc = driver(seed, train_loader, test_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
        "  acc_arr1.append(nrnn_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(0.0)\n",
        "  model_arr1.append(\"NRNN\")\n",
        "\n",
        "  lstm_acc = lstm_driver(seed, X_test, y_test, X_train, y_train)\n",
        "  acc_arr1.append(lstm_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(0.0)\n",
        "  model_arr1.append(\"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYqIjBk8Zl1z"
      },
      "outputs": [],
      "source": [
        "method = \"class\"\n",
        "acc_arr2 = []\n",
        "seed_arr2 = []\n",
        "method_arr2 = []\n",
        "specify_arr2 = []\n",
        "model_arr2 = []\n",
        "\n",
        "noise_matrices = [noise_pair_45, noise_sym_25, noise_sym_50]\n",
        "names = [\"noise_pair_45\", \"noise_sym_25\", \"noise_sym_50\"]\n",
        "\n",
        "for idx, noise_matrix in enumerate(noise_matrices):\n",
        "\n",
        "  for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    y_train_flipped = flip_HAR_labels(y_train, noise_matrix).reshape(-1, 1)\n",
        "    y_test_flipped = flip_HAR_labels(y_test, noise_matrix).reshape(-1, 1)\n",
        "\n",
        "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "    acc_arr2.append(rnn_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])\n",
        "    model_arr2.append(\"RNN\")\n",
        "\n",
        "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
        "    acc_arr2.append(nrnn_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])\n",
        "    model_arr2.append(\"NRNN\")\n",
        "\n",
        "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
        "    acc_arr2.append(lstm_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])\n",
        "    model_arr2.append(\"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXwkWtGyu6Na"
      },
      "outputs": [],
      "source": [
        "method = \"basic\"\n",
        "acc_arr3 = []\n",
        "seed_arr3 = []\n",
        "method_arr3 = []\n",
        "specify_arr3 = []\n",
        "model_arr3 = []\n",
        "\n",
        "flip_probabilities = [0.05, 0.1, 0.2, 0.4]\n",
        "for flip_probability in flip_probabilities:\n",
        "\n",
        "  for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    y_train_flipped = flip_HAR_labels_basic(y_train, flip_probability).reshape(-1, 1)\n",
        "    y_test_flipped = flip_HAR_labels_basic(y_test, flip_probability).reshape(-1, 1)\n",
        "\n",
        "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "    acc_arr3.append(rnn_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(flip_probability)\n",
        "    model_arr3.append(\"RNN\")\n",
        "\n",
        "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
        "    acc_arr3.append(nrnn_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(flip_probability)\n",
        "    model_arr3.append(\"NRNN\")\n",
        "    \n",
        "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
        "    acc_arr3.append(lstm_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(flip_probability)\n",
        "    model_arr3.append(\"LSTM\")\n",
        "\n",
        "#plt.boxplot(final_acc_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Z0vkBXQ_1v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "acc_arr = acc_arr1 + acc_arr2 + acc_arr3\n",
        "seed_arr = seed_arr1 + seed_arr2 + seed_arr3\n",
        "method_arr = method_arr1 + method_arr2 + method_arr3\n",
        "specify_arr = specify_arr1 + specify_arr2 + specify_arr3\n",
        "model_arr = model_arr1 + model_arr2 + model_arr3\n",
        "\n",
        "df1 = pd.DataFrame(list(zip(acc_arr, seed_arr, method_arr, specify_arr, model_arr)), columns = [\"Accuracies\", \"Seed\", \"Method\", \"Specify\", \"Model\"])\n",
        "df1\n",
        "df1.to_csv('out.csv')  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "noisy-times-series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}