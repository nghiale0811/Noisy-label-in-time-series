{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzBraGTpl7gT"
   },
   "source": [
    "## Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "r28CXXZps-g1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from torch._utils import _accumulate\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.utils.clip_grad as clip_grad\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import copy\n",
    "from subprocess import call\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
    "import datetime as dt\n",
    "import time\n",
    "import random\n",
    "from scipy import signal\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPwn7kurmByV"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYa-ggaVkzkI",
    "outputId": "07eaeaed-3bf9-46ea-d984-a92c2195bef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading...\n",
      "Dataset already downloaded. Did not download twice.\n",
      "\n",
      "Extracting...\n",
      "Dataset already extracted. Did not extract twice.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "\n",
    "print(\"Downloading...\")\n",
    "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
    "    call(\n",
    "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"Downloading done.\\n\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
    "\n",
    "\n",
    "print(\"Extracting...\")\n",
    "extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
    "if not os.path.exists(extract_directory):\n",
    "    call(\n",
    "        'unzip -nq \"UCI HAR Dataset.zip\"',\n",
    "        shell=True\n",
    "    )\n",
    "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
    "else:\n",
    "    print(\"Dataset already extracted. Did not extract twice.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwLgDKQytkRN",
    "outputId": "d4193575-565b-4f33-c08e-f749ede76871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 128, 9)\n",
      "300\n",
      "Some useful info to get an insight on dataset's shape and normalisation:\n",
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(300, 128, 9) (300, 1) 0.10143949 0.39385343\n"
     ]
    }
   ],
   "source": [
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "\n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "\n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "\n",
    "    # Substract 1 to each output class for friendly 0-based indexing\n",
    "    return y_ - 1\n",
    "\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
    "DATASET_PATH = \"/h/snagaraj/NoisyTS/UCI HAR Dataset/\"\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
    "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "\n",
    "#Take 700 training datapoints\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
    "X_train = X_train[:700]\n",
    "y_train = y_train[:700]\n",
    "\n",
    "#Take 300 testing datapoints\n",
    "X_test, y_test = shuffle(X_test, y_test, random_state = 0)\n",
    "X_test = X_test[:300]\n",
    "y_test = y_test[:300]\n",
    "\n",
    "# Input Data\n",
    "\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "print(X_train.shape)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "print(test_data_count)\n",
    "n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "\n",
    "train = data_utils.TensorDataset(torch.from_numpy(X_train), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
    "\n",
    "test = data_utils.TensorDataset(torch.from_numpy(X_test),torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = data_utils.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13RRzP01mLkI"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "cQz7VXgDFLfN"
   },
   "outputs": [],
   "source": [
    "#ADD NOISE TO HAR\n",
    "\n",
    "noise_pair_45= np.array([[.55,0.45,0.0,0.0,0.0,0.0],\n",
    "                        [0.0,.55,0.45,0.0,0.0,0.0],\n",
    "                        [0.0,0.0,.55,0.45,0.0,0.0],\n",
    "                        [0.0,0.0,0.0,.55,0.45,0.0],\n",
    "                        [0.0,0.0,0.0,0.0,.55,0.45],\n",
    "                        [0.45,0.0,0.0,0.0,0.0,.55]])\n",
    "noise_sym_50 = np.array([[.50,.10,.10,.10,.10,.10],\n",
    "                        [.10,.50,.10,.10,.10,.10],\n",
    "                        [.10,.10,.50,.10,.10,.10],\n",
    "                        [.10,.10,.10,.50,.10,.10],\n",
    "                        [.10,.10,.10,.10,.50,.10],\n",
    "                        [.10,.10,.10,.10,.10,.50]])\n",
    "\n",
    "noise_sym_25 = np.array([[.25,.15,.15,.15,.15,.15],\n",
    "                        [.15,.25,.15,.15,.15,.15],\n",
    "                        [.15,.15,.25,.15,.15,.15],\n",
    "                        [.15,.15,.15,.25,.15,.15],\n",
    "                        [.15,.15,.15,.15,.25,.15],\n",
    "                        [.15,.15,.15,.15,.15,.25]])\n",
    "\n",
    "def flip_HAR_labels(array, noise_matrix):\n",
    "    flipped = []\n",
    "    for elem in array.flatten():\n",
    "        flipped.append(np.random.choice([0,1,2,3,4,5], p=noise_matrix[int(elem)]))\n",
    "    \n",
    "    flipped = np.array(flipped)\n",
    "    flipped.reshape((-1, 1))\n",
    "    return flipped\n",
    "\n",
    "\n",
    "def flip_HAR_labels_basic(array, flip_probability):\n",
    "    flip_mask = np.random.binomial(1, flip_probability, len(array))\n",
    "    flipped = []\n",
    "    for i in range(len(array)):\n",
    "        if flip_mask[i]==1:\n",
    "            options = [0.0,1.0,2.0,3.0,4.0,5.0]\n",
    "            new_options = [x for x in options if x != array[i]]\n",
    "            flipped.append(np.random.choice(new_options, p=[0.2,0.2,0.2,0.2, 0.2]))\n",
    "        else:\n",
    "            flipped.append(array[i])\n",
    "    \n",
    "    flipped = np.array(flipped, dtype=np.int)\n",
    "    flipped.reshape((-1, 1))\n",
    "    return flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_L8xzQyNFPpW",
    "outputId": "9685c54c-7461-414c-9597-8c8c5cad0d97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([0.0,1.0,2.0,3.0,4.0,5.0])\n",
    "noise_matrix = noise_sym_25\n",
    "\n",
    "flip_HAR_labels_basic(array, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkrafFRyGKhG",
    "outputId": "32eda83a-e6b3-46f9-9719-f4d0e3b4f946"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 5, 3, 2, 3])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_HAR_labels(array, noise_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQ7gc-5cFT7S",
    "outputId": "3e49dd4b-8817-4e38-db9d-3ab660a1e588"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_mask = np.random.binomial(1, 0.5, len(array))\n",
    "flip_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "c4RXMRyAtFvC"
   },
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get a gpu if available.\"\"\"\n",
    "    if torch.cuda.device_count()>0:\n",
    "        device = torch.device('cuda')\n",
    "        print(\"Connected to a GPU\")\n",
    "    else:\n",
    "        print(\"Using the CPU\")\n",
    "        device = torch.device('cpu')\n",
    "    return device\n",
    "\n",
    "def which_device(model):\n",
    "    return next(model.parameters()).device\n",
    "\n",
    "\n",
    "def add_channels(X):\n",
    "    if len(X.shape) == 2:\n",
    "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
    "\n",
    "    elif len(X.shape) == 3:\n",
    "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
    "\n",
    "    else:\n",
    "        return \"dimenional error\"\n",
    "    \n",
    "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
    "\n",
    "    if strategy=='normal':\n",
    "        if epoch in decayEpoch:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= decay_eff\n",
    "            print('New learning rate is: ', param_group['lr'])\n",
    "    else:\n",
    "        print('wrong strategy')\n",
    "        raise ValueError('A very specific bad thing happened.')\n",
    "\n",
    "    return optimizer\n",
    "\n",
    "    \n",
    "    \n",
    "def gaussian_init_(n_units, std=1):    \n",
    "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
    "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
    "    return A_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PouKNWlImO5K"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "id": "j5KjAjajQA9p"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, feature_size, n_state, hidden_size=8, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
    "                 seed=random.seed('2021')):\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_state = n_state\n",
    "        self.seed = seed\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.rnn_type = rnn\n",
    "        self.regres = regres\n",
    "        self.return_all = return_all\n",
    "        \n",
    "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
    "\n",
    "        self.regressor = nn.Sequential(nn.Linear(self.hidden_size, self.n_state))\n",
    "                                      #nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, input, past_state=None, **kwargs):\n",
    "        input = input.to(self.device)\n",
    "        self.rnn.to(self.device)\n",
    "        self.regressor.to(self.device)\n",
    "        if not past_state:\n",
    "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
    "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            all_encodings, encoding = self.rnn(input, past_state)\n",
    "        else:\n",
    "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
    "        \n",
    "        if self.regres:\n",
    "            if not self.return_all:\n",
    "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
    "            else:\n",
    "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
    "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
    "        else:\n",
    "            return encoding.view(encoding.shape[1], -1)\n",
    "        \n",
    "        \n",
    "def format_time(elapsed):\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
    "            \n",
    "def save_ckpt(generator_model, output_dir, data):\n",
    "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
    "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
    "    os.makedirs(check_pt_dir, exist_ok=True)\n",
    "    torch.save(generator_model.state_dict(), fname)\n",
    "    \n",
    "def get_accuracy(model, loader):\n",
    "    \n",
    "    correct, total = 0, 0\n",
    "    for xs, ts in loader:\n",
    "        xs.to(device)\n",
    "        ts.to(device)\n",
    "        zs = model(xs)\n",
    "        \n",
    "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
    "        ts = torch.argmax(ts, 1)\n",
    "\n",
    "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
    "        total += int(ts.shape[0])\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "id": "Pe1d7EkrQC8s"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def train_model(model, train_dataloader, n_epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.1)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
    "    \n",
    "    best_loss = 10000.0\n",
    "    \n",
    "    n=0\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t0 = time.time()\n",
    "        #print(\"\")\n",
    "        #print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
    "        #print('Training...')\n",
    "\n",
    "        total_train_loss = 0\n",
    "        train_losses=[]\n",
    "        model = model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            #if step % 40 == 0 and not step == 0:\n",
    "            #    elapsed = (time.time() - t0)\n",
    "            #    print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            b_input= batch[0].to(device)\n",
    "            b_target =  batch[1].long().to(device)\n",
    "            iters.append(n)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(b_input)\n",
    "            #print(out)\n",
    "            #target = torch.argmax(b_target, 1)\n",
    "            target = b_target.squeeze()\n",
    "            #print(target)\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if n % 10 == 0:\n",
    "                iters_sub.append(n)\n",
    "                \n",
    "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
    "                #print(get_accuracy(model, train_dataloader))\n",
    "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
    "            # increment the iteration number\n",
    "            n += 1\n",
    "\n",
    "        training_time = (time.time() - t0)\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        #print(\"\")\n",
    "        #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        #print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "        #print(\"\")\n",
    "        #print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "    \n",
    "        history['train'].append(train_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch}: train loss {train_loss} ')\n",
    "    # plt.style.use('seaborn-white')\n",
    "    # plt.plot(history['train'])\n",
    "\n",
    "    # plt.title('LSTM  Training Curves')\n",
    "    # plt.ylabel('CE Loss')\n",
    "    # plt.xlabel('Epoch Number')\n",
    "    # plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    # plt.show()\n",
    "    \n",
    "    return model.eval(), history\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    #INFERENCE ON TEST SET\n",
    "    #print(x_test.shape)\n",
    "    #print(model(x_test))\n",
    "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
    "    \n",
    "    \n",
    "    y_test = y_test.squeeze().numpy()\n",
    "    y_pred = predictions.squeeze().cpu().numpy()\n",
    "    print(y_pred)\n",
    "    print(y_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #f1 = f1_score(y_test, y_pred, average='micro')\n",
    "    #precision = precision_score(y_test, y_pred, average='micro')\n",
    "    #print(accuracy)\n",
    "    #print(f1)\n",
    "    #print(precision)\n",
    "    return accuracy\n",
    "\n",
    "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
    "    train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.tensor(y_train))\n",
    "    train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
    "\n",
    "    x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "\n",
    "    y_test = torch.tensor(y_test, dtype=torch.int)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LSTMClassifier(9, #num features \n",
    "                  6, #num classes,\n",
    "                  seed = seed,\n",
    "                  rnn=\"GRU\" #rnn type    \n",
    "            )\n",
    "\n",
    "    model, history = train_model(model, train_loader, 50, 1e-3)\n",
    "    acc = evaluate_model(model, x_test, y_test)\n",
    "    print(\"Test Accuracy: \", acc)\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 1)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "id": "MELlsVkos0ZP"
   },
   "outputs": [],
   "source": [
    "class NoisyRNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_classes, n_units=16, eps=0.01, \n",
    "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
    "                 solver='base', add_noise=0, mult_noise=0):\n",
    "        super(NoisyRNN, self).__init__()\n",
    "\n",
    "        self.device = get_device()\n",
    "\n",
    "\n",
    "        self.n_units = n_units\n",
    "        self.eps = eps\n",
    "        self.solver = solver\n",
    "        self.beta = beta\n",
    "        self.alpha = alpha\n",
    "        self.gamma_A = gamma_A\n",
    "        self.gamma_W = gamma_W\n",
    "        self.add_noise = add_noise\n",
    "        self.mult_noise = mult_noise\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        self.E = nn.Linear(input_dim, n_units)\n",
    "        self.D = nn.Linear(n_units, output_classes)     \n",
    "                                            \n",
    "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
    "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
    "        self.I = torch.eye(n_units).to(self.device)   \n",
    "\n",
    "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
    "\n",
    "\n",
    "    def forward(self, x, mode='test'):\n",
    "        T = x.shape[1]\n",
    "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
    "\n",
    "        for i in range(T):\n",
    "            z = self.E(x[:,i,:])\n",
    "\n",
    "            if i == 0:\n",
    "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
    "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
    "                \n",
    "                        \n",
    "            add_noise = 0.0\n",
    "            mult_noise = 1.0\n",
    "            if mode == 'train':\n",
    "                if self.add_noise > 0:\n",
    "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
    "                            \n",
    "                if self.mult_noise > 0:\n",
    "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
    "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
    "                        \n",
    "\n",
    "            if self.solver == 'base': \n",
    "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
    "                h = h + self.eps * h_update\n",
    "            elif self.solver == 'noisy':\n",
    "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
    "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
    "                 \n",
    "                \n",
    "        # Decoder \n",
    "        #----------\n",
    "        out = self.D(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pK_RFvzqvl65",
    "outputId": "babb1630-8ce3-4084-851f-341b10699348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n"
     ]
    }
   ],
   "source": [
    "#code for the driver\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
    "#\n",
    "parser.add_argument('-f')\n",
    "#\n",
    "parser.add_argument('--name', type=str, default='HAR', metavar='N', help='dataset')\n",
    "#\n",
    "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
    "#\n",
    "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
    "#\n",
    "parser.add_argument('--epochs', type=int, default=50, metavar='N', help='number of epochs to train (default: 90)')\n",
    "#\n",
    "parser.add_argument('--lr', type=float, default=0.002, metavar='LR', help='learning rate (default: 0.1)')\n",
    "#\n",
    "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
    "#\n",
    "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
    "#\n",
    "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
    "#\n",
    "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
    "#\n",
    "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
    "#\n",
    "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
    "#\n",
    "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
    "#\n",
    "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
    "#\n",
    "parser.add_argument('--n_units', type=int, default=16, metavar='S', help='number of hidden units')\n",
    "#\n",
    "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
    "#\n",
    "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
    "#\n",
    "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
    "#\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
    "#\n",
    "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
    "#\n",
    "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
    "#\n",
    "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
    "#\n",
    "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
    "#\n",
    "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
    "#\n",
    "args = parser.parse_args()\n",
    "\n",
    "if not os.path.isdir(args.name + '_results'):\n",
    "    os.mkdir(args.name + '_results')\n",
    "\n",
    "#==============================================================================\n",
    "# set random seed to reproduce the work\n",
    "#==============================================================================\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "#==============================================================================\n",
    "# get device\n",
    "#==============================================================================\n",
    "device = get_device()\n",
    "\n",
    "#==============================================================================\n",
    "# initialize model\n",
    "#==============================================================================\n",
    "seeds = [1, 2, 3]\n",
    "\n",
    "\n",
    "def driver(seed, train, test, add_noise = 0, mult_noise = 0, nrnn = False):\n",
    "\n",
    "  final_acc = []\n",
    "  model = NoisyRNN(input_dim=int(9), output_classes=6, n_units=args.n_units, \n",
    "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
    "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
    "                add_noise=add_noise, mult_noise=mult_noise).to(device)\n",
    "\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  noise = torch.randn(1,700,128,9).float()\n",
    "\n",
    "\n",
    "  #==============================================================================\n",
    "  # set random seed to reproduce the work\n",
    "  #==============================================================================\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "  #==============================================================================\n",
    "  # Model summary\n",
    "  #==============================================================================\n",
    "  print(model)    \n",
    "  print('**** Setup ****')\n",
    "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
    "  print('************')    \n",
    "    \n",
    "\n",
    "  if args.optimizer == 'SGD':\n",
    "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
    "  elif  args.optimizer == 'Adam':\n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "  else:\n",
    "      print(\"Unexpected optimizer!\")\n",
    "      raise \n",
    "\n",
    "\n",
    "  loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "  # training and testing\n",
    "  count = 0\n",
    "  loss_hist = []\n",
    "  test_acc = []\n",
    "\n",
    "  t0 = timeit.default_timer()\n",
    "  for epoch in range(args.epochs):\n",
    "      model.train()\n",
    "      lossaccum = 0\n",
    "      \n",
    "      for step, (x, y) in enumerate(train):\n",
    "          count += 1\n",
    "          \n",
    "          # Reshape data for recurrent unit\n",
    "          inputs = Variable(x.view(-1, 128, int(9))).to(device) # reshape x to (batch, time_step, input_size)         \n",
    "          targets = Variable(y).to(device) \n",
    "\n",
    "                  \n",
    "          # send data to recurrent unit    \n",
    "          output = model(inputs, mode='train') \n",
    "          loss = loss_func(output, targets.squeeze(1).long())\n",
    "          \n",
    "          \n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()          \n",
    "          \n",
    "          if args.gclip != 0.0:\n",
    "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
    "              \n",
    "          optimizer.step() # update weights\n",
    "          lossaccum += loss.item()\n",
    "\n",
    "          if args.model == 'test':\n",
    "              D = model.W.weight.data.cpu().numpy()  \n",
    "              u, s, v = np.linalg.svd(D, 0)\n",
    "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
    "\n",
    "      loss_hist.append(lossaccum)    \n",
    "      \n",
    "      if epoch % 1 == 0:\n",
    "          model.eval()\n",
    "          correct = 0\n",
    "          total_num = 0\n",
    "          for data, target in test: \n",
    "              data, target = data.to(device), target.to(device)                \n",
    "              output = model(data.view(-1, 128, int(9)))\n",
    "              \n",
    "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                   \n",
    "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "              total_num += len(data)\n",
    "          \n",
    "          accuracy = correct / total_num\n",
    "          test_acc.append(accuracy)\n",
    "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
    "\n",
    "\n",
    "      if nrnn == True:\n",
    "          B = model.B.data.cpu().numpy()            \n",
    "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
    "          A = 0.5 * (A + A.T)\n",
    "          e, _ = np.linalg.eig(A)\n",
    "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
    "          \n",
    "          C = model.C.data.cpu().numpy()            \n",
    "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
    "          e, _ = np.linalg.eig(W)\n",
    "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
    "              \n",
    "              \n",
    "\n",
    "      # schedule learning rate decay    \n",
    "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
    "\n",
    "  print('total time: ', timeit.default_timer()  - t0 )\n",
    "\n",
    "  final_acc.append(max(test_acc))\n",
    "\n",
    "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
    "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
    "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
    "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
    "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
    "              + '_seed_' + str(args.seed) + '.pkl')  \n",
    "\n",
    "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
    "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
    "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
    "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
    "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
    "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
    "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
    "\n",
    "  pickle.dump(data,f)\n",
    "  f.close()\n",
    "  return max(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JY77AGBuZaQt",
    "outputId": "41ddf35b-8518-4c09-ff4f-8441e243e25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train loss 1.7769157120159693 \n",
      "Epoch 2: train loss 1.7383317964417593 \n",
      "Epoch 3: train loss 1.7010883109910147 \n",
      "Epoch 4: train loss 1.6523801922798156 \n",
      "Epoch 5: train loss 1.5942968811307634 \n",
      "Epoch 6: train loss 1.5383420825004577 \n",
      "Epoch 7: train loss 1.4885749765804837 \n",
      "Epoch 8: train loss 1.4643081784248353 \n",
      "Epoch 9: train loss 1.449504885503224 \n",
      "Epoch 10: train loss 1.4444634965487888 \n",
      "Epoch 11: train loss 1.4360510996409825 \n",
      "Epoch 12: train loss 1.4318484468119486 \n",
      "Epoch 13: train loss 1.4275486162730626 \n",
      "Epoch 14: train loss 1.4193004012107848 \n",
      "Epoch 15: train loss 1.4186759625162397 \n",
      "Epoch 16: train loss 1.4190871902874538 \n",
      "Epoch 17: train loss 1.4143922465188163 \n",
      "Epoch 18: train loss 1.4140129438468387 \n",
      "Epoch 19: train loss 1.411045628786087 \n",
      "Epoch 20: train loss 1.4131860554218292 \n",
      "Epoch 21: train loss 1.4086667452539716 \n",
      "Epoch 22: train loss 1.4083927358899797 \n",
      "Epoch 23: train loss 1.404966253893716 \n",
      "Epoch 24: train loss 1.4105659340109145 \n",
      "Epoch 25: train loss 1.4083675597395215 \n",
      "Epoch 26: train loss 1.4039632729121616 \n",
      "Epoch 27: train loss 1.4081163934298924 \n",
      "Epoch 28: train loss 1.4001895257404873 \n",
      "Epoch 29: train loss 1.4041130457605635 \n",
      "Epoch 30: train loss 1.4048526184899466 \n",
      "Epoch 31: train loss 1.3958822080067226 \n",
      "Epoch 32: train loss 1.4016017709459578 \n",
      "Epoch 33: train loss 1.3982220002583094 \n",
      "Epoch 34: train loss 1.4016853783811842 \n",
      "Epoch 35: train loss 1.3930688091686794 \n",
      "Epoch 36: train loss 1.404824845279966 \n",
      "Epoch 37: train loss 1.3959823438099452 \n",
      "Epoch 38: train loss 1.4021971923964365 \n",
      "Epoch 39: train loss 1.3950341650417872 \n",
      "Epoch 40: train loss 1.4017319219452995 \n",
      "Epoch 41: train loss 1.3992261622633253 \n",
      "Epoch 42: train loss 1.3939776182174684 \n",
      "Epoch 43: train loss 1.399654347555978 \n",
      "Epoch 44: train loss 1.3982890571866717 \n",
      "Epoch 45: train loss 1.3955886142594474 \n",
      "Epoch 46: train loss 1.3982933350971767 \n",
      "Epoch 47: train loss 1.3993760585784911 \n",
      "Epoch 48: train loss 1.3973144590854645 \n",
      "Epoch 49: train loss 1.3981439249856131 \n",
      "Epoch 50: train loss 1.3969664888722555 \n",
      "[4 5 5 4 4 5 4 5 4 4 5 4 5 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 5 4 4 4 5 3 5 4 4 4 5 4 4 3 4 4 4 4 4 5 5 3 4 3 4 4 5 4 4 3 5 5 4 5 5 3\n",
      " 4 4 5 5 4 4 4 4 4 4 4 4 4 3 4 4 4 4 5 4 5 3 4 4 3 4 5 4 5 5 4 3 4 5 4 4 5\n",
      " 4 5 5 5 3 4 4 5 4 4 4 4 5 3 4 4 5 4 4 4 5 4 4 4 5 4 4 4 5 4 3 4 4 4 4 4 3\n",
      " 5 4 3 4 4 3 3 4 4 5 4 4 4 4 4 4 4 4 4 4 4 4 5 4 4 4 4 4 4 4 5 4 4 4 4 4 4\n",
      " 3 4 5 3 4 4 5 4 4 3 4 5 5 5 4 4 4 5 5 4 4 5 3 4 4 5 4 4 4 5 4 4 4 4 4 4 4\n",
      " 5 5 3 4 5 4 4 4 5 4 4 4 3 4 4 4 3 4 4 4 5 4 4 4 4 4 4 4 5 4 3 4 5 4 4 4 4\n",
      " 4 4 4 4 4 4 5 4 4 5 4 4 3 4 4 4 5 4 4 4 4 4 4 4 5 4 5 4 4 4 4 5 4 5 4 4 4\n",
      " 4 4 5 4]\n",
      "[0 5 5 1 0 5 1 5 0 4 5 2 5 4 0 4 2 1 1 1 1 2 3 1 4 0 3 4 4 4 2 2 1 2 2 2 4\n",
      " 4 5 4 2 4 5 5 5 1 1 0 5 2 1 3 4 2 1 4 2 5 5 3 0 3 2 1 5 3 1 3 3 5 3 5 5 3\n",
      " 2 0 5 5 1 4 3 2 1 1 2 1 3 5 0 0 2 4 5 1 3 2 1 4 0 1 5 2 5 5 2 3 1 5 3 1 5\n",
      " 2 5 5 5 3 0 2 5 1 2 0 3 5 2 0 4 3 4 2 0 5 0 0 3 5 4 4 1 5 0 2 1 0 2 4 2 3\n",
      " 5 1 1 4 1 3 3 3 4 3 4 4 2 4 4 4 0 3 4 0 1 1 5 2 4 2 0 3 1 2 5 2 4 4 0 4 4\n",
      " 3 4 5 3 1 0 5 4 2 3 3 5 5 5 3 4 2 5 5 4 1 5 3 0 4 5 1 0 3 5 3 1 3 3 3 0 0\n",
      " 5 5 3 0 5 0 0 4 3 4 2 2 3 4 0 4 3 3 0 2 5 3 4 2 2 0 4 2 5 4 3 1 3 0 1 0 2\n",
      " 3 4 4 2 1 0 5 0 3 3 2 1 3 0 4 3 5 3 4 1 1 1 0 1 5 4 5 3 4 3 0 5 3 3 0 2 0\n",
      " 3 1 5 4]\n",
      "Test Accuracy:  0.4266666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4266666666666667"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "lstm_driver(seed, X_test, y_test, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n",
      "NoisyRNN(\n",
      "  (tanh): Tanh()\n",
      "  (E): Linear(in_features=9, out_features=16, bias=True)\n",
      "  (D): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n",
      "**** Setup ****\n",
      "Total params: 0.79k\n",
      "************\n",
      "Epoch:  0 Iteration:  120 | train loss: 1.3275 | test accuracy: 0.503\n",
      "Epoch:  1 Iteration:  240 | train loss: 1.7076 | test accuracy: 0.467\n",
      "Epoch:  2 Iteration:  360 | train loss: 1.0324 | test accuracy: 0.510\n",
      "Epoch:  3 Iteration:  480 | train loss: 1.0288 | test accuracy: 0.513\n",
      "Epoch:  4 Iteration:  600 | train loss: 0.4310 | test accuracy: 0.613\n",
      "Epoch:  5 Iteration:  720 | train loss: 0.8369 | test accuracy: 0.580\n",
      "Epoch:  6 Iteration:  840 | train loss: 0.3594 | test accuracy: 0.710\n",
      "Epoch:  7 Iteration:  960 | train loss: 0.4773 | test accuracy: 0.680\n",
      "Epoch:  8 Iteration:  1080 | train loss: 0.6692 | test accuracy: 0.703\n",
      "Epoch:  9 Iteration:  1200 | train loss: 1.0355 | test accuracy: 0.727\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-233-f885b891ea20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-231-4016a75a83b1>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(seed, train, test, add_noise, mult_noise, nrnn)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           \u001b[0;31m# send data to recurrent unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-230-a1840ac593a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "driver(seed, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "O7EmSIf4vSwB",
    "outputId": "4e8256ea-9a98-47f2-f8d2-4eb40596ff37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to a GPU\n",
      "NoisyRNN(\n",
      "  (tanh): Tanh()\n",
      "  (E): Linear(in_features=9, out_features=16, bias=True)\n",
      "  (D): Linear(in_features=16, out_features=6, bias=True)\n",
      ")\n",
      "**** Setup ****\n",
      "Total params: 0.79k\n",
      "************\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-f112c1807a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mrnn_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0macc_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mseed_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-4016a75a83b1>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(seed, train, test, add_noise, mult_noise, nrnn)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m           \u001b[0;31m# send data to recurrent unit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-a1840ac593a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'noisy'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mh_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmult_noise\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh_update\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "method = \"No noise\"\n",
    "acc_arr1 = []\n",
    "seed_arr1 = []\n",
    "method_arr1 = []\n",
    "specify_arr1 = []\n",
    "model_arr1 = []\n",
    "\n",
    "for seed in seeds:\n",
    "  rnn_acc = driver(seed, train_loader, test_loader)\n",
    "  acc_arr1.append(rnn_acc)\n",
    "  seed_arr1.append(seed)\n",
    "  method_arr1.append(method)\n",
    "  specify_arr1.append(0.0)\n",
    "  model_arr1.append(\"RNN\")\n",
    "\n",
    "  nrnn_acc = driver(seed, train_loader, test_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "  acc_arr1.append(nrnn_acc)\n",
    "  seed_arr1.append(seed)\n",
    "  method_arr1.append(method)\n",
    "  specify_arr1.append(0.0)\n",
    "  model_arr1.append(\"NRNN\")\n",
    "\n",
    "  lstm_acc = lstm_driver(seed, X_test, y_test, X_train, y_train)\n",
    "  acc_arr1.append(lstm_acc)\n",
    "  seed_arr1.append(seed)\n",
    "  method_arr1.append(method)\n",
    "  specify_arr1.append(0.0)\n",
    "  model_arr1.append(\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYqIjBk8Zl1z"
   },
   "outputs": [],
   "source": [
    "method = \"class\"\n",
    "acc_arr2 = []\n",
    "seed_arr2 = []\n",
    "method_arr2 = []\n",
    "specify_arr2 = []\n",
    "model_arr2 = []\n",
    "\n",
    "noise_matrices = [noise_pair_45, noise_sym_25, noise_sym_50]\n",
    "names = [\"noise_pair_45\", \"noise_sym_25\", \"noise_sym_50\"]\n",
    "\n",
    "for idx, noise_matrix in enumerate(noise_matrices):\n",
    "\n",
    "  for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    y_train_flipped = flip_HAR_labels(y_train, noise_matrix).reshape(-1, 1)\n",
    "    y_test_flipped = flip_HAR_labels(y_test, noise_matrix).reshape(-1, 1)\n",
    "\n",
    "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
    "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
    "    acc_arr2.append(rnn_acc)\n",
    "    seed_arr2.append(seed)\n",
    "    method_arr2.append(method)\n",
    "    specify_arr2.append(names[idx])\n",
    "    model_arr2.append(\"RNN\")\n",
    "\n",
    "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "    acc_arr2.append(nrnn_acc)\n",
    "    seed_arr2.append(seed)\n",
    "    method_arr2.append(method)\n",
    "    specify_arr2.append(names[idx])\n",
    "    model_arr2.append(\"NRNN\")\n",
    "\n",
    "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
    "    acc_arr2.append(lstm_acc)\n",
    "    seed_arr2.append(seed)\n",
    "    method_arr2.append(method)\n",
    "    specify_arr2.append(names[idx])\n",
    "    model_arr2.append(\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXwkWtGyu6Na"
   },
   "outputs": [],
   "source": [
    "method = \"basic\"\n",
    "acc_arr3 = []\n",
    "seed_arr3 = []\n",
    "method_arr3 = []\n",
    "specify_arr3 = []\n",
    "model_arr3 = []\n",
    "\n",
    "flip_probabilities = [0.05, 0.1, 0.2, 0.4]\n",
    "for flip_probability in flip_probabilities:\n",
    "\n",
    "  for seed in seeds:\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    y_train_flipped = flip_HAR_labels_basic(y_train, flip_probability).reshape(-1, 1)\n",
    "    y_test_flipped = flip_HAR_labels_basic(y_test, flip_probability).reshape(-1, 1)\n",
    "\n",
    "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
    "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=10, shuffle=True)\n",
    "\n",
    "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
    "    acc_arr3.append(rnn_acc)\n",
    "    seed_arr3.append(seed)\n",
    "    method_arr3.append(method)\n",
    "    specify_arr3.append(flip_probability)\n",
    "    model_arr3.append(\"RNN\")\n",
    "\n",
    "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
    "    acc_arr3.append(nrnn_acc)\n",
    "    seed_arr3.append(seed)\n",
    "    method_arr3.append(method)\n",
    "    specify_arr3.append(flip_probability)\n",
    "    model_arr3.append(\"NRNN\")\n",
    "    \n",
    "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
    "    acc_arr3.append(lstm_acc)\n",
    "    seed_arr3.append(seed)\n",
    "    method_arr3.append(method)\n",
    "    specify_arr3.append(flip_probability)\n",
    "    model_arr3.append(\"LSTM\")\n",
    "\n",
    "#plt.boxplot(final_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9Z0vkBXQ_1v"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "acc_arr = acc_arr1 + acc_arr2 + acc_arr3\n",
    "seed_arr = seed_arr1 + seed_arr2 + seed_arr3\n",
    "method_arr = method_arr1 + method_arr2 + method_arr3\n",
    "specify_arr = specify_arr1 + specify_arr2 + specify_arr3\n",
    "model_arr = model_arr1 + model_arr2 + model_arr3\n",
    "\n",
    "df1 = pd.DataFrame(list(zip(acc_arr, seed_arr, method_arr, specify_arr, model_arr)), columns = [\"Accuracies\", \"Seed\", \"Method\", \"Specify\", \"Model\"])\n",
    "df1\n",
    "df1.to_csv('out.csv')  "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "noisy-times-series.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Stress Recov(3.6)",
   "language": "python",
   "name": "myenv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
