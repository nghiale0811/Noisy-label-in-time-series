{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "noisy-times-series.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries\n"
      ],
      "metadata": {
        "id": "KzBraGTpl7gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy\n",
        "from subprocess import call"
      ],
      "metadata": {
        "id": "r28CXXZps-g1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "CPwn7kurmByV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "\n",
        "print(\"Downloading...\")\n",
        "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "    call(\n",
        "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Downloading done.\\n\")\n",
        "else:\n",
        "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "\n",
        "print(\"Extracting...\")\n",
        "extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
        "if not os.path.exists(extract_directory):\n",
        "    call(\n",
        "        'unzip -nq \"UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
        "else:\n",
        "    print(\"Dataset already extracted. Did not extract twice.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYa-ggaVkzkI",
        "outputId": "3ccf2e17-6c62-43a5-fae4-90919b4ebeca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading...\n",
            "Dataset already downloaded. Did not download twice.\n",
            "\n",
            "Extracting...\n",
            "Dataset already extracted. Did not extract twice.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "\n",
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "print(X_train.shape)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "print(test_data_count)\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=1000, shuffle=True)"
      ],
      "metadata": {
        "id": "FwLgDKQytkRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48757624-975d-4f93-f2ff-62a7c2acb54c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9)\n",
            "2947\n",
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "13RRzP01mLkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADD NOISE TO HAR\n",
        "\n",
        "noise_pair_45= np.array([[.55,0.45,0.0,0.0,0.0,0.0],\n",
        "                        [0.0,.55,0.45,0.0,0.0,0.0],\n",
        "                        [0.0,0.0,.55,0.45,0.0,0.0],\n",
        "                        [0.0,0.0,0.0,.55,0.45,0.0],\n",
        "                        [0.0,0.0,0.0,0.0,.55,0.0],\n",
        "                        [0.45,0.0,0.0,0.0,0.0,.55]])\n",
        "noise_sym_50 = np.array([[.50,.10,.10,.10,.10,.10],\n",
        "                        [.10,.50,.10,.10,.10,.10],\n",
        "                        [.10,.10,.50,.10,.10,.10],\n",
        "                        [.10,.10,.10,.50,.10,.10],\n",
        "                        [.10,.10,.10,.10,.50,.0],\n",
        "                        [.10,.10,.10,.10,.10,.50]])\n",
        "\n",
        "noise_sym_25 = np.array([[.25,.15,.15,.15,.15,.15],\n",
        "                        [.15,.25,.15,.15,.15,.15],\n",
        "                        [.15,.15,.25,.15,.15,.15],\n",
        "                        [.15,.15,.15,.25,.15,.15],\n",
        "                        [.15,.15,.15,.15,.25,.15],\n",
        "                        [.15,.15,.15,.15,.15,.25]])\n",
        "\n",
        "def flip_HAR_labels(array, noise_matrix):\n",
        "    flipped = []\n",
        "    for elem in array.flatten():\n",
        "      flipped.append(np.random.choice([0,1,2,3,4,5], p=noise_matrix[int(elem)]))\n",
        "    \n",
        "    flipped = np.array(flipped)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped\n",
        "\n",
        "\n",
        "def flip_HAR_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, flip_probability, len(array))\n",
        "    flipped = []\n",
        "    for i in range(len(array)):\n",
        "        if flip_mask[i]==1:\n",
        "            options = [0.0,1.0,2.0,3.0,4.0,5.0]\n",
        "            new_options = [x for x in options if x != array[i]]\n",
        "            flipped.append(np.random.choice(new_options, p=[0.2,0.2,0.2,0.2, 0.2]))\n",
        "        else:\n",
        "            flipped.append(array[i])\n",
        "    \n",
        "    flipped = np.array(flipped, dtype=np.int)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped"
      ],
      "metadata": {
        "id": "cQz7VXgDFLfN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.array([0.0,1.0,2.0,3.0,4.0,5.0])\n",
        "noise_matrix = noise_sym_25\n",
        "\n",
        "flip_HAR_labels_basic(array, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8xzQyNFPpW",
        "outputId": "3f86d537-14aa-4bc1-c532-40a4f58c43ff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flip_HAR_labels(array, noise_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkrafFRyGKhG",
        "outputId": "f6eb077b-5bc5-424d-b852-2af98899dc2d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3, 5, 1, 0, 1, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "flip_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ7gc-5cFT7S",
        "outputId": "689a6fb1-678f-472d-c71d-8cc2a576044b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "c4RXMRyAtFvC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "PouKNWlImO5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MELlsVkos0ZP"
      },
      "outputs": [],
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='HAR', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.0015, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# initialize model\n",
        "#==============================================================================\n",
        "seeds = [1, 2, 3]\n",
        "\n",
        "\n",
        "def driver(seed, train, test):\n",
        "\n",
        "  final_acc = []\n",
        "  model = NoisyRNN(input_dim=int(9), output_classes=6, n_units=args.n_units, \n",
        "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "                add_noise=args.add_noise, mult_noise=args.mult_noise).to(device)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  noise = torch.randn(1,7352,128,9).float()\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # set random seed to reproduce the work\n",
        "  #==============================================================================\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # Model summary\n",
        "  #==============================================================================\n",
        "  print(model)    \n",
        "  print('**** Setup ****')\n",
        "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "  print('************')    \n",
        "    \n",
        "\n",
        "  if args.optimizer == 'SGD':\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "  elif  args.optimizer == 'Adam':\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  else:\n",
        "      print(\"Unexpected optimizer!\")\n",
        "      raise \n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # training and testing\n",
        "  count = 0\n",
        "  loss_hist = []\n",
        "  test_acc = []\n",
        "\n",
        "  t0 = timeit.default_timer()\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      lossaccum = 0\n",
        "      \n",
        "      for step, (x, y) in enumerate(train):\n",
        "          count += 1\n",
        "          \n",
        "          # Reshape data for recurrent unit\n",
        "          inputs = Variable(x.view(-1, 128, int(9))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "          targets = Variable(y).to(device) \n",
        "\n",
        "                  \n",
        "          # send data to recurrent unit    \n",
        "          output = model(inputs, mode='train') \n",
        "          loss = loss_func(output, targets.squeeze(1).long())\n",
        "          \n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()          \n",
        "          \n",
        "          if args.gclip != 0.0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "              \n",
        "          optimizer.step() # update weights\n",
        "          lossaccum += loss.item()\n",
        "\n",
        "          if args.model == 'test':\n",
        "              D = model.W.weight.data.cpu().numpy()  \n",
        "              u, s, v = np.linalg.svd(D, 0)\n",
        "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "      loss_hist.append(lossaccum)    \n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          total_num = 0\n",
        "          for data, target in test: \n",
        "              data, target = data.to(device), target.to(device)                \n",
        "              output = model(data.view(-1, 128, int(9)))\n",
        "              \n",
        "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                   \n",
        "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "              total_num += len(data)\n",
        "          \n",
        "          accuracy = correct / total_num\n",
        "          test_acc.append(accuracy)\n",
        "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "  #        if args.model == 'NoisyRNN':\n",
        "  #            B = model.B.data.cpu().numpy()            \n",
        "  #            A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "  #            A = 0.5 * (A + A.T)\n",
        "  #            e, _ = np.linalg.eig(A)\n",
        "  #            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "  #            \n",
        "  #            C = model.C.data.cpu().numpy()            \n",
        "  #            W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "  #            e, _ = np.linalg.eig(W)\n",
        "  #            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "              \n",
        "              \n",
        "\n",
        "      # schedule learning rate decay    \n",
        "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "  print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "  final_acc.append(max(test_acc))\n",
        "\n",
        "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "  pickle.dump(data,f)\n",
        "  f.close()\n",
        "  return max(test_acc)"
      ],
      "metadata": {
        "id": "pK_RFvzqvl65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84e50d81-2036-4953-de17-0f54eb654122"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_acc = []\n",
        "for seed in seeds:\n",
        "  max_acc = driver(seed, train_loader, test_loader)\n",
        "  final_acc.append(max_acc)\n",
        "print(final_acc)\n",
        "plt.boxplot(final_acc)"
      ],
      "metadata": {
        "id": "O7EmSIf4vSwB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1149a2b7-8094-487a-ac4e-a3f3f4416a3d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  58 | train loss: 1.0199 | test accuracy: 0.529\n",
            "Epoch:  1 Iteration:  116 | train loss: 0.8434 | test accuracy: 0.605\n",
            "Epoch:  2 Iteration:  174 | train loss: 0.6489 | test accuracy: 0.590\n",
            "Epoch:  3 Iteration:  232 | train loss: 0.4747 | test accuracy: 0.664\n",
            "Epoch:  4 Iteration:  290 | train loss: 0.3670 | test accuracy: 0.767\n",
            "Epoch:  5 Iteration:  348 | train loss: 0.3122 | test accuracy: 0.770\n",
            "Epoch:  6 Iteration:  406 | train loss: 0.1784 | test accuracy: 0.749\n",
            "Epoch:  7 Iteration:  464 | train loss: 0.3391 | test accuracy: 0.745\n",
            "Epoch:  8 Iteration:  522 | train loss: 0.4290 | test accuracy: 0.816\n",
            "Epoch:  9 Iteration:  580 | train loss: 0.3400 | test accuracy: 0.819\n",
            "Epoch:  10 Iteration:  638 | train loss: 0.3817 | test accuracy: 0.828\n",
            "Epoch:  11 Iteration:  696 | train loss: 0.2707 | test accuracy: 0.847\n",
            "Epoch:  12 Iteration:  754 | train loss: 0.4688 | test accuracy: 0.829\n",
            "Epoch:  13 Iteration:  812 | train loss: 0.3103 | test accuracy: 0.844\n",
            "Epoch:  14 Iteration:  870 | train loss: 0.1941 | test accuracy: 0.825\n",
            "Epoch:  15 Iteration:  928 | train loss: 0.1762 | test accuracy: 0.872\n",
            "Epoch:  16 Iteration:  986 | train loss: 0.2475 | test accuracy: 0.849\n",
            "Epoch:  17 Iteration:  1044 | train loss: 0.2665 | test accuracy: 0.869\n",
            "Epoch:  18 Iteration:  1102 | train loss: 0.2077 | test accuracy: 0.859\n",
            "Epoch:  19 Iteration:  1160 | train loss: 0.2852 | test accuracy: 0.880\n",
            "Epoch:  20 Iteration:  1218 | train loss: 0.0896 | test accuracy: 0.880\n",
            "Epoch:  21 Iteration:  1276 | train loss: 0.1509 | test accuracy: 0.890\n",
            "Epoch:  22 Iteration:  1334 | train loss: 0.1468 | test accuracy: 0.883\n",
            "Epoch:  23 Iteration:  1392 | train loss: 0.1294 | test accuracy: 0.889\n",
            "Epoch:  24 Iteration:  1450 | train loss: 0.2086 | test accuracy: 0.882\n",
            "Epoch:  25 Iteration:  1508 | train loss: 0.1224 | test accuracy: 0.908\n",
            "Epoch:  26 Iteration:  1566 | train loss: 0.1465 | test accuracy: 0.890\n",
            "Epoch:  27 Iteration:  1624 | train loss: 0.1530 | test accuracy: 0.885\n",
            "Epoch:  28 Iteration:  1682 | train loss: 0.0821 | test accuracy: 0.905\n",
            "Epoch:  29 Iteration:  1740 | train loss: 0.1423 | test accuracy: 0.890\n",
            "Epoch:  30 Iteration:  1798 | train loss: 0.1658 | test accuracy: 0.901\n",
            "New learning rate is:  0.00015000000000000001\n",
            "Epoch:  31 Iteration:  1856 | train loss: 0.1256 | test accuracy: 0.917\n",
            "Epoch:  32 Iteration:  1914 | train loss: 0.0486 | test accuracy: 0.916\n",
            "Epoch:  33 Iteration:  1972 | train loss: 0.2394 | test accuracy: 0.915\n",
            "Epoch:  34 Iteration:  2030 | train loss: 0.2917 | test accuracy: 0.919\n",
            "Epoch:  35 Iteration:  2088 | train loss: 0.1054 | test accuracy: 0.916\n",
            "Epoch:  36 Iteration:  2146 | train loss: 0.2118 | test accuracy: 0.915\n",
            "Epoch:  37 Iteration:  2204 | train loss: 0.1638 | test accuracy: 0.910\n",
            "Epoch:  38 Iteration:  2262 | train loss: 0.2021 | test accuracy: 0.910\n",
            "Epoch:  39 Iteration:  2320 | train loss: 0.0526 | test accuracy: 0.909\n",
            "Epoch:  40 Iteration:  2378 | train loss: 0.1427 | test accuracy: 0.917\n",
            "Epoch:  41 Iteration:  2436 | train loss: 0.1904 | test accuracy: 0.918\n",
            "Epoch:  42 Iteration:  2494 | train loss: 0.1395 | test accuracy: 0.917\n",
            "Epoch:  43 Iteration:  2552 | train loss: 0.1989 | test accuracy: 0.907\n",
            "Epoch:  44 Iteration:  2610 | train loss: 0.0589 | test accuracy: 0.915\n",
            "Epoch:  45 Iteration:  2668 | train loss: 0.1634 | test accuracy: 0.908\n",
            "Epoch:  46 Iteration:  2726 | train loss: 0.0918 | test accuracy: 0.917\n",
            "Epoch:  47 Iteration:  2784 | train loss: 0.1087 | test accuracy: 0.910\n",
            "Epoch:  48 Iteration:  2842 | train loss: 0.0391 | test accuracy: 0.907\n",
            "Epoch:  49 Iteration:  2900 | train loss: 0.0782 | test accuracy: 0.910\n",
            "Epoch:  50 Iteration:  2958 | train loss: 0.0587 | test accuracy: 0.916\n",
            "Epoch:  51 Iteration:  3016 | train loss: 0.1160 | test accuracy: 0.913\n",
            "Epoch:  52 Iteration:  3074 | train loss: 0.0422 | test accuracy: 0.910\n",
            "Epoch:  53 Iteration:  3132 | train loss: 0.0443 | test accuracy: 0.909\n",
            "Epoch:  54 Iteration:  3190 | train loss: 0.0965 | test accuracy: 0.911\n",
            "Epoch:  55 Iteration:  3248 | train loss: 0.1556 | test accuracy: 0.916\n",
            "Epoch:  56 Iteration:  3306 | train loss: 0.0858 | test accuracy: 0.911\n",
            "Epoch:  57 Iteration:  3364 | train loss: 0.0969 | test accuracy: 0.912\n",
            "Epoch:  58 Iteration:  3422 | train loss: 0.0722 | test accuracy: 0.913\n",
            "Epoch:  59 Iteration:  3480 | train loss: 0.1664 | test accuracy: 0.909\n",
            "Epoch:  60 Iteration:  3538 | train loss: 0.1284 | test accuracy: 0.912\n",
            "Epoch:  61 Iteration:  3596 | train loss: 0.1955 | test accuracy: 0.914\n",
            "Epoch:  62 Iteration:  3654 | train loss: 0.0935 | test accuracy: 0.921\n",
            "Epoch:  63 Iteration:  3712 | train loss: 0.1728 | test accuracy: 0.911\n",
            "Epoch:  64 Iteration:  3770 | train loss: 0.1068 | test accuracy: 0.914\n",
            "Epoch:  65 Iteration:  3828 | train loss: 0.1121 | test accuracy: 0.913\n",
            "Epoch:  66 Iteration:  3886 | train loss: 0.1418 | test accuracy: 0.907\n",
            "Epoch:  67 Iteration:  3944 | train loss: 0.2597 | test accuracy: 0.912\n",
            "Epoch:  68 Iteration:  4002 | train loss: 0.0780 | test accuracy: 0.917\n",
            "Epoch:  69 Iteration:  4060 | train loss: 0.0618 | test accuracy: 0.914\n",
            "Epoch:  70 Iteration:  4118 | train loss: 0.1559 | test accuracy: 0.909\n",
            "Epoch:  71 Iteration:  4176 | train loss: 0.1059 | test accuracy: 0.914\n",
            "Epoch:  72 Iteration:  4234 | train loss: 0.1179 | test accuracy: 0.909\n",
            "Epoch:  73 Iteration:  4292 | train loss: 0.1814 | test accuracy: 0.917\n",
            "Epoch:  74 Iteration:  4350 | train loss: 0.0666 | test accuracy: 0.912\n",
            "Epoch:  75 Iteration:  4408 | train loss: 0.0711 | test accuracy: 0.913\n",
            "Epoch:  76 Iteration:  4466 | train loss: 0.1629 | test accuracy: 0.911\n",
            "Epoch:  77 Iteration:  4524 | train loss: 0.2039 | test accuracy: 0.912\n",
            "Epoch:  78 Iteration:  4582 | train loss: 0.1184 | test accuracy: 0.912\n",
            "Epoch:  79 Iteration:  4640 | train loss: 0.1594 | test accuracy: 0.913\n",
            "Epoch:  80 Iteration:  4698 | train loss: 0.0856 | test accuracy: 0.913\n",
            "Epoch:  81 Iteration:  4756 | train loss: 0.1149 | test accuracy: 0.910\n",
            "Epoch:  82 Iteration:  4814 | train loss: 0.1054 | test accuracy: 0.918\n",
            "Epoch:  83 Iteration:  4872 | train loss: 0.0677 | test accuracy: 0.914\n",
            "Epoch:  84 Iteration:  4930 | train loss: 0.0728 | test accuracy: 0.913\n",
            "Epoch:  85 Iteration:  4988 | train loss: 0.0668 | test accuracy: 0.911\n",
            "Epoch:  86 Iteration:  5046 | train loss: 0.0240 | test accuracy: 0.917\n",
            "Epoch:  87 Iteration:  5104 | train loss: 0.1356 | test accuracy: 0.905\n",
            "Epoch:  88 Iteration:  5162 | train loss: 0.1634 | test accuracy: 0.910\n",
            "Epoch:  89 Iteration:  5220 | train loss: 0.0499 | test accuracy: 0.913\n",
            "Epoch:  90 Iteration:  5278 | train loss: 0.0884 | test accuracy: 0.919\n",
            "Epoch:  91 Iteration:  5336 | train loss: 0.0463 | test accuracy: 0.910\n",
            "Epoch:  92 Iteration:  5394 | train loss: 0.1110 | test accuracy: 0.912\n",
            "Epoch:  93 Iteration:  5452 | train loss: 0.0665 | test accuracy: 0.918\n",
            "Epoch:  94 Iteration:  5510 | train loss: 0.1203 | test accuracy: 0.918\n",
            "Epoch:  95 Iteration:  5568 | train loss: 0.1299 | test accuracy: 0.921\n",
            "Epoch:  96 Iteration:  5626 | train loss: 0.0890 | test accuracy: 0.913\n",
            "Epoch:  97 Iteration:  5684 | train loss: 0.0692 | test accuracy: 0.913\n",
            "Epoch:  98 Iteration:  5742 | train loss: 0.1217 | test accuracy: 0.907\n",
            "Epoch:  99 Iteration:  5800 | train loss: 0.0735 | test accuracy: 0.914\n",
            "total time:  311.72272041999895\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  58 | train loss: 0.9306 | test accuracy: 0.545\n",
            "Epoch:  1 Iteration:  116 | train loss: 0.9133 | test accuracy: 0.572\n",
            "Epoch:  2 Iteration:  174 | train loss: 0.7753 | test accuracy: 0.604\n",
            "Epoch:  3 Iteration:  232 | train loss: 0.5070 | test accuracy: 0.636\n",
            "Epoch:  4 Iteration:  290 | train loss: 0.4595 | test accuracy: 0.694\n",
            "Epoch:  5 Iteration:  348 | train loss: 0.7787 | test accuracy: 0.742\n",
            "Epoch:  6 Iteration:  406 | train loss: 0.4338 | test accuracy: 0.748\n",
            "Epoch:  7 Iteration:  464 | train loss: 0.3038 | test accuracy: 0.789\n",
            "Epoch:  8 Iteration:  522 | train loss: 0.3023 | test accuracy: 0.813\n",
            "Epoch:  9 Iteration:  580 | train loss: 0.1844 | test accuracy: 0.834\n",
            "Epoch:  10 Iteration:  638 | train loss: 0.4068 | test accuracy: 0.815\n",
            "Epoch:  11 Iteration:  696 | train loss: 0.2660 | test accuracy: 0.827\n",
            "Epoch:  12 Iteration:  754 | train loss: 0.2869 | test accuracy: 0.871\n",
            "Epoch:  13 Iteration:  812 | train loss: 0.1449 | test accuracy: 0.854\n",
            "Epoch:  14 Iteration:  870 | train loss: 0.1851 | test accuracy: 0.857\n",
            "Epoch:  15 Iteration:  928 | train loss: 0.1040 | test accuracy: 0.859\n",
            "Epoch:  16 Iteration:  986 | train loss: 0.1771 | test accuracy: 0.862\n",
            "Epoch:  17 Iteration:  1044 | train loss: 0.1542 | test accuracy: 0.866\n",
            "Epoch:  18 Iteration:  1102 | train loss: 0.1142 | test accuracy: 0.887\n",
            "Epoch:  19 Iteration:  1160 | train loss: 0.1282 | test accuracy: 0.881\n",
            "Epoch:  20 Iteration:  1218 | train loss: 0.2202 | test accuracy: 0.862\n",
            "Epoch:  21 Iteration:  1276 | train loss: 0.1497 | test accuracy: 0.899\n",
            "Epoch:  22 Iteration:  1334 | train loss: 0.1762 | test accuracy: 0.899\n",
            "Epoch:  23 Iteration:  1392 | train loss: 0.1185 | test accuracy: 0.887\n",
            "Epoch:  24 Iteration:  1450 | train loss: 0.1171 | test accuracy: 0.893\n",
            "Epoch:  25 Iteration:  1508 | train loss: 0.1459 | test accuracy: 0.905\n",
            "Epoch:  26 Iteration:  1566 | train loss: 0.1690 | test accuracy: 0.881\n",
            "Epoch:  27 Iteration:  1624 | train loss: 0.2333 | test accuracy: 0.901\n",
            "Epoch:  28 Iteration:  1682 | train loss: 0.1172 | test accuracy: 0.894\n",
            "Epoch:  29 Iteration:  1740 | train loss: 0.1998 | test accuracy: 0.874\n",
            "Epoch:  30 Iteration:  1798 | train loss: 0.2157 | test accuracy: 0.897\n",
            "New learning rate is:  0.00015000000000000001\n",
            "Epoch:  31 Iteration:  1856 | train loss: 0.1090 | test accuracy: 0.911\n",
            "Epoch:  32 Iteration:  1914 | train loss: 0.0987 | test accuracy: 0.913\n",
            "Epoch:  33 Iteration:  1972 | train loss: 0.1687 | test accuracy: 0.909\n",
            "Epoch:  34 Iteration:  2030 | train loss: 0.0976 | test accuracy: 0.910\n",
            "Epoch:  35 Iteration:  2088 | train loss: 0.2547 | test accuracy: 0.919\n",
            "Epoch:  36 Iteration:  2146 | train loss: 0.0865 | test accuracy: 0.907\n",
            "Epoch:  37 Iteration:  2204 | train loss: 0.2018 | test accuracy: 0.911\n",
            "Epoch:  38 Iteration:  2262 | train loss: 0.1641 | test accuracy: 0.909\n",
            "Epoch:  39 Iteration:  2320 | train loss: 0.1692 | test accuracy: 0.907\n",
            "Epoch:  40 Iteration:  2378 | train loss: 0.1444 | test accuracy: 0.907\n",
            "Epoch:  41 Iteration:  2436 | train loss: 0.1066 | test accuracy: 0.913\n",
            "Epoch:  42 Iteration:  2494 | train loss: 0.1396 | test accuracy: 0.912\n",
            "Epoch:  43 Iteration:  2552 | train loss: 0.1086 | test accuracy: 0.907\n",
            "Epoch:  44 Iteration:  2610 | train loss: 0.0610 | test accuracy: 0.905\n",
            "Epoch:  45 Iteration:  2668 | train loss: 0.1828 | test accuracy: 0.908\n",
            "Epoch:  46 Iteration:  2726 | train loss: 0.1139 | test accuracy: 0.901\n",
            "Epoch:  47 Iteration:  2784 | train loss: 0.1381 | test accuracy: 0.912\n",
            "Epoch:  48 Iteration:  2842 | train loss: 0.1330 | test accuracy: 0.899\n",
            "Epoch:  49 Iteration:  2900 | train loss: 0.0450 | test accuracy: 0.898\n",
            "Epoch:  50 Iteration:  2958 | train loss: 0.0695 | test accuracy: 0.902\n",
            "Epoch:  51 Iteration:  3016 | train loss: 0.1000 | test accuracy: 0.903\n",
            "Epoch:  52 Iteration:  3074 | train loss: 0.0913 | test accuracy: 0.891\n",
            "Epoch:  53 Iteration:  3132 | train loss: 0.1542 | test accuracy: 0.904\n",
            "Epoch:  54 Iteration:  3190 | train loss: 0.1310 | test accuracy: 0.905\n",
            "Epoch:  55 Iteration:  3248 | train loss: 0.1520 | test accuracy: 0.900\n",
            "Epoch:  56 Iteration:  3306 | train loss: 0.1033 | test accuracy: 0.903\n",
            "Epoch:  57 Iteration:  3364 | train loss: 0.0538 | test accuracy: 0.904\n",
            "Epoch:  58 Iteration:  3422 | train loss: 0.0529 | test accuracy: 0.899\n",
            "Epoch:  59 Iteration:  3480 | train loss: 0.0537 | test accuracy: 0.898\n",
            "Epoch:  60 Iteration:  3538 | train loss: 0.1064 | test accuracy: 0.903\n",
            "Epoch:  61 Iteration:  3596 | train loss: 0.1019 | test accuracy: 0.903\n",
            "Epoch:  62 Iteration:  3654 | train loss: 0.1206 | test accuracy: 0.902\n",
            "Epoch:  63 Iteration:  3712 | train loss: 0.0441 | test accuracy: 0.905\n",
            "Epoch:  64 Iteration:  3770 | train loss: 0.0560 | test accuracy: 0.908\n",
            "Epoch:  65 Iteration:  3828 | train loss: 0.1636 | test accuracy: 0.902\n",
            "Epoch:  66 Iteration:  3886 | train loss: 0.1404 | test accuracy: 0.899\n",
            "Epoch:  67 Iteration:  3944 | train loss: 0.0651 | test accuracy: 0.903\n",
            "Epoch:  68 Iteration:  4002 | train loss: 0.0422 | test accuracy: 0.906\n",
            "Epoch:  69 Iteration:  4060 | train loss: 0.0785 | test accuracy: 0.904\n",
            "Epoch:  70 Iteration:  4118 | train loss: 0.0342 | test accuracy: 0.902\n",
            "Epoch:  71 Iteration:  4176 | train loss: 0.0793 | test accuracy: 0.903\n",
            "Epoch:  72 Iteration:  4234 | train loss: 0.0515 | test accuracy: 0.899\n",
            "Epoch:  73 Iteration:  4292 | train loss: 0.0915 | test accuracy: 0.906\n",
            "Epoch:  74 Iteration:  4350 | train loss: 0.0791 | test accuracy: 0.902\n",
            "Epoch:  75 Iteration:  4408 | train loss: 0.1642 | test accuracy: 0.902\n",
            "Epoch:  76 Iteration:  4466 | train loss: 0.1310 | test accuracy: 0.898\n",
            "Epoch:  77 Iteration:  4524 | train loss: 0.1454 | test accuracy: 0.904\n",
            "Epoch:  78 Iteration:  4582 | train loss: 0.0810 | test accuracy: 0.900\n",
            "Epoch:  79 Iteration:  4640 | train loss: 0.0475 | test accuracy: 0.897\n",
            "Epoch:  80 Iteration:  4698 | train loss: 0.1483 | test accuracy: 0.893\n",
            "Epoch:  81 Iteration:  4756 | train loss: 0.0454 | test accuracy: 0.904\n",
            "Epoch:  82 Iteration:  4814 | train loss: 0.0431 | test accuracy: 0.895\n",
            "Epoch:  83 Iteration:  4872 | train loss: 0.1573 | test accuracy: 0.898\n",
            "Epoch:  84 Iteration:  4930 | train loss: 0.0288 | test accuracy: 0.899\n",
            "Epoch:  85 Iteration:  4988 | train loss: 0.1473 | test accuracy: 0.901\n",
            "Epoch:  86 Iteration:  5046 | train loss: 0.0976 | test accuracy: 0.895\n",
            "Epoch:  87 Iteration:  5104 | train loss: 0.0585 | test accuracy: 0.901\n",
            "Epoch:  88 Iteration:  5162 | train loss: 0.1223 | test accuracy: 0.904\n",
            "Epoch:  89 Iteration:  5220 | train loss: 0.2054 | test accuracy: 0.899\n",
            "Epoch:  90 Iteration:  5278 | train loss: 0.1717 | test accuracy: 0.886\n",
            "Epoch:  91 Iteration:  5336 | train loss: 0.1576 | test accuracy: 0.895\n",
            "Epoch:  92 Iteration:  5394 | train loss: 0.0417 | test accuracy: 0.898\n",
            "Epoch:  93 Iteration:  5452 | train loss: 0.0902 | test accuracy: 0.906\n",
            "Epoch:  94 Iteration:  5510 | train loss: 0.1398 | test accuracy: 0.900\n",
            "Epoch:  95 Iteration:  5568 | train loss: 0.0443 | test accuracy: 0.906\n",
            "Epoch:  96 Iteration:  5626 | train loss: 0.0647 | test accuracy: 0.901\n",
            "Epoch:  97 Iteration:  5684 | train loss: 0.0566 | test accuracy: 0.897\n",
            "Epoch:  98 Iteration:  5742 | train loss: 0.1336 | test accuracy: 0.899\n",
            "Epoch:  99 Iteration:  5800 | train loss: 0.0545 | test accuracy: 0.895\n",
            "total time:  306.84396520299924\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  58 | train loss: 0.8934 | test accuracy: 0.560\n",
            "Epoch:  1 Iteration:  116 | train loss: 0.9500 | test accuracy: 0.499\n",
            "Epoch:  2 Iteration:  174 | train loss: 0.6491 | test accuracy: 0.642\n",
            "Epoch:  3 Iteration:  232 | train loss: 0.4820 | test accuracy: 0.748\n",
            "Epoch:  4 Iteration:  290 | train loss: 0.2821 | test accuracy: 0.756\n",
            "Epoch:  5 Iteration:  348 | train loss: 0.3339 | test accuracy: 0.781\n",
            "Epoch:  6 Iteration:  406 | train loss: 0.3692 | test accuracy: 0.767\n",
            "Epoch:  7 Iteration:  464 | train loss: 0.4741 | test accuracy: 0.779\n",
            "Epoch:  8 Iteration:  522 | train loss: 0.2537 | test accuracy: 0.815\n",
            "Epoch:  9 Iteration:  580 | train loss: 0.3628 | test accuracy: 0.807\n",
            "Epoch:  10 Iteration:  638 | train loss: 0.2615 | test accuracy: 0.804\n",
            "Epoch:  11 Iteration:  696 | train loss: 0.1732 | test accuracy: 0.817\n",
            "Epoch:  12 Iteration:  754 | train loss: 0.1899 | test accuracy: 0.852\n",
            "Epoch:  13 Iteration:  812 | train loss: 0.1709 | test accuracy: 0.833\n",
            "Epoch:  14 Iteration:  870 | train loss: 0.4258 | test accuracy: 0.866\n",
            "Epoch:  15 Iteration:  928 | train loss: 0.1521 | test accuracy: 0.845\n",
            "Epoch:  16 Iteration:  986 | train loss: 0.1946 | test accuracy: 0.885\n",
            "Epoch:  17 Iteration:  1044 | train loss: 0.2215 | test accuracy: 0.852\n",
            "Epoch:  18 Iteration:  1102 | train loss: 0.1894 | test accuracy: 0.846\n",
            "Epoch:  19 Iteration:  1160 | train loss: 0.2509 | test accuracy: 0.851\n",
            "Epoch:  20 Iteration:  1218 | train loss: 0.0809 | test accuracy: 0.881\n",
            "Epoch:  21 Iteration:  1276 | train loss: 0.1401 | test accuracy: 0.862\n",
            "Epoch:  22 Iteration:  1334 | train loss: 0.1558 | test accuracy: 0.861\n",
            "Epoch:  23 Iteration:  1392 | train loss: 0.2538 | test accuracy: 0.885\n",
            "Epoch:  24 Iteration:  1450 | train loss: 0.2051 | test accuracy: 0.871\n",
            "Epoch:  25 Iteration:  1508 | train loss: 0.2796 | test accuracy: 0.885\n",
            "Epoch:  26 Iteration:  1566 | train loss: 0.0891 | test accuracy: 0.888\n",
            "Epoch:  27 Iteration:  1624 | train loss: 0.1916 | test accuracy: 0.875\n",
            "Epoch:  28 Iteration:  1682 | train loss: 0.1402 | test accuracy: 0.872\n",
            "Epoch:  29 Iteration:  1740 | train loss: 0.1207 | test accuracy: 0.880\n",
            "Epoch:  30 Iteration:  1798 | train loss: 0.1881 | test accuracy: 0.894\n",
            "New learning rate is:  0.00015000000000000001\n",
            "Epoch:  31 Iteration:  1856 | train loss: 0.0752 | test accuracy: 0.898\n",
            "Epoch:  32 Iteration:  1914 | train loss: 0.1831 | test accuracy: 0.902\n",
            "Epoch:  33 Iteration:  1972 | train loss: 0.1073 | test accuracy: 0.899\n",
            "Epoch:  34 Iteration:  2030 | train loss: 0.1368 | test accuracy: 0.902\n",
            "Epoch:  35 Iteration:  2088 | train loss: 0.0303 | test accuracy: 0.904\n",
            "Epoch:  36 Iteration:  2146 | train loss: 0.0873 | test accuracy: 0.904\n",
            "Epoch:  37 Iteration:  2204 | train loss: 0.1539 | test accuracy: 0.897\n",
            "Epoch:  38 Iteration:  2262 | train loss: 0.0464 | test accuracy: 0.897\n",
            "Epoch:  39 Iteration:  2320 | train loss: 0.2186 | test accuracy: 0.901\n",
            "Epoch:  40 Iteration:  2378 | train loss: 0.1834 | test accuracy: 0.897\n",
            "Epoch:  41 Iteration:  2436 | train loss: 0.1637 | test accuracy: 0.899\n",
            "Epoch:  42 Iteration:  2494 | train loss: 0.1335 | test accuracy: 0.899\n",
            "Epoch:  43 Iteration:  2552 | train loss: 0.1304 | test accuracy: 0.901\n",
            "Epoch:  44 Iteration:  2610 | train loss: 0.0312 | test accuracy: 0.901\n",
            "Epoch:  45 Iteration:  2668 | train loss: 0.1492 | test accuracy: 0.903\n",
            "Epoch:  46 Iteration:  2726 | train loss: 0.0970 | test accuracy: 0.897\n",
            "Epoch:  47 Iteration:  2784 | train loss: 0.1405 | test accuracy: 0.901\n",
            "Epoch:  48 Iteration:  2842 | train loss: 0.1395 | test accuracy: 0.896\n",
            "Epoch:  49 Iteration:  2900 | train loss: 0.0878 | test accuracy: 0.898\n",
            "Epoch:  50 Iteration:  2958 | train loss: 0.1279 | test accuracy: 0.898\n",
            "Epoch:  51 Iteration:  3016 | train loss: 0.1057 | test accuracy: 0.901\n",
            "Epoch:  52 Iteration:  3074 | train loss: 0.0777 | test accuracy: 0.898\n",
            "Epoch:  53 Iteration:  3132 | train loss: 0.1202 | test accuracy: 0.900\n",
            "Epoch:  54 Iteration:  3190 | train loss: 0.0625 | test accuracy: 0.904\n",
            "Epoch:  55 Iteration:  3248 | train loss: 0.1501 | test accuracy: 0.898\n",
            "Epoch:  56 Iteration:  3306 | train loss: 0.1041 | test accuracy: 0.901\n",
            "Epoch:  57 Iteration:  3364 | train loss: 0.0673 | test accuracy: 0.899\n",
            "Epoch:  58 Iteration:  3422 | train loss: 0.0828 | test accuracy: 0.904\n",
            "Epoch:  59 Iteration:  3480 | train loss: 0.1065 | test accuracy: 0.903\n",
            "Epoch:  60 Iteration:  3538 | train loss: 0.0971 | test accuracy: 0.901\n",
            "Epoch:  61 Iteration:  3596 | train loss: 0.1101 | test accuracy: 0.900\n",
            "Epoch:  62 Iteration:  3654 | train loss: 0.1406 | test accuracy: 0.901\n",
            "Epoch:  63 Iteration:  3712 | train loss: 0.3267 | test accuracy: 0.894\n",
            "Epoch:  64 Iteration:  3770 | train loss: 0.1459 | test accuracy: 0.890\n",
            "Epoch:  65 Iteration:  3828 | train loss: 0.1318 | test accuracy: 0.898\n",
            "Epoch:  66 Iteration:  3886 | train loss: 0.1511 | test accuracy: 0.893\n",
            "Epoch:  67 Iteration:  3944 | train loss: 0.0612 | test accuracy: 0.894\n",
            "Epoch:  68 Iteration:  4002 | train loss: 0.1192 | test accuracy: 0.898\n",
            "Epoch:  69 Iteration:  4060 | train loss: 0.0561 | test accuracy: 0.898\n",
            "Epoch:  70 Iteration:  4118 | train loss: 0.3082 | test accuracy: 0.902\n",
            "Epoch:  71 Iteration:  4176 | train loss: 0.0507 | test accuracy: 0.903\n",
            "Epoch:  72 Iteration:  4234 | train loss: 0.0618 | test accuracy: 0.902\n",
            "Epoch:  73 Iteration:  4292 | train loss: 0.0940 | test accuracy: 0.898\n",
            "Epoch:  74 Iteration:  4350 | train loss: 0.0735 | test accuracy: 0.899\n",
            "Epoch:  75 Iteration:  4408 | train loss: 0.0837 | test accuracy: 0.905\n",
            "Epoch:  76 Iteration:  4466 | train loss: 0.0772 | test accuracy: 0.896\n",
            "Epoch:  77 Iteration:  4524 | train loss: 0.0576 | test accuracy: 0.890\n",
            "Epoch:  78 Iteration:  4582 | train loss: 0.1982 | test accuracy: 0.901\n",
            "Epoch:  79 Iteration:  4640 | train loss: 0.1261 | test accuracy: 0.899\n",
            "Epoch:  80 Iteration:  4698 | train loss: 0.0742 | test accuracy: 0.906\n",
            "Epoch:  81 Iteration:  4756 | train loss: 0.1225 | test accuracy: 0.895\n",
            "Epoch:  82 Iteration:  4814 | train loss: 0.0788 | test accuracy: 0.906\n",
            "Epoch:  83 Iteration:  4872 | train loss: 0.0572 | test accuracy: 0.901\n",
            "Epoch:  84 Iteration:  4930 | train loss: 0.1033 | test accuracy: 0.902\n",
            "Epoch:  85 Iteration:  4988 | train loss: 0.0645 | test accuracy: 0.890\n",
            "Epoch:  86 Iteration:  5046 | train loss: 0.1418 | test accuracy: 0.905\n",
            "Epoch:  87 Iteration:  5104 | train loss: 0.1341 | test accuracy: 0.904\n",
            "Epoch:  88 Iteration:  5162 | train loss: 0.1512 | test accuracy: 0.912\n",
            "Epoch:  89 Iteration:  5220 | train loss: 0.0864 | test accuracy: 0.912\n",
            "Epoch:  90 Iteration:  5278 | train loss: 0.1174 | test accuracy: 0.898\n",
            "Epoch:  91 Iteration:  5336 | train loss: 0.0818 | test accuracy: 0.908\n",
            "Epoch:  92 Iteration:  5394 | train loss: 0.0567 | test accuracy: 0.905\n",
            "Epoch:  93 Iteration:  5452 | train loss: 0.0385 | test accuracy: 0.902\n",
            "Epoch:  94 Iteration:  5510 | train loss: 0.0925 | test accuracy: 0.901\n",
            "Epoch:  95 Iteration:  5568 | train loss: 0.1277 | test accuracy: 0.903\n",
            "Epoch:  96 Iteration:  5626 | train loss: 0.0885 | test accuracy: 0.905\n",
            "Epoch:  97 Iteration:  5684 | train loss: 0.0837 | test accuracy: 0.894\n",
            "Epoch:  98 Iteration:  5742 | train loss: 0.1636 | test accuracy: 0.897\n",
            "Epoch:  99 Iteration:  5800 | train loss: 0.1674 | test accuracy: 0.919\n",
            "total time:  304.18901460599955\n",
            "[0.9212758737699356, 0.9189005768578216, 0.9189005768578216]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': [<matplotlib.lines.Line2D at 0x7f587a85a890>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7f587a82c950>,\n",
              "  <matplotlib.lines.Line2D at 0x7f587a82ce90>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7f587a833990>],\n",
              " 'means': [],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7f587a833450>],\n",
              " 'whiskers': [<matplotlib.lines.Line2D at 0x7f587a85ae90>,\n",
              "  <matplotlib.lines.Line2D at 0x7f587a82c410>]}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment with flipped labels (baseline model)"
      ],
      "metadata": {
        "id": "LyNBmgRYW8pX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_acc_2 = []\n",
        "for seed in seeds:\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  y_train_flipped = flip_HAR_labels(y_train, noise_matrix).reshape(-1, 1)\n",
        "  y_test_flipped = flip_HAR_labels(y_test, noise_matrix).reshape(-1, 1)\n",
        "\n",
        "  train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "  train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=128, shuffle=True)\n",
        "\n",
        "  test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test_flipped))\n",
        "  test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=1000, shuffle=True)\n",
        "\n",
        "  max_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "  final_acc_2.append(max_acc)\n",
        "print(final_acc_2)\n",
        "plt.boxplot(final_acc_2)"
      ],
      "metadata": {
        "id": "tYqIjBk8Zl1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c3d350f-af4c-4203-9469-521cd5b3db66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  58 | train loss: 1.8315 | test accuracy: 0.165\n",
            "Epoch:  1 Iteration:  116 | train loss: 1.8175 | test accuracy: 0.181\n",
            "Epoch:  2 Iteration:  174 | train loss: 1.8309 | test accuracy: 0.172\n",
            "Epoch:  3 Iteration:  232 | train loss: 1.7702 | test accuracy: 0.199\n",
            "Epoch:  4 Iteration:  290 | train loss: 1.8257 | test accuracy: 0.199\n",
            "Epoch:  5 Iteration:  348 | train loss: 1.8567 | test accuracy: 0.194\n",
            "Epoch:  6 Iteration:  406 | train loss: 1.7743 | test accuracy: 0.185\n",
            "Epoch:  7 Iteration:  464 | train loss: 1.8001 | test accuracy: 0.189\n",
            "Epoch:  8 Iteration:  522 | train loss: 1.7443 | test accuracy: 0.193\n",
            "Epoch:  9 Iteration:  580 | train loss: 1.7148 | test accuracy: 0.197\n",
            "Epoch:  10 Iteration:  638 | train loss: 1.7643 | test accuracy: 0.190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_acc_3 = []\n",
        "for seed in seeds:\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  y_train_flipped = flip_HAR_labels_basic(y_train, flip_probability = 0.1).reshape(-1, 1)\n",
        "  y_test_flipped = flip_HAR_labels_basic(y_test, flip_probability = 0.1).reshape(-1, 1)\n",
        "\n",
        "  train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "  train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=128, shuffle=True)\n",
        "\n",
        "  test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test_flipped))\n",
        "  test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=1000, shuffle=True)\n",
        "\n",
        "  max_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "  final_acc_3.append(max_acc[0])\n",
        "print(final_acc_3)\n",
        "plt.boxplot(final_acc_3)"
      ],
      "metadata": {
        "id": "wXwkWtGyu6Na"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}