{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "noisy-times-series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy\n",
        "\n",
        "from subprocess import call"
      ],
      "metadata": {
        "id": "r28CXXZps-g1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "\n",
        "print(\"Downloading...\")\n",
        "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "    call(\n",
        "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Downloading done.\\n\")\n",
        "else:\n",
        "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "\n",
        "print(\"Extracting...\")\n",
        "extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
        "if not os.path.exists(extract_directory):\n",
        "    call(\n",
        "        'unzip -nq \"UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
        "else:\n",
        "    print(\"Dataset already extracted. Did not extract twice.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYa-ggaVkzkI",
        "outputId": "1ba36281-c752-4308-a074-ceca438177e7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading...\n",
            "Dataset already downloaded. Did not download twice.\n",
            "\n",
            "Extracting...\n",
            "Dataset already extracted. Did not extract twice.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "\n",
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))"
      ],
      "metadata": {
        "id": "FwLgDKQytkRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6d34120-09b7-478f-913c-857137b60d87"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "n_epochs_hold = 100\n",
        "n_epochs_decay = batch_size - n_epochs_hold\n",
        "epochs = 120\n",
        "\n",
        "def extract_batch_size(_train, step, batch_size):\n",
        "    shape = list(_train.shape)\n",
        "    shape[0] = batch_size\n",
        "    batch = np.empty(shape)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        index = ((step - 1) * batch_size + i) % len(_train)\n",
        "        batch[i] = _train[index]\n",
        "\n",
        "    return batch\n",
        "\n",
        "\n",
        "def getLRScheduler(optimizer):\n",
        "    def lambdaRule(epoch):\n",
        "        lr_l = 1.0 - max(0, epoch - n_epochs_hold) / float(n_epochs_decay + 1)\n",
        "        return lr_l\n",
        "\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambdaRule)\n",
        "    #schedular = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "    return scheduler\n",
        "\n",
        "\n",
        "def plot(x_arg, param_train, param_test, label, lr):\n",
        "    plt.figure()\n",
        "    plt.plot(x_arg, param_train, color='blue', label='train')\n",
        "    plt.plot(x_arg, param_test, color='red', label='test')\n",
        "    plt.legend()\n",
        "    if (label == 'accuracy'):\n",
        "        plt.xlabel('Epoch', fontsize=14)\n",
        "        plt.ylabel('Accuracy (%)', fontsize=14)\n",
        "        plt.title('Training and Test Accuracy', fontsize=20)\n",
        "        plt.savefig('Accuracy_' + str(epochs) + str(lr) + '.png')\n",
        "        plt.show()\n",
        "    elif (label == 'loss'):\n",
        "        plt.xlabel('Epoch', fontsize=14)\n",
        "        plt.ylabel('Loss', fontsize=14)\n",
        "        plt.title('Training and Test Loss', fontsize=20)\n",
        "        plt.savefig('Loss_' + str(epochs) + str(lr) + '.png')\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.xlabel('Learning rate', fontsize=14)\n",
        "        plt.ylabel('Loss', fontsize=14)\n",
        "        plt.title('Training loss and Test loss with learning rate', fontsize=20)\n",
        "        plt.savefig('Loss_lr_' + str(epochs) + str(lr) + '.png')\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def evaluate(net, X_test, y_test, criterion):\n",
        "    test_batch = len(X_test)\n",
        "    net.eval()\n",
        "    test_h = net.init_hidden(test_batch)\n",
        "    inputs, targets = torch.from_numpy(X_test), torch.from_numpy(y_test.flatten('F'))\n",
        "    if (torch.cuda.is_available() ):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "    test_h = tuple([each.data for each in test_h])\n",
        "    output = net(inputs.float(), test_h)\n",
        "    test_loss = criterion(output, targets.long())\n",
        "    top_p, top_class = output.topk(1, dim=1)\n",
        "    targets = targets.view(*top_class.shape).long()\n",
        "    equals = top_class == targets\n",
        "\n",
        "    if (torch.cuda.is_available() ):\n",
        "            top_class, targets = top_class.cpu(), targets.cpu()\n",
        "\n",
        "    test_accuracy = torch.mean(equals.type(torch.FloatTensor))\n",
        "\n",
        "\n",
        "    print(\"Final loss is: {}\".format(test_loss.item()))\n",
        "    print(\"Final accuracy is: {}\". format(test_accuracy))"
      ],
      "metadata": {
        "id": "XmQk9QNHwc7b"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "c4RXMRyAtFvC"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "MELlsVkos0ZP"
      },
      "outputs": [],
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test(net, X_test, y_test, criterion, test_batch=64):\n",
        "    \n",
        "    net.eval()\n",
        "    test_losses = []\n",
        "    test_len = len(X_test)\n",
        "    test_h = net.init_hidden(test_batch)\n",
        "    test_accuracy = 0\n",
        "    step = 1\n",
        "\n",
        "    while step*test_batch <= test_len:\n",
        "        batch_xs = extract_batch_size(X_test, step, test_batch)\n",
        "        batch_ys = extract_batch_size(y_test, step, test_batch)\n",
        "\n",
        "        inputs, targets = torch.from_numpy(batch_xs), torch.from_numpy(batch_ys.flatten('F'))\n",
        "        #if (train_on_gpu):\n",
        "        if (torch.cuda.is_available() ):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "        test_h = tuple([each.data for each in test_h])\n",
        "        #print(\"Size of inputs is: {}\".format(X_test.shape))\n",
        "        output = net(inputs.float(), test_h)\n",
        "        test_loss = criterion(output, targets.long())\n",
        "        test_losses.append(test_loss.item())\n",
        "\n",
        "        top_p, top_class = output.topk(1, dim=1)\n",
        "        equals = top_class == targets.view(*top_class.shape).long()\n",
        "        #print(\"\\nDebugging here\")\n",
        "        #print(top_class.shape)\n",
        "        #print(output.shape)\n",
        "        #print(test_batch)\n",
        "        test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        #print(test_accuracy)\n",
        "        step += 1\n",
        "\n",
        "    test_loss_avg = np.mean(test_losses)\n",
        "    test_accuracy_avg = test_accuracy/(step-1)\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    return test_loss_avg, test_accuracy_avg"
      ],
      "metadata": {
        "id": "pK_RFvzqvl65"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, X_train, y_train, X_test, y_test, opt, criterion, epochs=100, clip_val=15):\n",
        "    print(\"\\n\\n********** Running training! ************\\n\\n\")\n",
        "\n",
        "    sched = getLRScheduler(optimizer=opt)\n",
        "    #if (train_on_gpu):\n",
        "    if (torch.cuda.is_available() ):\n",
        "        net.cuda()\n",
        "\n",
        "    train_losses = []\n",
        "    net.train()\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "    epoch_train_losses = []\n",
        "    epoch_train_acc = []\n",
        "    epoch_test_losses = []\n",
        "    epoch_test_acc = []\n",
        "    params = {\n",
        "        'epochs' : [],\n",
        "        'train_loss' : [],\n",
        "        'test_loss' : [],\n",
        "        'lr' : [],\n",
        "        'train_accuracy' : [],\n",
        "        'test_accuracy' : []\n",
        "    }\n",
        "    for epoch in range(epochs):\n",
        "        train_losses = []\n",
        "        step = 1\n",
        "\n",
        "        h = net.init_hidden(batch_size)\n",
        "\n",
        "        train_accuracy = 0\n",
        "        train_len = len(X_train)\n",
        "\n",
        "        while step * batch_size <= train_len:\n",
        "            batch_xs = extract_batch_size(X_train, step, batch_size)\n",
        "            # batch_ys = one_hot_vector(extract_batch_size(y_train, step, batch_size))\n",
        "            batch_ys = extract_batch_size(y_train, step, batch_size)\n",
        "\n",
        "            inputs, targets = torch.from_numpy(batch_xs), torch.from_numpy(batch_ys.flatten('F'))\n",
        "            #if (train_on_gpu):\n",
        "            if (torch.cuda.is_available() ):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            h = tuple([each.data for each in h])\n",
        "            opt.zero_grad()\n",
        "\n",
        "            output = net(inputs.float(), h)\n",
        "            # print(\"lenght of inputs is {} and target value is {}\".format(inputs.size(), targets.size()))\n",
        "            train_loss = criterion(output, targets.long())\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            top_p, top_class = output.topk(1, dim=1)\n",
        "            equals = top_class == targets.view(*top_class.shape).long()\n",
        "            train_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "            equals = top_class\n",
        "\n",
        "            train_loss.backward()\n",
        "            clip_grad.clip_grad_norm_(net.parameters(), clip_val)\n",
        "            opt.step()\n",
        "            step += 1\n",
        "\n",
        "        p = opt.param_groups[0]['lr']\n",
        "        params['lr'].append(p)\n",
        "        params['epochs'].append(epoch)\n",
        "        sched.step()\n",
        "        train_loss_avg = np.mean(train_losses)\n",
        "        train_accuracy_avg = train_accuracy/(step-1)\n",
        "        epoch_train_losses.append(train_loss_avg)\n",
        "        epoch_train_acc.append(train_accuracy_avg)\n",
        "        print(\"Epoch: {}/{}...\".format(epoch + 1, epochs),\n",
        "              ' ' * 16 + \"Train Loss: {:.4f}\".format(train_loss_avg),\n",
        "              \"Train accuracy: {:.4f}...\".format(train_accuracy_avg))\n",
        "        test_loss, test_accuracy, best_accuracy= test(net, X_test, y_test, criterion, test_batch=len(X_test))\n",
        "        epoch_test_losses.append(test_loss)\n",
        "        epoch_test_acc.append(test_accuracy)\n",
        "        if ((epoch+1) % 10 == 0):\n",
        "            print(\"Epoch: {}/{}...\".format(epoch + 1, epochs),\n",
        "                  ' ' * 16 + \"Test Loss: {:.4f}...\".format(test_loss),\n",
        "                  \"Test accuracy: {:.4f}...\".format(test_accuracy))\n",
        "\n",
        "    print('!!! Best accuracy is : {} !!!'.format(best_accuracy))\n",
        "    params['train_loss'] = epoch_train_losses\n",
        "    params['test_loss'] = epoch_test_losses\n",
        "    params['train_accuracy'] = epoch_train_acc\n",
        "    params['test_accuracy'] = epoch_test_acc\n",
        "    return params"
      ],
      "metadata": {
        "id": "6GUJ4Iznv_C4"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = [0.0015]\n",
        "clip_val = 20\n",
        "\n",
        "NoisyRNN = {\n",
        "\t'name' : 'NoisyRNN',\n",
        "\t'bidir' : False,\n",
        "\t'clip_val' : 10,\n",
        "\t'drop_prob' : 0.5,\n",
        "\t'n_epochs_hold' : 100,\n",
        "\t'n_layers' : 2,\n",
        "\t'learning_rate' : [0.01],\n",
        "\t'weight_decay' : 0.001,\n",
        "\t'n_residual_layers' : 0,\n",
        "\t'n_highway_layers' : 0,\n",
        "\t'diag' : 'Architecure chosen is baseline LSTM with 1 layer',\n",
        "\t'save_file' : 'results_lstm1.txt'\n",
        "}\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    torch.nn.init.orthogonal_(m.weight)\n",
        "    m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def main():\n",
        "    net = NoisyRNN()\n",
        "\n",
        "    for lr in learning_rate:\n",
        "        net.apply(init_weights)\n",
        "        opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        net = net.float()\n",
        "        params = train(net, X_train, y_train, X_test, y_test, opt=opt, criterion=criterion, epochs=epochs, clip_val=clip_val)\n",
        "        evaluate(params['best_model'], X_test, y_test, criterion)\n",
        "        plot(params['epochs'], params['train_loss'], params['test_loss'], 'loss', lr)\n",
        "        plot(params['epochs'], params['train_accuracy'], params['test_accuracy'], 'accuracy', lr)\n",
        "\n",
        "        #plot(params['lr'], params['train_loss'], params['test_loss'], 'loss_lr', lr)\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "pLMST8sWtb0p",
        "outputId": "afafe988-ede7-419d-f38a-e2e313888aff"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-6227098858d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-6227098858d3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNoisyRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'input_dim' and 'output_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7EmSIf4vSwB",
        "outputId": "05f1a9e3-66f8-47c8-ec45-3e022498e86f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-35617f318572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgclip\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}