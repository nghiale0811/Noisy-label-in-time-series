{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "noisy-times-series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy\n",
        "\n",
        "from subprocess import call"
      ],
      "metadata": {
        "id": "r28CXXZps-g1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "\n",
        "print(\"Downloading...\")\n",
        "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "    call(\n",
        "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Downloading done.\\n\")\n",
        "else:\n",
        "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "\n",
        "print(\"Extracting...\")\n",
        "extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
        "if not os.path.exists(extract_directory):\n",
        "    call(\n",
        "        'unzip -nq \"UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
        "else:\n",
        "    print(\"Dataset already extracted. Did not extract twice.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYa-ggaVkzkI",
        "outputId": "e262d9d2-92e5-4634-c921-ae81e08c11ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading...\n",
            "Dataset already downloaded. Did not download twice.\n",
            "\n",
            "Extracting...\n",
            "Dataset already extracted. Did not extract twice.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "\n",
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "print(X_train.shape)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "print(test_data_count)\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=1000, shuffle=True)"
      ],
      "metadata": {
        "id": "FwLgDKQytkRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dccdcabd-dc1d-4f58-913b-3fd52d025878"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7352, 128, 9)\n",
            "2947\n",
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(2947, 128, 9) (2947, 1) 0.09913992 0.39567086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "c4RXMRyAtFvC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MELlsVkos0ZP"
      },
      "outputs": [],
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='mnist', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.0015, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# get dataset\n",
        "#==============================================================================\n",
        "  \n",
        "model = NoisyRNN(input_dim=int(9), output_classes=6, n_units=args.n_units, \n",
        "              eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "              init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "              add_noise=args.add_noise, mult_noise=args.mult_noise).to(device)\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)        \n",
        "noise = torch.randn(1,7352,128,9).float()\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "\n",
        "#==============================================================================\n",
        "# Model summary\n",
        "#==============================================================================\n",
        "print(model)    \n",
        "print('**** Setup ****')\n",
        "print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "print('************')    \n",
        "   \n",
        "\n",
        "if args.optimizer == 'SGD':\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "elif  args.optimizer == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "else:\n",
        "    print(\"Unexpected optimizer!\")\n",
        "    raise \n",
        "\n",
        "\n",
        "loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# training and testing\n",
        "count = 0\n",
        "loss_hist = []\n",
        "test_acc = []\n",
        "\n",
        "t0 = timeit.default_timer()\n",
        "for epoch in range(args.epochs):\n",
        "    model.train()\n",
        "    lossaccum = 0\n",
        "    \n",
        "    for step, (x, y) in enumerate(train_loader):\n",
        "        count += 1\n",
        "        \n",
        "        # Reshape data for recurrent unit\n",
        "        inputs = Variable(x.view(-1, 128, int(9))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "        targets = Variable(y).to(device)   \n",
        "\n",
        "                 \n",
        "        # send data to recurrent unit    \n",
        "        output = model(inputs, mode='train')   \n",
        "        loss = loss_func(output, targets.squeeze(1).long())\n",
        "        \n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()          \n",
        "        \n",
        "        if args.gclip != 0.0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "            \n",
        "        optimizer.step() # update weights\n",
        "        lossaccum += loss.item()\n",
        "\n",
        "        if args.model == 'test':\n",
        "            D = model.W.weight.data.cpu().numpy()  \n",
        "            u, s, v = np.linalg.svd(D, 0)\n",
        "            model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "    loss_hist.append(lossaccum)    \n",
        "     \n",
        "    if epoch % 1 == 0:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total_num = 0\n",
        "        for data, target in test_loader: \n",
        "            data, target = data.to(device), target.to(device)                \n",
        "            output = model(data.view(-1, 128, int(9)))                    \n",
        "            \n",
        "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
        "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "            total_num += len(data)\n",
        "        \n",
        "        accuracy = correct / total_num\n",
        "        test_acc.append(accuracy)\n",
        "        print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "#        if args.model == 'NoisyRNN':\n",
        "#            B = model.B.data.cpu().numpy()            \n",
        "#            A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "#            A = 0.5 * (A + A.T)\n",
        "#            e, _ = np.linalg.eig(A)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "#            \n",
        "#            C = model.C.data.cpu().numpy()            \n",
        "#            W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "#            e, _ = np.linalg.eig(W)\n",
        "#            print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "            \n",
        "             \n",
        "\n",
        "    # schedule learning rate decay    \n",
        "    optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "\n",
        "torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "            + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "            + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "            + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "            + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "            + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "pickle.dump(data,f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "pK_RFvzqvl65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5846f6b2-996b-4c93-b104-9192c085c56d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  58 | train loss: 0.9853 | test accuracy: 0.512\n",
            "Epoch:  1 Iteration:  116 | train loss: 0.8614 | test accuracy: 0.572\n",
            "Epoch:  2 Iteration:  174 | train loss: 0.6753 | test accuracy: 0.572\n",
            "Epoch:  3 Iteration:  232 | train loss: 0.4102 | test accuracy: 0.729\n",
            "Epoch:  4 Iteration:  290 | train loss: 0.6330 | test accuracy: 0.657\n",
            "Epoch:  5 Iteration:  348 | train loss: 0.4301 | test accuracy: 0.784\n",
            "Epoch:  6 Iteration:  406 | train loss: 0.1945 | test accuracy: 0.798\n",
            "Epoch:  7 Iteration:  464 | train loss: 0.2902 | test accuracy: 0.775\n",
            "Epoch:  8 Iteration:  522 | train loss: 0.3314 | test accuracy: 0.821\n",
            "Epoch:  9 Iteration:  580 | train loss: 0.3822 | test accuracy: 0.835\n",
            "Epoch:  10 Iteration:  638 | train loss: 0.2640 | test accuracy: 0.818\n",
            "Epoch:  11 Iteration:  696 | train loss: 0.2887 | test accuracy: 0.838\n",
            "Epoch:  12 Iteration:  754 | train loss: 0.2454 | test accuracy: 0.867\n",
            "Epoch:  13 Iteration:  812 | train loss: 0.3547 | test accuracy: 0.861\n",
            "Epoch:  14 Iteration:  870 | train loss: 0.1047 | test accuracy: 0.864\n",
            "Epoch:  15 Iteration:  928 | train loss: 0.2399 | test accuracy: 0.816\n",
            "Epoch:  16 Iteration:  986 | train loss: 0.1882 | test accuracy: 0.861\n",
            "Epoch:  17 Iteration:  1044 | train loss: 0.2042 | test accuracy: 0.825\n",
            "Epoch:  18 Iteration:  1102 | train loss: 0.2272 | test accuracy: 0.874\n",
            "Epoch:  19 Iteration:  1160 | train loss: 0.2860 | test accuracy: 0.894\n",
            "Epoch:  20 Iteration:  1218 | train loss: 0.1057 | test accuracy: 0.873\n",
            "Epoch:  21 Iteration:  1276 | train loss: 0.1943 | test accuracy: 0.877\n",
            "Epoch:  22 Iteration:  1334 | train loss: 0.1487 | test accuracy: 0.879\n",
            "Epoch:  23 Iteration:  1392 | train loss: 0.0746 | test accuracy: 0.886\n",
            "Epoch:  24 Iteration:  1450 | train loss: 0.2142 | test accuracy: 0.857\n",
            "Epoch:  25 Iteration:  1508 | train loss: 0.1416 | test accuracy: 0.880\n",
            "Epoch:  26 Iteration:  1566 | train loss: 0.1561 | test accuracy: 0.886\n",
            "Epoch:  27 Iteration:  1624 | train loss: 0.1083 | test accuracy: 0.903\n",
            "Epoch:  28 Iteration:  1682 | train loss: 0.0918 | test accuracy: 0.884\n",
            "Epoch:  29 Iteration:  1740 | train loss: 0.1535 | test accuracy: 0.898\n",
            "Epoch:  30 Iteration:  1798 | train loss: 0.1668 | test accuracy: 0.889\n",
            "New learning rate is:  0.00017\n",
            "Epoch:  31 Iteration:  1856 | train loss: 0.1254 | test accuracy: 0.905\n",
            "Epoch:  32 Iteration:  1914 | train loss: 0.0493 | test accuracy: 0.904\n",
            "Epoch:  33 Iteration:  1972 | train loss: 0.2564 | test accuracy: 0.906\n",
            "Epoch:  34 Iteration:  2030 | train loss: 0.2862 | test accuracy: 0.912\n",
            "Epoch:  35 Iteration:  2088 | train loss: 0.1140 | test accuracy: 0.907\n",
            "Epoch:  36 Iteration:  2146 | train loss: 0.1916 | test accuracy: 0.907\n",
            "Epoch:  37 Iteration:  2204 | train loss: 0.1538 | test accuracy: 0.899\n",
            "Epoch:  38 Iteration:  2262 | train loss: 0.1940 | test accuracy: 0.893\n",
            "Epoch:  39 Iteration:  2320 | train loss: 0.0466 | test accuracy: 0.905\n",
            "Epoch:  40 Iteration:  2378 | train loss: 0.1439 | test accuracy: 0.901\n",
            "Epoch:  41 Iteration:  2436 | train loss: 0.1798 | test accuracy: 0.895\n",
            "Epoch:  42 Iteration:  2494 | train loss: 0.1317 | test accuracy: 0.897\n",
            "Epoch:  43 Iteration:  2552 | train loss: 0.1734 | test accuracy: 0.897\n",
            "Epoch:  44 Iteration:  2610 | train loss: 0.0546 | test accuracy: 0.899\n",
            "Epoch:  45 Iteration:  2668 | train loss: 0.1626 | test accuracy: 0.899\n",
            "Epoch:  46 Iteration:  2726 | train loss: 0.0834 | test accuracy: 0.898\n",
            "Epoch:  47 Iteration:  2784 | train loss: 0.1030 | test accuracy: 0.892\n",
            "Epoch:  48 Iteration:  2842 | train loss: 0.0331 | test accuracy: 0.893\n",
            "Epoch:  49 Iteration:  2900 | train loss: 0.0750 | test accuracy: 0.900\n",
            "Epoch:  50 Iteration:  2958 | train loss: 0.0611 | test accuracy: 0.895\n",
            "Epoch:  51 Iteration:  3016 | train loss: 0.1196 | test accuracy: 0.902\n",
            "Epoch:  52 Iteration:  3074 | train loss: 0.0374 | test accuracy: 0.888\n",
            "Epoch:  53 Iteration:  3132 | train loss: 0.0452 | test accuracy: 0.892\n",
            "Epoch:  54 Iteration:  3190 | train loss: 0.0915 | test accuracy: 0.893\n",
            "Epoch:  55 Iteration:  3248 | train loss: 0.1466 | test accuracy: 0.902\n",
            "Epoch:  56 Iteration:  3306 | train loss: 0.1002 | test accuracy: 0.890\n",
            "Epoch:  57 Iteration:  3364 | train loss: 0.0920 | test accuracy: 0.897\n",
            "Epoch:  58 Iteration:  3422 | train loss: 0.0697 | test accuracy: 0.895\n",
            "Epoch:  59 Iteration:  3480 | train loss: 0.1442 | test accuracy: 0.888\n",
            "Epoch:  60 Iteration:  3538 | train loss: 0.1305 | test accuracy: 0.897\n",
            "Epoch:  61 Iteration:  3596 | train loss: 0.1877 | test accuracy: 0.892\n",
            "Epoch:  62 Iteration:  3654 | train loss: 0.0920 | test accuracy: 0.897\n",
            "Epoch:  63 Iteration:  3712 | train loss: 0.1707 | test accuracy: 0.899\n",
            "Epoch:  64 Iteration:  3770 | train loss: 0.1171 | test accuracy: 0.906\n",
            "Epoch:  65 Iteration:  3828 | train loss: 0.1096 | test accuracy: 0.899\n",
            "Epoch:  66 Iteration:  3886 | train loss: 0.1307 | test accuracy: 0.891\n",
            "Epoch:  67 Iteration:  3944 | train loss: 0.2518 | test accuracy: 0.895\n",
            "Epoch:  68 Iteration:  4002 | train loss: 0.0835 | test accuracy: 0.895\n",
            "Epoch:  69 Iteration:  4060 | train loss: 0.0491 | test accuracy: 0.886\n",
            "Epoch:  70 Iteration:  4118 | train loss: 0.1574 | test accuracy: 0.893\n",
            "Epoch:  71 Iteration:  4176 | train loss: 0.1142 | test accuracy: 0.886\n",
            "Epoch:  72 Iteration:  4234 | train loss: 0.1049 | test accuracy: 0.905\n",
            "Epoch:  73 Iteration:  4292 | train loss: 0.1643 | test accuracy: 0.907\n",
            "Epoch:  74 Iteration:  4350 | train loss: 0.0679 | test accuracy: 0.903\n",
            "Epoch:  75 Iteration:  4408 | train loss: 0.0796 | test accuracy: 0.902\n",
            "Epoch:  76 Iteration:  4466 | train loss: 0.1724 | test accuracy: 0.897\n",
            "Epoch:  77 Iteration:  4524 | train loss: 0.1819 | test accuracy: 0.892\n",
            "Epoch:  78 Iteration:  4582 | train loss: 0.1034 | test accuracy: 0.906\n",
            "Epoch:  79 Iteration:  4640 | train loss: 0.1186 | test accuracy: 0.911\n",
            "Epoch:  80 Iteration:  4698 | train loss: 0.0710 | test accuracy: 0.906\n",
            "Epoch:  81 Iteration:  4756 | train loss: 0.0991 | test accuracy: 0.893\n",
            "Epoch:  82 Iteration:  4814 | train loss: 0.0871 | test accuracy: 0.892\n",
            "Epoch:  83 Iteration:  4872 | train loss: 0.0367 | test accuracy: 0.912\n",
            "Epoch:  84 Iteration:  4930 | train loss: 0.0369 | test accuracy: 0.895\n",
            "Epoch:  85 Iteration:  4988 | train loss: 0.0642 | test accuracy: 0.909\n",
            "Epoch:  86 Iteration:  5046 | train loss: 0.0105 | test accuracy: 0.892\n",
            "Epoch:  87 Iteration:  5104 | train loss: 0.0913 | test accuracy: 0.899\n",
            "Epoch:  88 Iteration:  5162 | train loss: 0.1477 | test accuracy: 0.893\n",
            "Epoch:  89 Iteration:  5220 | train loss: 0.0519 | test accuracy: 0.896\n",
            "Epoch:  90 Iteration:  5278 | train loss: 0.0784 | test accuracy: 0.893\n",
            "Epoch:  91 Iteration:  5336 | train loss: 0.0397 | test accuracy: 0.892\n",
            "Epoch:  92 Iteration:  5394 | train loss: 0.1207 | test accuracy: 0.909\n",
            "Epoch:  93 Iteration:  5452 | train loss: 0.0734 | test accuracy: 0.879\n",
            "Epoch:  94 Iteration:  5510 | train loss: 0.1199 | test accuracy: 0.889\n",
            "Epoch:  95 Iteration:  5568 | train loss: 0.0928 | test accuracy: 0.904\n",
            "Epoch:  96 Iteration:  5626 | train loss: 0.0566 | test accuracy: 0.905\n",
            "Epoch:  97 Iteration:  5684 | train loss: 0.0488 | test accuracy: 0.894\n",
            "Epoch:  98 Iteration:  5742 | train loss: 0.1174 | test accuracy: 0.891\n",
            "Epoch:  99 Iteration:  5800 | train loss: 0.0459 | test accuracy: 0.905\n",
            "total time:  369.41863481899964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "id": "O7EmSIf4vSwB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}