{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "noisy-times-series.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries\n"
      ],
      "metadata": {
        "id": "KzBraGTpl7gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy\n",
        "from subprocess import call\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
        "import datetime as dt\n",
        "import time\n",
        "import random\n",
        "from scipy import signal\n",
        "from sklearn.utils import shuffle\n"
      ],
      "metadata": {
        "id": "r28CXXZps-g1"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "CPwn7kurmByV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "\n",
        "print(\"Downloading...\")\n",
        "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "    call(\n",
        "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Downloading done.\\n\")\n",
        "else:\n",
        "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "\n",
        "print(\"Extracting...\")\n",
        "extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
        "if not os.path.exists(extract_directory):\n",
        "    call(\n",
        "        'unzip -nq \"UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
        "else:\n",
        "    print(\"Dataset already extracted. Did not extract twice.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYa-ggaVkzkI",
        "outputId": "ddf3e397-2000-4de4-e41b-f2682d819a5d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading...\n",
            "Dataset already downloaded. Did not download twice.\n",
            "\n",
            "Extracting...\n",
            "Dataset already extracted. Did not extract twice.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "\n",
        "#Take 700 training datapoints\n",
        "\n",
        "#X_train = signal.resample(X_train, 700)\n",
        "#X_test = signal.resample(X_test, 300)\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train)\n",
        "X_train = X_train[:700]\n",
        "y_train = y_train[:700]\n",
        "#X_test = np.random.permutation(X_test)[:300]\n",
        "\n",
        "#Take 300 testing datapoints\n",
        "X_test, y_test = shuffle(X_test, y_test)\n",
        "X_test = X_test[:300]\n",
        "y_test = y_test[:300]\n",
        "\n",
        "# y_train = signal.resample(y_train, 700)\n",
        "# y_test = signal.resample(y_test, 300)\n",
        "\n",
        "#y_train = np.random.permutation(y_train)[:700]\n",
        "#y_test = np.random.permutation(y_test)[:300]\n",
        "\n",
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "print(X_train.shape)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "print(test_data_count)\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=128, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=100, shuffle=True)"
      ],
      "metadata": {
        "id": "FwLgDKQytkRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73f8096-6787-4b20-94b9-e6f17044e1ec"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(700, 128, 9)\n",
            "300\n",
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(300, 128, 9) (300, 1) 0.10291834 0.3951399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "13RRzP01mLkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ADD NOISE TO HAR\n",
        "\n",
        "noise_pair_45= np.array([[.55,0.45,0.0,0.0,0.0,0.0],\n",
        "                        [0.0,.55,0.45,0.0,0.0,0.0],\n",
        "                        [0.0,0.0,.55,0.45,0.0,0.0],\n",
        "                        [0.0,0.0,0.0,.55,0.45,0.0],\n",
        "                        [0.0,0.0,0.0,0.0,.55,0.45],\n",
        "                        [0.45,0.0,0.0,0.0,0.0,.55]])\n",
        "noise_sym_50 = np.array([[.50,.10,.10,.10,.10,.10],\n",
        "                        [.10,.50,.10,.10,.10,.10],\n",
        "                        [.10,.10,.50,.10,.10,.10],\n",
        "                        [.10,.10,.10,.50,.10,.10],\n",
        "                        [.10,.10,.10,.10,.50,.10],\n",
        "                        [.10,.10,.10,.10,.10,.50]])\n",
        "\n",
        "noise_sym_25 = np.array([[.25,.15,.15,.15,.15,.15],\n",
        "                        [.15,.25,.15,.15,.15,.15],\n",
        "                        [.15,.15,.25,.15,.15,.15],\n",
        "                        [.15,.15,.15,.25,.15,.15],\n",
        "                        [.15,.15,.15,.15,.25,.15],\n",
        "                        [.15,.15,.15,.15,.15,.25]])\n",
        "\n",
        "def flip_HAR_labels(array, noise_matrix):\n",
        "    flipped = []\n",
        "    for elem in array.flatten():\n",
        "      flipped.append(np.random.choice([0,1,2,3,4,5], p=noise_matrix[int(elem)]))\n",
        "    \n",
        "    flipped = np.array(flipped)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped\n",
        "\n",
        "\n",
        "def flip_HAR_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, flip_probability, len(array))\n",
        "    flipped = []\n",
        "    for i in range(len(array)):\n",
        "        if flip_mask[i]==1:\n",
        "            options = [0.0,1.0,2.0,3.0,4.0,5.0]\n",
        "            new_options = [x for x in options if x != array[i]]\n",
        "            flipped.append(np.random.choice(new_options, p=[0.2,0.2,0.2,0.2, 0.2]))\n",
        "        else:\n",
        "            flipped.append(array[i])\n",
        "    \n",
        "    flipped = np.array(flipped, dtype=np.int)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped"
      ],
      "metadata": {
        "id": "cQz7VXgDFLfN"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array = np.array([0.0,1.0,2.0,3.0,4.0,5.0])\n",
        "noise_matrix = noise_sym_25\n",
        "\n",
        "flip_HAR_labels_basic(array, 0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8xzQyNFPpW",
        "outputId": "7de8361a-7131-438d-cd25-6cb38de68b83"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 2, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flip_HAR_labels(array, noise_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkrafFRyGKhG",
        "outputId": "2999a0ef-e407-47f9-da84-4973e78f6a26"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 5, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "flip_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ7gc-5cFT7S",
        "outputId": "fc942e5c-827e-4ce2-e010-5c25dbd78f46"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ],
      "metadata": {
        "id": "c4RXMRyAtFvC"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "PouKNWlImO5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, feature_size, n_state, hidden_size=128, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
        "                 seed=random.seed('2021')):\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_state = n_state\n",
        "        self.seed = seed\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.rnn_type = rnn\n",
        "        self.regres = regres\n",
        "        self.return_all = return_all\n",
        "        \n",
        "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "\n",
        "        self.regressor = nn.Sequential(nn.BatchNorm1d(num_features=self.hidden_size),\n",
        "                                       nn.ReLU(),\n",
        "                                       nn.Dropout(0.3),\n",
        "                                       nn.Linear(self.hidden_size, self.n_state),\n",
        "                                       nn.Softmax(-1))\n",
        "\n",
        "    def forward(self, input, past_state=None, **kwargs):\n",
        "        input = input.to(self.device)\n",
        "        self.rnn.to(self.device)\n",
        "        self.regressor.to(self.device)\n",
        "        if not past_state:\n",
        "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
        "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            all_encodings, encoding = self.rnn(input, past_state)\n",
        "        else:\n",
        "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
        "        \n",
        "        if self.regres:\n",
        "            if not self.return_all:\n",
        "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
        "            else:\n",
        "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
        "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
        "        else:\n",
        "            return encoding.view(encoding.shape[1], -1)\n",
        "        \n",
        "        \n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
        "            \n",
        "def save_ckpt(generator_model, output_dir, data):\n",
        "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
        "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
        "    os.makedirs(check_pt_dir, exist_ok=True)\n",
        "    torch.save(generator_model.state_dict(), fname)\n",
        "    \n",
        "def get_accuracy(model, loader):\n",
        "    \n",
        "    correct, total = 0, 0\n",
        "    for xs, ts in loader:\n",
        "        xs.to(device)\n",
        "        ts.to(device)\n",
        "        zs = model(xs)\n",
        "        \n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        ts = torch.argmax(ts, 1)\n",
        "\n",
        "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
        "        total += int(ts.shape[0])\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "j5KjAjajQA9p"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, n_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    history = dict(train=[], val=[])\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
        "    \n",
        "    best_loss = 10000.0\n",
        "    \n",
        "    n=0\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t0 = time.time()\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "        train_losses=[]\n",
        "        model = model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = (time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input= batch[0].to(device)\n",
        "            b_target =  batch[1].to(device)\n",
        "            iters.append(n)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(b_input)\n",
        "\n",
        "            target = torch.argmax(b_target, 1)\n",
        "\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if n % 10 == 0:\n",
        "                iters_sub.append(n)\n",
        "                \n",
        "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
        "                #print(get_accuracy(model, train_dataloader))\n",
        "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "        training_time = (time.time() - t0)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        val_losses = []\n",
        "        model = model.eval()\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "    \n",
        "        history['train'].append(train_loss)\n",
        "        \n",
        "        print(f'Epoch {epoch}: train loss {train_loss} ')\n",
        "    plt.style.use('seaborn-white')\n",
        "    plt.plot(history['train'])\n",
        "\n",
        "    plt.title('LSTM  Training Curves')\n",
        "    plt.ylabel('CE Loss')\n",
        "    plt.xlabel('Epoch Number')\n",
        "    plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    plt.show()\n",
        "    \n",
        "    return model.eval(), history\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    #INFERENCE ON TEST SET\n",
        "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
        "    \n",
        "    y_test = y_test.numpy()\n",
        "    y_pred = predictions.cpu().numpy()\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    #print(accuracy)\n",
        "    #print(f1)\n",
        "    #print(precision)\n",
        "    return accuracy\n",
        "\n",
        "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
        "  train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train))\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "  x_test = torch.flip(torch.tensor(x_test, dtype=torch.float), [1,2])\n",
        "  y_test = torch.tensor(y_test, dtype=torch.int)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = LSTMClassifier(9, #num features \n",
        "                  6, #num classes,\n",
        "                  seed = seed,\n",
        "                  rnn=\"GRU\" #rnn type    \n",
        "            )\n",
        "\n",
        "  model, history = train_model(model, train_loader, 128, 5e-4)\n",
        "  acc = evaluate_model(model, x_test, y_test)\n",
        "  return acc"
      ],
      "metadata": {
        "id": "Pe1d7EkrQC8s"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "MELlsVkos0ZP"
      },
      "outputs": [],
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=128, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='HAR', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=100, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.0015, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=64, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# initialize model\n",
        "#==============================================================================\n",
        "seeds = [1, 2, 3]\n",
        "\n",
        "\n",
        "def driver(seed, train, test, add_noise = 0, mult_noise = 0, nrnn = False):\n",
        "\n",
        "  final_acc = []\n",
        "  model = NoisyRNN(input_dim=int(9), output_classes=6, n_units=args.n_units, \n",
        "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "                add_noise=add_noise, mult_noise=mult_noise).to(device)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  noise = torch.randn(1,7352,128,9).float()\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # set random seed to reproduce the work\n",
        "  #==============================================================================\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # Model summary\n",
        "  #==============================================================================\n",
        "  print(model)    \n",
        "  print('**** Setup ****')\n",
        "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "  print('************')    \n",
        "    \n",
        "\n",
        "  if args.optimizer == 'SGD':\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "  elif  args.optimizer == 'Adam':\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  else:\n",
        "      print(\"Unexpected optimizer!\")\n",
        "      raise \n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # training and testing\n",
        "  count = 0\n",
        "  loss_hist = []\n",
        "  test_acc = []\n",
        "\n",
        "  t0 = timeit.default_timer()\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      lossaccum = 0\n",
        "      \n",
        "      for step, (x, y) in enumerate(train):\n",
        "          count += 1\n",
        "          \n",
        "          # Reshape data for recurrent unit\n",
        "          inputs = Variable(x.view(-1, 128, int(9))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "          targets = Variable(y).to(device) \n",
        "\n",
        "                  \n",
        "          # send data to recurrent unit    \n",
        "          output = model(inputs, mode='train') \n",
        "          loss = loss_func(output, targets.squeeze(1).long())\n",
        "          \n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()          \n",
        "          \n",
        "          if args.gclip != 0.0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "              \n",
        "          optimizer.step() # update weights\n",
        "          lossaccum += loss.item()\n",
        "\n",
        "          if args.model == 'test':\n",
        "              D = model.W.weight.data.cpu().numpy()  \n",
        "              u, s, v = np.linalg.svd(D, 0)\n",
        "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "      loss_hist.append(lossaccum)    \n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          total_num = 0\n",
        "          for data, target in test: \n",
        "              data, target = data.to(device), target.to(device)                \n",
        "              output = model(data.view(-1, 128, int(9)))\n",
        "              \n",
        "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                   \n",
        "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "              total_num += len(data)\n",
        "          \n",
        "          accuracy = correct / total_num\n",
        "          test_acc.append(accuracy)\n",
        "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "      if nrnn == True:\n",
        "          B = model.B.data.cpu().numpy()            \n",
        "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "          A = 0.5 * (A + A.T)\n",
        "          e, _ = np.linalg.eig(A)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "          \n",
        "          C = model.C.data.cpu().numpy()            \n",
        "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "          e, _ = np.linalg.eig(W)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "              \n",
        "              \n",
        "\n",
        "      # schedule learning rate decay    \n",
        "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "  print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "  final_acc.append(max(test_acc))\n",
        "\n",
        "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "  pickle.dump(data,f)\n",
        "  f.close()\n",
        "  return max(test_acc)"
      ],
      "metadata": {
        "id": "pK_RFvzqvl65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c07fafcc-00a7-4b22-a28a-54e6c30da15a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method = \"No noise\"\n",
        "rnn_arr1 = []\n",
        "nrnn_arr1 = []\n",
        "lstm_arr1 = []\n",
        "seed_arr1 = []\n",
        "method_arr1 = []\n",
        "specify_arr1 = []\n",
        "\n",
        "for seed in seeds:\n",
        "  rnn_acc = driver(seed, train_loader, test_loader)\n",
        "  nrnn_acc = driver(seed, train_loader, test_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "  lstm_acc = lstm_driver(seed, X_test, y_test, X_train, y_train)\n",
        "\n",
        "  rnn_arr1.append(rnn_acc)\n",
        "  nrnn_arr1.append(nrnn_acc)\n",
        "  lstm_arr1.append(lstm_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(None)\n"
      ],
      "metadata": {
        "id": "O7EmSIf4vSwB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2c303f-ac28-4cb6-8673-9fbfd1b2fbf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  6 | train loss: 1.6802 | test accuracy: 0.353\n",
            "Epoch:  1 Iteration:  12 | train loss: 1.3397 | test accuracy: 0.397\n",
            "Epoch:  2 Iteration:  18 | train loss: 1.1950 | test accuracy: 0.537\n",
            "Epoch:  3 Iteration:  24 | train loss: 1.0620 | test accuracy: 0.490\n",
            "Epoch:  4 Iteration:  30 | train loss: 1.2600 | test accuracy: 0.510\n",
            "Epoch:  5 Iteration:  36 | train loss: 1.2186 | test accuracy: 0.537\n",
            "Epoch:  6 Iteration:  42 | train loss: 1.2019 | test accuracy: 0.537\n",
            "Epoch:  7 Iteration:  48 | train loss: 1.0502 | test accuracy: 0.530\n",
            "Epoch:  8 Iteration:  54 | train loss: 1.0350 | test accuracy: 0.497\n",
            "Epoch:  9 Iteration:  60 | train loss: 1.1910 | test accuracy: 0.503\n",
            "Epoch:  10 Iteration:  66 | train loss: 0.8967 | test accuracy: 0.533\n",
            "Epoch:  11 Iteration:  72 | train loss: 0.9464 | test accuracy: 0.533\n",
            "Epoch:  12 Iteration:  78 | train loss: 0.9472 | test accuracy: 0.537\n",
            "Epoch:  13 Iteration:  84 | train loss: 0.9834 | test accuracy: 0.543\n",
            "Epoch:  14 Iteration:  90 | train loss: 0.9902 | test accuracy: 0.547\n",
            "Epoch:  15 Iteration:  96 | train loss: 0.7512 | test accuracy: 0.513\n",
            "Epoch:  16 Iteration:  102 | train loss: 0.9981 | test accuracy: 0.490\n",
            "Epoch:  17 Iteration:  108 | train loss: 1.0009 | test accuracy: 0.520\n",
            "Epoch:  18 Iteration:  114 | train loss: 0.8581 | test accuracy: 0.563\n",
            "Epoch:  19 Iteration:  120 | train loss: 0.7914 | test accuracy: 0.570\n",
            "Epoch:  20 Iteration:  126 | train loss: 0.8981 | test accuracy: 0.593\n",
            "Epoch:  21 Iteration:  132 | train loss: 0.8536 | test accuracy: 0.613\n",
            "Epoch:  22 Iteration:  138 | train loss: 0.8538 | test accuracy: 0.620\n",
            "Epoch:  23 Iteration:  144 | train loss: 0.6102 | test accuracy: 0.653\n",
            "Epoch:  24 Iteration:  150 | train loss: 0.6204 | test accuracy: 0.627\n",
            "Epoch:  25 Iteration:  156 | train loss: 0.6704 | test accuracy: 0.657\n",
            "Epoch:  26 Iteration:  162 | train loss: 0.7138 | test accuracy: 0.647\n",
            "Epoch:  27 Iteration:  168 | train loss: 0.5855 | test accuracy: 0.717\n",
            "Epoch:  28 Iteration:  174 | train loss: 0.5751 | test accuracy: 0.703\n",
            "Epoch:  29 Iteration:  180 | train loss: 0.7648 | test accuracy: 0.687\n",
            "Epoch:  30 Iteration:  186 | train loss: 0.6751 | test accuracy: 0.700\n",
            "New learning rate is:  0.00015000000000000001\n",
            "Epoch:  31 Iteration:  192 | train loss: 0.6777 | test accuracy: 0.677\n",
            "Epoch:  32 Iteration:  198 | train loss: 0.5283 | test accuracy: 0.723\n",
            "Epoch:  33 Iteration:  204 | train loss: 0.3403 | test accuracy: 0.707\n",
            "Epoch:  34 Iteration:  210 | train loss: 0.3473 | test accuracy: 0.697\n",
            "Epoch:  35 Iteration:  216 | train loss: 0.5316 | test accuracy: 0.710\n",
            "Epoch:  36 Iteration:  222 | train loss: 0.4346 | test accuracy: 0.713\n",
            "Epoch:  37 Iteration:  228 | train loss: 0.5981 | test accuracy: 0.713\n",
            "Epoch:  38 Iteration:  234 | train loss: 0.3974 | test accuracy: 0.717\n",
            "Epoch:  39 Iteration:  240 | train loss: 0.3927 | test accuracy: 0.717\n",
            "Epoch:  40 Iteration:  246 | train loss: 0.4764 | test accuracy: 0.723\n",
            "Epoch:  41 Iteration:  252 | train loss: 0.4237 | test accuracy: 0.723\n",
            "Epoch:  42 Iteration:  258 | train loss: 0.4213 | test accuracy: 0.720\n",
            "Epoch:  43 Iteration:  264 | train loss: 0.3953 | test accuracy: 0.720\n",
            "Epoch:  44 Iteration:  270 | train loss: 0.3288 | test accuracy: 0.717\n",
            "Epoch:  45 Iteration:  276 | train loss: 0.4512 | test accuracy: 0.720\n",
            "Epoch:  46 Iteration:  282 | train loss: 0.4061 | test accuracy: 0.727\n",
            "Epoch:  47 Iteration:  288 | train loss: 0.3749 | test accuracy: 0.723\n",
            "Epoch:  48 Iteration:  294 | train loss: 0.3894 | test accuracy: 0.727\n",
            "Epoch:  49 Iteration:  300 | train loss: 0.4834 | test accuracy: 0.727\n",
            "Epoch:  50 Iteration:  306 | train loss: 0.4534 | test accuracy: 0.723\n",
            "Epoch:  51 Iteration:  312 | train loss: 0.4420 | test accuracy: 0.720\n",
            "Epoch:  52 Iteration:  318 | train loss: 0.4296 | test accuracy: 0.723\n",
            "Epoch:  53 Iteration:  324 | train loss: 0.3476 | test accuracy: 0.723\n",
            "Epoch:  54 Iteration:  330 | train loss: 0.3271 | test accuracy: 0.720\n",
            "Epoch:  55 Iteration:  336 | train loss: 0.4527 | test accuracy: 0.717\n",
            "Epoch:  56 Iteration:  342 | train loss: 0.3242 | test accuracy: 0.723\n",
            "Epoch:  57 Iteration:  348 | train loss: 0.2820 | test accuracy: 0.733\n",
            "Epoch:  58 Iteration:  354 | train loss: 0.4160 | test accuracy: 0.730\n",
            "Epoch:  59 Iteration:  360 | train loss: 0.2812 | test accuracy: 0.727\n",
            "Epoch:  60 Iteration:  366 | train loss: 0.4634 | test accuracy: 0.730\n",
            "Epoch:  61 Iteration:  372 | train loss: 0.4024 | test accuracy: 0.733\n",
            "Epoch:  62 Iteration:  378 | train loss: 0.4193 | test accuracy: 0.743\n",
            "Epoch:  63 Iteration:  384 | train loss: 0.3561 | test accuracy: 0.727\n",
            "Epoch:  64 Iteration:  390 | train loss: 0.5408 | test accuracy: 0.743\n",
            "Epoch:  65 Iteration:  396 | train loss: 0.3683 | test accuracy: 0.730\n",
            "Epoch:  66 Iteration:  402 | train loss: 0.3362 | test accuracy: 0.740\n",
            "Epoch:  67 Iteration:  408 | train loss: 0.4072 | test accuracy: 0.723\n",
            "Epoch:  68 Iteration:  414 | train loss: 0.5283 | test accuracy: 0.743\n",
            "Epoch:  69 Iteration:  420 | train loss: 0.4115 | test accuracy: 0.730\n",
            "Epoch:  70 Iteration:  426 | train loss: 0.4894 | test accuracy: 0.747\n",
            "Epoch:  71 Iteration:  432 | train loss: 0.3699 | test accuracy: 0.730\n",
            "Epoch:  72 Iteration:  438 | train loss: 0.3365 | test accuracy: 0.733\n",
            "Epoch:  73 Iteration:  444 | train loss: 0.4083 | test accuracy: 0.747\n",
            "Epoch:  74 Iteration:  450 | train loss: 0.3946 | test accuracy: 0.730\n",
            "Epoch:  75 Iteration:  456 | train loss: 0.3372 | test accuracy: 0.760\n",
            "Epoch:  76 Iteration:  462 | train loss: 0.3532 | test accuracy: 0.740\n",
            "Epoch:  77 Iteration:  468 | train loss: 0.3817 | test accuracy: 0.740\n",
            "Epoch:  78 Iteration:  474 | train loss: 0.3914 | test accuracy: 0.747\n",
            "Epoch:  79 Iteration:  480 | train loss: 0.4156 | test accuracy: 0.757\n",
            "Epoch:  80 Iteration:  486 | train loss: 0.3022 | test accuracy: 0.743\n",
            "Epoch:  81 Iteration:  492 | train loss: 0.3515 | test accuracy: 0.743\n",
            "Epoch:  82 Iteration:  498 | train loss: 0.4821 | test accuracy: 0.750\n",
            "Epoch:  83 Iteration:  504 | train loss: 0.3252 | test accuracy: 0.740\n",
            "Epoch:  84 Iteration:  510 | train loss: 0.3156 | test accuracy: 0.740\n",
            "Epoch:  85 Iteration:  516 | train loss: 0.2178 | test accuracy: 0.747\n",
            "Epoch:  86 Iteration:  522 | train loss: 0.2757 | test accuracy: 0.747\n",
            "Epoch:  87 Iteration:  528 | train loss: 0.3134 | test accuracy: 0.753\n",
            "Epoch:  88 Iteration:  534 | train loss: 0.4267 | test accuracy: 0.757\n",
            "Epoch:  89 Iteration:  540 | train loss: 0.2690 | test accuracy: 0.737\n",
            "Epoch:  90 Iteration:  546 | train loss: 0.2824 | test accuracy: 0.760\n",
            "Epoch:  91 Iteration:  552 | train loss: 0.2738 | test accuracy: 0.757\n",
            "Epoch:  92 Iteration:  558 | train loss: 0.3530 | test accuracy: 0.763\n",
            "Epoch:  93 Iteration:  564 | train loss: 0.3535 | test accuracy: 0.753\n",
            "Epoch:  94 Iteration:  570 | train loss: 0.3195 | test accuracy: 0.753\n",
            "Epoch:  95 Iteration:  576 | train loss: 0.3563 | test accuracy: 0.763\n",
            "Epoch:  96 Iteration:  582 | train loss: 0.3675 | test accuracy: 0.753\n",
            "Epoch:  97 Iteration:  588 | train loss: 0.3115 | test accuracy: 0.770\n",
            "Epoch:  98 Iteration:  594 | train loss: 0.3331 | test accuracy: 0.757\n",
            "Epoch:  99 Iteration:  600 | train loss: 0.2442 | test accuracy: 0.767\n",
            "total time:  58.1304843009998\n",
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=64, bias=True)\n",
            "  (D): Linear(in_features=64, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 9.29k\n",
            "************\n",
            "Epoch:  0 Iteration:  6 | train loss: 2.0020 | test accuracy: 0.377\n",
            "Epoch:  1 Iteration:  12 | train loss: 1.7476 | test accuracy: 0.463\n",
            "Epoch:  2 Iteration:  18 | train loss: 1.3892 | test accuracy: 0.463\n",
            "Epoch:  3 Iteration:  24 | train loss: 1.1095 | test accuracy: 0.483\n",
            "Epoch:  4 Iteration:  30 | train loss: 1.4955 | test accuracy: 0.480\n",
            "Epoch:  5 Iteration:  36 | train loss: 1.5528 | test accuracy: 0.537\n",
            "Epoch:  6 Iteration:  42 | train loss: 1.2258 | test accuracy: 0.513\n",
            "Epoch:  7 Iteration:  48 | train loss: 1.2523 | test accuracy: 0.547\n",
            "Epoch:  8 Iteration:  54 | train loss: 1.4981 | test accuracy: 0.500\n",
            "Epoch:  9 Iteration:  60 | train loss: 1.1843 | test accuracy: 0.513\n",
            "Epoch:  10 Iteration:  66 | train loss: 0.9616 | test accuracy: 0.483\n",
            "Epoch:  11 Iteration:  72 | train loss: 0.8572 | test accuracy: 0.570\n",
            "Epoch:  12 Iteration:  78 | train loss: 1.0396 | test accuracy: 0.523\n",
            "Epoch:  13 Iteration:  84 | train loss: 1.1248 | test accuracy: 0.550\n",
            "Epoch:  14 Iteration:  90 | train loss: 1.2928 | test accuracy: 0.493\n",
            "Epoch:  15 Iteration:  96 | train loss: 1.3532 | test accuracy: 0.487\n",
            "Epoch:  16 Iteration:  102 | train loss: 1.1704 | test accuracy: 0.503\n",
            "Epoch:  17 Iteration:  108 | train loss: 1.1485 | test accuracy: 0.557\n",
            "Epoch:  18 Iteration:  114 | train loss: 1.0501 | test accuracy: 0.510\n",
            "Epoch:  19 Iteration:  120 | train loss: 1.3206 | test accuracy: 0.530\n",
            "Epoch:  20 Iteration:  126 | train loss: 1.0989 | test accuracy: 0.530\n",
            "Epoch:  21 Iteration:  132 | train loss: 1.0080 | test accuracy: 0.540\n",
            "Epoch:  22 Iteration:  138 | train loss: 1.3187 | test accuracy: 0.547\n",
            "Epoch:  23 Iteration:  144 | train loss: 1.0612 | test accuracy: 0.537\n",
            "Epoch:  24 Iteration:  150 | train loss: 1.0796 | test accuracy: 0.577\n",
            "Epoch:  25 Iteration:  156 | train loss: 1.1766 | test accuracy: 0.547\n",
            "Epoch:  26 Iteration:  162 | train loss: 1.1749 | test accuracy: 0.550\n",
            "Epoch:  27 Iteration:  168 | train loss: 0.9950 | test accuracy: 0.553\n",
            "Epoch:  28 Iteration:  174 | train loss: 1.1980 | test accuracy: 0.543\n",
            "Epoch:  29 Iteration:  180 | train loss: 1.1606 | test accuracy: 0.517\n",
            "Epoch:  30 Iteration:  186 | train loss: 0.9442 | test accuracy: 0.347\n",
            "New learning rate is:  0.00015000000000000001\n",
            "Epoch:  31 Iteration:  192 | train loss: 1.0247 | test accuracy: 0.557\n",
            "Epoch:  32 Iteration:  198 | train loss: 0.9738 | test accuracy: 0.553\n",
            "Epoch:  33 Iteration:  204 | train loss: 0.9687 | test accuracy: 0.553\n",
            "Epoch:  34 Iteration:  210 | train loss: 0.9882 | test accuracy: 0.577\n",
            "Epoch:  35 Iteration:  216 | train loss: 1.0039 | test accuracy: 0.567\n",
            "Epoch:  36 Iteration:  222 | train loss: 0.8709 | test accuracy: 0.567\n",
            "Epoch:  37 Iteration:  228 | train loss: 0.9314 | test accuracy: 0.567\n",
            "Epoch:  38 Iteration:  234 | train loss: 0.8585 | test accuracy: 0.563\n",
            "Epoch:  39 Iteration:  240 | train loss: 1.0709 | test accuracy: 0.553\n",
            "Epoch:  40 Iteration:  246 | train loss: 0.7685 | test accuracy: 0.550\n",
            "Epoch:  41 Iteration:  252 | train loss: 1.0514 | test accuracy: 0.563\n",
            "Epoch:  42 Iteration:  258 | train loss: 0.8394 | test accuracy: 0.563\n",
            "Epoch:  43 Iteration:  264 | train loss: 0.9982 | test accuracy: 0.553\n",
            "Epoch:  44 Iteration:  270 | train loss: 0.9864 | test accuracy: 0.560\n",
            "Epoch:  45 Iteration:  276 | train loss: 0.9208 | test accuracy: 0.553\n",
            "Epoch:  46 Iteration:  282 | train loss: 0.9467 | test accuracy: 0.550\n",
            "Epoch:  47 Iteration:  288 | train loss: 0.9873 | test accuracy: 0.563\n",
            "Epoch:  48 Iteration:  294 | train loss: 1.1859 | test accuracy: 0.580\n",
            "Epoch:  49 Iteration:  300 | train loss: 1.0510 | test accuracy: 0.567\n",
            "Epoch:  50 Iteration:  306 | train loss: 0.9936 | test accuracy: 0.547\n",
            "Epoch:  51 Iteration:  312 | train loss: 0.9760 | test accuracy: 0.563\n",
            "Epoch:  52 Iteration:  318 | train loss: 0.8447 | test accuracy: 0.563\n",
            "Epoch:  53 Iteration:  324 | train loss: 1.0999 | test accuracy: 0.583\n",
            "Epoch:  54 Iteration:  330 | train loss: 0.9586 | test accuracy: 0.587\n",
            "Epoch:  55 Iteration:  336 | train loss: 0.9295 | test accuracy: 0.560\n",
            "Epoch:  56 Iteration:  342 | train loss: 0.9565 | test accuracy: 0.563\n",
            "Epoch:  57 Iteration:  348 | train loss: 1.0307 | test accuracy: 0.573\n",
            "Epoch:  58 Iteration:  354 | train loss: 0.9132 | test accuracy: 0.580\n",
            "Epoch:  59 Iteration:  360 | train loss: 0.9328 | test accuracy: 0.563\n",
            "Epoch:  60 Iteration:  366 | train loss: 0.9448 | test accuracy: 0.557\n",
            "Epoch:  61 Iteration:  372 | train loss: 1.0810 | test accuracy: 0.567\n",
            "Epoch:  62 Iteration:  378 | train loss: 0.8314 | test accuracy: 0.580\n",
            "Epoch:  63 Iteration:  384 | train loss: 1.0474 | test accuracy: 0.583\n",
            "Epoch:  64 Iteration:  390 | train loss: 1.0064 | test accuracy: 0.557\n",
            "Epoch:  65 Iteration:  396 | train loss: 0.9516 | test accuracy: 0.570\n",
            "Epoch:  66 Iteration:  402 | train loss: 0.9549 | test accuracy: 0.573\n",
            "Epoch:  67 Iteration:  408 | train loss: 1.0390 | test accuracy: 0.580\n",
            "Epoch:  68 Iteration:  414 | train loss: 0.8832 | test accuracy: 0.590\n",
            "Epoch:  69 Iteration:  420 | train loss: 1.0024 | test accuracy: 0.563\n",
            "Epoch:  70 Iteration:  426 | train loss: 1.1507 | test accuracy: 0.557\n",
            "Epoch:  71 Iteration:  432 | train loss: 0.7586 | test accuracy: 0.610\n",
            "Epoch:  72 Iteration:  438 | train loss: 0.7845 | test accuracy: 0.620\n",
            "Epoch:  73 Iteration:  444 | train loss: 1.0418 | test accuracy: 0.553\n",
            "Epoch:  74 Iteration:  450 | train loss: 1.1356 | test accuracy: 0.570\n",
            "Epoch:  75 Iteration:  456 | train loss: 1.0177 | test accuracy: 0.603\n",
            "Epoch:  76 Iteration:  462 | train loss: 0.8939 | test accuracy: 0.587\n",
            "Epoch:  77 Iteration:  468 | train loss: 0.8192 | test accuracy: 0.563\n",
            "Epoch:  78 Iteration:  474 | train loss: 0.9331 | test accuracy: 0.570\n",
            "Epoch:  79 Iteration:  480 | train loss: 0.9911 | test accuracy: 0.587\n",
            "Epoch:  80 Iteration:  486 | train loss: 0.9918 | test accuracy: 0.587\n",
            "Epoch:  81 Iteration:  492 | train loss: 0.8335 | test accuracy: 0.573\n",
            "Epoch:  82 Iteration:  498 | train loss: 0.9477 | test accuracy: 0.610\n",
            "Epoch:  83 Iteration:  504 | train loss: 1.1709 | test accuracy: 0.613\n",
            "Epoch:  84 Iteration:  510 | train loss: 0.8919 | test accuracy: 0.603\n",
            "Epoch:  85 Iteration:  516 | train loss: 0.9925 | test accuracy: 0.583\n",
            "Epoch:  86 Iteration:  522 | train loss: 0.9320 | test accuracy: 0.583\n",
            "Epoch:  87 Iteration:  528 | train loss: 0.7664 | test accuracy: 0.620\n",
            "Epoch:  88 Iteration:  534 | train loss: 1.1162 | test accuracy: 0.577\n",
            "Epoch:  89 Iteration:  540 | train loss: 1.0716 | test accuracy: 0.610\n",
            "Epoch:  90 Iteration:  546 | train loss: 0.8442 | test accuracy: 0.577\n",
            "Epoch:  91 Iteration:  552 | train loss: 0.7931 | test accuracy: 0.603\n",
            "Epoch:  92 Iteration:  558 | train loss: 0.9717 | test accuracy: 0.597\n",
            "Epoch:  93 Iteration:  564 | train loss: 0.9134 | test accuracy: 0.600\n",
            "Epoch:  94 Iteration:  570 | train loss: 0.9726 | test accuracy: 0.583\n",
            "Epoch:  95 Iteration:  576 | train loss: 0.8624 | test accuracy: 0.617\n",
            "Epoch:  96 Iteration:  582 | train loss: 0.9607 | test accuracy: 0.620\n",
            "Epoch:  97 Iteration:  588 | train loss: 0.9812 | test accuracy: 0.640\n",
            "Epoch:  98 Iteration:  594 | train loss: 0.7089 | test accuracy: 0.617\n",
            "Epoch:  99 Iteration:  600 | train loss: 0.8624 | test accuracy: 0.613\n",
            "total time:  72.71995417900007\n",
            "\n",
            "======== Epoch 1 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.054943561553955.\n",
            "\n",
            "  Average training loss: 1.76\n",
            "  Training epoch took: 3.5976927280426025\n",
            "\n",
            "Running Validation...\n",
            "Epoch 1: train loss 1.7558837073189872 \n",
            "\n",
            "======== Epoch 2 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1509485244750977.\n",
            "\n",
            "  Average training loss: 1.62\n",
            "  Training epoch took: 3.735246419906616\n",
            "\n",
            "Running Validation...\n",
            "Epoch 2: train loss 1.6151999916349138 \n",
            "\n",
            "======== Epoch 3 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2477307319641113.\n",
            "\n",
            "  Average training loss: 1.46\n",
            "  Training epoch took: 3.8942339420318604\n",
            "\n",
            "Running Validation...\n",
            "Epoch 3: train loss 1.4645753605025156 \n",
            "\n",
            "======== Epoch 4 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1342642307281494.\n",
            "\n",
            "  Average training loss: 1.34\n",
            "  Training epoch took: 3.7281084060668945\n",
            "\n",
            "Running Validation...\n",
            "Epoch 4: train loss 1.343975269794464 \n",
            "\n",
            "======== Epoch 5 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1635608673095703.\n",
            "\n",
            "  Average training loss: 1.26\n",
            "  Training epoch took: 3.786658525466919\n",
            "\n",
            "Running Validation...\n",
            "Epoch 5: train loss 1.2557665279933385 \n",
            "\n",
            "======== Epoch 6 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.145160436630249.\n",
            "\n",
            "  Average training loss: 1.20\n",
            "  Training epoch took: 3.812877655029297\n",
            "\n",
            "Running Validation...\n",
            "Epoch 6: train loss 1.1981021744864329 \n",
            "\n",
            "======== Epoch 7 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.21120548248291.\n",
            "\n",
            "  Average training loss: 1.15\n",
            "  Training epoch took: 3.83762788772583\n",
            "\n",
            "Running Validation...\n",
            "Epoch 7: train loss 1.1544615813664028 \n",
            "\n",
            "======== Epoch 8 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1942050457000732.\n",
            "\n",
            "  Average training loss: 1.12\n",
            "  Training epoch took: 3.841066360473633\n",
            "\n",
            "Running Validation...\n",
            "Epoch 8: train loss 1.1235076018742152 \n",
            "\n",
            "======== Epoch 9 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2394185066223145.\n",
            "\n",
            "  Average training loss: 1.11\n",
            "  Training epoch took: 3.9302217960357666\n",
            "\n",
            "Running Validation...\n",
            "Epoch 9: train loss 1.1070193716457912 \n",
            "\n",
            "======== Epoch 10 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2784945964813232.\n",
            "\n",
            "  Average training loss: 1.09\n",
            "  Training epoch took: 3.907811164855957\n",
            "\n",
            "Running Validation...\n",
            "Epoch 10: train loss 1.0921200684138708 \n",
            "\n",
            "======== Epoch 11 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2893869876861572.\n",
            "\n",
            "  Average training loss: 1.08\n",
            "  Training epoch took: 3.901928663253784\n",
            "\n",
            "Running Validation...\n",
            "Epoch 11: train loss 1.0792273725782122 \n",
            "\n",
            "======== Epoch 12 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2904245853424072.\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 3.9966840744018555\n",
            "\n",
            "Running Validation...\n",
            "Epoch 12: train loss 1.0738668714250836 \n",
            "\n",
            "======== Epoch 13 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2501978874206543.\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 3.86952543258667\n",
            "\n",
            "Running Validation...\n",
            "Epoch 13: train loss 1.0682410257203239 \n",
            "\n",
            "======== Epoch 14 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2468833923339844.\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epoch took: 3.919039726257324\n",
            "\n",
            "Running Validation...\n",
            "Epoch 14: train loss 1.0670920252799987 \n",
            "\n",
            "======== Epoch 15 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.3444364070892334.\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epoch took: 4.109461069107056\n",
            "\n",
            "Running Validation...\n",
            "Epoch 15: train loss 1.062580670629229 \n",
            "\n",
            "======== Epoch 16 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.344456672668457.\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epoch took: 4.1112143993377686\n",
            "\n",
            "Running Validation...\n",
            "Epoch 16: train loss 1.0602327585220337 \n",
            "\n",
            "======== Epoch 17 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.4409682750701904.\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epoch took: 4.179989576339722\n",
            "\n",
            "Running Validation...\n",
            "Epoch 17: train loss 1.05798123223441 \n",
            "\n",
            "======== Epoch 18 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.386091470718384.\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epoch took: 4.126049280166626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 18: train loss 1.0564082741737366 \n",
            "\n",
            "======== Epoch 19 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.35097074508667.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.130650758743286\n",
            "\n",
            "Running Validation...\n",
            "Epoch 19: train loss 1.0546846100262233 \n",
            "\n",
            "======== Epoch 20 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.328277587890625.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.087216377258301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 20: train loss 1.0537666167531694 \n",
            "\n",
            "======== Epoch 21 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.361518621444702.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.107402086257935\n",
            "\n",
            "Running Validation...\n",
            "Epoch 21: train loss 1.052288704259055 \n",
            "\n",
            "======== Epoch 22 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.4539530277252197.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.313499927520752\n",
            "\n",
            "Running Validation...\n",
            "Epoch 22: train loss 1.0532881685665676 \n",
            "\n",
            "======== Epoch 23 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.53566837310791.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.278105020523071\n",
            "\n",
            "Running Validation...\n",
            "Epoch 23: train loss 1.0519859092576163 \n",
            "\n",
            "======== Epoch 24 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.3253960609436035.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.026431083679199\n",
            "\n",
            "Running Validation...\n",
            "Epoch 24: train loss 1.0503560866628374 \n",
            "\n",
            "======== Epoch 25 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.294093608856201.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 4.050349235534668\n",
            "\n",
            "Running Validation...\n",
            "Epoch 25: train loss 1.0500883715493339 \n",
            "\n",
            "======== Epoch 26 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.173002243041992.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.7928614616394043\n",
            "\n",
            "Running Validation...\n",
            "Epoch 26: train loss 1.049528271811349 \n",
            "\n",
            "======== Epoch 27 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1834053993225098.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.8080694675445557\n",
            "\n",
            "Running Validation...\n",
            "Epoch 27: train loss 1.048432011263711 \n",
            "\n",
            "======== Epoch 28 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2475028038024902.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.914120674133301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 28: train loss 1.0482677391597204 \n",
            "\n",
            "======== Epoch 29 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2186317443847656.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.8801815509796143\n",
            "\n",
            "Running Validation...\n",
            "Epoch 29: train loss 1.0486165693828038 \n",
            "\n",
            "======== Epoch 30 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.158478021621704.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.7757441997528076\n",
            "\n",
            "Running Validation...\n",
            "Epoch 30: train loss 1.0478677170617239 \n",
            "\n",
            "======== Epoch 31 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0959312915802.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6272354125976562\n",
            "\n",
            "Running Validation...\n",
            "Epoch 31: train loss 1.0474453670637949 \n",
            "\n",
            "======== Epoch 32 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0921881198883057.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.64501690864563\n",
            "\n",
            "Running Validation...\n",
            "Epoch 32: train loss 1.047648595060621 \n",
            "\n",
            "======== Epoch 33 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.151693820953369.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.726516008377075\n",
            "\n",
            "Running Validation...\n",
            "Epoch 33: train loss 1.0472601430756705 \n",
            "\n",
            "======== Epoch 34 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.101447105407715.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.788043260574341\n",
            "\n",
            "Running Validation...\n",
            "Epoch 34: train loss 1.0469789249556405 \n",
            "\n",
            "======== Epoch 35 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2724597454071045.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.957348108291626\n",
            "\n",
            "Running Validation...\n",
            "Epoch 35: train loss 1.0465301445552282 \n",
            "\n",
            "======== Epoch 36 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2196128368377686.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.8661091327667236\n",
            "\n",
            "Running Validation...\n",
            "Epoch 36: train loss 1.0462711487497602 \n",
            "\n",
            "======== Epoch 37 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0636277198791504.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.5973918437957764\n",
            "\n",
            "Running Validation...\n",
            "Epoch 37: train loss 1.0465714045933314 \n",
            "\n",
            "======== Epoch 38 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0981976985931396.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6669015884399414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 38: train loss 1.0462912883077349 \n",
            "\n",
            "======== Epoch 39 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.07611346244812.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6207480430603027\n",
            "\n",
            "Running Validation...\n",
            "Epoch 39: train loss 1.0459206138338362 \n",
            "\n",
            "======== Epoch 40 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.081925392150879.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6434109210968018\n",
            "\n",
            "Running Validation...\n",
            "Epoch 40: train loss 1.0458362988063268 \n",
            "\n",
            "======== Epoch 41 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.086402416229248.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6333372592926025\n",
            "\n",
            "Running Validation...\n",
            "Epoch 41: train loss 1.0456681506974357 \n",
            "\n",
            "======== Epoch 42 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.030005931854248.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6018893718719482\n",
            "\n",
            "Running Validation...\n",
            "Epoch 42: train loss 1.0454996722085135 \n",
            "\n",
            "======== Epoch 43 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1498982906341553.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.7970285415649414\n",
            "\n",
            "Running Validation...\n",
            "Epoch 43: train loss 1.0455894351005555 \n",
            "\n",
            "======== Epoch 44 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2642621994018555.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.9218666553497314\n",
            "\n",
            "Running Validation...\n",
            "Epoch 44: train loss 1.0454130291938781 \n",
            "\n",
            "======== Epoch 45 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2213339805603027.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.847050189971924\n",
            "\n",
            "Running Validation...\n",
            "Epoch 45: train loss 1.0455979892185756 \n",
            "\n",
            "======== Epoch 46 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.135849952697754.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.6714260578155518\n",
            "\n",
            "Running Validation...\n",
            "Epoch 46: train loss 1.0452482734407698 \n",
            "\n",
            "======== Epoch 47 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0518510341644287.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.5975005626678467\n",
            "\n",
            "Running Validation...\n",
            "Epoch 47: train loss 1.0451477970395768 \n",
            "\n",
            "======== Epoch 48 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0726425647735596.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.62270450592041\n",
            "\n",
            "Running Validation...\n",
            "Epoch 48: train loss 1.0452408398900712 \n",
            "\n",
            "======== Epoch 49 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1599061489105225.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.798124313354492\n",
            "\n",
            "Running Validation...\n",
            "Epoch 49: train loss 1.0453270503452845 \n",
            "\n",
            "======== Epoch 50 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2215402126312256.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.9216973781585693\n",
            "\n",
            "Running Validation...\n",
            "Epoch 50: train loss 1.0450665542057582 \n",
            "\n",
            "======== Epoch 51 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.061673879623413.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.685567617416382\n",
            "\n",
            "Running Validation...\n",
            "Epoch 51: train loss 1.0447844862937927 \n",
            "\n",
            "======== Epoch 52 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1081783771514893.\n",
            "\n",
            "  Average training loss: 1.05\n",
            "  Training epoch took: 3.674254894256592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 52: train loss 1.0451430525098528 \n",
            "\n",
            "======== Epoch 53 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0926640033721924.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.6331822872161865\n",
            "\n",
            "Running Validation...\n",
            "Epoch 53: train loss 1.0449144652911595 \n",
            "\n",
            "======== Epoch 54 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1317555904388428.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.736295700073242\n",
            "\n",
            "Running Validation...\n",
            "Epoch 54: train loss 1.0447542616299221 \n",
            "\n",
            "======== Epoch 55 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.125868082046509.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.697463274002075\n",
            "\n",
            "Running Validation...\n",
            "Epoch 55: train loss 1.0448129330362592 \n",
            "\n",
            "======== Epoch 56 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1558773517608643.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.722323179244995\n",
            "\n",
            "Running Validation...\n",
            "Epoch 56: train loss 1.0446246300424848 \n",
            "\n",
            "======== Epoch 57 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.072162389755249.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.613091468811035\n",
            "\n",
            "Running Validation...\n",
            "Epoch 57: train loss 1.0445574079241071 \n",
            "\n",
            "======== Epoch 58 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0179378986358643.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5232253074645996\n",
            "\n",
            "Running Validation...\n",
            "Epoch 58: train loss 1.0446844441550118 \n",
            "\n",
            "======== Epoch 59 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0649948120117188.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.633878469467163\n",
            "\n",
            "Running Validation...\n",
            "Epoch 59: train loss 1.0446965285709926 \n",
            "\n",
            "======== Epoch 60 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0119404792785645.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5119519233703613\n",
            "\n",
            "Running Validation...\n",
            "Epoch 60: train loss 1.0445004446165902 \n",
            "\n",
            "======== Epoch 61 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0509448051452637.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.620225429534912\n",
            "\n",
            "Running Validation...\n",
            "Epoch 61: train loss 1.0444259899003165 \n",
            "\n",
            "======== Epoch 62 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.061321973800659.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.623281717300415\n",
            "\n",
            "Running Validation...\n",
            "Epoch 62: train loss 1.044512244633266 \n",
            "\n",
            "======== Epoch 63 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.096400737762451.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.6174092292785645\n",
            "\n",
            "Running Validation...\n",
            "Epoch 63: train loss 1.0446863753455027 \n",
            "\n",
            "======== Epoch 64 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0098650455474854.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5268478393554688\n",
            "\n",
            "Running Validation...\n",
            "Epoch 64: train loss 1.0444908738136292 \n",
            "\n",
            "======== Epoch 65 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0157346725463867.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.509941577911377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 65: train loss 1.0443991337503706 \n",
            "\n",
            "======== Epoch 66 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.005829095840454.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5363824367523193\n",
            "\n",
            "Running Validation...\n",
            "Epoch 66: train loss 1.0444544400487628 \n",
            "\n",
            "======== Epoch 67 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.104074001312256.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.6982696056365967\n",
            "\n",
            "Running Validation...\n",
            "Epoch 67: train loss 1.0443574156079973 \n",
            "\n",
            "======== Epoch 68 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1404128074645996.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.7006256580352783\n",
            "\n",
            "Running Validation...\n",
            "Epoch 68: train loss 1.0442860347884042 \n",
            "\n",
            "======== Epoch 69 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0535225868225098.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.583678722381592\n",
            "\n",
            "Running Validation...\n",
            "Epoch 69: train loss 1.0443610770361764 \n",
            "\n",
            "======== Epoch 70 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0367956161499023.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.601132392883301\n",
            "\n",
            "Running Validation...\n",
            "Epoch 70: train loss 1.0442394392830985 \n",
            "\n",
            "======== Epoch 71 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0843093395233154.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.599604368209839\n",
            "\n",
            "Running Validation...\n",
            "Epoch 71: train loss 1.0443066188267298 \n",
            "\n",
            "======== Epoch 72 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0558931827545166.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5855414867401123\n",
            "\n",
            "Running Validation...\n",
            "Epoch 72: train loss 1.0443201439721244 \n",
            "\n",
            "======== Epoch 73 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.04276967048645.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.603022813796997\n",
            "\n",
            "Running Validation...\n",
            "Epoch 73: train loss 1.0442191447530473 \n",
            "\n",
            "======== Epoch 74 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.148705005645752.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.693692684173584\n",
            "\n",
            "Running Validation...\n",
            "Epoch 74: train loss 1.0442084193229675 \n",
            "\n",
            "======== Epoch 75 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.033484935760498.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5678598880767822\n",
            "\n",
            "Running Validation...\n",
            "Epoch 75: train loss 1.0442145228385926 \n",
            "\n",
            "======== Epoch 76 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0183193683624268.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5214672088623047\n",
            "\n",
            "Running Validation...\n",
            "Epoch 76: train loss 1.0442460111209324 \n",
            "\n",
            "======== Epoch 77 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0485808849334717.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.5623278617858887\n",
            "\n",
            "Running Validation...\n",
            "Epoch 77: train loss 1.0441929715020315 \n",
            "\n",
            "======== Epoch 78 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0941741466522217.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.6530232429504395\n",
            "\n",
            "Running Validation...\n",
            "Epoch 78: train loss 1.0440502728734697 \n",
            "\n",
            "======== Epoch 79 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1017189025878906.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.65396785736084\n",
            "\n",
            "Running Validation...\n",
            "Epoch 79: train loss 1.0441903250558036 \n",
            "\n",
            "======== Epoch 80 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.0554306507110596.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.622882127761841\n",
            "\n",
            "Running Validation...\n",
            "Epoch 80: train loss 1.0440338850021362 \n",
            "\n",
            "======== Epoch 81 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1173079013824463.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.733734369277954\n",
            "\n",
            "Running Validation...\n",
            "Epoch 81: train loss 1.0441877092633929 \n",
            "\n",
            "======== Epoch 82 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.181907892227173.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.7958898544311523\n",
            "\n",
            "Running Validation...\n",
            "Epoch 82: train loss 1.0442269495555332 \n",
            "\n",
            "======== Epoch 83 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.119809627532959.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.6944267749786377\n",
            "\n",
            "Running Validation...\n",
            "Epoch 83: train loss 1.0440695558275495 \n",
            "\n",
            "======== Epoch 84 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.168043613433838.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.79447340965271\n",
            "\n",
            "Running Validation...\n",
            "Epoch 84: train loss 1.0440040622438704 \n",
            "\n",
            "======== Epoch 85 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.234504461288452.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.891835927963257\n",
            "\n",
            "Running Validation...\n",
            "Epoch 85: train loss 1.0439751846449716 \n",
            "\n",
            "======== Epoch 86 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1675665378570557.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.7816102504730225\n",
            "\n",
            "Running Validation...\n",
            "Epoch 86: train loss 1.0440889716148376 \n",
            "\n",
            "======== Epoch 87 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1883931159973145.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.7988529205322266\n",
            "\n",
            "Running Validation...\n",
            "Epoch 87: train loss 1.0440678238868712 \n",
            "\n",
            "======== Epoch 88 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.137953996658325.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.7508113384246826\n",
            "\n",
            "Running Validation...\n",
            "Epoch 88: train loss 1.0439750109400068 \n",
            "\n",
            "======== Epoch 89 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.1746134757995605.\n",
            "\n",
            "  Average training loss: 1.04\n",
            "  Training epoch took: 3.788715362548828\n",
            "\n",
            "Running Validation...\n",
            "Epoch 89: train loss 1.043986737728119 \n",
            "\n",
            "======== Epoch 90 / 128 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 2.2432711124420166.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "method = \"class\"\n",
        "rnn_arr2 = []\n",
        "nrnn_arr2 = []\n",
        "lstm_arr2 = []\n",
        "seed_arr2 = []\n",
        "method_arr2 = []\n",
        "specify_arr2 = []\n",
        "\n",
        "noise_matrices = [noise_pair_45, noise_sym_25, noise_sym_50]\n",
        "names = [\"noise_pair_45\", \"noise_sym_25\", \"noise_sym_50\"]\n",
        "\n",
        "for idx, noise_matrix in enumerate(noise_matrices):\n",
        "\n",
        "  for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    y_train_flipped = flip_HAR_labels(y_train, noise_matrix).reshape(-1, 1)\n",
        "    y_test_flipped = flip_HAR_labels(y_test, noise_matrix).reshape(-1, 1)\n",
        "\n",
        "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=128, shuffle=True)\n",
        "\n",
        "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test_flipped))\n",
        "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=100, shuffle=True)\n",
        "\n",
        "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
        "\n",
        "    rnn_arr2.append(rnn_acc)\n",
        "    nrnn_arr2.append(nrnn_acc)\n",
        "    lstm_arr2.append(lstm_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])"
      ],
      "metadata": {
        "id": "tYqIjBk8Zl1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "method = \"basic\"\n",
        "rnn_arr3 = []\n",
        "nrnn_arr3 = []\n",
        "lstm_arr3 = []\n",
        "seed_arr3 = []\n",
        "method_arr3 = []\n",
        "specify_arr3 = []\n",
        "\n",
        "flip_probabilities = [0.1, 0.5, 0.9]\n",
        "for flip_probability in flip_probabilities:\n",
        "\n",
        "  for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    y_train_flipped = flip_HAR_labels_basic(y_train, flip_probability).reshape(-1, 1)\n",
        "    y_test_flipped = flip_HAR_labels_basic(y_test, flip_probability).reshape(-1, 1)\n",
        "\n",
        "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=128, shuffle=True)\n",
        "\n",
        "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test_flipped))\n",
        "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=100, shuffle=True)\n",
        "\n",
        "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.2, mult_noise = 0.2, nrnn = True)\n",
        "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
        "\n",
        "    rnn_arr3.append(rnn_acc)\n",
        "    nrnn_arr3.append(nrnn_acc)\n",
        "    lstm_arr3.append(lstm_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(names[idx])\n",
        "\n",
        "#plt.boxplot(final_acc_3)"
      ],
      "metadata": {
        "id": "wXwkWtGyu6Na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "rnn_arr = rnn_arr1 + rnn_arr2 + rnn_arr3\n",
        "nrnn_arr = nrnn_arr1 + nrnn_arr2 + nrnn_arr3\n",
        "lstm_arr = lstm_arr1 + lstm_arr2 + lstm_arr3\n",
        "seed_arr = seed_arr1 + seed_arr2 + seed_arr3\n",
        "method_arr = method_arr1 + method_arr2 + method_arr3\n",
        "specify_arr = specify_arr1 + specify_arr2 + specify_arr3\n",
        "\n",
        "df1 = pd.DataFrame(list(zip(rnn_arr, nrnn_arr, lstm_arr, seed_arr, method_arr, specify_arr)), columns = [\"RNN Accuracy\", \"NRNN Accuracy\", \"LSTM Accuracy\", \"Seed\", \"Method\", \"Specify\"])\n",
        "df1\n",
        "df1.to_csv('out.csv')  "
      ],
      "metadata": {
        "id": "k9Z0vkBXQ_1v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}