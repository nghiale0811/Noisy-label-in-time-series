{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzBraGTpl7gT"
      },
      "source": [
        "## Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "r28CXXZps-g1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import pickle\n",
        "import timeit\n",
        "\n",
        "from torch._utils import _accumulate\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data import Subset\n",
        "import torch.nn.utils.clip_grad as clip_grad\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "import copy\n",
        "from subprocess import call\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, classification_report\n",
        "import datetime as dt\n",
        "import time\n",
        "import random\n",
        "from scipy import signal\n",
        "from sklearn.utils import shuffle\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwn7kurmByV"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYa-ggaVkzkI",
        "outputId": "07eaeaed-3bf9-46ea-d984-a92c2195bef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading...\n",
            "Dataset already downloaded. Did not download twice.\n",
            "\n",
            "Extracting...\n",
            "Dataset already extracted. Did not extract twice.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"\")\n",
        "\n",
        "print(\"Downloading...\")\n",
        "if not os.path.exists(\"UCI HAR Dataset.zip\"):\n",
        "    call(\n",
        "        'wget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Downloading done.\\n\")\n",
        "else:\n",
        "    print(\"Dataset already downloaded. Did not download twice.\\n\")\n",
        "\n",
        "\n",
        "print(\"Extracting...\")\n",
        "extract_directory = os.path.abspath(\"UCI HAR Dataset\")\n",
        "if not os.path.exists(extract_directory):\n",
        "    call(\n",
        "        'unzip -nq \"UCI HAR Dataset.zip\"',\n",
        "        shell=True\n",
        "    )\n",
        "    print(\"Extracting successfully done to {}.\".format(extract_directory))\n",
        "else:\n",
        "    print(\"Dataset already extracted. Did not extract twice.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwLgDKQytkRN",
        "outputId": "d4193575-565b-4f33-c08e-f749ede76871"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(700, 128, 9)\n",
            "300\n",
            "Some useful info to get an insight on dataset's shape and normalisation:\n",
            "(X shape, y shape, every X's mean, every X's standard deviation)\n",
            "(300, 128, 9) (300, 1) 0.10143949 0.39385343\n"
          ]
        }
      ],
      "source": [
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "\n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "\n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]],\n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "\n",
        "    # Substract 1 to each output class for friendly 0-based indexing\n",
        "    return y_ - 1\n",
        "\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "DATASET_PATH = \"/content/UCI HAR Dataset/\"\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    DATASET_PATH + TRAIN + \"Inertial Signals/\" + signal + \"train.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    DATASET_PATH + TEST + \"Inertial Signals/\" + signal + \"test.txt\" for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "y_train_path = DATASET_PATH + TRAIN + \"y_train.txt\"\n",
        "y_test_path = DATASET_PATH + TEST + \"y_test.txt\"\n",
        "\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)\n",
        "\n",
        "#Take 700 training datapoints\n",
        "\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
        "X_train = X_train[:700]\n",
        "y_train = y_train[:700]\n",
        "\n",
        "#Take 300 testing datapoints\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state = 0)\n",
        "X_test = X_test[:300]\n",
        "y_test = y_test[:300]\n",
        "\n",
        "# Input Data\n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "print(X_train.shape)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "print(test_data_count)\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "\n",
        "train = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "test = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "test_loader = data_utils.DataLoader(test, batch_size=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13RRzP01mLkI"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cQz7VXgDFLfN"
      },
      "outputs": [],
      "source": [
        "#ADD NOISE TO HAR\n",
        "\n",
        "noise_pair_45= np.array([[.55,0.45,0.0,0.0,0.0,0.0],\n",
        "                        [0.0,.55,0.45,0.0,0.0,0.0],\n",
        "                        [0.0,0.0,.55,0.45,0.0,0.0],\n",
        "                        [0.0,0.0,0.0,.55,0.45,0.0],\n",
        "                        [0.0,0.0,0.0,0.0,.55,0.45],\n",
        "                        [0.45,0.0,0.0,0.0,0.0,.55]])\n",
        "noise_sym_50 = np.array([[.50,.10,.10,.10,.10,.10],\n",
        "                        [.10,.50,.10,.10,.10,.10],\n",
        "                        [.10,.10,.50,.10,.10,.10],\n",
        "                        [.10,.10,.10,.50,.10,.10],\n",
        "                        [.10,.10,.10,.10,.50,.10],\n",
        "                        [.10,.10,.10,.10,.10,.50]])\n",
        "\n",
        "noise_sym_25 = np.array([[.25,.15,.15,.15,.15,.15],\n",
        "                        [.15,.25,.15,.15,.15,.15],\n",
        "                        [.15,.15,.25,.15,.15,.15],\n",
        "                        [.15,.15,.15,.25,.15,.15],\n",
        "                        [.15,.15,.15,.15,.25,.15],\n",
        "                        [.15,.15,.15,.15,.15,.25]])\n",
        "\n",
        "def flip_HAR_labels(array, noise_matrix):\n",
        "    flipped = []\n",
        "    for elem in array.flatten():\n",
        "      flipped.append(np.random.choice([0,1,2,3,4,5], p=noise_matrix[int(elem)]))\n",
        "    \n",
        "    flipped = np.array(flipped)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped\n",
        "\n",
        "\n",
        "def flip_HAR_labels_basic(array, flip_probability):\n",
        "    flip_mask = np.random.binomial(1, flip_probability, len(array))\n",
        "    flipped = []\n",
        "    for i in range(len(array)):\n",
        "        if flip_mask[i]==1:\n",
        "            options = [0.0,1.0,2.0,3.0,4.0,5.0]\n",
        "            new_options = [x for x in options if x != array[i]]\n",
        "            flipped.append(np.random.choice(new_options, p=[0.2,0.2,0.2,0.2, 0.2]))\n",
        "        else:\n",
        "            flipped.append(array[i])\n",
        "    \n",
        "    flipped = np.array(flipped, dtype=np.int)\n",
        "    flipped.reshape((-1, 1))\n",
        "    return flipped"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8xzQyNFPpW",
        "outputId": "9685c54c-7461-414c-9597-8c8c5cad0d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "array = np.array([0.0,1.0,2.0,3.0,4.0,5.0])\n",
        "noise_matrix = noise_sym_25\n",
        "\n",
        "flip_HAR_labels_basic(array, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkrafFRyGKhG",
        "outputId": "32eda83a-e6b3-46f9-9719-f4d0e3b4f946"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 1, 2, 2, 5, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "flip_HAR_labels(array, noise_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ7gc-5cFT7S",
        "outputId": "3e49dd4b-8817-4e38-db9d-3ab660a1e588"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "flip_mask = np.random.binomial(1, 0.5, len(array))\n",
        "flip_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "c4RXMRyAtFvC"
      },
      "outputs": [],
      "source": [
        "def get_device():\n",
        "    \"\"\"Get a gpu if available.\"\"\"\n",
        "    if torch.cuda.device_count()>0:\n",
        "        device = torch.device('cuda')\n",
        "        print(\"Connected to a GPU\")\n",
        "    else:\n",
        "        print(\"Using the CPU\")\n",
        "        device = torch.device('cpu')\n",
        "    return device\n",
        "\n",
        "def which_device(model):\n",
        "    return next(model.parameters()).device\n",
        "\n",
        "\n",
        "def add_channels(X):\n",
        "    if len(X.shape) == 2:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1],1)\n",
        "\n",
        "    elif len(X.shape) == 3:\n",
        "        return X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])\n",
        "\n",
        "    else:\n",
        "        return \"dimenional error\"\n",
        "    \n",
        "def exp_lr_scheduler(epoch, optimizer, strategy='normal', decay_eff=0.1, decayEpoch=[]):\n",
        "    \"\"\"Decay learning rate by a factor of lr_decay every lr_decay_epoch epochs\"\"\"\n",
        "\n",
        "    if strategy=='normal':\n",
        "        if epoch in decayEpoch:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= decay_eff\n",
        "            print('New learning rate is: ', param_group['lr'])\n",
        "    else:\n",
        "        print('wrong strategy')\n",
        "        raise ValueError('A very specific bad thing happened.')\n",
        "\n",
        "    return optimizer\n",
        "\n",
        "    \n",
        "    \n",
        "def gaussian_init_(n_units, std=1):    \n",
        "    sampler = torch.distributions.Normal(torch.Tensor([0]), torch.Tensor([std/n_units]))\n",
        "    A_init = sampler.sample((n_units, n_units))[..., 0]  \n",
        "    return A_init"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PouKNWlImO5K"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "j5KjAjajQA9p"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, feature_size, n_state, hidden_size=16, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
        "                 seed=random.seed('2021')):\n",
        "        \n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_state = n_state\n",
        "        self.seed = seed\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.rnn_type = rnn\n",
        "        self.regres = regres\n",
        "        self.return_all = return_all\n",
        "        \n",
        "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "        else:\n",
        "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
        "\n",
        "        self.regressor = nn.Sequential(nn.Linear(self.hidden_size, self.n_state))\n",
        "\n",
        "    def forward(self, input, past_state=None, **kwargs):\n",
        "        input = input.to(self.device)\n",
        "        self.rnn.to(self.device)\n",
        "        self.regressor.to(self.device)\n",
        "        if not past_state:\n",
        "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
        "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
        "        if self.rnn_type == 'GRU':\n",
        "            all_encodings, encoding = self.rnn(input, past_state)\n",
        "        else:\n",
        "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
        "        \n",
        "        if self.regres:\n",
        "            if not self.return_all:\n",
        "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
        "            else:\n",
        "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
        "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
        "        else:\n",
        "            return encoding.view(encoding.shape[1], -1)\n",
        "        \n",
        "        \n",
        "def format_time(elapsed):\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(dt.timedelta(seconds=elapsed_rounded))\n",
        "            \n",
        "def save_ckpt(generator_model, output_dir, data):\n",
        "    check_pt_dir = os.path.join(output_dir,data,'ckpt')\n",
        "    fname = os.path.join(check_pt_dir,'generator.pt')\n",
        "    os.makedirs(check_pt_dir, exist_ok=True)\n",
        "    torch.save(generator_model.state_dict(), fname)\n",
        "    \n",
        "def get_accuracy(model, loader):\n",
        "    \n",
        "    correct, total = 0, 0\n",
        "    for xs, ts in loader:\n",
        "        xs.to(device)\n",
        "        ts.to(device)\n",
        "        zs = model(xs)\n",
        "        \n",
        "        pred = zs.max(1, keepdim=True)[1] # get the index of the max logit\n",
        "        ts = torch.argmax(ts, 1)\n",
        "\n",
        "        correct += pred.eq(ts.view_as(pred)).sum().item()\n",
        "        total += int(ts.shape[0])\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Pe1d7EkrQC8s"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "def train_model(model, train_dataloader, n_epochs, lr):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.002, weight_decay=0.1)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    history = dict(train=[], val=[])\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
        "    \n",
        "    best_loss = 10000.0\n",
        "    \n",
        "    n=0\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        t0 = time.time()\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "        train_losses=[]\n",
        "        model = model.train()\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = (time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            b_input= batch[0].to(device)\n",
        "            b_target =  batch[1].to(device)\n",
        "            iters.append(n)\n",
        "            optimizer.zero_grad()\n",
        "            out = model(b_input)\n",
        "\n",
        "            target = torch.argmax(b_target, 1)\n",
        "\n",
        "            loss = criterion(out, target)\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            \n",
        "            if n % 10 == 0:\n",
        "                iters_sub.append(n)\n",
        "                \n",
        "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
        "                #print(get_accuracy(model, train_dataloader))\n",
        "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
        "            # increment the iteration number\n",
        "            n += 1\n",
        "\n",
        "        training_time = (time.time() - t0)\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "        #print(\"\")\n",
        "        #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        #print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "        #print(\"\")\n",
        "        #print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "        val_losses = []\n",
        "        model = model.eval()\n",
        "\n",
        "        train_loss = np.mean(train_losses)\n",
        "    \n",
        "        history['train'].append(train_loss)\n",
        "        \n",
        "        #print(f'Epoch {epoch}: train loss {train_loss} ')\n",
        "    # plt.style.use('seaborn-white')\n",
        "    # plt.plot(history['train'])\n",
        "\n",
        "    # plt.title('LSTM  Training Curves')\n",
        "    # plt.ylabel('CE Loss')\n",
        "    # plt.xlabel('Epoch Number')\n",
        "    # plt.legend(['Train', 'Val'], loc='upper right')\n",
        "    # plt.show()\n",
        "    \n",
        "    return model.eval(), history\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    #INFERENCE ON TEST SET\n",
        "    predictions = model(x_test).max(1, keepdim=True)[1]\n",
        "    \n",
        "    y_test = y_test.numpy()\n",
        "    y_pred = predictions.cpu().numpy()\n",
        "    \n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    #f1 = f1_score(y_test, y_pred, average='micro')\n",
        "    #precision = precision_score(y_test, y_pred, average='micro')\n",
        "    #print(accuracy)\n",
        "    #print(f1)\n",
        "    #print(precision)\n",
        "    return accuracy\n",
        "\n",
        "def lstm_driver(seed, x_test, y_test, x_train, y_train):\n",
        "  train = data_utils.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train))\n",
        "  train_loader = data_utils.DataLoader(train, batch_size=10, shuffle=True)\n",
        "\n",
        "  x_test = torch.tensor(x_test, dtype=torch.float)\n",
        "    \n",
        "  y_test = torch.tensor(y_test, dtype=torch.int)\n",
        "\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  model = LSTMClassifier(9, #num features \n",
        "                  6, #num classes,\n",
        "                  seed = seed,\n",
        "                  rnn=\"GRU\" #rnn type    \n",
        "            )\n",
        "\n",
        "  model, history = train_model(model, train_loader, 50, 2e-3)\n",
        "  acc = evaluate_model(model, x_test, y_test)\n",
        "  print(\"Test Accuracy: \", acc)\n",
        "\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "MELlsVkos0ZP"
      },
      "outputs": [],
      "source": [
        "class NoisyRNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_classes, n_units=16, eps=0.01, \n",
        "                 beta=0.8, gamma_A=0.01, gamma_W=0.01, init_std=1, alpha=1,\n",
        "                 solver='base', add_noise=0, mult_noise=0):\n",
        "        super(NoisyRNN, self).__init__()\n",
        "\n",
        "        self.device = get_device()\n",
        "\n",
        "\n",
        "        self.n_units = n_units\n",
        "        self.eps = eps\n",
        "        self.solver = solver\n",
        "        self.beta = beta\n",
        "        self.alpha = alpha\n",
        "        self.gamma_A = gamma_A\n",
        "        self.gamma_W = gamma_W\n",
        "        self.add_noise = add_noise\n",
        "        self.mult_noise = mult_noise\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        self.E = nn.Linear(input_dim, n_units)\n",
        "        self.D = nn.Linear(n_units, output_classes)     \n",
        "                                            \n",
        "        self.C = nn.Parameter(gaussian_init_(n_units, std=init_std))            \n",
        "        self.B = nn.Parameter(gaussian_init_(n_units, std=init_std))    \n",
        "        self.I = torch.eye(n_units).to(self.device)   \n",
        "\n",
        "        self.d = nn.Parameter(torch.rand(self.n_units).float().to(self.device)*0 + eps)           \n",
        "\n",
        "\n",
        "    def forward(self, x, mode='test'):\n",
        "        T = x.shape[1]\n",
        "        h = torch.zeros(x.shape[0], self.n_units).to(which_device(self))\n",
        "\n",
        "        for i in range(T):\n",
        "            z = self.E(x[:,i,:])\n",
        "\n",
        "            if i == 0:\n",
        "                    A = self.beta * (self.B - self.B.transpose(1, 0)) + (1-self.beta) * (self.B + self.B.transpose(1, 0)) - self.gamma_A * self.I\n",
        "                    W = self.beta * (self.C - self.C.transpose(1, 0)) + (1-self.beta) * (self.C + self.C.transpose(1, 0)) - self.gamma_W * self.I\n",
        "                \n",
        "                        \n",
        "            add_noise = 0.0\n",
        "            mult_noise = 1.0\n",
        "            if mode == 'train':\n",
        "                if self.add_noise > 0:\n",
        "                    add_noise = self.add_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device)\n",
        "                            \n",
        "                if self.mult_noise > 0:\n",
        "                    #mult_noise = self.mult_noise * torch.randn(h.shape[0], h.shape[1]).float().to(self.device) + 1\n",
        "                    mult_noise = self.mult_noise * torch.rand(h.shape[0], h.shape[1]).float().to(self.device) + (1-self.mult_noise)\n",
        "                        \n",
        "\n",
        "            if self.solver == 'base': \n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.eps * h_update\n",
        "            elif self.solver == 'noisy':\n",
        "                h_update = self.alpha * torch.matmul(h, A) + self.tanh(torch.matmul(h, W) + z)                \n",
        "                h = h + self.d * mult_noise * h_update + add_noise                              \n",
        "                 \n",
        "                \n",
        "        # Decoder \n",
        "        #----------\n",
        "        out = self.D(h)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK_RFvzqvl65",
        "outputId": "babb1630-8ce3-4084-851f-341b10699348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n"
          ]
        }
      ],
      "source": [
        "#code for the driver\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Human Activity Data')\n",
        "#\n",
        "parser.add_argument('-f')\n",
        "#\n",
        "parser.add_argument('--name', type=str, default='HAR', metavar='N', help='dataset')\n",
        "#\n",
        "parser.add_argument('--batch-size', type=int, default=128, metavar='N', help='input batch size for training (default: 128)')\n",
        "#\n",
        "parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N', help='input batch size for testing (default: 1000)')\n",
        "#\n",
        "parser.add_argument('--epochs', type=int, default=50, metavar='N', help='number of epochs to train (default: 90)')\n",
        "#\n",
        "parser.add_argument('--lr', type=float, default=0.002, metavar='LR', help='learning rate (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay', type=float, default=0.1, help='learning rate decay value (default: 0.1)')\n",
        "#\n",
        "parser.add_argument('--lr_decay_epoch', type=int, nargs='+', default=[30], help='decrease learning rate at these epochs.')\n",
        "#\n",
        "parser.add_argument('--wd', default=0.0, type=float, metavar='W', help='weight decay (default: 0.0)')\n",
        "#\n",
        "parser.add_argument('--gamma_W', default=0.001, type=float, metavar='W', help='diffiusion rate for W')\n",
        "#\n",
        "parser.add_argument('--gamma_A', default=0.001, type=float, metavar='W', help='diffiusion rate for A')\n",
        "#\n",
        "parser.add_argument('--beta', default=0.75, type=float, metavar='W', help='skew level')\n",
        "#\n",
        "parser.add_argument('--model', type=str, default='NoisyRNN', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--solver', type=str, default='noisy', metavar='N', help='model name')\n",
        "#\n",
        "parser.add_argument('--n_units', type=int, default=16, metavar='S', help='number of hidden units')\n",
        "#\n",
        "parser.add_argument('--eps', default=0.1, type=float, metavar='W', help='time step for euler scheme')\n",
        "#\n",
        "parser.add_argument('--T', default=49, type=int, metavar='W', help='time steps')\n",
        "#\n",
        "parser.add_argument('--init_std', type=float, default=0.1, metavar='S', help='control of std for initilization')\n",
        "#\n",
        "parser.add_argument('--seed', type=int, default=1, metavar='S', help='random seed (default: 0)')\n",
        "#\n",
        "parser.add_argument('--gclip', type=int, default=0, metavar='S', help='gradient clipping')\n",
        "#\n",
        "parser.add_argument('--optimizer', type=str, default='Adam', metavar='N', help='optimizer')\n",
        "#\n",
        "parser.add_argument('--alpha', type=float, default=1, metavar='S', help='for ablation study')\n",
        "#\n",
        "parser.add_argument('--add_noise', type=float, default=0.0, metavar='S', help='level of additive noise')\n",
        "#\n",
        "parser.add_argument('--mult_noise', type=float, default=0.0, metavar='S', help='level of multiplicative noise')\n",
        "#\n",
        "args = parser.parse_args()\n",
        "\n",
        "if not os.path.isdir(args.name + '_results'):\n",
        "    os.mkdir(args.name + '_results')\n",
        "\n",
        "#==============================================================================\n",
        "# set random seed to reproduce the work\n",
        "#==============================================================================\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "#==============================================================================\n",
        "# get device\n",
        "#==============================================================================\n",
        "device = get_device()\n",
        "\n",
        "#==============================================================================\n",
        "# initialize model\n",
        "#==============================================================================\n",
        "seeds = [1, 2, 3]\n",
        "\n",
        "\n",
        "def driver(seed, train, test, add_noise = 0, mult_noise = 0, nrnn = False):\n",
        "\n",
        "  final_acc = []\n",
        "  model = NoisyRNN(input_dim=int(9), output_classes=6, n_units=args.n_units, \n",
        "                eps=args.eps, beta=args.beta, gamma_A=args.gamma_A, gamma_W=args.gamma_W,\n",
        "                init_std=args.init_std, alpha=args.alpha,  solver=args.solver, \n",
        "                add_noise=add_noise, mult_noise=mult_noise).to(device)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  noise = torch.randn(1,700,128,9).float()\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # set random seed to reproduce the work\n",
        "  #==============================================================================\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "\n",
        "\n",
        "  #==============================================================================\n",
        "  # Model summary\n",
        "  #==============================================================================\n",
        "  print(model)    \n",
        "  print('**** Setup ****')\n",
        "  print('Total params: %.2fk' % (sum(p.numel() for p in model.parameters())/1000.0))\n",
        "  print('************')    \n",
        "    \n",
        "\n",
        "  if args.optimizer == 'SGD':\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
        "  elif  args.optimizer == 'Adam':\n",
        "      optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
        "  else:\n",
        "      print(\"Unexpected optimizer!\")\n",
        "      raise \n",
        "\n",
        "\n",
        "  loss_func = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  # training and testing\n",
        "  count = 0\n",
        "  loss_hist = []\n",
        "  test_acc = []\n",
        "\n",
        "  t0 = timeit.default_timer()\n",
        "  for epoch in range(args.epochs):\n",
        "      model.train()\n",
        "      lossaccum = 0\n",
        "      \n",
        "      for step, (x, y) in enumerate(train):\n",
        "          count += 1\n",
        "          \n",
        "          # Reshape data for recurrent unit\n",
        "          inputs = Variable(x.view(-1, 128, int(9))).to(device) # reshape x to (batch, time_step, input_size)         \n",
        "          targets = Variable(y).to(device) \n",
        "\n",
        "                  \n",
        "          # send data to recurrent unit    \n",
        "          output = model(inputs, mode='train') \n",
        "          loss = loss_func(output, targets.squeeze(1).long())\n",
        "          \n",
        "          \n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()          \n",
        "          \n",
        "          if args.gclip != 0.0:\n",
        "              torch.nn.utils.clip_grad_norm_(model.parameters(), args.gclip) # gradient clip\n",
        "              \n",
        "          optimizer.step() # update weights\n",
        "          lossaccum += loss.item()\n",
        "\n",
        "          if args.model == 'test':\n",
        "              D = model.W.weight.data.cpu().numpy()  \n",
        "              u, s, v = np.linalg.svd(D, 0)\n",
        "              model.W.weight.data = torch.from_numpy(u.dot(v)).float().cuda()\n",
        "\n",
        "      loss_hist.append(lossaccum)    \n",
        "      \n",
        "      if epoch % 1 == 0:\n",
        "          model.eval()\n",
        "          correct = 0\n",
        "          total_num = 0\n",
        "          for data, target in test: \n",
        "              data, target = data.to(device), target.to(device)                \n",
        "              output = model(data.view(-1, 128, int(9)))\n",
        "              \n",
        "              pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                   \n",
        "              correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "              total_num += len(data)\n",
        "          \n",
        "          accuracy = correct / total_num\n",
        "          test_acc.append(accuracy)\n",
        "          print('Epoch: ', epoch, 'Iteration: ', count, '| train loss: %.4f' % loss.item(), '| test accuracy: %.3f' % accuracy)\n",
        "\n",
        "\n",
        "      if nrnn == True:\n",
        "          B = model.B.data.cpu().numpy()            \n",
        "          A = args.alpha * (args.beta * (B - B.T) + (1-args.beta) * (B + B.T) - args.gamma_A * np.eye(args.n_units))\n",
        "          A = 0.5 * (A + A.T)\n",
        "          e, _ = np.linalg.eig(A)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "          \n",
        "          C = model.C.data.cpu().numpy()            \n",
        "          W = args.beta * (C - C.T) + (1-args.beta) * (C + C.T) - args.gamma_W * np.eye(args.n_units)\n",
        "          e, _ = np.linalg.eig(W)\n",
        "          # print('Eigenvalues of A (min and max): ', (np.min(np.abs(e)), np.max(np.abs(e))))\n",
        "              \n",
        "              \n",
        "\n",
        "      # schedule learning rate decay    \n",
        "      optimizer=exp_lr_scheduler(epoch, optimizer, decay_eff=args.lr_decay, decayEpoch=args.lr_decay_epoch)\n",
        "\n",
        "  print('total time: ', timeit.default_timer()  - t0 )\n",
        "\n",
        "  final_acc.append(max(test_acc))\n",
        "\n",
        "  torch.save(model, args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '.pkl')  \n",
        "\n",
        "  data = {'loss': lossaccum, 'testacc': test_acc}\n",
        "  f = open(args.name + '_results/' + args.model + '_' + args.name + '_T_' + str(args.T) \n",
        "              + '_units_' + str(args.n_units) + '_beta_' + str(args.beta) \n",
        "              + '_gamma_A_' + str(args.gamma_A) + '_gamma_W_' + str(args.gamma_W) + '_eps_' + str(args.eps) \n",
        "              + '_solver_' + str(args.solver) + '_gclip_' + str(args.gclip) + '_optimizer_' + str(args.optimizer)\n",
        "              + '_addnoise_' + str(args.add_noise) + '_multnoise_' + str(args.mult_noise) \n",
        "              + '_seed_' + str(args.seed) + '_loss.pkl',\"wb\")\n",
        "\n",
        "  pickle.dump(data,f)\n",
        "  f.close()\n",
        "  return max(test_acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_driver(seed, X_test, y_test, X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JY77AGBuZaQt",
        "outputId": "41ddf35b-8518-4c09-ff4f-8441e243e25b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3238234519958496.\n",
            "\n",
            "======== Epoch 2 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3137454986572266.\n",
            "\n",
            "======== Epoch 3 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2353830337524414.\n",
            "\n",
            "======== Epoch 4 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2533743381500244.\n",
            "\n",
            "======== Epoch 5 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.25132155418396.\n",
            "\n",
            "======== Epoch 6 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2804439067840576.\n",
            "\n",
            "======== Epoch 7 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2705585956573486.\n",
            "\n",
            "======== Epoch 8 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2658321857452393.\n",
            "\n",
            "======== Epoch 9 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2680871486663818.\n",
            "\n",
            "======== Epoch 10 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2632315158843994.\n",
            "\n",
            "======== Epoch 11 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2690329551696777.\n",
            "\n",
            "======== Epoch 12 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2467124462127686.\n",
            "\n",
            "======== Epoch 13 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3111872673034668.\n",
            "\n",
            "======== Epoch 14 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2863891124725342.\n",
            "\n",
            "======== Epoch 15 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.263922929763794.\n",
            "\n",
            "======== Epoch 16 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.307931661605835.\n",
            "\n",
            "======== Epoch 17 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.310837984085083.\n",
            "\n",
            "======== Epoch 18 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2826933860778809.\n",
            "\n",
            "======== Epoch 19 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.295447587966919.\n",
            "\n",
            "======== Epoch 20 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2436537742614746.\n",
            "\n",
            "======== Epoch 21 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2748544216156006.\n",
            "\n",
            "======== Epoch 22 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2769179344177246.\n",
            "\n",
            "======== Epoch 23 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.277641773223877.\n",
            "\n",
            "======== Epoch 24 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3155462741851807.\n",
            "\n",
            "======== Epoch 25 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.303746223449707.\n",
            "\n",
            "======== Epoch 26 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2349517345428467.\n",
            "\n",
            "======== Epoch 27 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2725379467010498.\n",
            "\n",
            "======== Epoch 28 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3200252056121826.\n",
            "\n",
            "======== Epoch 29 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3247833251953125.\n",
            "\n",
            "======== Epoch 30 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2486910820007324.\n",
            "\n",
            "======== Epoch 31 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.242931842803955.\n",
            "\n",
            "======== Epoch 32 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2734425067901611.\n",
            "\n",
            "======== Epoch 33 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3426990509033203.\n",
            "\n",
            "======== Epoch 34 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2977371215820312.\n",
            "\n",
            "======== Epoch 35 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.283712387084961.\n",
            "\n",
            "======== Epoch 36 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.28599214553833.\n",
            "\n",
            "======== Epoch 37 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2529747486114502.\n",
            "\n",
            "======== Epoch 38 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.310703992843628.\n",
            "\n",
            "======== Epoch 39 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2590067386627197.\n",
            "\n",
            "======== Epoch 40 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.268568992614746.\n",
            "\n",
            "======== Epoch 41 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2575619220733643.\n",
            "\n",
            "======== Epoch 42 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2767798900604248.\n",
            "\n",
            "======== Epoch 43 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.2920184135437012.\n",
            "\n",
            "======== Epoch 44 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.8692057132720947.\n",
            "\n",
            "======== Epoch 45 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.261568307876587.\n",
            "\n",
            "======== Epoch 46 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.255702257156372.\n",
            "\n",
            "======== Epoch 47 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3064074516296387.\n",
            "\n",
            "======== Epoch 48 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.8766050338745117.\n",
            "\n",
            "======== Epoch 49 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.321925163269043.\n",
            "\n",
            "======== Epoch 50 / 50 ========\n",
            "Training...\n",
            "  Batch    40  of     70.    Elapsed: 1.3581337928771973.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy:  0.14333333333333334\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14333333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "O7EmSIf4vSwB",
        "outputId": "4e8256ea-9a98-47f2-f8d2-4eb40596ff37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using the CPU\n",
            "NoisyRNN(\n",
            "  (tanh): Tanh()\n",
            "  (E): Linear(in_features=9, out_features=16, bias=True)\n",
            "  (D): Linear(in_features=16, out_features=6, bias=True)\n",
            ")\n",
            "**** Setup ****\n",
            "Total params: 0.79k\n",
            "************\n",
            "Epoch:  0 Iteration:  70 | train loss: 1.2677 | test accuracy: 0.490\n",
            "Epoch:  1 Iteration:  140 | train loss: 1.4267 | test accuracy: 0.497\n",
            "Epoch:  2 Iteration:  210 | train loss: 1.1627 | test accuracy: 0.510\n",
            "Epoch:  3 Iteration:  280 | train loss: 1.1240 | test accuracy: 0.510\n",
            "Epoch:  4 Iteration:  350 | train loss: 0.6071 | test accuracy: 0.517\n",
            "Epoch:  5 Iteration:  420 | train loss: 0.5281 | test accuracy: 0.657\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-f112c1807a9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseeds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mrnn_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0macc_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mseed_arr1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-4016a75a83b1>\u001b[0m in \u001b[0;36mdriver\u001b[0;34m(seed, train, test, add_noise, mult_noise, nrnn)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgclip\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "method = \"No noise\"\n",
        "acc_arr1 = []\n",
        "seed_arr1 = []\n",
        "method_arr1 = []\n",
        "specify_arr1 = []\n",
        "model_arr1 = []\n",
        "\n",
        "for seed in seeds:\n",
        "  rnn_acc = driver(seed, train_loader, test_loader)\n",
        "  acc_arr1.append(rnn_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(0.0)\n",
        "  model_arr1.append(\"RNN\")\n",
        "\n",
        "  nrnn_acc = driver(seed, train_loader, test_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
        "  acc_arr1.append(nrnn_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(0.0)\n",
        "  model_arr1.append(\"NRNN\")\n",
        "\n",
        "  lstm_acc = lstm_driver(seed, X_test, y_test, X_train, y_train)\n",
        "  acc_arr1.append(lstm_acc)\n",
        "  seed_arr1.append(seed)\n",
        "  method_arr1.append(method)\n",
        "  specify_arr1.append(0.0)\n",
        "  model_arr1.append(\"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYqIjBk8Zl1z"
      },
      "outputs": [],
      "source": [
        "method = \"class\"\n",
        "acc_arr2 = []\n",
        "seed_arr2 = []\n",
        "method_arr2 = []\n",
        "specify_arr2 = []\n",
        "model_arr2 = []\n",
        "\n",
        "noise_matrices = [noise_pair_45, noise_sym_25, noise_sym_50]\n",
        "names = [\"noise_pair_45\", \"noise_sym_25\", \"noise_sym_50\"]\n",
        "\n",
        "for idx, noise_matrix in enumerate(noise_matrices):\n",
        "\n",
        "  for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    y_train_flipped = flip_HAR_labels(y_train, noise_matrix).reshape(-1, 1)\n",
        "    y_test_flipped = flip_HAR_labels(y_test, noise_matrix).reshape(-1, 1)\n",
        "\n",
        "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "    acc_arr2.append(rnn_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])\n",
        "    model_arr2.append(\"RNN\")\n",
        "\n",
        "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
        "    acc_arr2.append(nrnn_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])\n",
        "    model_arr2.append(\"NRNN\")\n",
        "\n",
        "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
        "    acc_arr2.append(lstm_acc)\n",
        "    seed_arr2.append(seed)\n",
        "    method_arr2.append(method)\n",
        "    specify_arr2.append(names[idx])\n",
        "    model_arr2.append(\"LSTM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXwkWtGyu6Na"
      },
      "outputs": [],
      "source": [
        "method = \"basic\"\n",
        "acc_arr3 = []\n",
        "seed_arr3 = []\n",
        "method_arr3 = []\n",
        "specify_arr3 = []\n",
        "model_arr3 = []\n",
        "\n",
        "flip_probabilities = [0.05, 0.1, 0.2, 0.4]\n",
        "for flip_probability in flip_probabilities:\n",
        "\n",
        "  for seed in seeds:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    y_train_flipped = flip_HAR_labels_basic(y_train, flip_probability).reshape(-1, 1)\n",
        "    y_test_flipped = flip_HAR_labels_basic(y_test, flip_probability).reshape(-1, 1)\n",
        "\n",
        "    train_flipped = data_utils.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train_flipped))\n",
        "    train_flipped_loader = data_utils.DataLoader(train_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    test_flipped = data_utils.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "    test_flipped_loader = data_utils.DataLoader(test_flipped, batch_size=10, shuffle=True)\n",
        "\n",
        "    rnn_acc = driver(seed, train_flipped_loader, test_flipped_loader)\n",
        "    acc_arr3.append(rnn_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(flip_probability)\n",
        "    model_arr3.append(\"RNN\")\n",
        "\n",
        "    nrnn_acc = driver(seed, train_flipped_loader, test_flipped_loader, add_noise = 0.02, mult_noise = 0.02, nrnn = True)\n",
        "    acc_arr3.append(nrnn_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(flip_probability)\n",
        "    model_arr3.append(\"NRNN\")\n",
        "    \n",
        "    lstm_acc = lstm_driver(seed, X_test, y_test_flipped, X_train, y_train_flipped)\n",
        "    acc_arr3.append(lstm_acc)\n",
        "    seed_arr3.append(seed)\n",
        "    method_arr3.append(method)\n",
        "    specify_arr3.append(flip_probability)\n",
        "    model_arr3.append(\"LSTM\")\n",
        "\n",
        "#plt.boxplot(final_acc_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Z0vkBXQ_1v"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "acc_arr = acc_arr1 + acc_arr2 + acc_arr3\n",
        "seed_arr = seed_arr1 + seed_arr2 + seed_arr3\n",
        "method_arr = method_arr1 + method_arr2 + method_arr3\n",
        "specify_arr = specify_arr1 + specify_arr2 + specify_arr3\n",
        "model_arr = model_arr1 + model_arr2 + model_arr3\n",
        "\n",
        "df1 = pd.DataFrame(list(zip(acc_arr, seed_arr, method_arr, specify_arr, model_arr)), columns = [\"Accuracies\", \"Seed\", \"Method\", \"Specify\", \"Model\"])\n",
        "df1\n",
        "df1.to_csv('out.csv')  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "noisy-times-series.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}